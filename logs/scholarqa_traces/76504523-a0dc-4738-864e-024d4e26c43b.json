{
    "query": "What are the key papers and models where multimodal large language models (MLLMs) can generate both text outputs and images containing embedded or edited scene text (e.g., signboards, documents) within a single end-to-end framework?",
    "user_id": "lib_user",
    "task_id": "76504523-a0dc-4738-864e-024d4e26c43b",
    "timestamp": "2025-06-23T22:55:13.511352",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 42,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.32924699999999996,
    "decomposed_query": {
        "rewritten_query": "Multimodal large language models that can generate both text outputs and images containing embedded or edited scene text within a single end-to-end framework.",
        "keyword_query": "multimodal large language models generate text outputs images embedded edited scene text signboards documents end-to-end framework",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010107,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Multi-Modal Generative Embedding Model",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218044713",
                    "name": "Feipeng Ma"
                },
                {
                    "authorId": "1878753365",
                    "name": "Hongwei Xue"
                },
                {
                    "authorId": "2303832848",
                    "name": "Guangting Wang"
                },
                {
                    "authorId": "2141747950",
                    "name": "Yizhou Zhou"
                },
                {
                    "authorId": "2276428924",
                    "name": "Fengyun Rao"
                },
                {
                    "authorId": "2303910455",
                    "name": "Shilin Yan"
                },
                {
                    "authorId": "2240587447",
                    "name": "Yueyi Zhang"
                },
                {
                    "authorId": "2303712729",
                    "name": "Siying Wu"
                },
                {
                    "authorId": "2303652188",
                    "name": "Mike Zheng Shou"
                },
                {
                    "authorId": "2303715824",
                    "name": "Xiaoyan Sun"
                }
            ],
            "abstract": "Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding. To explore the minimalism of multi-modal paradigms, we attempt to achieve only one model per modality in this work. We propose a Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and embedding objectives are encapsulated in one Large Language Model. We also propose a PoolAggregator to boost efficiency and enable the ability of fine-grained embedding and generation. A surprising finding is that these two objectives do not significantly conflict with each other. For example, MM-GEM instantiated from ViT-Large and TinyLlama shows competitive performance on benchmarks for multimodal embedding models such as cross-modal retrieval and zero-shot classification, while has good ability of image captioning. Additionally, MM-GEM can seamlessly execute region-level image caption generation and retrieval tasks. Besides, the advanced text model in MM-GEM brings over 5% improvement in Recall@1 for long text and image retrieval.",
            "corpus_id": 270094821,
            "sentences": [
                {
                    "corpus_id": "270094821",
                    "title": "Multi-Modal Generative Embedding Model",
                    "text": "The Multi-Modal Generative Embedding Model (MM-GEM) presents a unified approach to multimodal learning by integrating generative and embedding objectives within a single Large Language Model (LLM).Our experiments demonstrate that these two objectives do not significantly conflict with each other.MM-GEM achieves competitive performance across a range of tasks, including cross-modal retrieval, zero-shot classification, and image captioning.A key contribution is the PoolAggregator, enhancing the model's ability to handle fine-grained tasks.Additionally, MM-GEM's advanced text module significantly improves performance on long-form text retrieval, showcasing the benefits of leveraging a robust LLM for text processing.\n\nMM-GEM represents a significant step towards unified multi-modal models, yet there are still many subsequent potential directions: 1) We mainly focus on image captioning for generative tasks in this work, the performance impact of adding plain language data needs to be further investigated.2) MM-GEM enables LLMs generate discriminative outputs besides language tokens, this may benefit multi-modal large language model by retrieving or grounding visual information efficiently.Further investigation into these aspects will be explored in future work.",
                    "score": 0.5187408305999397,
                    "section_title": "Conclusion",
                    "char_start_offset": 25786,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 197
                        },
                        {
                            "start": 197,
                            "end": 297
                        },
                        {
                            "start": 297,
                            "end": 442
                        },
                        {
                            "start": 442,
                            "end": 543
                        },
                        {
                            "start": 543,
                            "end": 722
                        },
                        {
                            "start": 724,
                            "end": 1015
                        },
                        {
                            "start": 1015,
                            "end": 1203
                        },
                        {
                            "start": 1203,
                            "end": 1276
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.896484375
                }
            ],
            "relevance_judgement": 0.896484375,
            "relevance_judgment_input_expanded": "# Title: Multi-Modal Generative Embedding Model\n# Venue: arXiv.org\n# Authors: Feipeng Ma, Hongwei Xue, Guangting Wang, Yizhou Zhou, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, Xiaoyan Sun\n## Abstract\nMost multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding. To explore the minimalism of multi-modal paradigms, we attempt to achieve only one model per modality in this work. We propose a Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and embedding objectives are encapsulated in one Large Language Model. We also propose a PoolAggregator to boost efficiency and enable the ability of fine-grained embedding and generation. A surprising finding is that these two objectives do not significantly conflict with each other. For example, MM-GEM instantiated from ViT-Large and TinyLlama shows competitive performance on benchmarks for multimodal embedding models such as cross-modal retrieval and zero-shot classification, while has good ability of image captioning. Additionally, MM-GEM can seamlessly execute region-level image caption generation and retrieval tasks. Besides, the advanced text model in MM-GEM brings over 5% improvement in Recall@1 for long text and image retrieval.\n## Conclusion\nThe Multi-Modal Generative Embedding Model (MM-GEM) presents a unified approach to multimodal learning by integrating generative and embedding objectives within a single Large Language Model (LLM).Our experiments demonstrate that these two objectives do not significantly conflict with each other.MM-GEM achieves competitive performance across a range of tasks, including cross-modal retrieval, zero-shot classification, and image captioning.A key contribution is the PoolAggregator, enhancing the model's ability to handle fine-grained tasks.Additionally, MM-GEM's advanced text module significantly improves performance on long-form text retrieval, showcasing the benefits of leveraging a robust LLM for text processing.\n\nMM-GEM represents a significant step towards unified multi-modal models, yet there are still many subsequent potential directions: 1) We mainly focus on image captioning for generative tasks in this work, the performance impact of adding plain language data needs to be further investigated.2) MM-GEM enables LLMs generate discriminative outputs besides language tokens, this may benefit multi-modal large language model by retrieving or grounding visual information efficiently.Further investigation into these aspects will be explored in future work.",
            "reference_string": "[270094821 | Ma et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
            "venue": "Machine Learning with Applications",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.12293, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309151434",
                    "name": "Averi Bates"
                },
                {
                    "authorId": "2363651648",
                    "name": "Ryan Vavricka"
                },
                {
                    "authorId": "2350512740",
                    "name": "Shane Carleton"
                },
                {
                    "authorId": "2350505892",
                    "name": "Ruosi Shao"
                },
                {
                    "authorId": "2084643963",
                    "name": "Chongle Pan"
                }
            ],
            "abstract": null,
            "corpus_id": 277065986,
            "sentences": [
                {
                    "corpus_id": "277065986",
                    "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
                    "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating human-like text. Advanced transformer-based architectures such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and T5 (Raffel et al., 2020) capture contextual relationships within text, enabling tasks like summarization, question answering, and code generation. However, LLMs process only textual data, limiting their utility in domains requiring multimodal understanding, such as image or video analysis. Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation.",
                    "score": 0.5612261349265448,
                    "section_title": "Addressing the Problem",
                    "char_start_offset": 4189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1429
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 232,
                            "end": 253,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88916015625
                }
            ],
            "relevance_judgement": 0.88916015625,
            "relevance_judgment_input_expanded": "# Title: Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models\n# Venue: Machine Learning with Applications\n# Authors: Averi Bates, Ryan Vavricka, Shane Carleton, Ruosi Shao, Chongle Pan\n## Abstract\nNone\n## Addressing the Problem\nLarge Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating human-like text. Advanced transformer-based architectures such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and T5 (Raffel et al., 2020) capture contextual relationships within text, enabling tasks like summarization, question answering, and code generation. However, LLMs process only textual data, limiting their utility in domains requiring multimodal understanding, such as image or video analysis. Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation.",
            "reference_string": "[277065986 | Bates et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Jointly Training Large Autoregressive Multimodal Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 31,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.15564",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.15564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2185505465",
                    "name": "Emanuele Aiello"
                },
                {
                    "authorId": "49297123",
                    "name": "L. Yu"
                },
                {
                    "authorId": "2247227174",
                    "name": "Yixin Nie"
                },
                {
                    "authorId": "2201435",
                    "name": "Armen Aghajanyan"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                }
            ],
            "abstract": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
            "corpus_id": 262898422,
            "sentences": [
                {
                    "corpus_id": "262898422",
                    "title": "Jointly Training Large Autoregressive Multimodal Models",
                    "text": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
                    "score": 0.5171846090662012,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8779296875
                }
            ],
            "relevance_judgement": 0.8779296875,
            "relevance_judgment_input_expanded": "# Title: Jointly Training Large Autoregressive Multimodal Models\n# Venue: International Conference on Learning Representations\n# Authors: Emanuele Aiello, L. Yu, Yixin Nie, Armen Aghajanyan, Barlas O\u011fuz\n## Abstract\nIn recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.\n",
            "reference_string": "[262898422 | Aiello et al. | 2023 | Citations: 31]"
        },
        {
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.06646, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319817038",
                    "name": "Alessandro Serra"
                },
                {
                    "authorId": "2284691756",
                    "name": "Francesco Ortu"
                },
                {
                    "authorId": "2334479581",
                    "name": "Emanuele Panizon"
                },
                {
                    "authorId": "108193025",
                    "name": "L. Valeriani"
                },
                {
                    "authorId": "2308039508",
                    "name": "Lorenzo Basile"
                },
                {
                    "authorId": "5366502",
                    "name": "A. Ansuini"
                },
                {
                    "authorId": "104350259",
                    "name": "Diego Doimo"
                },
                {
                    "authorId": "2269294135",
                    "name": "Alberto Cazzaniga"
                }
            ],
            "abstract": "Recent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. This study investigates how vision-language models (VLMs) handle image-understanding tasks, specifically focusing on how visual information is processed and transferred to the textual domain. We compare VLMs that generate both images and text with those that output only text, highlighting key differences in information flow. We find that in models with multimodal outputs, image and text embeddings are more separated within the residual stream. Additionally, models vary in how information is exchanged from visual to textual tokens. VLMs that only output text exhibit a distributed communication pattern, where information is exchanged through multiple image tokens. In contrast, models trained for image and text generation tend to rely on a single token that acts as a narrow gate for visual information. We demonstrate that ablating this single token significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token enables effective steering of the image semantics, showing that targeted, local interventions can reliably control the model's global behavior.",
            "corpus_id": 274597781,
            "sentences": [
                {
                    "corpus_id": "274597781",
                    "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
                    "text": "The rise of foundation models [1] trained on vast amounts of text has transformed natural language processing (NLP), \n\nshowing that a single large language model (LLM) [2] can handle many different linguistic tasks [3][4][5]. The rich set of features encoded in LLM embeddings has been then used as an effective prior knowledge both for text-conditional image generation [6][7][8] and image understanding [9][10][11][12][13]. Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings [16] have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture [17][18][19][20]. This unification allows a richer understanding of the visual world, as generative tasks often require insight into the fundamental concepts and relationships within the data [21]. For example, a model that generates images from text descriptions must grasp the semantic content of those images to ensure they faithfully reflect the details and intent of the text [22,23]. As a result, research has rapidly progressed to integrate multiple modalities into a unified framework with increasingly deeper multimodal fusion. \n\nEarly approaches used cross-attention modules between modality-specific encoders [17]. Further advancements highlighted the importance of using a pre-trained LLM backbone combined with lightweight projection layers, often fine-tuning the LLM to refine the multimodal representations [19,24,25]. More recently, research has shown success in training multimodal-output vision-language models (VLMs) from scratch [20,26], achieving performance close to VLMs that only output text on visual understanding tasks.",
                    "score": 0.6541430977835287,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 119,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 879
                        },
                        {
                            "start": 880,
                            "end": 1071
                        },
                        {
                            "start": 1072,
                            "end": 1218
                        },
                        {
                            "start": 1221,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1515
                        },
                        {
                            "start": 1516,
                            "end": 1728
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 30,
                            "end": 33,
                            "matchedPaperCorpusId": "237091588"
                        },
                        {
                            "start": 215,
                            "end": 218,
                            "matchedPaperCorpusId": "6401679"
                        },
                        {
                            "start": 374,
                            "end": 377,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 405,
                            "end": 408,
                            "matchedPaperCorpusId": "244954250"
                        },
                        {
                            "start": 408,
                            "end": 412,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 412,
                            "end": 416,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 543,
                            "end": 547,
                            "matchedPaperCorpusId": "263605855"
                        },
                        {
                            "start": 682,
                            "end": 686,
                            "matchedPaperCorpusId": "258822817"
                        },
                        {
                            "start": 690,
                            "end": 694,
                            "matchedPaperCorpusId": "267750101"
                        },
                        {
                            "start": 874,
                            "end": 878,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 1302,
                            "end": 1306,
                            "matchedPaperCorpusId": "258822817"
                        },
                        {
                            "start": 1504,
                            "end": 1508,
                            "matchedPaperCorpusId": "267750101"
                        },
                        {
                            "start": 1511,
                            "end": 1514,
                            "matchedPaperCorpusId": "261696650"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.869140625
                }
            ],
            "relevance_judgement": 0.869140625,
            "relevance_judgment_input_expanded": "# Title: The Narrow Gate: Localized Image-Text Communication in Vision-Language Models\n# Venue: arXiv.org\n# Authors: Alessandro Serra, Francesco Ortu, Emanuele Panizon, L. Valeriani, Lorenzo Basile, A. Ansuini, Diego Doimo, Alberto Cazzaniga\n## Abstract\nRecent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. This study investigates how vision-language models (VLMs) handle image-understanding tasks, specifically focusing on how visual information is processed and transferred to the textual domain. We compare VLMs that generate both images and text with those that output only text, highlighting key differences in information flow. We find that in models with multimodal outputs, image and text embeddings are more separated within the residual stream. Additionally, models vary in how information is exchanged from visual to textual tokens. VLMs that only output text exhibit a distributed communication pattern, where information is exchanged through multiple image tokens. In contrast, models trained for image and text generation tend to rely on a single token that acts as a narrow gate for visual information. We demonstrate that ablating this single token significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token enables effective steering of the image semantics, showing that targeted, local interventions can reliably control the model's global behavior.\n## Introduction\nThe rise of foundation models [1] trained on vast amounts of text has transformed natural language processing (NLP), \n\nshowing that a single large language model (LLM) [2] can handle many different linguistic tasks [3][4][5]. The rich set of features encoded in LLM embeddings has been then used as an effective prior knowledge both for text-conditional image generation [6][7][8] and image understanding [9][10][11][12][13]. Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings [16] have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture [17][18][19][20]. This unification allows a richer understanding of the visual world, as generative tasks often require insight into the fundamental concepts and relationships within the data [21]. For example, a model that generates images from text descriptions must grasp the semantic content of those images to ensure they faithfully reflect the details and intent of the text [22,23]. As a result, research has rapidly progressed to integrate multiple modalities into a unified framework with increasingly deeper multimodal fusion. \n\nEarly approaches used cross-attention modules between modality-specific encoders [17]. Further advancements highlighted the importance of using a pre-trained LLM backbone combined with lightweight projection layers, often fine-tuning the LLM to refine the multimodal representations [19,24,25]. More recently, research has shown success in training multimodal-output vision-language models (VLMs) from scratch [20,26], achieving performance close to VLMs that only output text on visual understanding tasks.",
            "reference_string": "[274597781 | Serra et al. | 2024 | Citations: 0]"
        },
        {
            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 82,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.03651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2248946985",
                    "name": "Rui Zhao"
                },
                {
                    "authorId": "2258958963",
                    "name": "Weijia Mao"
                },
                {
                    "authorId": "2344762475",
                    "name": "Mike Zheng Shou"
                }
            ],
            "abstract": "Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle.",
            "corpus_id": 276782523,
            "sentences": [
                {
                    "corpus_id": "276782523",
                    "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
                    "text": "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,2,15,19,59,62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder [45] are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM [15] extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities. Our work leverages the advancements made by these foundational models and explores how to adapt the foundational model to specific domains.",
                    "score": 0.5083459429967252,
                    "section_title": "Unified Multimodal Generative Models",
                    "char_start_offset": 6395,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1413
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 234,
                            "end": 236,
                            "matchedPaperCorpusId": "262898422"
                        },
                        {
                            "start": 236,
                            "end": 239,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 242,
                            "end": 245,
                            "matchedPaperCorpusId": "266374640"
                        },
                        {
                            "start": 373,
                            "end": 377,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 517,
                            "end": 521,
                            "matchedPaperCorpusId": "261975252"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86474609375
                }
            ],
            "relevance_judgement": 0.86474609375,
            "relevance_judgment_input_expanded": "# Title: DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles\n# Venue: arXiv.org\n# Authors: Rui Zhao, Weijia Mao, Mike Zheng Shou\n## Abstract\nAdapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle.\n## Unified Multimodal Generative Models\nUnified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,2,15,19,59,62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder [45] are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM [15] extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities. Our work leverages the advancements made by these foundational models and explores how to adapt the foundational model to specific domains.",
            "reference_string": "[276782523 | Zhao et al. | 2025 | Citations: 1]"
        },
        {
            "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.13388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289660121",
                    "name": "Wei Li"
                },
                {
                    "authorId": "2118734325",
                    "name": "Xue Xu"
                },
                {
                    "authorId": "2288740225",
                    "name": "Jiachen Liu"
                },
                {
                    "authorId": "2288393805",
                    "name": "Xinyan Xiao"
                }
            ],
            "abstract": "Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.",
            "corpus_id": 267200040,
            "sentences": [
                {
                    "corpus_id": "267200040",
                    "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
                    "text": "Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.",
                    "score": 0.5965984751231097,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8623046875
                },
                {
                    "corpus_id": "267200040",
                    "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
                    "text": "In this paper, we propose UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts comprising freeform interleaved vision-language inputs. Unlike traditional text-only prompts, multimodal prompts encompass various combinations of image entities and textual elements, as demonstrated in Figure 1. UNIMO-G is designed to faithfully reproduce all image entities, render textual content, and follow the instructions in multimodal prompts. Specifically, we leverage the perception capabilities of Multimodal Large Language Models (MLLMs) to encode multimodal prompts into a unified visionlanguage semantic space. Subsequently, a conditional diffusion network generates images from these encoded representations. \n\nTo train UNIMO-G efficiently, we implement a two-phase strategy. Initially, the model undergoes pre-training on a large-scale dataset of text-image pairs, enhancing its proficiency in conditional image generation. This is followed by a phase of instruction tuning with multimodal prompts, learns to generate images that align with the detailed specifications provided in these prompts. A carefully designed data processing pipeline, incorporating language grounding and image segmentation, is employed to construct these multimodal prompts. This approach enables UNIMO-G to harness rich features from the MLLM encoder to generate images faithfully reproducing the contents across various contexts. \n\nUNIMO-G exhibits a comprehensive capability for controllable image generation, excelling not only in text-to-image synthesis but also in zero-shot subject-driven generation. It adeptly produces highfidelity images from multimodal prompts, even those containing multiple image entities. To assess its performance, we conducted evaluations in both text-to-image and subject-driven generation contexts using the MS-COCO (Lin et al., 2014) and DreamBench (Ruiz et al., 2023) datasets, respectively. The results consistently highlight UNIMO-G's superior performance in these scenarios. Additionally, recognizing DreamBench's focus on single-subject generation, we introduce Multi-Bench, a new benchmark featuring images with multiple entities. The evaluation on MultiBench confirms UNIMO-G's effectiveness in zero-shot multi-entity subject-driven generation.",
                    "score": 0.5395419042024179,
                    "section_title": "Introduction",
                    "char_start_offset": 1856,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 474
                        },
                        {
                            "start": 475,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 746
                        },
                        {
                            "start": 749,
                            "end": 813
                        },
                        {
                            "start": 814,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1289
                        },
                        {
                            "start": 1290,
                            "end": 1446
                        },
                        {
                            "start": 1449,
                            "end": 1622
                        },
                        {
                            "start": 1623,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 1943
                        },
                        {
                            "start": 1944,
                            "end": 2029
                        },
                        {
                            "start": 2030,
                            "end": 2187
                        },
                        {
                            "start": 2188,
                            "end": 2302
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1866,
                            "end": 1884,
                            "matchedPaperCorpusId": "14113767"
                        },
                        {
                            "start": 1900,
                            "end": 1919,
                            "matchedPaperCorpusId": "251800180"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.818359375
                },
                {
                    "corpus_id": "267200040",
                    "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
                    "text": "Multimodal Large Language Models (MLLMs) have significantly broadened the capabilities of language models to process various modalities (Liu et al., 2023a;Li et al., 2021Li et al., , 2022a;;Wang et al., 2023;Driess et al., 2023). These models inherently facilitate interleaved vision-language input, effectively handling multiple images. Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder. The key component of KOSMOS-G is an AlignerNet, which is trained solely on textual data to align the output embedding space of the frozen SDv1.5 U-Net with the MLLM. In contrast, our approach centers on training the U-Net model end-to-end specifically for multimodal diffusion, significantly enhancing both the faithfulness and relevance of generated images in multimodal contexts. Differing from alignmentbased approaches, our two-stage training strategy markedly improves the model's proficiency in following multimodal instructions, particularly in complex multi-entity scenarios.",
                    "score": 0.5324202213532938,
                    "section_title": "Generating with Multi-modal Language Models",
                    "char_start_offset": 14730,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 864
                        },
                        {
                            "start": 865,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1738
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 155,
                            "end": 170,
                            "matchedPaperCorpusId": "229924402"
                        },
                        {
                            "start": 170,
                            "end": 190,
                            "matchedPaperCorpusId": "247519008"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.798828125
                }
            ],
            "relevance_judgement": 0.8623046875,
            "relevance_judgment_input_expanded": "# Title: UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Wei Li, Xue Xu, Jiachen Liu, Xinyan Xiao\n## Abstract\nExisting text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.\n## Introduction\nIn this paper, we propose UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts comprising freeform interleaved vision-language inputs. Unlike traditional text-only prompts, multimodal prompts encompass various combinations of image entities and textual elements, as demonstrated in Figure 1. UNIMO-G is designed to faithfully reproduce all image entities, render textual content, and follow the instructions in multimodal prompts. Specifically, we leverage the perception capabilities of Multimodal Large Language Models (MLLMs) to encode multimodal prompts into a unified visionlanguage semantic space. Subsequently, a conditional diffusion network generates images from these encoded representations. \n\nTo train UNIMO-G efficiently, we implement a two-phase strategy. Initially, the model undergoes pre-training on a large-scale dataset of text-image pairs, enhancing its proficiency in conditional image generation. This is followed by a phase of instruction tuning with multimodal prompts, learns to generate images that align with the detailed specifications provided in these prompts. A carefully designed data processing pipeline, incorporating language grounding and image segmentation, is employed to construct these multimodal prompts. This approach enables UNIMO-G to harness rich features from the MLLM encoder to generate images faithfully reproducing the contents across various contexts. \n\nUNIMO-G exhibits a comprehensive capability for controllable image generation, excelling not only in text-to-image synthesis but also in zero-shot subject-driven generation. It adeptly produces highfidelity images from multimodal prompts, even those containing multiple image entities. To assess its performance, we conducted evaluations in both text-to-image and subject-driven generation contexts using the MS-COCO (Lin et al., 2014) and DreamBench (Ruiz et al., 2023) datasets, respectively. The results consistently highlight UNIMO-G's superior performance in these scenarios. Additionally, recognizing DreamBench's focus on single-subject generation, we introduce Multi-Bench, a new benchmark featuring images with multiple entities. The evaluation on MultiBench confirms UNIMO-G's effectiveness in zero-shot multi-entity subject-driven generation.\n\n## Generating with Multi-modal Language Models\nMultimodal Large Language Models (MLLMs) have significantly broadened the capabilities of language models to process various modalities (Liu et al., 2023a;Li et al., 2021Li et al., , 2022a;;Wang et al., 2023;Driess et al., 2023). These models inherently facilitate interleaved vision-language input, effectively handling multiple images. Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder. The key component of KOSMOS-G is an AlignerNet, which is trained solely on textual data to align the output embedding space of the frozen SDv1.5 U-Net with the MLLM. In contrast, our approach centers on training the U-Net model end-to-end specifically for multimodal diffusion, significantly enhancing both the faithfulness and relevance of generated images in multimodal contexts. Differing from alignmentbased approaches, our two-stage training strategy markedly improves the model's proficiency in following multimodal instructions, particularly in complex multi-entity scenarios.",
            "reference_string": "[267200040 | Li et al. | 2024 | Citations: 8]"
        },
        {
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 15,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144484147",
                    "name": "Rongyao Fang"
                },
                {
                    "authorId": "2326302007",
                    "name": "Chengqi Duan"
                },
                {
                    "authorId": "2267252383",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2274232642",
                    "name": "Hao Li"
                },
                {
                    "authorId": "2274118690",
                    "name": "Hao Tian"
                },
                {
                    "authorId": "2257433221",
                    "name": "Xingyu Zeng"
                },
                {
                    "authorId": "2319817758",
                    "name": "Rui Zhao"
                },
                {
                    "authorId": "2292283383",
                    "name": "Jifeng Dai"
                },
                {
                    "authorId": "2268799275",
                    "name": "Hongsheng Li"
                },
                {
                    "authorId": "2286520347",
                    "name": "Xihui Liu"
                }
            ],
            "abstract": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.",
            "corpus_id": 273403986,
            "sentences": [
                {
                    "corpus_id": "273403986",
                    "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
                    "text": "Unifying multimodal understanding and generation capabilities within a single model is a critical milestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu et al., 2024b;Zhu et al., 2023a) in multimodal large language models (MLLMs) have made significant progress in integrating visual reasoning and understanding with natural language interfaces. However, developing a unified framework that excels at both comprehending and generating multimodal content remains a significant challenge in the field of artificial intelligence. \n\nRecent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis. \n\nHowever, existing MLLM-based methods (Sun et al., 2023;2024b) face a common challenge in the trade-off between diversity for text-to-image generation and high controllability for tasks such as image editing. Previous methods mostly rely on single-granular features extracted from a visual encoder and neglect the varying granularity requirements of different tasks. On the one hand, generating diverse images reflecting the real world from text descriptions requires features that encode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image decoder, allowing the diffusion model to generate diverse images that semantically align with the text prompt. On the other hand, tasks demanding precise control over output images, such as image editing and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed visual information for the image decoder. This dichotomy presents a significant challenge for current MLLM-based methods, which typically generate single-granular feature representations for all tasks.",
                    "score": 0.6074280164830536,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1200,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 154,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 570
                        },
                        {
                            "start": 573,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1193
                        },
                        {
                            "start": 1196,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1709
                        },
                        {
                            "start": 1710,
                            "end": 1887
                        },
                        {
                            "start": 1888,
                            "end": 2115
                        },
                        {
                            "start": 2116,
                            "end": 2275
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 193,
                            "end": 212,
                            "matchedPaperCorpusId": "258179774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85888671875
                },
                {
                    "corpus_id": "273403986",
                    "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
                    "text": "The rapid advancement of large language models (LLMs) has catalyzed significant progress in multimodal large language models (MLLMs) for multimodal understanding tasks Dai et al. (2023); Li et al. (2024b); Zhang et al. (2023a); Chen et al. (2024); Lin et al. (2024); Zhang et al. (2024b); Li et al. (2024a). Pioneering works such as LLaVA (Liu et al., 2024b) and MiniGPT-4 (Zhu et al., 2023a) have demonstrated remarkable performance across diverse image understanding tasks, including visual question answering (VQA), visual reasoning, optical character recognition (OCR), and object grounding. These approaches typically employ visual encoders, such as the CLIP encoder (Radford et al., 2021), to extract continuous image features, which are then projected into the LLM's embedding space for subsequent tasks. While successfully unifying various image understanding tasks within a single model, these methods mostly adhere to a multimodal-input, text-output paradigm. Consequently, they excel at text-based responses to visual inputs but cannot generate multimodal outputs beyond text, limiting their applicability in tasks requiring visual content generation.",
                    "score": 0.5501752015248307,
                    "section_title": "MULTIMODAL UNDERSTANDING",
                    "char_start_offset": 6299,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 595
                        },
                        {
                            "start": 596,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1162
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 228,
                            "end": 246,
                            "matchedPaperCorpusId": "266521410"
                        },
                        {
                            "start": 248,
                            "end": 265,
                            "matchedPaperCorpusId": "266174746"
                        },
                        {
                            "start": 339,
                            "end": 358,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 672,
                            "end": 694,
                            "matchedPaperCorpusId": "231591445"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7607421875
                }
            ],
            "relevance_judgement": 0.85888671875,
            "relevance_judgment_input_expanded": "# Title: PUMA: Empowering Unified MLLM with Multi-granular Visual Generation\n# Venue: arXiv.org\n# Authors: Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, Xihui Liu\n## Abstract\nRecent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.\n## INTRODUCTION\nUnifying multimodal understanding and generation capabilities within a single model is a critical milestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu et al., 2024b;Zhu et al., 2023a) in multimodal large language models (MLLMs) have made significant progress in integrating visual reasoning and understanding with natural language interfaces. However, developing a unified framework that excels at both comprehending and generating multimodal content remains a significant challenge in the field of artificial intelligence. \n\nRecent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis. \n\nHowever, existing MLLM-based methods (Sun et al., 2023;2024b) face a common challenge in the trade-off between diversity for text-to-image generation and high controllability for tasks such as image editing. Previous methods mostly rely on single-granular features extracted from a visual encoder and neglect the varying granularity requirements of different tasks. On the one hand, generating diverse images reflecting the real world from text descriptions requires features that encode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image decoder, allowing the diffusion model to generate diverse images that semantically align with the text prompt. On the other hand, tasks demanding precise control over output images, such as image editing and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed visual information for the image decoder. This dichotomy presents a significant challenge for current MLLM-based methods, which typically generate single-granular feature representations for all tasks.\n\n## MULTIMODAL UNDERSTANDING\nThe rapid advancement of large language models (LLMs) has catalyzed significant progress in multimodal large language models (MLLMs) for multimodal understanding tasks Dai et al. (2023); Li et al. (2024b); Zhang et al. (2023a); Chen et al. (2024); Lin et al. (2024); Zhang et al. (2024b); Li et al. (2024a). Pioneering works such as LLaVA (Liu et al., 2024b) and MiniGPT-4 (Zhu et al., 2023a) have demonstrated remarkable performance across diverse image understanding tasks, including visual question answering (VQA), visual reasoning, optical character recognition (OCR), and object grounding. These approaches typically employ visual encoders, such as the CLIP encoder (Radford et al., 2021), to extract continuous image features, which are then projected into the LLM's embedding space for subsequent tasks. While successfully unifying various image understanding tasks within a single model, these methods mostly adhere to a multimodal-input, text-output paradigm. Consequently, they excel at text-based responses to visual inputs but cannot generate multimodal outputs beyond text, limiting their applicability in tasks requiring visual content generation.",
            "reference_string": "[273403986 | Fang et al. | 2024 | Citations: 15]"
        },
        {
            "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 61,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.13436, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347484699",
                    "name": "Lijie Fan"
                },
                {
                    "authorId": "34689393",
                    "name": "Luming Tang"
                },
                {
                    "authorId": "2333872078",
                    "name": "Siyang Qin"
                },
                {
                    "authorId": "2307269819",
                    "name": "Tianhong Li"
                },
                {
                    "authorId": "2350843695",
                    "name": "Xuan Yang"
                },
                {
                    "authorId": "2266238535",
                    "name": "Siyuan Qiao"
                },
                {
                    "authorId": "2350755056",
                    "name": "Andreas Steiner"
                },
                {
                    "authorId": "2333512682",
                    "name": "Chen Sun"
                },
                {
                    "authorId": "2281034398",
                    "name": "Yuanzhen Li"
                },
                {
                    "authorId": "2351408307",
                    "name": "Tao Zhu"
                },
                {
                    "authorId": "2325950285",
                    "name": "Michael Rubinstein"
                },
                {
                    "authorId": "3063676",
                    "name": "Michalis Raptis"
                },
                {
                    "authorId": "2262516617",
                    "name": "Deqing Sun"
                },
                {
                    "authorId": "1737285",
                    "name": "Radu Soricut"
                }
            ],
            "abstract": "We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image. We find though there is an inherent trade-off between the image generation and understanding task, a carefully tuned training recipe enables them to improve each other. By selecting an appropriate loss balance weight, the unified model achieves results comparable to or exceeding those of single-task baselines on both tasks. Furthermore, we demonstrate that employing stronger pre-trained LLMs and random-order generation during training is important to achieve high-fidelity image generation within this unified framework. Built upon the Gemma model series, UniFluid exhibits competitive performance across both image generation and understanding, demonstrating strong transferability to various downstream tasks, including image editing for generation, as well as visual captioning and question answering for understanding.",
            "corpus_id": 277103675,
            "sentences": [
                {
                    "corpus_id": "277103675",
                    "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
                    "text": "Large Language Models (LLMs) have recently advanced from text-centric architectures, such as BERT [11] and GPT [35], toward multimodal systems capable of understanding and generating content across different modalities. GPT-3 [5] and PaLM [9] show that scaling language models leads to emergent capabilities, while Flamingo [2] further demonstrates that incorporating visual inputs facilitates unified multimodal reasoning. This trend toward unified vision-language model-using a single model for diverse tasks of visual understanding and generation-has promising potential for leveraging knowledge and reasoning abilities that transfer across different vision and language tasks, ultimately facilitating more robust and generalizable multimodal representation and modeling capabilities. \n\nMotivated by the advantages and strong scaling properties of autoregressive models, coupled with their simplicity, we investigate a pure autoregressive framework for unified visual generation and understanding, without the limitations introduced by vector quantization (VQ). In this paper, we introduce UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,21] for image generation and a pre-trained SigLIP [59] image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities.",
                    "score": 0.5181928472540605,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 423
                        },
                        {
                            "start": 424,
                            "end": 787
                        },
                        {
                            "start": 790,
                            "end": 1064
                        },
                        {
                            "start": 1065,
                            "end": 1270
                        },
                        {
                            "start": 1271,
                            "end": 1552
                        },
                        {
                            "start": 1553,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 2005
                        },
                        {
                            "start": 2006,
                            "end": 2126
                        },
                        {
                            "start": 2127,
                            "end": 2436
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 98,
                            "end": 102,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 226,
                            "end": 229,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 239,
                            "end": 242,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 324,
                            "end": 327,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 1830,
                            "end": 1833,
                            "matchedPaperCorpusId": "270560593"
                        },
                        {
                            "start": 1880,
                            "end": 1884,
                            "matchedPaperCorpusId": "257767223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8583984375
                }
            ],
            "relevance_judgement": 0.8583984375,
            "relevance_judgment_input_expanded": "# Title: Unified Autoregressive Visual Generation and Understanding with Continuous Tokens\n# Venue: arXiv.org\n# Authors: Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, Michael Rubinstein, Michalis Raptis, Deqing Sun, Radu Soricut\n## Abstract\nWe present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image. We find though there is an inherent trade-off between the image generation and understanding task, a carefully tuned training recipe enables them to improve each other. By selecting an appropriate loss balance weight, the unified model achieves results comparable to or exceeding those of single-task baselines on both tasks. Furthermore, we demonstrate that employing stronger pre-trained LLMs and random-order generation during training is important to achieve high-fidelity image generation within this unified framework. Built upon the Gemma model series, UniFluid exhibits competitive performance across both image generation and understanding, demonstrating strong transferability to various downstream tasks, including image editing for generation, as well as visual captioning and question answering for understanding.\n## Introduction\nLarge Language Models (LLMs) have recently advanced from text-centric architectures, such as BERT [11] and GPT [35], toward multimodal systems capable of understanding and generating content across different modalities. GPT-3 [5] and PaLM [9] show that scaling language models leads to emergent capabilities, while Flamingo [2] further demonstrates that incorporating visual inputs facilitates unified multimodal reasoning. This trend toward unified vision-language model-using a single model for diverse tasks of visual understanding and generation-has promising potential for leveraging knowledge and reasoning abilities that transfer across different vision and language tasks, ultimately facilitating more robust and generalizable multimodal representation and modeling capabilities. \n\nMotivated by the advantages and strong scaling properties of autoregressive models, coupled with their simplicity, we investigate a pure autoregressive framework for unified visual generation and understanding, without the limitations introduced by vector quantization (VQ). In this paper, we introduce UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,21] for image generation and a pre-trained SigLIP [59] image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities.",
            "reference_string": "[277103675 | Fan et al. | 2025 | Citations: 8]"
        },
        {
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 317,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359211182",
                    "name": "Xinjie Zhang"
                },
                {
                    "authorId": "2359201363",
                    "name": "Jintao Guo"
                },
                {
                    "authorId": "2337775059",
                    "name": "Shanshan Zhao"
                },
                {
                    "authorId": "2345947101",
                    "name": "Minghao Fu"
                },
                {
                    "authorId": "2275153684",
                    "name": "Lunhao Duan"
                },
                {
                    "authorId": "2339237238",
                    "name": "Guo-Hua Wang"
                },
                {
                    "authorId": "2304395071",
                    "name": "Qing-Guo Chen"
                },
                {
                    "authorId": "2304361003",
                    "name": "Zhao Xu"
                },
                {
                    "authorId": "2305289815",
                    "name": "Weihua Luo"
                },
                {
                    "authorId": "2304530663",
                    "name": "Kaifu Zhang"
                }
            ],
            "abstract": "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
            "corpus_id": 278327487,
            "sentences": [
                {
                    "corpus_id": "278327487",
                    "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
                    "text": "One major direction in unified multimodal understanding and generation models adopts autoregressive (AR) architectures, where both vision and language tokens are typically serialized and modeled sequentially. In these models, a backbone Transformer, typically adapted from large language models (LLMs) such as LLaMA family [1], [2], [133], Vicuna [58], Gemma series [134], [135], [136], and Qwen series [5], [6], [9], [10], serves as the unified modality-fusion module to autoregressively predict multimodal outputs. \n\nTo integrate visual information into the AR framework, as shown in Fig. 5, existing methods propose different strategies for image tokenization during modality encoding. These approaches can be broadly categorized into four types: pixel-based, semantic-based, learnable query-based, hybrid-based encoding methods. 1) Pixel-based Encoding. As shown in Fig. 5 (b-1), pixelbased encoding typically refers to the representation of images as continuous or discrete tokens obtained from pretrained autoencoders supervised purely by image reconstruction, such as VQGAN-like models [32], [137], [138], [139]. These encoders compress the high-dimensional pixel space into a compact latent space, where each spatial patch corresponds to an image token. In unified multimodal autoregressive models, image tokens serialized from such encoders are processed analogously to text tokens, allowing both modalities to be modeled within a single sequence. \n\nRecent works have adopted and enhanced pixel-based tokenization with various encoder designs. LWM [29] employs a VQGAN tokenizer [32] to encode images into discrete latent codes without requiring semantic supervision. It proposes a multimodal world modeling framework, wherein visual and textual tokens are serialized together for unified autoregressive modeling. By learning world dynamics purely through reconstruction-based visual tokens and textual descriptions, LWM demonstrates that largescale multimodal generation is feasible without specialized semantic tokenization.",
                    "score": 0.5234393517505439,
                    "section_title": "Auto-Regressive Models",
                    "char_start_offset": 27036,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 516
                        },
                        {
                            "start": 519,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1456
                        },
                        {
                            "start": 1459,
                            "end": 1552
                        },
                        {
                            "start": 1553,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1822
                        },
                        {
                            "start": 1823,
                            "end": 2035
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1093,
                            "end": 1097,
                            "matchedPaperCorpusId": "229297973"
                        },
                        {
                            "start": 1099,
                            "end": 1104,
                            "matchedPaperCorpusId": "252367709"
                        },
                        {
                            "start": 1106,
                            "end": 1111,
                            "matchedPaperCorpusId": "263671912"
                        },
                        {
                            "start": 1113,
                            "end": 1118,
                            "matchedPaperCorpusId": "247628171"
                        },
                        {
                            "start": 1588,
                            "end": 1592,
                            "matchedPaperCorpusId": "229297973"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85791015625
                },
                {
                    "corpus_id": "278327487",
                    "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
                    "text": "A unified model capable of both understanding and generating multimodal content holds immense potential: it could generate images based on complex instructions, reason about visual data, and visualize multimodal analyses through generated outputs. The unveiling of GPT-4o's enhanced capabilities [27] in March 2025 has further highlighted this potential, sparking widespread interest in unification. \n\nHowever, designing such a unified framework presents significant challenges. It requires integrating the strengths of autoregressive models for reasoning and text generation with the robustness of diffusion-based models for highquality image synthesis. Key questions remain unresolved, state of unified multimodal models (as illustrated in Fig. 1), thereby benefiting future research endeavors, we present this survey. We begin by introducing the foundational concepts and recent advancements in both multimodal understanding and image generation, covering both autoregressive and diffusion-based paradigms. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. Within the autoregressive and hybrid categories, we further classify models based on their image tokenization strategies, reflecting the diversity of approaches in this area. \n\nBeyond architecture, we assemble datasets and benchmarks tailored for training and evaluating unified multimodal models. These resources span multimodal understanding, text-to-image generation, image editing, and other relevant tasks, providing a foundation for future exploration. Finally, we discuss the key challenges facing this nascent field, including efficient tokenization strategy, data construction, model evaluation, etc. Tackling these challenges will be crucial for advancing the capabilities and scalability of unified multimodal models. \n\nIn the community, there exist excellent surveys on large language models [40], [41], multimodal understanding [42], [43], [44], and image generation [45], [46], while our work focuses specifically on the integration of understanding and generation tasks. Readers are encouraged to consult these complementary surveys for a broader perspective on related topics. We aim to inspire further research in this rapidly evolving field and provide a valuable reference for the community.",
                    "score": 0.576975105131979,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1860,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 247
                        },
                        {
                            "start": 248,
                            "end": 399
                        },
                        {
                            "start": 402,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 654
                        },
                        {
                            "start": 655,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 1009
                        },
                        {
                            "start": 1010,
                            "end": 1221
                        },
                        {
                            "start": 1222,
                            "end": 1396
                        },
                        {
                            "start": 1399,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1950
                        },
                        {
                            "start": 1953,
                            "end": 2207
                        },
                        {
                            "start": 2208,
                            "end": 2314
                        },
                        {
                            "start": 2315,
                            "end": 2432
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2032,
                            "end": 2036,
                            "matchedPaperCorpusId": "259360395"
                        },
                        {
                            "start": 2063,
                            "end": 2067,
                            "matchedPaperCorpusId": "271750640"
                        },
                        {
                            "start": 2069,
                            "end": 2073,
                            "matchedPaperCorpusId": "265308931"
                        },
                        {
                            "start": 2075,
                            "end": 2079,
                            "matchedPaperCorpusId": "259243718"
                        },
                        {
                            "start": 2102,
                            "end": 2106,
                            "matchedPaperCorpusId": "252199918"
                        },
                        {
                            "start": 2108,
                            "end": 2112,
                            "matchedPaperCorpusId": "265821110"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77880859375
                }
            ],
            "relevance_judgement": 0.85791015625,
            "relevance_judgment_input_expanded": "# Title: Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities\n# Venue: arXiv.org\n# Authors: Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang\n## Abstract\nRecent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).\n## INTRODUCTION\nA unified model capable of both understanding and generating multimodal content holds immense potential: it could generate images based on complex instructions, reason about visual data, and visualize multimodal analyses through generated outputs. The unveiling of GPT-4o's enhanced capabilities [27] in March 2025 has further highlighted this potential, sparking widespread interest in unification. \n\nHowever, designing such a unified framework presents significant challenges. It requires integrating the strengths of autoregressive models for reasoning and text generation with the robustness of diffusion-based models for highquality image synthesis. Key questions remain unresolved, state of unified multimodal models (as illustrated in Fig. 1), thereby benefiting future research endeavors, we present this survey. We begin by introducing the foundational concepts and recent advancements in both multimodal understanding and image generation, covering both autoregressive and diffusion-based paradigms. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. Within the autoregressive and hybrid categories, we further classify models based on their image tokenization strategies, reflecting the diversity of approaches in this area. \n\nBeyond architecture, we assemble datasets and benchmarks tailored for training and evaluating unified multimodal models. These resources span multimodal understanding, text-to-image generation, image editing, and other relevant tasks, providing a foundation for future exploration. Finally, we discuss the key challenges facing this nascent field, including efficient tokenization strategy, data construction, model evaluation, etc. Tackling these challenges will be crucial for advancing the capabilities and scalability of unified multimodal models. \n\nIn the community, there exist excellent surveys on large language models [40], [41], multimodal understanding [42], [43], [44], and image generation [45], [46], while our work focuses specifically on the integration of understanding and generation tasks. Readers are encouraged to consult these complementary surveys for a broader perspective on related topics. We aim to inspire further research in this rapidly evolving field and provide a valuable reference for the community.\n\n## Auto-Regressive Models\nOne major direction in unified multimodal understanding and generation models adopts autoregressive (AR) architectures, where both vision and language tokens are typically serialized and modeled sequentially. In these models, a backbone Transformer, typically adapted from large language models (LLMs) such as LLaMA family [1], [2], [133], Vicuna [58], Gemma series [134], [135], [136], and Qwen series [5], [6], [9], [10], serves as the unified modality-fusion module to autoregressively predict multimodal outputs. \n\nTo integrate visual information into the AR framework, as shown in Fig. 5, existing methods propose different strategies for image tokenization during modality encoding. These approaches can be broadly categorized into four types: pixel-based, semantic-based, learnable query-based, hybrid-based encoding methods. 1) Pixel-based Encoding. As shown in Fig. 5 (b-1), pixelbased encoding typically refers to the representation of images as continuous or discrete tokens obtained from pretrained autoencoders supervised purely by image reconstruction, such as VQGAN-like models [32], [137], [138], [139]. These encoders compress the high-dimensional pixel space into a compact latent space, where each spatial patch corresponds to an image token. In unified multimodal autoregressive models, image tokens serialized from such encoders are processed analogously to text tokens, allowing both modalities to be modeled within a single sequence. \n\nRecent works have adopted and enhanced pixel-based tokenization with various encoder designs. LWM [29] employs a VQGAN tokenizer [32] to encode images into discrete latent codes without requiring semantic supervision. It proposes a multimodal world modeling framework, wherein visual and textual tokens are serialized together for unified autoregressive modeling. By learning world dynamics purely through reconstruction-based visual tokens and textual descriptions, LWM demonstrates that largescale multimodal generation is feasible without specialized semantic tokenization.",
            "reference_string": "[278327487 | Zhang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 145,
            "citation_count": 23,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.16602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115941903",
                    "name": "Jiaxing Huang"
                },
                {
                    "authorId": "2276743977",
                    "name": "Jingyi Zhang"
                },
                {
                    "authorId": "2276609830",
                    "name": "Kai Jiang"
                },
                {
                    "authorId": "49660254",
                    "name": "Han Qiu"
                },
                {
                    "authorId": "2237947102",
                    "name": "Shijian Lu"
                }
            ],
            "abstract": "Traditional computer vision generally solves each single task independently by a dedicated model with the task instruction implicitly designed in the model architecture, arising two limitations: (1) it leads to task-specific models, which require multiple models for different tasks and restrict the potential synergies from diverse tasks; (2) it leads to a pre-defined and fixed model interface that has limited interactivity and adaptability in following user' task instructions. To address them, Visual Instruction Tuning (VIT) has been intensively studied recently, which finetunes a large vision model with language as task instructions, aiming to learn from a wide range of vision tasks described by language instructions a general-purpose multimodal model that can follow arbitrary instructions and thus solve arbitrary tasks specified by the user. This work aims to provide a systematic review of visual instruction tuning, covering (1) the background that presents computer vision task paradigms and the development of VIT; (2) the foundations of VIT that introduce commonly used network architectures, visual instruction tuning frameworks and objectives, and evaluation setups and tasks; (3) the commonly used datasets in visual instruction tuning and evaluation; (4) the review of existing VIT methods that categorizes them with a taxonomy according to both the studied vision task and the method design and highlights the major contributions, strengths, and shortcomings of them; (5) the comparison and discussion of VIT methods over various instruction-following benchmarks; (6) several challenges, open directions and possible future works in visual instruction tuning research.",
            "corpus_id": 266573642,
            "sentences": [
                {
                    "corpus_id": "266573642",
                    "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
                    "text": "integrating multiple modalities. Overall, the architecture enables the integration of diverse modalities into a large language model for multimodal understanding and response generation. \n\nPandaGPT is a model that integrates multimodal encoders from ImageBind and language models from Vicuna to perform instruction-following tasks across six modalities: image/video, text, audio, depth, thermal, and IMU. It demonstrates the ability to connect information from different modalities and compose their semantics naturally, enabling tasks such as image description generation, story writing inspired by videos, and answering questions about audios. PandaGPT's training on aligned image-text pairs allows it to display emergent cross-modal capabilities for data other than image and text, paving the way for holistic understanding of inputs across different modalities. \n\nLAMM introduces the Language-Assisted multimodal (LAMM) dataset, framework, and benchmark, aiming to facilitate the and evaluation of multimodal large language models (MLLMs). The main contributions include the comprehensive dataset and benchmark covering a wide range of vision tasks for 2D and 3D vision, a detailed methodology for constructing multimodal instruction tuning datasets, and a primary MLLM training framework optimized for modality extension. Additionally, the paper provides baseline models, extensive experimental observations, and analysis to accelerate future research in the field of multimodal language models. \n\nLLaVAR As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework. \n\nOtterHD introduces OtterHD-8B model, which addresses the limitations of fixed-resolution inputs in Large Multimodal Models (LMMs). It leverages the Fuyu-8B ar-Fig. 28: Illustration of SEED-LLaMA [137].",
                    "score": 0.5657981395533265,
                    "section_title": "Visual Assistant",
                    "char_start_offset": 68635,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 33,
                            "end": 186
                        },
                        {
                            "start": 189,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 865
                        },
                        {
                            "start": 868,
                            "end": 1043
                        },
                        {
                            "start": 1044,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1500
                        },
                        {
                            "start": 1503,
                            "end": 1737
                        },
                        {
                            "start": 1738,
                            "end": 1903
                        },
                        {
                            "start": 1904,
                            "end": 2111
                        },
                        {
                            "start": 2114,
                            "end": 2244
                        },
                        {
                            "start": 2245,
                            "end": 2277
                        },
                        {
                            "start": 2278,
                            "end": 2315
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8525390625
                },
                {
                    "corpus_id": "266573642",
                    "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
                    "text": "EmbodiedGPT is an end-to-end multimodal foundation model for embodied AI with a \"chain-of-thought\" capability, enabling embodied agents to interact with the physical world more naturally. It also develops two datasets, Ego-COT and EgoVQA, and proposes a cost-effective training Fig. 29: The training paradigm of of ImageBind-LLM [139]. Figure is from [139]. approach for extracting task-relevant features from planning queries. The approach demonstrates state-of-the-art or comparable performance on multiple embodied tasks, including embodied control, planning, video captioning, and video QA, outperforming existing models on benchmark tasks. \n\nChatBridge is a novel multimodal language model that leverages large language models to bridge the gap between various modalities. It proposes a two-stage training approach to align different modalities with language and introduces a new multimodal instruction tuning dataset called MULTIS. \n\nVideoChat is a chat-centric video understanding system that integrates video foundation models and large language models. It proposes a video-centric instruction dataset emphasizing spatiotemporal reasoning and causal relationships, providing a valuable asset for training chatcentric video understanding systems. It also presents qualitative experiments showcasing the system's potential across various video applications and sets a standard for future research in the field of video understanding. As shown in Figure 30, the framework of VideoChat consists of two main components: VideoChat-Text and VideoChat-Embed. VideoChat-Text textualizes videos in stream by converting visual data into textual format using various vision models and prompts, allowing a pretrained large language model to address user-specified tasks based on the video text descriptions. On the other hand, VideoChat-Embed encodes videos as embeddings and combines video and language foundation models with a Video-Language Token Interface (VLTF) to optimize cross-modality, enabling the model to effectively communicate with users through a large language model. \n\nVideo-ChatGPT is a multimodal model that merges a pretrained visual encoder with a Large Language Model (LLM) to understand and generate detailed conversations about videos.",
                    "score": 0.5101551817905602,
                    "section_title": "Visual Assistant",
                    "char_start_offset": 73028,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 427
                        },
                        {
                            "start": 428,
                            "end": 644
                        },
                        {
                            "start": 647,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 937
                        },
                        {
                            "start": 940,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1802
                        },
                        {
                            "start": 1803,
                            "end": 2078
                        },
                        {
                            "start": 2081,
                            "end": 2254
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76123046875
                }
            ],
            "relevance_judgement": 0.8525390625,
            "relevance_judgment_input_expanded": "# Title: Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey\n# Venue: arXiv.org\n# Authors: Jiaxing Huang, Jingyi Zhang, Kai Jiang, Han Qiu, Shijian Lu\n## Abstract\nTraditional computer vision generally solves each single task independently by a dedicated model with the task instruction implicitly designed in the model architecture, arising two limitations: (1) it leads to task-specific models, which require multiple models for different tasks and restrict the potential synergies from diverse tasks; (2) it leads to a pre-defined and fixed model interface that has limited interactivity and adaptability in following user' task instructions. To address them, Visual Instruction Tuning (VIT) has been intensively studied recently, which finetunes a large vision model with language as task instructions, aiming to learn from a wide range of vision tasks described by language instructions a general-purpose multimodal model that can follow arbitrary instructions and thus solve arbitrary tasks specified by the user. This work aims to provide a systematic review of visual instruction tuning, covering (1) the background that presents computer vision task paradigms and the development of VIT; (2) the foundations of VIT that introduce commonly used network architectures, visual instruction tuning frameworks and objectives, and evaluation setups and tasks; (3) the commonly used datasets in visual instruction tuning and evaluation; (4) the review of existing VIT methods that categorizes them with a taxonomy according to both the studied vision task and the method design and highlights the major contributions, strengths, and shortcomings of them; (5) the comparison and discussion of VIT methods over various instruction-following benchmarks; (6) several challenges, open directions and possible future works in visual instruction tuning research.\n## Visual Assistant\nintegrating multiple modalities. Overall, the architecture enables the integration of diverse modalities into a large language model for multimodal understanding and response generation. \n\nPandaGPT is a model that integrates multimodal encoders from ImageBind and language models from Vicuna to perform instruction-following tasks across six modalities: image/video, text, audio, depth, thermal, and IMU. It demonstrates the ability to connect information from different modalities and compose their semantics naturally, enabling tasks such as image description generation, story writing inspired by videos, and answering questions about audios. PandaGPT's training on aligned image-text pairs allows it to display emergent cross-modal capabilities for data other than image and text, paving the way for holistic understanding of inputs across different modalities. \n\nLAMM introduces the Language-Assisted multimodal (LAMM) dataset, framework, and benchmark, aiming to facilitate the and evaluation of multimodal large language models (MLLMs). The main contributions include the comprehensive dataset and benchmark covering a wide range of vision tasks for 2D and 3D vision, a detailed methodology for constructing multimodal instruction tuning datasets, and a primary MLLM training framework optimized for modality extension. Additionally, the paper provides baseline models, extensive experimental observations, and analysis to accelerate future research in the field of multimodal language models. \n\nLLaVAR As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework. \n\nOtterHD introduces OtterHD-8B model, which addresses the limitations of fixed-resolution inputs in Large Multimodal Models (LMMs). It leverages the Fuyu-8B ar-Fig. 28: Illustration of SEED-LLaMA [137].\n...\nEmbodiedGPT is an end-to-end multimodal foundation model for embodied AI with a \"chain-of-thought\" capability, enabling embodied agents to interact with the physical world more naturally. It also develops two datasets, Ego-COT and EgoVQA, and proposes a cost-effective training Fig. 29: The training paradigm of of ImageBind-LLM [139]. Figure is from [139]. approach for extracting task-relevant features from planning queries. The approach demonstrates state-of-the-art or comparable performance on multiple embodied tasks, including embodied control, planning, video captioning, and video QA, outperforming existing models on benchmark tasks. \n\nChatBridge is a novel multimodal language model that leverages large language models to bridge the gap between various modalities. It proposes a two-stage training approach to align different modalities with language and introduces a new multimodal instruction tuning dataset called MULTIS. \n\nVideoChat is a chat-centric video understanding system that integrates video foundation models and large language models. It proposes a video-centric instruction dataset emphasizing spatiotemporal reasoning and causal relationships, providing a valuable asset for training chatcentric video understanding systems. It also presents qualitative experiments showcasing the system's potential across various video applications and sets a standard for future research in the field of video understanding. As shown in Figure 30, the framework of VideoChat consists of two main components: VideoChat-Text and VideoChat-Embed. VideoChat-Text textualizes videos in stream by converting visual data into textual format using various vision models and prompts, allowing a pretrained large language model to address user-specified tasks based on the video text descriptions. On the other hand, VideoChat-Embed encodes videos as embeddings and combines video and language foundation models with a Video-Language Token Interface (VLTF) to optimize cross-modality, enabling the model to effectively communicate with users through a large language model. \n\nVideo-ChatGPT is a multimodal model that merges a pretrained visual encoder with a Large Language Model (LLM) to understand and generate detailed conversations about videos.",
            "reference_string": "[266573642 | Huang et al. | 2023 | Citations: 23]"
        },
        {
            "title": "i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data",
            "venue": "NAACL-HLT",
            "year": 2023,
            "reference_count": 87,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.12311",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.12311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155459391",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "1736464",
                    "name": "M. Khademi"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "4099006",
                    "name": "Reid Pryzant"
                },
                {
                    "authorId": "51444591",
                    "name": "Yuwei Fang"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "49025801",
                    "name": "Dongdong Chen"
                },
                {
                    "authorId": "2114898054",
                    "name": "Yao Qian"
                },
                {
                    "authorId": "2147416198",
                    "name": "Mei Gao"
                },
                {
                    "authorId": "2109182290",
                    "name": "Yi-Ling Chen"
                },
                {
                    "authorId": "2983603",
                    "name": "R. Gmyr"
                },
                {
                    "authorId": "1833359",
                    "name": "Naoyuki Kanda"
                },
                {
                    "authorId": "40589056",
                    "name": "N. Codella"
                },
                {
                    "authorId": "2054421528",
                    "name": "Bin Xiao"
                },
                {
                    "authorId": "1844953096",
                    "name": "Yu Shi"
                },
                {
                    "authorId": "2150687325",
                    "name": "Lu Yuan"
                },
                {
                    "authorId": "34638725",
                    "name": "Takuya Yoshioka"
                },
                {
                    "authorId": "48262024",
                    "name": "Michael Zeng"
                },
                {
                    "authorId": "144531812",
                    "name": "Xuedong Huang"
                }
            ],
            "abstract": "The convergence of text, visual, and audio data is a key step towards human-like artificial intelligence, however the current Vision-Language-Speech landscape is dominated by encoder-only models which lack generative abilities. We propose closing this gap with i-Code V2, the first model capable of generating natural language from any combination of Vision, Language, and Speech data. i-Code V2 is an integrative system that leverages state-of-the-art single-modality encoders, combining their outputs with a new modality-fusing encoder in order to flexibly project combinations of modalities into a shared representational space. Next, language tokens are generated from these representations via an autoregressive decoder. The whole framework is pretrained end-to-end on a large collection of dual- and single-modality datasets using a novel text completion objective that can be generalized across arbitrary combinations of modalities. i-Code V2 matches or outperforms state-of-the-art single- and dual-modality baselines on 7 multimodal tasks, demonstrating the power of generative multimodal pretraining across a diversity of tasks and signals.",
            "corpus_id": 258833320,
            "sentences": [
                {
                    "corpus_id": "258833320",
                    "title": "i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data",
                    "text": "The convergence of text, visual, and audio data is a key step towards human-like artificial intelligence, however the current Vision-Language-Speech landscape is dominated by encoder-only models which lack generative abilities. We propose closing this gap with i-Code V2, the first model capable of generating natural language from any combination of Vision, Language, and Speech data. i-Code V2 is an integrative system that leverages state-of-the-art single-modality encoders, combining their outputs with a new modality-fusing encoder in order to flexibly project combinations of modalities into a shared representational space. Next, language tokens are generated from these representations via an autoregressive decoder. The whole framework is pretrained end-to-end on a large collection of dual- and single-modality datasets using a novel text completion objective that can be generalized across arbitrary combinations of modalities. i-Code V2 matches or outperforms state-of-the-art single- and dual-modality baselines on 7 multimodal tasks, demonstrating the power of generative multimodal pretraining across a diversity of tasks and signals.",
                    "score": 0.5437683591514479,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85107421875
                }
            ],
            "relevance_judgement": 0.85107421875,
            "relevance_judgment_input_expanded": "# Title: i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data\n# Venue: NAACL-HLT\n# Authors: Ziyi Yang, M. Khademi, Yichong Xu, Reid Pryzant, Yuwei Fang, Chenguang Zhu, Dongdong Chen, Yao Qian, Mei Gao, Yi-Ling Chen, R. Gmyr, Naoyuki Kanda, N. Codella, Bin Xiao, Yu Shi, Lu Yuan, Takuya Yoshioka, Michael Zeng, Xuedong Huang\n## Abstract\nThe convergence of text, visual, and audio data is a key step towards human-like artificial intelligence, however the current Vision-Language-Speech landscape is dominated by encoder-only models which lack generative abilities. We propose closing this gap with i-Code V2, the first model capable of generating natural language from any combination of Vision, Language, and Speech data. i-Code V2 is an integrative system that leverages state-of-the-art single-modality encoders, combining their outputs with a new modality-fusing encoder in order to flexibly project combinations of modalities into a shared representational space. Next, language tokens are generated from these representations via an autoregressive decoder. The whole framework is pretrained end-to-end on a large collection of dual- and single-modality datasets using a novel text completion objective that can be generalized across arbitrary combinations of modalities. i-Code V2 matches or outperforms state-of-the-art single- and dual-modality baselines on 7 multimodal tasks, demonstrating the power of generative multimodal pretraining across a diversity of tasks and signals.\n",
            "reference_string": "[258833320 | Yang et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Recognition-Synergistic Scene Text Editing",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08387, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313679002",
                    "name": "Zhengyao Fang"
                },
                {
                    "authorId": "10344582",
                    "name": "Pengyuan Lyu"
                },
                {
                    "authorId": "2313920076",
                    "name": "Jingjing Wu"
                },
                {
                    "authorId": "2248958848",
                    "name": "Chengquan Zhang"
                },
                {
                    "authorId": "2349736964",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2261251059",
                    "name": "Guan-Lun Lu"
                },
                {
                    "authorId": "1678473",
                    "name": "Wenjie Pei"
                }
            ],
            "abstract": "Scene text editing aims to modify text content within scene images while maintaining style consistency. Traditional methods achieve this by explicitly disentangling style and content from the source image and then fusing the style with the target content, while ensuring content consistency using a pre-trained recognition model. Despite notable progress, these methods suffer from complex pipelines, leading to suboptimal performance in complex scenarios. In this work, we introduce Recognition-Synergistic Scene Text Editing (RS-STE), a novel approach that fully exploits the intrinsic synergy of text recognition for editing. Our model seamlessly integrates text recognition with text editing within a unified framework, and leverages the recognition model's ability to implicitly disentangle style and content while ensuring content consistency. Specifically, our approach employs a multi-modal parallel decoder based on transformer architecture, which predicts both text content and stylized images in parallel. Additionally, our cyclic self-supervised fine-tuning strategy enables effective training on unpaired real-world data without ground truth, enhancing style and content consistency through a twice-cyclic generation process. Built on a relatively simple architecture, RS-STE achieves state-of-the-art performance on both synthetic and real-world benchmarks, and further demonstrates the effectiveness of leveraging the generated hard cases to boost the performance of downstream recognition tasks. Code is available at https://github.com/ZhengyaoFang/RS-STE.",
            "corpus_id": 276929408,
            "sentences": [
                {
                    "corpus_id": "276929408",
                    "title": "Recognition-Synergistic Scene Text Editing",
                    "text": "In response to the notable progress of large language models in natural language processing [1,2,6], the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models [15], but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,38,40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs [3,13,29] offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation. Inspired by these methods, our approach integrates the multi-modal language model RS-STE, which is specialized in scene text editing.",
                    "score": 0.7037648091296218,
                    "section_title": "MLLM for Image Generation and Editing",
                    "char_start_offset": 6474,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 757
                        },
                        {
                            "start": 758,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1336
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 97,
                            "end": 99,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 470,
                            "end": 474,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 583,
                            "end": 586,
                            "matchedPaperCorpusId": "266374640"
                        },
                        {
                            "start": 966,
                            "end": 969,
                            "matchedPaperCorpusId": "253581213"
                        },
                        {
                            "start": 972,
                            "end": 975,
                            "matchedPaperCorpusId": "253581838"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.845703125
                }
            ],
            "relevance_judgement": 0.845703125,
            "relevance_judgment_input_expanded": "# Title: Recognition-Synergistic Scene Text Editing\n# Venue: arXiv.org\n# Authors: Zhengyao Fang, Pengyuan Lyu, Jingjing Wu, Chengquan Zhang, Jun Yu, Guan-Lun Lu, Wenjie Pei\n## Abstract\nScene text editing aims to modify text content within scene images while maintaining style consistency. Traditional methods achieve this by explicitly disentangling style and content from the source image and then fusing the style with the target content, while ensuring content consistency using a pre-trained recognition model. Despite notable progress, these methods suffer from complex pipelines, leading to suboptimal performance in complex scenarios. In this work, we introduce Recognition-Synergistic Scene Text Editing (RS-STE), a novel approach that fully exploits the intrinsic synergy of text recognition for editing. Our model seamlessly integrates text recognition with text editing within a unified framework, and leverages the recognition model's ability to implicitly disentangle style and content while ensuring content consistency. Specifically, our approach employs a multi-modal parallel decoder based on transformer architecture, which predicts both text content and stylized images in parallel. Additionally, our cyclic self-supervised fine-tuning strategy enables effective training on unpaired real-world data without ground truth, enhancing style and content consistency through a twice-cyclic generation process. Built on a relatively simple architecture, RS-STE achieves state-of-the-art performance on both synthetic and real-world benchmarks, and further demonstrates the effectiveness of leveraging the generated hard cases to boost the performance of downstream recognition tasks. Code is available at https://github.com/ZhengyaoFang/RS-STE.\n## MLLM for Image Generation and Editing\nIn response to the notable progress of large language models in natural language processing [1,2,6], the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models [15], but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,38,40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs [3,13,29] offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation. Inspired by these methods, our approach integrates the multi-modal language model RS-STE, which is specialized in scene text editing.",
            "reference_string": "[276929408 | Fang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 58,
            "citation_count": 26,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350309448",
                    "name": "Yuwei Niu"
                },
                {
                    "authorId": "2253434320",
                    "name": "Munan Ning"
                },
                {
                    "authorId": "2349744842",
                    "name": "Mengren Zheng"
                },
                {
                    "authorId": "2333420566",
                    "name": "Bin Lin"
                },
                {
                    "authorId": "2328406001",
                    "name": "Peng Jin"
                },
                {
                    "authorId": "2315613899",
                    "name": "Jiaqi Liao"
                },
                {
                    "authorId": "2253467098",
                    "name": "Kun-Peng Ning"
                },
                {
                    "authorId": "2256419313",
                    "name": "Bin Zhu"
                },
                {
                    "authorId": "2280992738",
                    "name": "Li Yuan"
                }
            ],
            "abstract": "Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose $\\textbf{WISE}$, the first benchmark specifically designed for $\\textbf{W}$orld Knowledge-$\\textbf{I}$nformed $\\textbf{S}$emantic $\\textbf{E}$valuation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce $\\textbf{WiScore}$, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE.",
            "corpus_id": 276929205,
            "sentences": [
                {
                    "corpus_id": "276929205",
                    "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
                    "text": "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45]48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model. A crucial question concerning unified multimodal models is whether their understanding and generation capabilities can mutually enhance each other. Some studies [41,45] have provided evidence supporting this phenomenon. However, in contrast to the rich and comprehensive benchmarks for multimodal understanding, T2I benchmarks are often relatively",
                    "score": 0.5878907759862659,
                    "section_title": "Unified Multimodal Models",
                    "char_start_offset": 6123,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1083
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83984375
                }
            ],
            "relevance_judgement": 0.83984375,
            "relevance_judgment_input_expanded": "# Title: WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation\n# Venue: arXiv.org\n# Authors: Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kun-Peng Ning, Bin Zhu, Li Yuan\n## Abstract\nText-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose $\\textbf{WISE}$, the first benchmark specifically designed for $\\textbf{W}$orld Knowledge-$\\textbf{I}$nformed $\\textbf{S}$emantic $\\textbf{E}$valuation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce $\\textbf{WiScore}$, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE.\n## Unified Multimodal Models\nUnified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45]48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model. A crucial question concerning unified multimodal models is whether their understanding and generation capabilities can mutually enhance each other. Some studies [41,45] have provided evidence supporting this phenomenon. However, in contrast to the rich and comprehensive benchmarks for multimodal understanding, T2I benchmarks are often relatively",
            "reference_string": "[276929205 | Niu et al. | 2025 | Citations: 26]"
        },
        {
            "title": "Multi-aspect Knowledge Distillation with Large Language Model",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.13341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238019356",
                    "name": "Taegyeong Lee"
                },
                {
                    "authorId": "2341715815",
                    "name": "Jinsik Bang"
                },
                {
                    "authorId": "2294517321",
                    "name": "Soyeong Kwon"
                },
                {
                    "authorId": "2174161125",
                    "name": "Taehwan Kim"
                }
            ],
            "abstract": "Recent advancements in deep learning have significantly improved performance on computer vision tasks. Previous image classification methods primarily modify model architectures or add features, and they optimize models using cross-entropy loss on class logits. Since they focus on classifying images with considering class labels, these methods may struggle to learn various \\emph{aspects} of classes (e.g., natural positions and shape changes). Rethinking the previous approach from a novel view, we propose a multi-aspect knowledge distillation method using Multimodal Large Language Models (MLLMs). Our approach involves: 1) querying Large Language Model with multi-aspect questions relevant to the knowledge we want to transfer to the model, 2) extracting corresponding logits from MLLM, and 3) expanding the model's output dimensions to distill these multi-aspect logits. We then apply cross-entropy loss to class logits and binary cross-entropy loss to multi-aspect logits. Through our method, the model can learn not only the knowledge about visual aspects but also the abstract and complex aspects that require a deeper understanding. We primarily apply our method to image classification, and to explore the potential for extending our model, such as object detection. In all experimental results, our method improves the performance of the baselines. Additionally, we analyze the effect of multi-aspect knowledge distillation. These results demonstrate that our method can transfer knowledge about various aspects to the model and the aspect knowledge can enhance model performance in computer vision tasks.",
            "corpus_id": 275820726,
            "sentences": [
                {
                    "corpus_id": "275820726",
                    "title": "Multi-aspect Knowledge Distillation with Large Language Model",
                    "text": "Multimodal Large Language Models. Recently, Multimodal Large Language Models (MLLMs) [1,2,14,40,41] have shown significant performance improvements in multi-modal problems such as visual question answering and image captioning by leveraging large-scale datasets to learn a joint embedding space where images and their corresponding textual descriptions are closely aligned. GPT-4o [1] has the ability to get the context and has a humanlike text generation ability, showing strong performance not only in the natural language processing area but also in multi-modal tasks. InternVL [4] can address both text and image data and shows better performances in various multimodal tasks (such as visual understanding, language generation, and visual QA) while using fewer computing resources compared to other MLLMs. Motivated by this, we apply the rich knowledge of MLLMs to image classification. \n\nVisual tasks with linguistic information. Many studies [3,18,21,23,27,37,38] try to extract linguistic information from a large language model and use it to settle the visual problems. One method [18] leverages the linguistic knowledge for each visual category from LLM to generate the descriptions and use the descriptions in zero-shot image classification. Another method [37] creates a concise set of representative visual attributes from LLM by leveraging their learning-to-search method for interpretable visual recognition. While these methods focus on generating attributes for model training, our approach distills knowledge about various aspects, extending the model's output dimensions.",
                    "score": 0.5035367617621954,
                    "section_title": "Related Work",
                    "char_start_offset": 4702,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 33
                        },
                        {
                            "start": 34,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 571
                        },
                        {
                            "start": 572,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 890
                        },
                        {
                            "start": 893,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1251
                        },
                        {
                            "start": 1252,
                            "end": 1422
                        },
                        {
                            "start": 1423,
                            "end": 1589
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 88,
                            "end": 90,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 90,
                            "end": 93,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 581,
                            "end": 584,
                            "matchedPaperCorpusId": "266521410"
                        },
                        {
                            "start": 957,
                            "end": 960,
                            "matchedPaperCorpusId": "252111028"
                        },
                        {
                            "start": 960,
                            "end": 963,
                            "matchedPaperCorpusId": "258866192"
                        },
                        {
                            "start": 963,
                            "end": 966,
                            "matchedPaperCorpusId": "260682240"
                        },
                        {
                            "start": 966,
                            "end": 969,
                            "matchedPaperCorpusId": "253735286"
                        },
                        {
                            "start": 1267,
                            "end": 1271,
                            "matchedPaperCorpusId": "260682240"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8359375
                }
            ],
            "relevance_judgement": 0.8359375,
            "relevance_judgment_input_expanded": "# Title: Multi-aspect Knowledge Distillation with Large Language Model\n# Venue: arXiv.org\n# Authors: Taegyeong Lee, Jinsik Bang, Soyeong Kwon, Taehwan Kim\n## Abstract\nRecent advancements in deep learning have significantly improved performance on computer vision tasks. Previous image classification methods primarily modify model architectures or add features, and they optimize models using cross-entropy loss on class logits. Since they focus on classifying images with considering class labels, these methods may struggle to learn various \\emph{aspects} of classes (e.g., natural positions and shape changes). Rethinking the previous approach from a novel view, we propose a multi-aspect knowledge distillation method using Multimodal Large Language Models (MLLMs). Our approach involves: 1) querying Large Language Model with multi-aspect questions relevant to the knowledge we want to transfer to the model, 2) extracting corresponding logits from MLLM, and 3) expanding the model's output dimensions to distill these multi-aspect logits. We then apply cross-entropy loss to class logits and binary cross-entropy loss to multi-aspect logits. Through our method, the model can learn not only the knowledge about visual aspects but also the abstract and complex aspects that require a deeper understanding. We primarily apply our method to image classification, and to explore the potential for extending our model, such as object detection. In all experimental results, our method improves the performance of the baselines. Additionally, we analyze the effect of multi-aspect knowledge distillation. These results demonstrate that our method can transfer knowledge about various aspects to the model and the aspect knowledge can enhance model performance in computer vision tasks.\n## Related Work\nMultimodal Large Language Models. Recently, Multimodal Large Language Models (MLLMs) [1,2,14,40,41] have shown significant performance improvements in multi-modal problems such as visual question answering and image captioning by leveraging large-scale datasets to learn a joint embedding space where images and their corresponding textual descriptions are closely aligned. GPT-4o [1] has the ability to get the context and has a humanlike text generation ability, showing strong performance not only in the natural language processing area but also in multi-modal tasks. InternVL [4] can address both text and image data and shows better performances in various multimodal tasks (such as visual understanding, language generation, and visual QA) while using fewer computing resources compared to other MLLMs. Motivated by this, we apply the rich knowledge of MLLMs to image classification. \n\nVisual tasks with linguistic information. Many studies [3,18,21,23,27,37,38] try to extract linguistic information from a large language model and use it to settle the visual problems. One method [18] leverages the linguistic knowledge for each visual category from LLM to generate the descriptions and use the descriptions in zero-shot image classification. Another method [37] creates a concise set of representative visual attributes from LLM by leveraging their learning-to-search method for interpretable visual recognition. While these methods focus on generating attributes for model training, our approach distills knowledge about various aspects, extending the model's output dimensions.",
            "reference_string": "[275820726 | Lee et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Generating Images with Multimodal Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 73,
            "citation_count": 259,
            "influential_citation_count": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.17216",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "23978705",
                    "name": "Jing Yu Koh"
                },
                {
                    "authorId": "47070750",
                    "name": "Daniel Fried"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                }
            ],
            "abstract": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
            "corpus_id": 258959284,
            "sentences": [
                {
                    "corpus_id": "258959284",
                    "title": "Generating Images with Multimodal Language Models",
                    "text": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
                    "score": 0.5870240973458767,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8349609375
                }
            ],
            "relevance_judgement": 0.8349609375,
            "relevance_judgment_input_expanded": "# Title: Generating Images with Multimodal Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Jing Yu Koh, Daniel Fried, R. Salakhutdinov\n## Abstract\nWe propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.\n",
            "reference_string": "[258959284 | Koh et al. | 2023 | Citations: 259]"
        },
        {
            "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 80,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.01934, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2140386536",
                    "name": "Runhu Huang"
                },
                {
                    "authorId": "2258786929",
                    "name": "Chunwei Wang"
                },
                {
                    "authorId": "2367457293",
                    "name": "Junwei Yang"
                },
                {
                    "authorId": "46182317",
                    "name": "Guansong Lu"
                },
                {
                    "authorId": "2157637784",
                    "name": "Yunlong Yuan"
                },
                {
                    "authorId": "47180442",
                    "name": "Jianhua Han"
                },
                {
                    "authorId": "2311312463",
                    "name": "Lu Hou"
                },
                {
                    "authorId": "2256596407",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2353906756",
                    "name": "Lanqing Hong"
                },
                {
                    "authorId": "2310758544",
                    "name": "Hengshuang Zhao"
                },
                {
                    "authorId": "2320227478",
                    "name": "Hang Xu"
                }
            ],
            "abstract": "We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.",
            "corpus_id": 277501830,
            "sentences": [
                {
                    "corpus_id": "277501830",
                    "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement",
                    "text": "\"What I cannot create, I do not understand.\" --Richard Feynman Recent advancements in Large Language Models (LLMs) have significantly enhanced their capability to handle multimodal tasks, particularly by integrating visual inputs into language models. Efforts such as the LLaVA series and the QwenVL series [38,37,1,64] have demonstrated remarkable visual comprehension performance. Meanwhile, the development of text-to-image generation models, such as diffusion-based approaches [53,50,51,54] and more recent autoregressive approaches [52,16,74], has made substantial strides in generating high-fidelity images. These developments have driven the push towards creating unified Multimodal Large Language Models (MLLMs) that seamlessly integrate both visual understanding and generation capabilities. A unified model holds promise not only for advancing task coordination and generalization but also for contributing to the exploration of artificial general intelligence (AGI). By merging understanding and generation capabilities within a single framework, unified models can genuinely grasp the deep relationships between visual and textual information, enabling more intelligent and flexible interactions and task execution in complex real-world scenarios. \n\nTo build such unified models in an autoregressive framework, existing approaches have explored several distinct paradigms. As illustrated in Fig. 2 (a), the earliest models, e.g., Chameleon [59] and Emu3 [65], utilized VQGAN [18] to discretize images, enabling a shared vocabulary for text and vision within an autoregressive (AR) framework. However, even with large-scale training, they lag behind models like LLaVA [38] in visual understanding tasks. To address this issue, works like LaViT [28] and ILLUME [63] (Fig. 2 (b)) learn the semantic codebook from the pretrained text-aligned semantic encoders [78,84], and employ diffusion models to reconstruct images based on semantic tokens. This improves image-text alignment in MLLM pretraining and achieves strong performance in both understanding and generation tasks. However, the lack of texture preservation in vision tokenizers limits their capability in fine-grained image editing.",
                    "score": 0.6312456189658229,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 44
                        },
                        {
                            "start": 45,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 613
                        },
                        {
                            "start": 614,
                            "end": 800
                        },
                        {
                            "start": 801,
                            "end": 977
                        },
                        {
                            "start": 978,
                            "end": 1259
                        },
                        {
                            "start": 1262,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1714
                        },
                        {
                            "start": 1715,
                            "end": 1952
                        },
                        {
                            "start": 1953,
                            "end": 2083
                        },
                        {
                            "start": 2084,
                            "end": 2201
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 311,
                            "end": 314,
                            "matchedPaperCorpusId": "263672058"
                        },
                        {
                            "start": 481,
                            "end": 485,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 485,
                            "end": 488,
                            "matchedPaperCorpusId": "254854389"
                        },
                        {
                            "start": 491,
                            "end": 494,
                            "matchedPaperCorpusId": "251800180"
                        },
                        {
                            "start": 537,
                            "end": 541,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 541,
                            "end": 544,
                            "matchedPaperCorpusId": "235212350"
                        },
                        {
                            "start": 1487,
                            "end": 1491,
                            "matchedPaperCorpusId": "229297973"
                        },
                        {
                            "start": 1755,
                            "end": 1759,
                            "matchedPaperCorpusId": "263889455"
                        },
                        {
                            "start": 1868,
                            "end": 1872,
                            "matchedPaperCorpusId": "257767223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.830078125
                }
            ],
            "relevance_judgement": 0.830078125,
            "relevance_judgment_input_expanded": "# Title: ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement\n# Venue: arXiv.org\n# Authors: Runhu Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu\n## Abstract\nWe present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.\n## Introduction\n\"What I cannot create, I do not understand.\" --Richard Feynman Recent advancements in Large Language Models (LLMs) have significantly enhanced their capability to handle multimodal tasks, particularly by integrating visual inputs into language models. Efforts such as the LLaVA series and the QwenVL series [38,37,1,64] have demonstrated remarkable visual comprehension performance. Meanwhile, the development of text-to-image generation models, such as diffusion-based approaches [53,50,51,54] and more recent autoregressive approaches [52,16,74], has made substantial strides in generating high-fidelity images. These developments have driven the push towards creating unified Multimodal Large Language Models (MLLMs) that seamlessly integrate both visual understanding and generation capabilities. A unified model holds promise not only for advancing task coordination and generalization but also for contributing to the exploration of artificial general intelligence (AGI). By merging understanding and generation capabilities within a single framework, unified models can genuinely grasp the deep relationships between visual and textual information, enabling more intelligent and flexible interactions and task execution in complex real-world scenarios. \n\nTo build such unified models in an autoregressive framework, existing approaches have explored several distinct paradigms. As illustrated in Fig. 2 (a), the earliest models, e.g., Chameleon [59] and Emu3 [65], utilized VQGAN [18] to discretize images, enabling a shared vocabulary for text and vision within an autoregressive (AR) framework. However, even with large-scale training, they lag behind models like LLaVA [38] in visual understanding tasks. To address this issue, works like LaViT [28] and ILLUME [63] (Fig. 2 (b)) learn the semantic codebook from the pretrained text-aligned semantic encoders [78,84], and employ diffusion models to reconstruct images based on semantic tokens. This improves image-text alignment in MLLM pretraining and achieves strong performance in both understanding and generation tasks. However, the lack of texture preservation in vision tokenizers limits their capability in fine-grained image editing.",
            "reference_string": "[277501830 | Huang et al. | 2025 | Citations: 7]"
        },
        {
            "title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 457,
            "citation_count": 47,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10466766.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260382962",
                    "name": "Abolfazl Younesi"
                },
                {
                    "authorId": "153631156",
                    "name": "Mohsen Ansari"
                },
                {
                    "authorId": "2260384993",
                    "name": "Mohammadamin Fazli"
                },
                {
                    "authorId": "2490667",
                    "name": "A. Ejlali"
                },
                {
                    "authorId": "2238832044",
                    "name": "Muhammad Shafique"
                },
                {
                    "authorId": "2247781719",
                    "name": "Joerg Henkel"
                }
            ],
            "abstract": "In today\u2019s digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation. There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others. Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks. It\u2019s crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or development from various perspectives. Additionally, we explore the main research fields of CNN like 6D vision, generative models, and meta-learning. This survey paper provides a comprehensive examination and comparison of various CNN architectures, highlighting their architectural differences and emphasizing their respective advantages, disadvantages, applications, challenges, and future trends.",
            "corpus_id": 267897956,
            "sentences": [
                {
                    "corpus_id": "267897956",
                    "title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends",
                    "text": "In recent epochs, the domain of large language models (LLMs) for natural language processing has witnessed a precipitous progression. Prototypes such as BERT, GPT-3, and PaLM have demonstrated exceptional aptitude in language apprehension and generation, courtesy of self-supervised pretraining on voluminous text corpora [85]. As LLMs expand in magnitude and range, incorporating additional modalities beyond text is a burgeoning field of study. Multimodal LLMs strive to amalgamate language, vision, and other sensory inputs within a singular model architecture. They hold the potential to attain a more holistic understanding of the world by concurrently learning representations across diverse data types [96]. A significant hurdle is the effective fusion of the strengths of CNNs for computer vision and transformer architectures for language modeling. \n\nOne strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks. Self-supervised pretraining continues to be vital for multimodal LLMs to learn effective joint representations before downstream task tuning. Contrastive learning objectives that predict associations between modalities have proven highly effective [99]. Models pre-trained on large datasets of image-text pairs have demonstrated robust zeroshot transfer performance on multimodal tasks. \n\nAs multimodal LLMs increase in scale, the efficient combination of diverse convolution types and attention mechanisms will be crucial. Compact CNN architectures could help to reduce the cost of computing. Sparse attention and memory compression techniques can assist with scalability.",
                    "score": 0.5291483572215102,
                    "section_title": "H. Evolution of Language Models and Multimodal LLMs",
                    "char_start_offset": 60107,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 564
                        },
                        {
                            "start": 565,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 857
                        },
                        {
                            "start": 860,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1219
                        },
                        {
                            "start": 1220,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 1981
                        },
                        {
                            "start": 1982,
                            "end": 2114
                        },
                        {
                            "start": 2117,
                            "end": 2251
                        },
                        {
                            "start": 2252,
                            "end": 2321
                        },
                        {
                            "start": 2322,
                            "end": 2401
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82275390625
                }
            ],
            "relevance_judgement": 0.82275390625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends\n# Venue: IEEE Access\n# Authors: Abolfazl Younesi, Mohsen Ansari, Mohammadamin Fazli, A. Ejlali, Muhammad Shafique, Joerg Henkel\n## Abstract\nIn today\u2019s digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation. There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others. Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks. It\u2019s crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or development from various perspectives. Additionally, we explore the main research fields of CNN like 6D vision, generative models, and meta-learning. This survey paper provides a comprehensive examination and comparison of various CNN architectures, highlighting their architectural differences and emphasizing their respective advantages, disadvantages, applications, challenges, and future trends.\n## H. Evolution of Language Models and Multimodal LLMs\nIn recent epochs, the domain of large language models (LLMs) for natural language processing has witnessed a precipitous progression. Prototypes such as BERT, GPT-3, and PaLM have demonstrated exceptional aptitude in language apprehension and generation, courtesy of self-supervised pretraining on voluminous text corpora [85]. As LLMs expand in magnitude and range, incorporating additional modalities beyond text is a burgeoning field of study. Multimodal LLMs strive to amalgamate language, vision, and other sensory inputs within a singular model architecture. They hold the potential to attain a more holistic understanding of the world by concurrently learning representations across diverse data types [96]. A significant hurdle is the effective fusion of the strengths of CNNs for computer vision and transformer architectures for language modeling. \n\nOne strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks. Self-supervised pretraining continues to be vital for multimodal LLMs to learn effective joint representations before downstream task tuning. Contrastive learning objectives that predict associations between modalities have proven highly effective [99]. Models pre-trained on large datasets of image-text pairs have demonstrated robust zeroshot transfer performance on multimodal tasks. \n\nAs multimodal LLMs increase in scale, the efficient combination of diverse convolution types and attention mechanisms will be crucial. Compact CNN architectures could help to reduce the cost of computing. Sparse attention and memory compression techniques can assist with scalability.",
            "reference_string": "[267897956 | Younesi et al. | 2024 | Citations: 47]"
        },
        {
            "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis",
            "venue": "ECCV Workshops",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.18038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1490936425",
                    "name": "Donggoo Kang"
                },
                {
                    "authorId": "1783599864",
                    "name": "Dasol Jeong"
                },
                {
                    "authorId": "2321673431",
                    "name": "Hyunmin Lee"
                },
                {
                    "authorId": "2205412863",
                    "name": "Sangwoo Park"
                },
                {
                    "authorId": "31237187",
                    "name": "Hasil Park"
                },
                {
                    "authorId": "2211171908",
                    "name": "Sunkyu Kwon"
                },
                {
                    "authorId": "2268097132",
                    "name": "Yeongjoon Kim"
                },
                {
                    "authorId": "2261728538",
                    "name": "Joonki Paik"
                }
            ],
            "abstract": "The Large Vision Language Model (VLM) has recently addressed remarkable progress in bridging two fundamental modalities. VLM, trained by a sufficiently large dataset, exhibits a comprehensive understanding of both visual and linguistic to perform diverse tasks. To distill this knowledge accurately, in this paper, we introduce a novel approach that explicitly utilizes VLM as an objective function form for the Human-Object Interaction (HOI) detection task (\\textbf{VLM-HOI}). Specifically, we propose a method that quantifies the similarity of the predicted HOI triplet using the Image-Text matching technique. We represent HOI triplets linguistically to fully utilize the language comprehension of VLMs, which are more suitable than CLIP models due to their localization and object-centric nature. This matching score is used as an objective for contrastive optimization. To our knowledge, this is the first utilization of VLM language abilities for HOI detection. Experiments demonstrate the effectiveness of our method, achieving state-of-the-art HOI detection accuracy on benchmarks. We believe integrating VLMs into HOI detection represents important progress towards more advanced and interpretable analysis of human-object interactions.",
            "corpus_id": 274305669,
            "sentences": [
                {
                    "corpus_id": "274305669",
                    "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis",
                    "text": "Integrating vision and language has been a long-standing goal in artificial intelligence. Early work focused on image captioning [39], generating textual descriptions of image contents. Recent years have seen rapid progress in visual question answering [1,2], enabled by large-scale datasets [7] and deep neural encoder-decoder models [43]. \n\nMore advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework [3]. \n\nOne line of work explores joint embedding models to learn aligned vector representations for image regions and language fragments [15,19]. However, these approaches do not explicitly model interactions between modalities. \n\nRecently, pretrained language models, such as BERT [11] and GPT [4], have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text. More recent methods like VisualBERT [23] and LXMERT [38] utilize pretrained BERT weights for joint image-text comprehension. \n\nUnified architectures like ViLBERT [30], VL-BERT [36], UNITER [8], BLIP [21] integrate masked language modeling objectives alongside paired image-text prediction tasks within a single Transformer model. These models set new stateof-the-art results on downstream tasks like visual question answering (VQA), visual reasoning, and image retrieval. \n\nSpecifically, we utilize BLIP as our vision-language model backbone. BLIP uses a flexible multimodal encoder-decoder model that can handle both understanding and generation tasks. It also improves training data quality through generating new image captions and filtering noise. These advantages lead to strong performance. \n\nInstruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks. A key advantage these models have over traditional fine-tuning is they can adapt to new tasks without needing gradient updates or lots of specific training data. The instruction format allows rapid adaptation.",
                    "score": 0.5093169895333786,
                    "section_title": "Vision-Lauguage Model",
                    "char_start_offset": 7308,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 89
                        },
                        {
                            "start": 90,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 340
                        },
                        {
                            "start": 343,
                            "end": 454
                        },
                        {
                            "start": 455,
                            "end": 603
                        },
                        {
                            "start": 606,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 827
                        },
                        {
                            "start": 830,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1200
                        },
                        {
                            "start": 1203,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1547
                        },
                        {
                            "start": 1550,
                            "end": 1618
                        },
                        {
                            "start": 1619,
                            "end": 1729
                        },
                        {
                            "start": 1730,
                            "end": 1827
                        },
                        {
                            "start": 1828,
                            "end": 1872
                        },
                        {
                            "start": 1875,
                            "end": 2090
                        },
                        {
                            "start": 2091,
                            "end": 2252
                        },
                        {
                            "start": 2253,
                            "end": 2300
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 129,
                            "end": 133,
                            "matchedPaperCorpusId": "1169492"
                        },
                        {
                            "start": 253,
                            "end": 256,
                            "matchedPaperCorpusId": "3753452"
                        },
                        {
                            "start": 256,
                            "end": 258,
                            "matchedPaperCorpusId": "3180429"
                        },
                        {
                            "start": 335,
                            "end": 339,
                            "matchedPaperCorpusId": "1055111"
                        },
                        {
                            "start": 599,
                            "end": 602,
                            "matchedPaperCorpusId": "10137425"
                        },
                        {
                            "start": 736,
                            "end": 740,
                            "matchedPaperCorpusId": "8517067"
                        },
                        {
                            "start": 894,
                            "end": 897,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1238,
                            "end": 1242,
                            "matchedPaperCorpusId": "208637516"
                        },
                        {
                            "start": 1265,
                            "end": 1268,
                            "matchedPaperCorpusId": "216080982"
                        },
                        {
                            "start": 1275,
                            "end": 1279,
                            "matchedPaperCorpusId": "246411402"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82275390625
                }
            ],
            "relevance_judgement": 0.82275390625,
            "relevance_judgment_input_expanded": "# Title: VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis\n# Venue: ECCV Workshops\n# Authors: Donggoo Kang, Dasol Jeong, Hyunmin Lee, Sangwoo Park, Hasil Park, Sunkyu Kwon, Yeongjoon Kim, Joonki Paik\n## Abstract\nThe Large Vision Language Model (VLM) has recently addressed remarkable progress in bridging two fundamental modalities. VLM, trained by a sufficiently large dataset, exhibits a comprehensive understanding of both visual and linguistic to perform diverse tasks. To distill this knowledge accurately, in this paper, we introduce a novel approach that explicitly utilizes VLM as an objective function form for the Human-Object Interaction (HOI) detection task (\\textbf{VLM-HOI}). Specifically, we propose a method that quantifies the similarity of the predicted HOI triplet using the Image-Text matching technique. We represent HOI triplets linguistically to fully utilize the language comprehension of VLMs, which are more suitable than CLIP models due to their localization and object-centric nature. This matching score is used as an objective for contrastive optimization. To our knowledge, this is the first utilization of VLM language abilities for HOI detection. Experiments demonstrate the effectiveness of our method, achieving state-of-the-art HOI detection accuracy on benchmarks. We believe integrating VLMs into HOI detection represents important progress towards more advanced and interpretable analysis of human-object interactions.\n## Vision-Lauguage Model\nIntegrating vision and language has been a long-standing goal in artificial intelligence. Early work focused on image captioning [39], generating textual descriptions of image contents. Recent years have seen rapid progress in visual question answering [1,2], enabled by large-scale datasets [7] and deep neural encoder-decoder models [43]. \n\nMore advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework [3]. \n\nOne line of work explores joint embedding models to learn aligned vector representations for image regions and language fragments [15,19]. However, these approaches do not explicitly model interactions between modalities. \n\nRecently, pretrained language models, such as BERT [11] and GPT [4], have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text. More recent methods like VisualBERT [23] and LXMERT [38] utilize pretrained BERT weights for joint image-text comprehension. \n\nUnified architectures like ViLBERT [30], VL-BERT [36], UNITER [8], BLIP [21] integrate masked language modeling objectives alongside paired image-text prediction tasks within a single Transformer model. These models set new stateof-the-art results on downstream tasks like visual question answering (VQA), visual reasoning, and image retrieval. \n\nSpecifically, we utilize BLIP as our vision-language model backbone. BLIP uses a flexible multimodal encoder-decoder model that can handle both understanding and generation tasks. It also improves training data quality through generating new image captions and filtering noise. These advantages lead to strong performance. \n\nInstruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks. A key advantage these models have over traditional fine-tuning is they can adapt to new tasks without needing gradient updates or lots of specific training data. The instruction format allows rapid adaptation.",
            "reference_string": "[274305669 | Kang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 28,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.15188, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254168373",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2290545053",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "2333891614",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "151253861",
                    "name": "Weixin Liang"
                },
                {
                    "authorId": "2255374957",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "2256736753",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2296724476",
                    "name": "Lili Yu"
                }
            ],
            "abstract": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
            "corpus_id": 274860024,
            "sentences": [
                {
                    "corpus_id": "274860024",
                    "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
                    "text": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
                    "score": 0.6025430145104524,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80908203125
                },
                {
                    "corpus_id": "274860024",
                    "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
                    "text": "Over the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023;Koh et al., 2024;Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and Unified-IO (Lu et al., 2022(Lu et al., , 2024) ) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities. However, these models typically train from scratch, demanding significant computational resources to achieve proficiency across all modalities. The computational cost of mastering even a single modality is substantial-training a state-of-the-art text-only large language models (LLMs) like Llama-3 (Dubey et al., 2024) requires training over 15 trillion tokens. \n\nGiven these computational demands, we investigate an alternative paradigm that reuses and adapts existing pretrained LLMs (Ge et al., 2023;Sun et al., 2023;Wu et al., 2024b). We address a fundamental research question: How to preserve the text-only performance of pretrained LLMs while equipping them with visual understanding and generation abilities? Our experiments show that naive finetuning of pretrained text-only LLMs on multimodal data leads to significant degradation of their language processing capabilities. \n\nTo address this challenge, we introduce LMFusion, a framework that enhances a pretrained text-only LLM, Llama-3 (Dubey et al., 2024) with multimodal capabilities by building upon the recipe of Transfusion (Zhou et al., 2024). Drawing from recent and parallel work on modality separation (Shen et al., 2023;Chen et al., 2023;Liang et al., 2024;Liu et al., 2024a), LMFusion integrates the original Llama modules pretrained for language processing while introducing additional dedicated transformer modules for visual understanding and generation tasks.",
                    "score": 0.5216649653138572,
                    "section_title": "Introduction",
                    "char_start_offset": 2915,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 610
                        },
                        {
                            "start": 611,
                            "end": 828
                        },
                        {
                            "start": 831,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1350
                        },
                        {
                            "start": 1353,
                            "end": 1578
                        },
                        {
                            "start": 1579,
                            "end": 1903
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 201,
                            "end": 218,
                            "matchedPaperCorpusId": "258959284"
                        },
                        {
                            "start": 323,
                            "end": 339,
                            "matchedPaperCorpusId": "249848272"
                        },
                        {
                            "start": 339,
                            "end": 360,
                            "matchedPaperCorpusId": "266573555"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78369140625
                }
            ],
            "relevance_judgement": 0.80908203125,
            "relevance_judgment_input_expanded": "# Title: LMFusion: Adapting Pretrained Language Models for Multimodal Generation\n# Venue: arXiv.org\n# Authors: Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke S. Zettlemoyer, Lili Yu\n## Abstract\nWe present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.\n## Introduction\nOver the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023;Koh et al., 2024;Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and Unified-IO (Lu et al., 2022(Lu et al., , 2024) ) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities. However, these models typically train from scratch, demanding significant computational resources to achieve proficiency across all modalities. The computational cost of mastering even a single modality is substantial-training a state-of-the-art text-only large language models (LLMs) like Llama-3 (Dubey et al., 2024) requires training over 15 trillion tokens. \n\nGiven these computational demands, we investigate an alternative paradigm that reuses and adapts existing pretrained LLMs (Ge et al., 2023;Sun et al., 2023;Wu et al., 2024b). We address a fundamental research question: How to preserve the text-only performance of pretrained LLMs while equipping them with visual understanding and generation abilities? Our experiments show that naive finetuning of pretrained text-only LLMs on multimodal data leads to significant degradation of their language processing capabilities. \n\nTo address this challenge, we introduce LMFusion, a framework that enhances a pretrained text-only LLM, Llama-3 (Dubey et al., 2024) with multimodal capabilities by building upon the recipe of Transfusion (Zhou et al., 2024). Drawing from recent and parallel work on modality separation (Shen et al., 2023;Chen et al., 2023;Liang et al., 2024;Liu et al., 2024a), LMFusion integrates the original Llama modules pretrained for language processing while introducing additional dedicated transformer modules for visual understanding and generation tasks.",
            "reference_string": "[274860024 | Shi et al. | 2024 | Citations: 28]"
        },
        {
            "title": "A Generative Framework for Bidirectional Image-Report Understanding in Chest Radiography",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 26,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.05926, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2344750800",
                    "name": "Nicholas Evans"
                },
                {
                    "authorId": "2344809861",
                    "name": "Stephen Baker"
                },
                {
                    "authorId": "2344750681",
                    "name": "Miles Reed"
                }
            ],
            "abstract": "The rapid advancements in large language models (LLMs) have unlocked their potential for multimodal tasks, where text and visual data are processed jointly. However, applying LLMs to medical imaging, particularly for chest X-rays (CXR), poses significant challenges due to the need for precise visual-textual alignment and the preservation of critical diagnostic details. In this paper, we propose Multi-Stage Adaptive Vision-Language Tuning (MAViLT), a novel framework designed to enhance multimodal reasoning and generation for CXR understanding. MAViLT incorporates a clinical gradient-weighted tokenization process and a hierarchical fine-tuning strategy, enabling it to generate accurate radiology reports, synthesize realistic CXRs from text, and answer vision-based clinical questions. We evaluate MAViLT on two benchmark datasets, MIMIC-CXR and Indiana University CXR, achieving state-of-the-art results across all tasks. Human evaluations further validate the clinical relevance and utility of MAViLT, making it a robust tool for real-world medical applications. This work demonstrates the feasibility of leveraging LLMs for multimodal medical imaging while addressing key challenges in vision-language integration.",
            "corpus_id": 276249583,
            "sentences": [
                {
                    "corpus_id": "276249583",
                    "title": "A Generative Framework for Bidirectional Image-Report Understanding in Chest Radiography",
                    "text": "MAViLT is based on a pre-trained large language model (LLM) extended to multimodal settings. It adopts a generative architecture that processes and outputs both textual and visual data. The key components of MAViLT include: \n\n\u2022 Tokenization Module: VQ-GAN-based tokenization is used to encode images into discrete latent tokens, which are integrated with textual tokens in a shared embedding space. \n\n\u2022 Multimodal Embedding Layer: The LLM's embedding table is expanded to accommodate image tokens, allowing joint modeling of text and visual data. \n\n\u2022 Decoder: The decoder generates sequences for the specified modality (text or image) in an autoregressive manner. Formally, given an input x, which consists of visual inputs x v (e.g., CXR images) and textual inputs x t (e.g., radiology reports), the goal is to model the joint distribution: \n\ndepending on the specific task. The model generates outputs y by maximizing the conditional likelihood: \u0177 = arg max y P (y|x). \n\n(2)",
                    "score": 0.5150736837544448,
                    "section_title": "A. Model Overview",
                    "char_start_offset": 10688,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 92
                        },
                        {
                            "start": 93,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 223
                        },
                        {
                            "start": 226,
                            "end": 398
                        },
                        {
                            "start": 401,
                            "end": 546
                        },
                        {
                            "start": 549,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 841
                        },
                        {
                            "start": 844,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 970
                        },
                        {
                            "start": 973,
                            "end": 976
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80908203125
                }
            ],
            "relevance_judgement": 0.80908203125,
            "relevance_judgment_input_expanded": "# Title: A Generative Framework for Bidirectional Image-Report Understanding in Chest Radiography\n# Venue: arXiv.org\n# Authors: Nicholas Evans, Stephen Baker, Miles Reed\n## Abstract\nThe rapid advancements in large language models (LLMs) have unlocked their potential for multimodal tasks, where text and visual data are processed jointly. However, applying LLMs to medical imaging, particularly for chest X-rays (CXR), poses significant challenges due to the need for precise visual-textual alignment and the preservation of critical diagnostic details. In this paper, we propose Multi-Stage Adaptive Vision-Language Tuning (MAViLT), a novel framework designed to enhance multimodal reasoning and generation for CXR understanding. MAViLT incorporates a clinical gradient-weighted tokenization process and a hierarchical fine-tuning strategy, enabling it to generate accurate radiology reports, synthesize realistic CXRs from text, and answer vision-based clinical questions. We evaluate MAViLT on two benchmark datasets, MIMIC-CXR and Indiana University CXR, achieving state-of-the-art results across all tasks. Human evaluations further validate the clinical relevance and utility of MAViLT, making it a robust tool for real-world medical applications. This work demonstrates the feasibility of leveraging LLMs for multimodal medical imaging while addressing key challenges in vision-language integration.\n## A. Model Overview\nMAViLT is based on a pre-trained large language model (LLM) extended to multimodal settings. It adopts a generative architecture that processes and outputs both textual and visual data. The key components of MAViLT include: \n\n\u2022 Tokenization Module: VQ-GAN-based tokenization is used to encode images into discrete latent tokens, which are integrated with textual tokens in a shared embedding space. \n\n\u2022 Multimodal Embedding Layer: The LLM's embedding table is expanded to accommodate image tokens, allowing joint modeling of text and visual data. \n\n\u2022 Decoder: The decoder generates sequences for the specified modality (text or image) in an autoregressive manner. Formally, given an input x, which consists of visual inputs x v (e.g., CXR images) and textual inputs x t (e.g., radiology reports), the goal is to model the joint distribution: \n\ndepending on the specific task. The model generates outputs y by maximizing the conditional likelihood: \u0177 = arg max y P (y|x). \n\n(2)",
            "reference_string": "[276249583 | Evans et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Chain of Images for Intuitively Reasoning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 33,
            "citation_count": 10,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256983328",
                    "name": "Fanxu Meng"
                },
                {
                    "authorId": "2185454298",
                    "name": "Haotong Yang"
                },
                {
                    "authorId": "2267025893",
                    "name": "Yiding Wang"
                },
                {
                    "authorId": "2257092769",
                    "name": "Muhan Zhang"
                }
            ],
            "abstract": "The human brain is naturally equipped to comprehend and interpret visual information rapidly. When confronted with complex problems or concepts, we use flowcharts, sketches, and diagrams to aid our thought process. Leveraging this inherent ability can significantly enhance logical reasoning. However, current Large Language Models (LLMs) do not utilize such visual intuition to help their thinking. Even the most advanced version language models (e.g., GPT-4V and LLaVA) merely align images into textual space, which means their reasoning processes remain purely verbal. To mitigate such limitations, we present a Chain of Images (CoI) approach, which can convert complex language reasoning problems to simple pattern recognition by generating a series of images as intermediate representations. Furthermore, we have developed a CoI evaluation dataset encompassing 15 distinct domains where images can intuitively aid problem-solving. Based on this dataset, we aim to construct a benchmark to assess the capability of future multimodal large-scale models to leverage images for reasoning. In supporting our CoI reasoning, we introduce a symbolic multimodal large language model (SyMLLM) that generates images strictly based on language instructions and accepts both text and image as input. Experiments on Geometry, Chess and Common Sense tasks sourced from the CoI evaluation dataset show that CoI improves performance significantly over the pure-language Chain of Thoughts (CoT) baselines. The code is available at https://github.com/GraphPKU/CoI.",
            "corpus_id": 265220926,
            "sentences": [
                {
                    "corpus_id": "265220926",
                    "title": "Chain of Images for Intuitively Reasoning",
                    "text": "The monumental advancements in large language models within the domain of natural language processing have spurred a surge in efforts to adapt these successes to other modalities. Unlike these language models, multimodal models are designed to accept or produce input in various modalities beyond just natural language. These models incorporate additional modules to understand and generate content in formats such as images, voice, or video. \n\nThe underlying principle of this work involves integrating the pre-trained language model with another pre-train model for a specific modality, such as images. For instance, Li et al. (2023) proposes to train a lightweight Transformer to align the image representation from an image encoder with the textual representation from an LLM. Drawing inspiration from the instruction tuning techniques used in pure language models, Liu et al. (2023a) employs data generated by GPT-4 to enhance question-answering performance. Alayrac et al. (2022) achieves closer integration of image and text data by incorporating image information into the LLM through cross attention layer instead of embedding. Moreover, Girdhar et al. (2023) proposes a joint embedding across six different modalities, including images, text, audio, depth, terminal, and IMU data, without the need for training on the last four modalities. This is achieved by leveraging the zero-shot capacity of the large image-text models. The above-mentioned models do not possess image-generation capabilities. To make up for this shortcoming, Dong et al. (2023) realize image generation by combining the model proposed by Liu et al. (2023a) with the Stable diffusion (Rombach et al., 2022). Building on this, Wu et al. (2023) further integrates it with the model from Girdhar et al. (2023), utilizing its ability to align multiple modalities to generate and accept multiple modalities. Very recently, GPT-4 (OpenAI, 2023) has demonstrated its astounding ability to understand and generate in the modalities of images and voice. In the examples it showcased, the GPT-4 has already reached a level close to human capabilities, adding both confidence and pressure to the research in this field.",
                    "score": 0.5561857393985821,
                    "section_title": "MULTIMODEL LARGE LANGUAGE MODEL",
                    "char_start_offset": 25133,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 442
                        },
                        {
                            "start": 445,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 780
                        },
                        {
                            "start": 781,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1349
                        },
                        {
                            "start": 1350,
                            "end": 1435
                        },
                        {
                            "start": 1436,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1884
                        },
                        {
                            "start": 1885,
                            "end": 2026
                        },
                        {
                            "start": 2027,
                            "end": 2190
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 964,
                            "end": 985,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 1147,
                            "end": 1168,
                            "matchedPaperCorpusId": "258564264"
                        },
                        {
                            "start": 1666,
                            "end": 1688,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1767,
                            "end": 1788,
                            "matchedPaperCorpusId": "258564264"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80712890625
                }
            ],
            "relevance_judgement": 0.80712890625,
            "relevance_judgment_input_expanded": "# Title: Chain of Images for Intuitively Reasoning\n# Venue: arXiv.org\n# Authors: Fanxu Meng, Haotong Yang, Yiding Wang, Muhan Zhang\n## Abstract\nThe human brain is naturally equipped to comprehend and interpret visual information rapidly. When confronted with complex problems or concepts, we use flowcharts, sketches, and diagrams to aid our thought process. Leveraging this inherent ability can significantly enhance logical reasoning. However, current Large Language Models (LLMs) do not utilize such visual intuition to help their thinking. Even the most advanced version language models (e.g., GPT-4V and LLaVA) merely align images into textual space, which means their reasoning processes remain purely verbal. To mitigate such limitations, we present a Chain of Images (CoI) approach, which can convert complex language reasoning problems to simple pattern recognition by generating a series of images as intermediate representations. Furthermore, we have developed a CoI evaluation dataset encompassing 15 distinct domains where images can intuitively aid problem-solving. Based on this dataset, we aim to construct a benchmark to assess the capability of future multimodal large-scale models to leverage images for reasoning. In supporting our CoI reasoning, we introduce a symbolic multimodal large language model (SyMLLM) that generates images strictly based on language instructions and accepts both text and image as input. Experiments on Geometry, Chess and Common Sense tasks sourced from the CoI evaluation dataset show that CoI improves performance significantly over the pure-language Chain of Thoughts (CoT) baselines. The code is available at https://github.com/GraphPKU/CoI.\n## MULTIMODEL LARGE LANGUAGE MODEL\nThe monumental advancements in large language models within the domain of natural language processing have spurred a surge in efforts to adapt these successes to other modalities. Unlike these language models, multimodal models are designed to accept or produce input in various modalities beyond just natural language. These models incorporate additional modules to understand and generate content in formats such as images, voice, or video. \n\nThe underlying principle of this work involves integrating the pre-trained language model with another pre-train model for a specific modality, such as images. For instance, Li et al. (2023) proposes to train a lightweight Transformer to align the image representation from an image encoder with the textual representation from an LLM. Drawing inspiration from the instruction tuning techniques used in pure language models, Liu et al. (2023a) employs data generated by GPT-4 to enhance question-answering performance. Alayrac et al. (2022) achieves closer integration of image and text data by incorporating image information into the LLM through cross attention layer instead of embedding. Moreover, Girdhar et al. (2023) proposes a joint embedding across six different modalities, including images, text, audio, depth, terminal, and IMU data, without the need for training on the last four modalities. This is achieved by leveraging the zero-shot capacity of the large image-text models. The above-mentioned models do not possess image-generation capabilities. To make up for this shortcoming, Dong et al. (2023) realize image generation by combining the model proposed by Liu et al. (2023a) with the Stable diffusion (Rombach et al., 2022). Building on this, Wu et al. (2023) further integrates it with the model from Girdhar et al. (2023), utilizing its ability to align multiple modalities to generate and accept multiple modalities. Very recently, GPT-4 (OpenAI, 2023) has demonstrated its astounding ability to understand and generate in the modalities of images and voice. In the examples it showcased, the GPT-4 has already reached a level close to human capabilities, adding both confidence and pressure to the research in this field.",
            "reference_string": "[265220926 | Meng et al. | 2023 | Citations: 10]"
        },
        {
            "title": "Multimodal Graph Learning for Generative Tasks",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.07478",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07478, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50736424",
                    "name": "Minji Yoon"
                },
                {
                    "authorId": "2257036795",
                    "name": "Jing Yu Koh"
                },
                {
                    "authorId": "2314827895",
                    "name": "Bryan Hooi"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                }
            ],
            "abstract": "Multimodal learning combines multiple data modalities, broadening the types and complexity of data our models can utilize: for example, from plain text to image-caption pairs. Most multimodal learning algorithms focus on modeling simple one-to-one pairs of data from two modalities, such as image-caption pairs, or audio-text pairs. However, in most real-world settings, entities of different modalities interact with each other in more complex and multifaceted ways, going beyond one-to-one mappings. We propose to represent these complex relationships as graphs, allowing us to capture data with any number of modalities, and with complex relationships between modalities that can flexibly vary from one sample to another. Toward this goal, we propose Multimodal Graph Learning (MMGL), a general and systematic framework for capturing information from multiple multimodal neighbors with relational structures among them. In particular, we focus on MMGL for generative tasks, building upon pretrained Language Models (LMs), aiming to augment their text generation with multimodal neighbor contexts. We study three research questions raised by MMGL: (1) how can we infuse multiple neighbor information into the pretrained LMs, while avoiding scalability issues? (2) how can we infuse the graph structure information among multimodal neighbors into the LMs? and (3) how can we finetune the pretrained LMs to learn from the neighbor context in a parameter-efficient manner? We conduct extensive experiments to answer these three questions on MMGL and analyze the empirical results to pave the way for future MMGL research.",
            "corpus_id": 263835060,
            "sentences": [
                {
                    "corpus_id": "263835060",
                    "title": "Multimodal Graph Learning for Generative Tasks",
                    "text": "End-to-End Multimodal Learning: While many discriminative multimodal models [14,22] have also been developed, we primarily consider related work on generative multimodal models, as this is most closely related with our approach. Several recent approaches tackle multimodal learning by building upon the Transformer [32] architecture. Multimodal extensions typically use either full self-attention over modalities concatenated across the sequence dimension [3,28] or a crossmodal attention layer [30]. Self-supervised multimodal pretraining methods train these architectures from large-scale unlabeled multimodal data before transferring them to downstream multimodal tasks via fine-tuning [9,19]. These methods perform end-to-end pre-training, incurring extremely high computation costs, especially as model parameters increase [17]. Moreover, this framework is relatively inflexible for end-to-end pre-trained models to leverage readily available unimodal pre-trained models, such as text-only LMs or pretrained vision models. \n\nMultimodal Learning with Frozen Image Encoders and Large Language Models: Recently, various vision-language models have been proposed to leverage off-the-shelf pre-trained models and keep them frozen during pretrainig [1,17,16]. To input visual information directly to a frozen text-only LLM, a key challenge is to align visual features to the text space. Motivated by Frozen [31], which finetunes a visual encoder to map images into the hidden space of a text-only LLM, Blip-2 [17] and GILL [16] finetune separate image mapping networks whose inputs are precomputed by frozen image encoders and outputs are directly used as soft prompts to LLMs. On the other hand, Flamingo [1] inserts new cross-attention layers into the LLM to inject visual features and pre-trains the new layers on image-text pairs. Note that all these methods primarily focus on processing interleaved image and text inputs to generate text outputs.",
                    "score": 0.5364696458518261,
                    "section_title": "Related Work",
                    "char_start_offset": 6104,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 500
                        },
                        {
                            "start": 501,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 1027
                        },
                        {
                            "start": 1030,
                            "end": 1258
                        },
                        {
                            "start": 1259,
                            "end": 1385
                        },
                        {
                            "start": 1386,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1833
                        },
                        {
                            "start": 1834,
                            "end": 1951
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 76,
                            "end": 80,
                            "matchedPaperCorpusId": "231879586"
                        },
                        {
                            "start": 80,
                            "end": 83,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 456,
                            "end": 459,
                            "matchedPaperCorpusId": "216080982"
                        },
                        {
                            "start": 495,
                            "end": 499,
                            "matchedPaperCorpusId": "173990158"
                        },
                        {
                            "start": 689,
                            "end": 692,
                            "matchedPaperCorpusId": "231740629"
                        },
                        {
                            "start": 692,
                            "end": 695,
                            "matchedPaperCorpusId": "259275338"
                        },
                        {
                            "start": 1248,
                            "end": 1251,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 1406,
                            "end": 1410,
                            "matchedPaperCorpusId": "235658331"
                        },
                        {
                            "start": 1705,
                            "end": 1708,
                            "matchedPaperCorpusId": "248476411"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80224609375
                }
            ],
            "relevance_judgement": 0.80224609375,
            "relevance_judgment_input_expanded": "# Title: Multimodal Graph Learning for Generative Tasks\n# Venue: arXiv.org\n# Authors: Minji Yoon, Jing Yu Koh, Bryan Hooi, R. Salakhutdinov\n## Abstract\nMultimodal learning combines multiple data modalities, broadening the types and complexity of data our models can utilize: for example, from plain text to image-caption pairs. Most multimodal learning algorithms focus on modeling simple one-to-one pairs of data from two modalities, such as image-caption pairs, or audio-text pairs. However, in most real-world settings, entities of different modalities interact with each other in more complex and multifaceted ways, going beyond one-to-one mappings. We propose to represent these complex relationships as graphs, allowing us to capture data with any number of modalities, and with complex relationships between modalities that can flexibly vary from one sample to another. Toward this goal, we propose Multimodal Graph Learning (MMGL), a general and systematic framework for capturing information from multiple multimodal neighbors with relational structures among them. In particular, we focus on MMGL for generative tasks, building upon pretrained Language Models (LMs), aiming to augment their text generation with multimodal neighbor contexts. We study three research questions raised by MMGL: (1) how can we infuse multiple neighbor information into the pretrained LMs, while avoiding scalability issues? (2) how can we infuse the graph structure information among multimodal neighbors into the LMs? and (3) how can we finetune the pretrained LMs to learn from the neighbor context in a parameter-efficient manner? We conduct extensive experiments to answer these three questions on MMGL and analyze the empirical results to pave the way for future MMGL research.\n## Related Work\nEnd-to-End Multimodal Learning: While many discriminative multimodal models [14,22] have also been developed, we primarily consider related work on generative multimodal models, as this is most closely related with our approach. Several recent approaches tackle multimodal learning by building upon the Transformer [32] architecture. Multimodal extensions typically use either full self-attention over modalities concatenated across the sequence dimension [3,28] or a crossmodal attention layer [30]. Self-supervised multimodal pretraining methods train these architectures from large-scale unlabeled multimodal data before transferring them to downstream multimodal tasks via fine-tuning [9,19]. These methods perform end-to-end pre-training, incurring extremely high computation costs, especially as model parameters increase [17]. Moreover, this framework is relatively inflexible for end-to-end pre-trained models to leverage readily available unimodal pre-trained models, such as text-only LMs or pretrained vision models. \n\nMultimodal Learning with Frozen Image Encoders and Large Language Models: Recently, various vision-language models have been proposed to leverage off-the-shelf pre-trained models and keep them frozen during pretrainig [1,17,16]. To input visual information directly to a frozen text-only LLM, a key challenge is to align visual features to the text space. Motivated by Frozen [31], which finetunes a visual encoder to map images into the hidden space of a text-only LLM, Blip-2 [17] and GILL [16] finetune separate image mapping networks whose inputs are precomputed by frozen image encoders and outputs are directly used as soft prompts to LLMs. On the other hand, Flamingo [1] inserts new cross-attention layers into the LLM to inject visual features and pre-trains the new layers on image-text pairs. Note that all these methods primarily focus on processing interleaved image and text inputs to generate text outputs.",
            "reference_string": "[263835060 | Yoon et al. | 2023 | Citations: 11]"
        },
        {
            "title": "Efficient Multimodal Diffusion Models Using Joint Data Infilling with Partially Shared U-Net",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.16488, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2148424224",
                    "name": "Zizhao Hu"
                },
                {
                    "authorId": "2268492825",
                    "name": "Shaochong Jia"
                },
                {
                    "authorId": "2256992795",
                    "name": "Mohammad Rostami"
                }
            ],
            "abstract": "Recently, diffusion models have been used successfully to fit distributions for cross-modal data translation and multimodal data generation. However, these methods rely on extensive scaling, overlooking the inefficiency and interference between modalities. We develop Partially Shared U-Net (PS-U-Net) architecture which is an efficient multimodal diffusion model that allows text and image inputs to pass through dedicated layers and skip-connections for preserving modality-specific fine-grained details. Inspired by image inpainting, we also propose a new efficient multimodal sampling method that introduces new scenarios for conditional generation while only requiring a simple joint distribution to be learned. Our empirical exploration of the MS-COCO dataset demonstrates that our method generates multimodal text and image data with higher quality compared to existing multimodal diffusion models while having a comparable size, faster training, faster multimodal sampling, and more flexible generation.",
            "corpus_id": 265466568,
            "sentences": [
                {
                    "corpus_id": "265466568",
                    "title": "Efficient Multimodal Diffusion Models Using Joint Data Infilling with Partially Shared U-Net",
                    "text": "Diffusion models [25] have emerged as a potent framework for generating data across diverse domains such as language and vision. They have found broad applications across various domains, enabling high-quality and detail-oriented conditional data generation, as well as cross-modality data generation such as text-to-image [3,9,18,21], text-to-video [10], etc. A case in point is Stable Diffusion [21] which has demonstrated human-like text-to-image generation capabilities, showcasing the robustness and versatility of diffusion models. In addition to continuous domain, diffusion models have also shown promising futures in discrete data spaces such as language generation [8,11,14,26]. \n\nThe above-mentioned models can only generate data in a single data modality, a major discrepancy from human intelligence. For example, a human painter can use language to describe the scene they see, and when a text description of a scene is seen, they can reconstruct the visual scene either in imagination or through painting. In the pursuit of a more powerful generative system, it is necessary to develop architectures that have the ability to generate multimodal data given only partial data modality. To offer this ability, diffusion model architectures have recently been extended to accommodate cross-modal and multimodal generation scenarios [2,27,30]. These works enable any-to-any generation with bidirectional conditioning and simultaneous generation of multiple modalities. They utilize different techniques to bring data from different domains into a shared embedding space. Versatile Diffusion (VD) [30] aligns separate modality flows using shared global model layers. Conditional sampling is achieved by injecting the context and generation modality pairs in a shared context layer. Composable Diffusion (CoDi) [27] selects text as the \"bridging\" modality, leveraging its availability in multimodal paired data. It then trains separate encoders for each modality to align with the text space. Unidiffuser [2] concatenates both images and text into a shared continuous latent space, and learns the joint diffusion of both modalities which may not lead to optimal learning. For a more thorough discussion of related work, please refer to the Appendix.",
                    "score": 0.5433312048901344,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 688
                        },
                        {
                            "start": 691,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 1019
                        },
                        {
                            "start": 1020,
                            "end": 1197
                        },
                        {
                            "start": 1198,
                            "end": 1352
                        },
                        {
                            "start": 1353,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1579
                        },
                        {
                            "start": 1580,
                            "end": 1674
                        },
                        {
                            "start": 1675,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 1999
                        },
                        {
                            "start": 2000,
                            "end": 2178
                        },
                        {
                            "start": 2179,
                            "end": 2256
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 328,
                            "end": 331,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 331,
                            "end": 334,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 397,
                            "end": 401,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80029296875
                }
            ],
            "relevance_judgement": 0.80029296875,
            "relevance_judgment_input_expanded": "# Title: Efficient Multimodal Diffusion Models Using Joint Data Infilling with Partially Shared U-Net\n# Venue: arXiv.org\n# Authors: Zizhao Hu, Shaochong Jia, Mohammad Rostami\n## Abstract\nRecently, diffusion models have been used successfully to fit distributions for cross-modal data translation and multimodal data generation. However, these methods rely on extensive scaling, overlooking the inefficiency and interference between modalities. We develop Partially Shared U-Net (PS-U-Net) architecture which is an efficient multimodal diffusion model that allows text and image inputs to pass through dedicated layers and skip-connections for preserving modality-specific fine-grained details. Inspired by image inpainting, we also propose a new efficient multimodal sampling method that introduces new scenarios for conditional generation while only requiring a simple joint distribution to be learned. Our empirical exploration of the MS-COCO dataset demonstrates that our method generates multimodal text and image data with higher quality compared to existing multimodal diffusion models while having a comparable size, faster training, faster multimodal sampling, and more flexible generation.\n## Introduction\nDiffusion models [25] have emerged as a potent framework for generating data across diverse domains such as language and vision. They have found broad applications across various domains, enabling high-quality and detail-oriented conditional data generation, as well as cross-modality data generation such as text-to-image [3,9,18,21], text-to-video [10], etc. A case in point is Stable Diffusion [21] which has demonstrated human-like text-to-image generation capabilities, showcasing the robustness and versatility of diffusion models. In addition to continuous domain, diffusion models have also shown promising futures in discrete data spaces such as language generation [8,11,14,26]. \n\nThe above-mentioned models can only generate data in a single data modality, a major discrepancy from human intelligence. For example, a human painter can use language to describe the scene they see, and when a text description of a scene is seen, they can reconstruct the visual scene either in imagination or through painting. In the pursuit of a more powerful generative system, it is necessary to develop architectures that have the ability to generate multimodal data given only partial data modality. To offer this ability, diffusion model architectures have recently been extended to accommodate cross-modal and multimodal generation scenarios [2,27,30]. These works enable any-to-any generation with bidirectional conditioning and simultaneous generation of multiple modalities. They utilize different techniques to bring data from different domains into a shared embedding space. Versatile Diffusion (VD) [30] aligns separate modality flows using shared global model layers. Conditional sampling is achieved by injecting the context and generation modality pairs in a shared context layer. Composable Diffusion (CoDi) [27] selects text as the \"bridging\" modality, leveraging its availability in multimodal paired data. It then trains separate encoders for each modality to align with the text space. Unidiffuser [2] concatenates both images and text into a shared continuous latent space, and learns the joint diffusion of both modalities which may not lead to optimal learning. For a more thorough discussion of related work, please refer to the Appendix.",
            "reference_string": "[265466568 | Hu et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Unifying Multimodal Transformer for Bi-directional Image and Text Generation",
            "venue": "ACM Multimedia",
            "year": 2021,
            "reference_count": 54,
            "citation_count": 59,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2110.09753",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.09753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "102665943",
                    "name": "Yupan Huang"
                },
                {
                    "authorId": "1878753365",
                    "name": "Hongwei Xue"
                },
                {
                    "authorId": "2127734772",
                    "name": "Bei Liu"
                },
                {
                    "authorId": "2140026613",
                    "name": "Yutong Lu"
                }
            ],
            "abstract": "We study the joint learning of image-to-text and text-to-image generations, which are naturally bi-directional tasks. Typical existing works design two separate task-specific models for each task, which impose expensive design efforts. In this work, we propose a unified image-and-text generative framework based on a single multimodal model to jointly study the bi-directional tasks. We adopt Transformer as our unified architecture for its strong performance and task-agnostic design. Specifically, we formulate both tasks as sequence generation tasks, where we represent images and text as unified sequences of tokens, and the Transformer learns multimodal interactions to generate sequences. We further propose two-level granularity feature representations and sequence-level training to improve the Transformer-based unified framework. Experiments show that our approach significantly improves previous Transformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for text-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for fine-tuned image-to-text generation on the MS-COCO dataset. Our code is available online.",
            "corpus_id": 239011704,
            "sentences": [
                {
                    "corpus_id": "239011704",
                    "title": "Unifying Multimodal Transformer for Bi-directional Image and Text Generation",
                    "text": "We study the joint learning of image-to-text and text-to-image generations, which are naturally bi-directional tasks. Typical existing works design two separate task-specific models for each task, which impose expensive design efforts. In this work, we propose a unified image-and-text generative framework based on a single multimodal model to jointly study the bi-directional tasks. We adopt Transformer as our unified architecture for its strong performance and task-agnostic design. Specifically, we formulate both tasks as sequence generation tasks, where we represent images and text as unified sequences of tokens, and the Transformer learns multimodal interactions to generate sequences. We further propose two-level granularity feature representations and sequence-level training to improve the Transformer-based unified framework. Experiments show that our approach significantly improves previous Transformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for text-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for fine-tuned image-to-text generation on the MS-COCO dataset. Our code is available online.",
                    "score": 0.5223071190491124,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7978515625
                },
                {
                    "corpus_id": "239011704",
                    "title": "Unifying Multimodal Transformer for Bi-directional Image and Text Generation",
                    "text": "Our model mainly consists of a unified multimodal Transformer [45], which has a multi-layer architecture and each layer mainly consists of multi-head self-attention and position-wise fully connected feedforward networks. We adopt LXMERT [43] as our Transformerbased architecture following X-LXMERT [6] for a direct comparison, since our proposal is mainly based on feature representation and training mechanism but not on the specific model. LXMERT is a cross-modality Transformer consisting of an object-relation encoder, a language encoder and a cross-modality encoder. We omit an exhaustive background description of the model architecture and refer readers to [6,43,45] for additional details.\n\nTo enable both image-to-text and text-to-image generation tasks, we formulate both tasks as sequence generation tasks. Specifically, we firstly pre-process image-and-text pairs into sequences of image tokens and text tokens. Then the Transformer accepts sequences with masked tokens as input, and maps the input to a contextualized representation via attention networks. Lastly, an image or text linear transformation classifier projects the contextualized representation to predicted target tokens. In our model, we share the parameters of most Transformer modules, except for the last classifiers since image and text vocabularies have different sizes and semantics. For text-to-image generation, we use an additional GAN-based image generator to convert the size of 8 \u00d7 8 image token predictions to a 256 \u00d7 256 resolution visible image.\n\nText representation is a combination of the position embedding and the word embedding, where the position refers to the index of the word within the caption, and the word embedding is initialized from pre-trained models like BERT [8] or LXMERT [43].",
                    "score": 0.5235133754947741,
                    "section_title": "Unified Multimodal Framework",
                    "char_start_offset": 9005,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 62,
                            "end": 66,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 237,
                            "end": 241,
                            "matchedPaperCorpusId": "201103729"
                        },
                        {
                            "start": 667,
                            "end": 670,
                            "matchedPaperCorpusId": "201103729"
                        },
                        {
                            "start": 670,
                            "end": 673,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1770,
                            "end": 1773,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1784,
                            "end": 1788,
                            "matchedPaperCorpusId": "201103729"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7822265625
                }
            ],
            "relevance_judgement": 0.7978515625,
            "relevance_judgment_input_expanded": "# Title: Unifying Multimodal Transformer for Bi-directional Image and Text Generation\n# Venue: ACM Multimedia\n# Authors: Yupan Huang, Hongwei Xue, Bei Liu, Yutong Lu\n## Abstract\nWe study the joint learning of image-to-text and text-to-image generations, which are naturally bi-directional tasks. Typical existing works design two separate task-specific models for each task, which impose expensive design efforts. In this work, we propose a unified image-and-text generative framework based on a single multimodal model to jointly study the bi-directional tasks. We adopt Transformer as our unified architecture for its strong performance and task-agnostic design. Specifically, we formulate both tasks as sequence generation tasks, where we represent images and text as unified sequences of tokens, and the Transformer learns multimodal interactions to generate sequences. We further propose two-level granularity feature representations and sequence-level training to improve the Transformer-based unified framework. Experiments show that our approach significantly improves previous Transformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for text-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for fine-tuned image-to-text generation on the MS-COCO dataset. Our code is available online.\n## Unified Multimodal Framework\nOur model mainly consists of a unified multimodal Transformer [45], which has a multi-layer architecture and each layer mainly consists of multi-head self-attention and position-wise fully connected feedforward networks. We adopt LXMERT [43] as our Transformerbased architecture following X-LXMERT [6] for a direct comparison, since our proposal is mainly based on feature representation and training mechanism but not on the specific model. LXMERT is a cross-modality Transformer consisting of an object-relation encoder, a language encoder and a cross-modality encoder. We omit an exhaustive background description of the model architecture and refer readers to [6,43,45] for additional details.\n\nTo enable both image-to-text and text-to-image generation tasks, we formulate both tasks as sequence generation tasks. Specifically, we firstly pre-process image-and-text pairs into sequences of image tokens and text tokens. Then the Transformer accepts sequences with masked tokens as input, and maps the input to a contextualized representation via attention networks. Lastly, an image or text linear transformation classifier projects the contextualized representation to predicted target tokens. In our model, we share the parameters of most Transformer modules, except for the last classifiers since image and text vocabularies have different sizes and semantics. For text-to-image generation, we use an additional GAN-based image generator to convert the size of 8 \u00d7 8 image token predictions to a 256 \u00d7 256 resolution visible image.\n\nText representation is a combination of the position embedding and the word embedding, where the position refers to the index of the word within the caption, and the word embedding is initialized from pre-trained models like BERT [8] or LXMERT [43].",
            "reference_string": "[239011704 | Huang et al. | 2021 | Citations: 59]"
        },
        {
            "title": "GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation",
            "venue": "ACM Multimedia",
            "year": 2023,
            "reference_count": 66,
            "citation_count": 27,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2311.16511",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.16511, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268638120",
                    "name": "Zhanyu Wang"
                },
                {
                    "authorId": "2111542852",
                    "name": "Longyue Wang"
                },
                {
                    "authorId": "145737114",
                    "name": "Zhen Zhao"
                },
                {
                    "authorId": "2145209409",
                    "name": "Minghao Wu"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "91956362",
                    "name": "Huayang Li"
                },
                {
                    "authorId": "2266753374",
                    "name": "Deng Cai"
                },
                {
                    "authorId": "2268723641",
                    "name": "Luping Zhou"
                },
                {
                    "authorId": "2239383048",
                    "name": "Shuming Shi"
                },
                {
                    "authorId": "2256685351",
                    "name": "Zhaopeng Tu"
                }
            ],
            "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have constituted a significant leap forward in the field, particularly in the processing of videos, which encompasses inherent challenges such as spatiotemporal relationships. However, existing MLLMs are predominantly focused on the comprehension of video inputs, with limited capabilities in generating video content. In this paper, we present GPT4Video, a unified framework that seamlessly and lightly integrates with LLMs, visual feature extractors, and stable diffusion generative models for cohesive video understanding and generation. Moreover, we explore a text-only finetuning approach to equip models for instruction-following and safeguarding in multimodal conversations, enhancing training efficiency and generalization capabilities. Additionally, we construct multi-turn and caption-interleaved datasets for finetuning and benchmarking MLLMs, which serve as solid resources for advancing this field. Through quantitative and qualitative assessments, GPT4Video demonstrates the following advantages: 1) The framework incorporates video generation ability without adding extra training parameters, ensuring seamless compatibility with various video generators. 2) The model achieves superior performances across a variety of benchmarks. For instance, it outperforms Valley by 11.8% on video question answering, and surpasses NExt-GPT by 2.3% on text-to-video generation. 3) As safety pioneers in open-source MLLMs, we developed finetuning and evaluation datasets, securing an F1 score exceeding 80% in blocking harmful content during understanding and generating videos. In general, GPT4Video shows potential to function as a real-life assistant, marked by its effectiveness, adaptability, and safety.",
            "corpus_id": 265466543,
            "sentences": [
                {
                    "corpus_id": "265466543",
                    "title": "GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation",
                    "text": "Multimodal Language Models Numerous studies have developed multimodal language models that can handle visual inputs and text outputs, or vice versa, such as [6,58]. With these advancements of LLMs, some researches have focused on learning a joint embedding space for multiple modalities, as demonstrated in [16,42]. Others have combined pre-trained single-modality models to showcase impressive zero-shot capabilities [1,26]. More recently, there has been a growing interest in enabling multi-modal LLMs to follow instructions, as shown in [12,59,69]. To facilitate research in this area, [57] introduced MultiInstruct, the first multi-modal instruction tuning benchmark dataset covering a wide range of tasks and categories. \n\nAdditionally, [33] explored multi-modal instruction-tuning using machine-generated data, while [36] fine-tuned all model parameters to allow the textual LLM to process four modalities. Large Language Models Large language models (LLMs) commonly refer to as Transformer-based language models with billions of parameters [50] and have revolutionized the research paradigm in natural language processing community [13,49]. Furthermore, recent works have demonstrated that supervised fine-tuning, also known as instruction-tuning, can effectively improve the zero-shot performance of these LLMs [11,48]. Zhao et al. [66] present a comprehensive survey on the research of LLMs. These prompts are then used to create videos from the Text-to-Video Model Gallery. Icons of a snowflake and a flame visually distinguish between the non-trainable and trainable parameters within the system. The two bullet points highlight GPT4Video's dual capabilities in understanding and generating video content. \n\nText-to-Image/Video Generation Text-to-image/video generation refers to the task of producing realistic images or videos based on natural language descriptions. One of the earliest approaches to this task was the use of conditional GANs [43]. Since then, various techniques have been developed to improve the quality of the generated images [39]. Compared to text-to-image generation, textto-video generation is relative new and still remains challenging.",
                    "score": 0.5286527855795332,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 5722,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 315
                        },
                        {
                            "start": 316,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 725
                        },
                        {
                            "start": 728,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1327
                        },
                        {
                            "start": 1328,
                            "end": 1400
                        },
                        {
                            "start": 1401,
                            "end": 1483
                        },
                        {
                            "start": 1484,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1716
                        },
                        {
                            "start": 1719,
                            "end": 1879
                        },
                        {
                            "start": 1880,
                            "end": 1961
                        },
                        {
                            "start": 1962,
                            "end": 2065
                        },
                        {
                            "start": 2066,
                            "end": 2174
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 157,
                            "end": 160,
                            "matchedPaperCorpusId": "257663916"
                        },
                        {
                            "start": 160,
                            "end": 163,
                            "matchedPaperCorpusId": "249712016"
                        },
                        {
                            "start": 311,
                            "end": 314,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 418,
                            "end": 421,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 1139,
                            "end": 1143,
                            "matchedPaperCorpusId": "247519241"
                        },
                        {
                            "start": 1956,
                            "end": 1960,
                            "matchedPaperCorpusId": "1563370"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79248046875
                }
            ],
            "relevance_judgement": 0.79248046875,
            "relevance_judgment_input_expanded": "# Title: GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation\n# Venue: ACM Multimedia\n# Authors: Zhanyu Wang, Longyue Wang, Zhen Zhao, Minghao Wu, Chenyang Lyu, Huayang Li, Deng Cai, Luping Zhou, Shuming Shi, Zhaopeng Tu\n## Abstract\nRecent advances in Multimodal Large Language Models (MLLMs) have constituted a significant leap forward in the field, particularly in the processing of videos, which encompasses inherent challenges such as spatiotemporal relationships. However, existing MLLMs are predominantly focused on the comprehension of video inputs, with limited capabilities in generating video content. In this paper, we present GPT4Video, a unified framework that seamlessly and lightly integrates with LLMs, visual feature extractors, and stable diffusion generative models for cohesive video understanding and generation. Moreover, we explore a text-only finetuning approach to equip models for instruction-following and safeguarding in multimodal conversations, enhancing training efficiency and generalization capabilities. Additionally, we construct multi-turn and caption-interleaved datasets for finetuning and benchmarking MLLMs, which serve as solid resources for advancing this field. Through quantitative and qualitative assessments, GPT4Video demonstrates the following advantages: 1) The framework incorporates video generation ability without adding extra training parameters, ensuring seamless compatibility with various video generators. 2) The model achieves superior performances across a variety of benchmarks. For instance, it outperforms Valley by 11.8% on video question answering, and surpasses NExt-GPT by 2.3% on text-to-video generation. 3) As safety pioneers in open-source MLLMs, we developed finetuning and evaluation datasets, securing an F1 score exceeding 80% in blocking harmful content during understanding and generating videos. In general, GPT4Video shows potential to function as a real-life assistant, marked by its effectiveness, adaptability, and safety.\n## RELATED WORK\nMultimodal Language Models Numerous studies have developed multimodal language models that can handle visual inputs and text outputs, or vice versa, such as [6,58]. With these advancements of LLMs, some researches have focused on learning a joint embedding space for multiple modalities, as demonstrated in [16,42]. Others have combined pre-trained single-modality models to showcase impressive zero-shot capabilities [1,26]. More recently, there has been a growing interest in enabling multi-modal LLMs to follow instructions, as shown in [12,59,69]. To facilitate research in this area, [57] introduced MultiInstruct, the first multi-modal instruction tuning benchmark dataset covering a wide range of tasks and categories. \n\nAdditionally, [33] explored multi-modal instruction-tuning using machine-generated data, while [36] fine-tuned all model parameters to allow the textual LLM to process four modalities. Large Language Models Large language models (LLMs) commonly refer to as Transformer-based language models with billions of parameters [50] and have revolutionized the research paradigm in natural language processing community [13,49]. Furthermore, recent works have demonstrated that supervised fine-tuning, also known as instruction-tuning, can effectively improve the zero-shot performance of these LLMs [11,48]. Zhao et al. [66] present a comprehensive survey on the research of LLMs. These prompts are then used to create videos from the Text-to-Video Model Gallery. Icons of a snowflake and a flame visually distinguish between the non-trainable and trainable parameters within the system. The two bullet points highlight GPT4Video's dual capabilities in understanding and generating video content. \n\nText-to-Image/Video Generation Text-to-image/video generation refers to the task of producing realistic images or videos based on natural language descriptions. One of the earliest approaches to this task was the use of conditional GANs [43]. Since then, various techniques have been developed to improve the quality of the generated images [39]. Compared to text-to-image generation, textto-video generation is relative new and still remains challenging.",
            "reference_string": "[265466543 | Wang et al. | 2023 | Citations: 27]"
        },
        {
            "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 61,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261083308",
                    "name": "Peng Xia"
                },
                {
                    "authorId": "2283875337",
                    "name": "Siwei Han"
                },
                {
                    "authorId": "2325909382",
                    "name": "Shi Qiu"
                },
                {
                    "authorId": "2250759613",
                    "name": "Yiyang Zhou"
                },
                {
                    "authorId": "2303664290",
                    "name": "Zhaoyang Wang"
                },
                {
                    "authorId": "2283848842",
                    "name": "Wenhao Zheng"
                },
                {
                    "authorId": "1641681688",
                    "name": "Zhaorun Chen"
                },
                {
                    "authorId": "2249763944",
                    "name": "Chenhang Cui"
                },
                {
                    "authorId": "2273648094",
                    "name": "Mingyu Ding"
                },
                {
                    "authorId": "50703697",
                    "name": "Linjie Li"
                },
                {
                    "authorId": "2273909761",
                    "name": "Lijuan Wang"
                },
                {
                    "authorId": "2287977579",
                    "name": "Huaxiu Yao"
                }
            ],
            "abstract": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.",
            "corpus_id": 273346517,
            "sentences": [
                {
                    "corpus_id": "273346517",
                    "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
                    "text": "\"True evaluation lies in the seamless interweaving of diverse modalities.\" \n\nMultimodal learning has made remarkable progress with the development of Large Vision-Language Models (LVLMs) (Liu et al., 2023a;Zhu et al., 2023;Dai et al., 2023), which are capable of handling diverse tasks that involve both images and text. Despite their advancements, most of these models are limited to multimodal tasks for text generation, such as visual question answering (VQA) and image captioning, which do not fully reflect the potential of multimodal capacity. To broaden their application, interleaved text-and-image generation has emerged as a critical area of research (Liu et al., 2024). It requires models to generate images and text in any sequence, thereby enhancing the versatility and effectiveness of multimodal systems. It opens up possibilities for various complex applications, such as multi-step inference (Lu et al., 2024;Kazemi et al., 2024), multimodal situational analysis (Yang et al., 2021), and visual storytelling (Huang et al., 2016). \n\nWhile recent LVLMs are evolving to support interleaved text-and-image generation (Team, 2024;Xie et al., 2024;Chern et al., 2024;Zhou et al., 2024), a comprehensive evaluation benchmark is still falling behind due to the following two challenges: Figure 1: Typical samples from the MMIE Benchmark showcase its support for multiple image inputs and outputs, with ground truth provided for every query. MMIE evaluates models across diverse fields, ensuring a comprehensive evaluation of their capabilities. \n\n\u2022 Difficulty in Constructing Modality-Coherent Benchmarks. The first challenge lies in the difficulty of constructing modality-aligned multimodal datasets, where both the input and output contain images and text.",
                    "score": 0.5135492427720594,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 77,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 1046
                        },
                        {
                            "start": 1049,
                            "end": 1449
                        },
                        {
                            "start": 1450,
                            "end": 1553
                        },
                        {
                            "start": 1556,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1768
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 909,
                            "end": 926,
                            "matchedPaperCorpusId": "264491155"
                        },
                        {
                            "start": 1025,
                            "end": 1045,
                            "matchedPaperCorpusId": "2574224"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78466796875
                }
            ],
            "relevance_judgement": 0.78466796875,
            "relevance_judgment_input_expanded": "# Title: MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, Lijuan Wang, Huaxiu Yao\n## Abstract\nInterleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.\n## INTRODUCTION\n\"True evaluation lies in the seamless interweaving of diverse modalities.\" \n\nMultimodal learning has made remarkable progress with the development of Large Vision-Language Models (LVLMs) (Liu et al., 2023a;Zhu et al., 2023;Dai et al., 2023), which are capable of handling diverse tasks that involve both images and text. Despite their advancements, most of these models are limited to multimodal tasks for text generation, such as visual question answering (VQA) and image captioning, which do not fully reflect the potential of multimodal capacity. To broaden their application, interleaved text-and-image generation has emerged as a critical area of research (Liu et al., 2024). It requires models to generate images and text in any sequence, thereby enhancing the versatility and effectiveness of multimodal systems. It opens up possibilities for various complex applications, such as multi-step inference (Lu et al., 2024;Kazemi et al., 2024), multimodal situational analysis (Yang et al., 2021), and visual storytelling (Huang et al., 2016). \n\nWhile recent LVLMs are evolving to support interleaved text-and-image generation (Team, 2024;Xie et al., 2024;Chern et al., 2024;Zhou et al., 2024), a comprehensive evaluation benchmark is still falling behind due to the following two challenges: Figure 1: Typical samples from the MMIE Benchmark showcase its support for multiple image inputs and outputs, with ground truth provided for every query. MMIE evaluates models across diverse fields, ensuring a comprehensive evaluation of their capabilities. \n\n\u2022 Difficulty in Constructing Modality-Coherent Benchmarks. The first challenge lies in the difficulty of constructing modality-aligned multimodal datasets, where both the input and output contain images and text.",
            "reference_string": "[273346517 | Xia et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2123130842",
                    "name": "Minqian Liu"
                },
                {
                    "authorId": "2136442661",
                    "name": "Zhiyang Xu"
                },
                {
                    "authorId": "2254334449",
                    "name": "Zihao Lin"
                },
                {
                    "authorId": "2243992517",
                    "name": "Trevor Ashby"
                },
                {
                    "authorId": "2218905547",
                    "name": "Joy Rimchala"
                },
                {
                    "authorId": "2307975528",
                    "name": "Jiaxin Zhang"
                },
                {
                    "authorId": "2238885968",
                    "name": "Lifu Huang"
                }
            ],
            "abstract": "Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate and explainable evaluation. We carefully define five essential evaluation aspects for InterleavedEval, including text quality, perceptual quality, image coherence, text-image coherence, and helpfulness, to ensure a comprehensive and fine-grained assessment. Through extensive experiments and rigorous human evaluation, we show that our benchmark and metric can effectively evaluate the existing models with a strong correlation with human judgments surpassing previous reference-based metrics. We also provide substantial findings and insights to foster future research in interleaved generation and its evaluation.",
            "corpus_id": 270688372,
            "sentences": [
                {
                    "corpus_id": "270688372",
                    "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
                    "text": "Large Multimodal Models for Interleaved Generation The advent of large multimodal models (LMMs) (Koh et al., 2023;Sun et al., 2023a) has significantly advanced the field of interleaved textand-image generation. Previous models such as DALL-E (Ramesh et al., 2021) and Stable Diffusion (Podell et al., 2023) have demonstrated impressive capabilities in generating high-quality images conditioned on textual descriptions. However, previous focus has predominantly been on unidirectional generation tasks, either from text to image or image to text, without considering the interleaved generation scenarios where text and images are seamlessly integrated within the same output. Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023;Sun et al., 2023b;Dong et al., 2024;Tian et al., 2024;Zhan et al., 2024;Chen et al., 2023). These models leverage advanced architectures and training techniques to enhance their ability to produce coherent and contextually relevant interleaved content. Despite these advancements, the evaluation of such models remains an underexplored area, with most evaluations still relying on separate assessments of text and image quality or simplistic reference-based metrics. Our proposed INTERLEAVEDBENCH benchmark aims to bridge this gap by providing a holistic evaluation framework tailored specifically for interleaved text-and-image generation.",
                    "score": 0.5382066672284536,
                    "section_title": "Related Work",
                    "char_start_offset": 5358,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 210
                        },
                        {
                            "start": 211,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1308
                        },
                        {
                            "start": 1309,
                            "end": 1482
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 96,
                            "end": 114,
                            "matchedPaperCorpusId": "258959284"
                        },
                        {
                            "start": 242,
                            "end": 263,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 824,
                            "end": 842,
                            "matchedPaperCorpusId": "258959284"
                        },
                        {
                            "start": 860,
                            "end": 878,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 896,
                            "end": 914,
                            "matchedPaperCorpusId": "267750101"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78125
                }
            ],
            "relevance_judgement": 0.78125,
            "relevance_judgment_input_expanded": "# Title: Holistic Evaluation for Interleaved Text-and-Image Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Minqian Liu, Zhiyang Xu, Zihao Lin, Trevor Ashby, Joy Rimchala, Jiaxin Zhang, Lifu Huang\n## Abstract\nInterleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate and explainable evaluation. We carefully define five essential evaluation aspects for InterleavedEval, including text quality, perceptual quality, image coherence, text-image coherence, and helpfulness, to ensure a comprehensive and fine-grained assessment. Through extensive experiments and rigorous human evaluation, we show that our benchmark and metric can effectively evaluate the existing models with a strong correlation with human judgments surpassing previous reference-based metrics. We also provide substantial findings and insights to foster future research in interleaved generation and its evaluation.\n## Related Work\nLarge Multimodal Models for Interleaved Generation The advent of large multimodal models (LMMs) (Koh et al., 2023;Sun et al., 2023a) has significantly advanced the field of interleaved textand-image generation. Previous models such as DALL-E (Ramesh et al., 2021) and Stable Diffusion (Podell et al., 2023) have demonstrated impressive capabilities in generating high-quality images conditioned on textual descriptions. However, previous focus has predominantly been on unidirectional generation tasks, either from text to image or image to text, without considering the interleaved generation scenarios where text and images are seamlessly integrated within the same output. Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023;Sun et al., 2023b;Dong et al., 2024;Tian et al., 2024;Zhan et al., 2024;Chen et al., 2023). These models leverage advanced architectures and training techniques to enhance their ability to produce coherent and contextually relevant interleaved content. Despite these advancements, the evaluation of such models remains an underexplored area, with most evaluations still relying on separate assessments of text and image quality or simplistic reference-based metrics. Our proposed INTERLEAVEDBENCH benchmark aims to bridge this gap by providing a holistic evaluation framework tailored specifically for interleaved text-and-image generation.",
            "reference_string": "[270688372 | Liu et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 127,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2270181751",
                    "name": "Zilun Zhang"
                },
                {
                    "authorId": "2174678931",
                    "name": "Haozhan Shen"
                },
                {
                    "authorId": "8200875",
                    "name": "Tiancheng Zhao"
                },
                {
                    "authorId": "2330774884",
                    "name": "Yuhao Wang"
                },
                {
                    "authorId": "2330612748",
                    "name": "Bin Chen"
                },
                {
                    "authorId": "2149196373",
                    "name": "Yuxiang Cai"
                },
                {
                    "authorId": "2093090552",
                    "name": "Yongheng Shang"
                },
                {
                    "authorId": "2111612160",
                    "name": "Jianwei Yin"
                }
            ],
            "abstract": "Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 $\\times$ 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient. Codebase will be released in https://github.com/om-ai-lab/ImageRAG",
            "corpus_id": 273969615,
            "sentences": [
                {
                    "corpus_id": "273969615",
                    "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
                    "text": "Multimodal RAG technology is an extension of the traditional RAG model, designed to enhance the performance of generative tasks by incorporating information from multiple data modalities [118]. Unlike the traditional RAG, which processes only textual data, multimodal RAG can handle not only text but also other modalities such as images, audio, and video. It is capable of extracting information from various modalities and integrating this information to generate richer and more accurate outputs. In multimodal RAG systems, embeddings for various data types, such as text and images, are generated through modality-specific encoders [119]. These encoders share a unified embedding space, which is also employed for encoding the query. \n\nThe latest advancements in RAG in the image domain have led to significant improvements [120]. RA-CM3 [121] enhances both text-to-image and image-to-text generation by combining the CLIP retriever and the CM3 Transformer generator, achieving a performance boost while reducing computational costs by over 30%. Mortaheb et al. introduced a reranking mechanism based on a relevance score model [122], improving context selection during retrieval and reducing hallucinations, thereby enhancing the quality of generated responses. Yu et al.'s VisRAG [123] framework bypasses the text parsing stage to directly process multi-modal documents containing both text and images, achieving substantial improvements in multi-modal tasks. Bonomo and Bianco's Visual RAG [124] expands the visual knowledge of large MLLMs without the need for fine-tuning by dynamically retrieving relevant examples, offering high computational efficiency. Riedler and Langer's work on multimodal inputs for industrial applications demonstrates that integrating both images and text in RAG systems significantly improves performance [125]. \n\nRAG has also made significant progress in the video domain, driving innovations in long video comprehension. Yongdong Luo et al. propose Video-RAG [126], which enhances long video understanding by integrating visually-aligned auxiliary texts into large video-language models, surpassing models like Gemini1.5-Pro and GPT-4o.",
                    "score": 0.523344515976994,
                    "section_title": "D. Multimodal RAG",
                    "char_start_offset": 70659,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 499
                        },
                        {
                            "start": 500,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 737
                        },
                        {
                            "start": 740,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1847
                        },
                        {
                            "start": 1850,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2174
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78125
                }
            ],
            "relevance_judgement": 0.78125,
            "relevance_judgment_input_expanded": "# Title: Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG\n# Venue: arXiv.org\n# Authors: Zilun Zhang, Haozhan Shen, Tiancheng Zhao, Yuhao Wang, Bin Chen, Yuxiang Cai, Yongheng Shang, Jianwei Yin\n## Abstract\nUltra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 $\\times$ 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient. Codebase will be released in https://github.com/om-ai-lab/ImageRAG\n## D. Multimodal RAG\nMultimodal RAG technology is an extension of the traditional RAG model, designed to enhance the performance of generative tasks by incorporating information from multiple data modalities [118]. Unlike the traditional RAG, which processes only textual data, multimodal RAG can handle not only text but also other modalities such as images, audio, and video. It is capable of extracting information from various modalities and integrating this information to generate richer and more accurate outputs. In multimodal RAG systems, embeddings for various data types, such as text and images, are generated through modality-specific encoders [119]. These encoders share a unified embedding space, which is also employed for encoding the query. \n\nThe latest advancements in RAG in the image domain have led to significant improvements [120]. RA-CM3 [121] enhances both text-to-image and image-to-text generation by combining the CLIP retriever and the CM3 Transformer generator, achieving a performance boost while reducing computational costs by over 30%. Mortaheb et al. introduced a reranking mechanism based on a relevance score model [122], improving context selection during retrieval and reducing hallucinations, thereby enhancing the quality of generated responses. Yu et al.'s VisRAG [123] framework bypasses the text parsing stage to directly process multi-modal documents containing both text and images, achieving substantial improvements in multi-modal tasks. Bonomo and Bianco's Visual RAG [124] expands the visual knowledge of large MLLMs without the need for fine-tuning by dynamically retrieving relevant examples, offering high computational efficiency. Riedler and Langer's work on multimodal inputs for industrial applications demonstrates that integrating both images and text in RAG systems significantly improves performance [125]. \n\nRAG has also made significant progress in the video domain, driving innovations in long video comprehension. Yongdong Luo et al. propose Video-RAG [126], which enhances long video understanding by integrating visually-aligned auxiliary texts into large video-language models, surpassing models like Gemini1.5-Pro and GPT-4o.",
            "reference_string": "[273969615 | Zhang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "From text to multimodal: a survey of adversarial example generation in question answering systems",
            "venue": "Knowledge and Information Systems",
            "year": 2023,
            "reference_count": 100,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.16156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1476820716",
                    "name": "Gulsum Yigit"
                },
                {
                    "authorId": "1775715",
                    "name": "M. Amasyal\u0131"
                }
            ],
            "abstract": "Integrating adversarial machine learning with question answering (QA) systems has emerged as a critical area for understanding the vulnerabilities and robustness of these systems. This article aims to review adversarial example-generation techniques in the QA field, including textual and multimodal contexts. We examine the techniques employed through systematic categorization, providing a structured review. Beginning with an overview of traditional QA models, we traverse the adversarial example generation by exploring rule-based perturbations and advanced generative models. We then extend our research to include multimodal QA systems, analyze them across various methods, and examine generative models, seq2seq architectures, and hybrid methodologies. Our research grows to different defense strategies, adversarial datasets, and evaluation metrics and illustrates the literature on adversarial QA. Finally, the paper considers the future landscape of adversarial question generation, highlighting potential research directions that can advance textual and multimodal QA systems in the context of adversarial challenges.",
            "corpus_id": 266551693,
            "sentences": [
                {
                    "corpus_id": "266551693",
                    "title": "From text to multimodal: a survey of adversarial example generation in question answering systems",
                    "text": "In [61], VX2TEXT is proposed to generate text from multimodal inputs such as video, text, speech, or audio. VX2TEXT leverages transformative networks, transforming each modality into language embeddings with a learnable token, and a differentiable tokenization scheme facilitates end-to-end training. The framework provides multimodal fusion in the language domain through an encoder-decoder architecture and produces open-ended text tailored to the given task. Xie et al. focused on generating questions based on a given image and target answer, called conditional VQG [62]. They proposed a model called MOAG, which consists of four modules. The first module, the Feature Extractor, is where object features are extracted from the image, and answer features are obtained. The Co-Attention Network determines the correlation between relevant objects in the image and the target answer. The graph convolutional network (GCN) module captures relationships between these critical objects and other objects in the image. Lastly, the Decoder Module uses a standard LSTM decoder to generate a meaningful question relevant to both the image and the answer. \n\nIn [63], Li et al. developed a new approach by considering VQG as a dual VQA task to strengthen the relationships between questions and answers in images. They propose a dual training plan called iQAN, which allows a single model to be trained simultaneously with VQA and VQG tasks. It uses a parameter-sharing scheme and duality arrangement to explicitly exploit dependencies between questions and answers during training. Mostafazadeh et al. introduced a novel approach to QG using images as context [64]. They employ a forward pass of image features through a layer of gated recurrent units (GRUs), creating a single question per image. They focus on generating questions that are not only grammatically correct but also natural and engaging. \n\nWang et al. introduced Video QG [65]. Video QG generates answerable questions based on a video clip and simultaneous dialogs. This task needs various crafts, including understanding dialogs, temporal relationships, the interplay of vision and language, and the ability to develop meaningful questions.",
                    "score": 0.5329880316166887,
                    "section_title": "Encoder-decoder models",
                    "char_start_offset": 29780,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 300
                        },
                        {
                            "start": 301,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 772
                        },
                        {
                            "start": 773,
                            "end": 885
                        },
                        {
                            "start": 886,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1149
                        },
                        {
                            "start": 1152,
                            "end": 1306
                        },
                        {
                            "start": 1307,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1575
                        },
                        {
                            "start": 1576,
                            "end": 1659
                        },
                        {
                            "start": 1660,
                            "end": 1791
                        },
                        {
                            "start": 1792,
                            "end": 1897
                        },
                        {
                            "start": 1900,
                            "end": 1937
                        },
                        {
                            "start": 1938,
                            "end": 2025
                        },
                        {
                            "start": 2026,
                            "end": 2201
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 3,
                            "end": 7,
                            "matchedPaperCorpusId": "231719672"
                        },
                        {
                            "start": 570,
                            "end": 574,
                            "matchedPaperCorpusId": "239012092"
                        },
                        {
                            "start": 1155,
                            "end": 1159,
                            "matchedPaperCorpusId": "3184591"
                        },
                        {
                            "start": 1932,
                            "end": 1936,
                            "matchedPaperCorpusId": "195833622"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77783203125
                }
            ],
            "relevance_judgement": 0.77783203125,
            "relevance_judgment_input_expanded": "# Title: From text to multimodal: a survey of adversarial example generation in question answering systems\n# Venue: Knowledge and Information Systems\n# Authors: Gulsum Yigit, M. Amasyal\u0131\n## Abstract\nIntegrating adversarial machine learning with question answering (QA) systems has emerged as a critical area for understanding the vulnerabilities and robustness of these systems. This article aims to review adversarial example-generation techniques in the QA field, including textual and multimodal contexts. We examine the techniques employed through systematic categorization, providing a structured review. Beginning with an overview of traditional QA models, we traverse the adversarial example generation by exploring rule-based perturbations and advanced generative models. We then extend our research to include multimodal QA systems, analyze them across various methods, and examine generative models, seq2seq architectures, and hybrid methodologies. Our research grows to different defense strategies, adversarial datasets, and evaluation metrics and illustrates the literature on adversarial QA. Finally, the paper considers the future landscape of adversarial question generation, highlighting potential research directions that can advance textual and multimodal QA systems in the context of adversarial challenges.\n## Encoder-decoder models\nIn [61], VX2TEXT is proposed to generate text from multimodal inputs such as video, text, speech, or audio. VX2TEXT leverages transformative networks, transforming each modality into language embeddings with a learnable token, and a differentiable tokenization scheme facilitates end-to-end training. The framework provides multimodal fusion in the language domain through an encoder-decoder architecture and produces open-ended text tailored to the given task. Xie et al. focused on generating questions based on a given image and target answer, called conditional VQG [62]. They proposed a model called MOAG, which consists of four modules. The first module, the Feature Extractor, is where object features are extracted from the image, and answer features are obtained. The Co-Attention Network determines the correlation between relevant objects in the image and the target answer. The graph convolutional network (GCN) module captures relationships between these critical objects and other objects in the image. Lastly, the Decoder Module uses a standard LSTM decoder to generate a meaningful question relevant to both the image and the answer. \n\nIn [63], Li et al. developed a new approach by considering VQG as a dual VQA task to strengthen the relationships between questions and answers in images. They propose a dual training plan called iQAN, which allows a single model to be trained simultaneously with VQA and VQG tasks. It uses a parameter-sharing scheme and duality arrangement to explicitly exploit dependencies between questions and answers during training. Mostafazadeh et al. introduced a novel approach to QG using images as context [64]. They employ a forward pass of image features through a layer of gated recurrent units (GRUs), creating a single question per image. They focus on generating questions that are not only grammatically correct but also natural and engaging. \n\nWang et al. introduced Video QG [65]. Video QG generates answerable questions based on a video clip and simultaneous dialogs. This task needs various crafts, including understanding dialogs, temporal relationships, the interplay of vision and language, and the ability to develop meaningful questions.",
            "reference_string": "[266551693 | Yigit et al. | 2023 | Citations: 3]"
        },
        {
            "title": "Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications",
            "venue": "ACM Trans. Multim. Comput. Commun. Appl.",
            "year": 2023,
            "reference_count": 218,
            "citation_count": 32,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.00389",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.00389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282539097",
                    "name": "Muhammad Arslan Manzoor"
                },
                {
                    "authorId": "2129589403",
                    "name": "S. Albarri"
                },
                {
                    "authorId": "2203864747",
                    "name": "Ziting Xian"
                },
                {
                    "authorId": "3451645",
                    "name": "Zaiqiao Meng"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "3279808",
                    "name": "Shangsong Liang"
                }
            ],
            "abstract": "Multimodality Representation Learning, as a technique of learning to embed information from different modalities and their correlations, has achieved remarkable success on a variety of applications, such as Visual Question Answering (VQA), Natural Language for Visual Reasoning (NLVR), and Vision Language Retrieval (VLR). Among these applications, cross-modal interaction and complementary information from different modalities are crucial for advanced models to perform any multimodal task, e.g., understand, recognize, retrieve, or generate optimally. Researchers have proposed diverse methods to address these tasks. The different variants of transformer-based architectures performed extraordinarily on multiple modalities. This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodal tasks. This study summarizes the (i) recent task-specific deep learning methodologies, (ii) the pretraining types and multimodal pretraining objectives, (iii) from state-of-the-art pretrained multimodal approaches to unifying architectures, and (iv) multimodal task categories and possible future improvements that can be devised for better multimodal learning. Moreover, we prepare a dataset section for new researchers that covers most of the benchmarks for pretraining and finetuning. Finally, major challenges, gaps, and potential research topics are explored. A constantly-updated paperlist related to our survey is maintained at https://github.com/marslanm/multimodality-representation-learning.",
            "corpus_id": 256459691,
            "sentences": [
                {
                    "corpus_id": "256459691",
                    "title": "Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications",
                    "text": "BLIP-2 [98], an improved visual-language model based on comprehensive pretraining strategy, achieves state-of-the-art performance across numerous vision-language tasks. BLIP-2 exhibits a wide range of zero-shot image-to-text abilities, which include visual knowledge reasoning, visual common sense reasoning, visual conversation, and personalized image-to-text generation. \n\nLike its predecessor, BLIP-2 uses the same evaluation datasets and continues to implement the Captioning and Filtering (CapFilt) method. The UNITER [82] is a pretraining approach conducted at a large scale over four benchmark datasets. Joint embedding from both modalities is learned to perform heterogeneous downstream tasks. MLM, MRM (Masked Region Modeling), ITM (Image-Text Matching), and WRA (Word-Region Alignment) are employed as a pretraining task. Additionally, the pretraining task achieves global image text alignment using Conditional masking. Optimal Transport is the author's second concept for the WRA task to improve the alignment between images and words. Hu et al. [108] proposed a unified transformer for multitasking based on multimodal learning at a time. The proposed architecture uses a specialized encoder for each modality and a shared decoder for every task. DETR [87] is used for visual features encoding and BERT [35] performs the textual feature encoding. Contrastive learning is employed on multimodal data by Akbari et al. [109] to train a transformer encoder that processes audio, text, and video simultaneously. \n\nWang et al. [110] proposed the One For All (OFA) method, which unifies tasks and modalities via a sequence-tosequence framework based on Unified vocabulary (for all kinds of the modality). OFA represents data of different modalities in a unified space that discretizes images and text to form a unified output vocabulary. They presented three qualities that a unifying model should support to maintain multitasking for any modality: (1) Task-Agnostic: handcraft instruction-based learning is utilized to achieve this property.",
                    "score": 0.5244769466266627,
                    "section_title": "Transformer-Based Architectures. Recent work on large scale pretraining has inspired the research community to",
                    "char_start_offset": 24156,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 372
                        },
                        {
                            "start": 375,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 610
                        },
                        {
                            "start": 611,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 930
                        },
                        {
                            "start": 931,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1359
                        },
                        {
                            "start": 1360,
                            "end": 1519
                        },
                        {
                            "start": 1522,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1843
                        },
                        {
                            "start": 1844,
                            "end": 2048
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 523,
                            "end": 527,
                            "matchedPaperCorpusId": "216080982"
                        },
                        {
                            "start": 1058,
                            "end": 1063,
                            "matchedPaperCorpusId": "237204499"
                        },
                        {
                            "start": 1265,
                            "end": 1269,
                            "matchedPaperCorpusId": "218889832"
                        },
                        {
                            "start": 1316,
                            "end": 1320,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1429,
                            "end": 1434,
                            "matchedPaperCorpusId": "233346984"
                        },
                        {
                            "start": 1534,
                            "end": 1539,
                            "matchedPaperCorpusId": "246634906"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.771484375
                }
            ],
            "relevance_judgement": 0.771484375,
            "relevance_judgment_input_expanded": "# Title: Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications\n# Venue: ACM Trans. Multim. Comput. Commun. Appl.\n# Authors: Muhammad Arslan Manzoor, S. Albarri, Ziting Xian, Zaiqiao Meng, Preslav Nakov, Shangsong Liang\n## Abstract\nMultimodality Representation Learning, as a technique of learning to embed information from different modalities and their correlations, has achieved remarkable success on a variety of applications, such as Visual Question Answering (VQA), Natural Language for Visual Reasoning (NLVR), and Vision Language Retrieval (VLR). Among these applications, cross-modal interaction and complementary information from different modalities are crucial for advanced models to perform any multimodal task, e.g., understand, recognize, retrieve, or generate optimally. Researchers have proposed diverse methods to address these tasks. The different variants of transformer-based architectures performed extraordinarily on multiple modalities. This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodal tasks. This study summarizes the (i) recent task-specific deep learning methodologies, (ii) the pretraining types and multimodal pretraining objectives, (iii) from state-of-the-art pretrained multimodal approaches to unifying architectures, and (iv) multimodal task categories and possible future improvements that can be devised for better multimodal learning. Moreover, we prepare a dataset section for new researchers that covers most of the benchmarks for pretraining and finetuning. Finally, major challenges, gaps, and potential research topics are explored. A constantly-updated paperlist related to our survey is maintained at https://github.com/marslanm/multimodality-representation-learning.\n## Transformer-Based Architectures. Recent work on large scale pretraining has inspired the research community to\nBLIP-2 [98], an improved visual-language model based on comprehensive pretraining strategy, achieves state-of-the-art performance across numerous vision-language tasks. BLIP-2 exhibits a wide range of zero-shot image-to-text abilities, which include visual knowledge reasoning, visual common sense reasoning, visual conversation, and personalized image-to-text generation. \n\nLike its predecessor, BLIP-2 uses the same evaluation datasets and continues to implement the Captioning and Filtering (CapFilt) method. The UNITER [82] is a pretraining approach conducted at a large scale over four benchmark datasets. Joint embedding from both modalities is learned to perform heterogeneous downstream tasks. MLM, MRM (Masked Region Modeling), ITM (Image-Text Matching), and WRA (Word-Region Alignment) are employed as a pretraining task. Additionally, the pretraining task achieves global image text alignment using Conditional masking. Optimal Transport is the author's second concept for the WRA task to improve the alignment between images and words. Hu et al. [108] proposed a unified transformer for multitasking based on multimodal learning at a time. The proposed architecture uses a specialized encoder for each modality and a shared decoder for every task. DETR [87] is used for visual features encoding and BERT [35] performs the textual feature encoding. Contrastive learning is employed on multimodal data by Akbari et al. [109] to train a transformer encoder that processes audio, text, and video simultaneously. \n\nWang et al. [110] proposed the One For All (OFA) method, which unifies tasks and modalities via a sequence-tosequence framework based on Unified vocabulary (for all kinds of the modality). OFA represents data of different modalities in a unified space that discretizes images and text to form a unified output vocabulary. They presented three qualities that a unifying model should support to maintain multitasking for any modality: (1) Task-Agnostic: handcraft instruction-based learning is utilized to achieve this property.",
            "reference_string": "[256459691 | Manzoor et al. | 2023 | Citations: 32]"
        },
        {
            "title": "Harmonizing Visual Text Comprehension and Generation",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 84,
            "citation_count": 26,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.16364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268032623",
                    "name": "Zhen Zhao"
                },
                {
                    "authorId": "2299523904",
                    "name": "Jingqun Tang"
                },
                {
                    "authorId": "2267874275",
                    "name": "Binghong Wu"
                },
                {
                    "authorId": "2268054519",
                    "name": "Chunhui Lin"
                },
                {
                    "authorId": "2269740461",
                    "name": "Shubo Wei"
                },
                {
                    "authorId": "2268188994",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "2249851862",
                    "name": "Xin Tan"
                },
                {
                    "authorId": "2241419999",
                    "name": "Zhizhong Zhang"
                },
                {
                    "authorId": "2267862616",
                    "name": "Can Huang"
                },
                {
                    "authorId": "2293421634",
                    "name": "Yuan Xie"
                }
            ],
            "abstract": "In this work, we present TextHarmony, a unified and versatile multimodal generative model proficient in comprehending and generating visual text. Simultaneously generating images and texts typically results in performance degradation due to the inherent inconsistency between vision and language modalities. To overcome this challenge, existing approaches resort to modality-specific data for supervised fine-tuning, necessitating distinct model instances. We propose Slide-LoRA, which dynamically aggregates modality-specific and modality-agnostic LoRA experts, partially decoupling the multimodal generation space. Slide-LoRA harmonizes the generation of vision and language within a singular model instance, thereby facilitating a more unified generative process. Additionally, we develop a high-quality image caption dataset, DetailedTextCaps-100K, synthesized with a sophisticated closed-source MLLM to enhance visual text generation capabilities further. Comprehensive experiments across various benchmarks demonstrate the effectiveness of the proposed approach. Empowered by Slide-LoRA, TextHarmony achieves comparable performance to modality-specific fine-tuning results with only a 2% increase in parameters and shows an average improvement of 2.5% in visual text comprehension tasks and 4.0% in visual text generation tasks. Our work delineates the viability of an integrated approach to multimodal generation within the visual text domain, setting a foundation for subsequent inquiries. Code is available at https://github.com/bytedance/TextHarmony.",
            "corpus_id": 271333958,
            "sentences": [
                {
                    "corpus_id": "271333958",
                    "title": "Harmonizing Visual Text Comprehension and Generation",
                    "text": "Visual text comprehension and generation tasks such as scene text detection and recognition [70,54,31,66,53,52,24,49], document understanding [55,23], visual question answering (VQA) [26,15,27,30,50], key information extraction (KIE) [55,23], visual text generation, editing, and erasure [57,6,5] are consistently of significant value for both academic research and practical applications. Recently, remarkable advancements have been achieved in visual text comprehension and generation, driven by the evolution of Multimodal Large Language Models (MLLMs) and diffusion models. Foremost text-centric MLLMs [64,20,27,30] utilize a cohesive framework to comprehend text-rich images comprehensively, whereas diffusion-based approaches [57,6,5] introduce innovative modifications to enhance visual text generation capabilities. As depicted in Figure 1, text-centric MLLMs and diffusion models are capable of handling language and vision modalities adeptly, with MLLMs generating texts and diffusion models producing images. However, integrating language and vision generation capabilities within a large multimodal model for visual text scenarios remains unexplored. This paper focuses on the simultaneous manipulation of language and vision generations to further streamline the processing of diverse text-centric multimodal tasks. In the general multimodal domain, some pioneering efforts [48,16,71,56] empower MLLMs with the ability to generate images beyond texts, vastly extending the versatility of multimodal models. Such advancements inspire us to develop a text-centric multimodal generative model. Our foundational model follows these approaches, incorporating a VIT-based image encoder, a text tokenizer, an LLM, a text detokenizer, and a diffusion-based image decoder. \n\nPrevious works [48,71,56] and our pilot experiments (Figure 2) have shown that multimodal generation often leads to a notable decline in performance due to the substantial inconsistency between language and vision modalities in the generation space. Prior studies [48,16,71,56] commonly rely on modality-specific supervised fine-tuning to bolster generative capacities.",
                    "score": 0.6245975184023917,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 823
                        },
                        {
                            "start": 824,
                            "end": 1019
                        },
                        {
                            "start": 1020,
                            "end": 1162
                        },
                        {
                            "start": 1163,
                            "end": 1328
                        },
                        {
                            "start": 1329,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1776
                        },
                        {
                            "start": 1779,
                            "end": 2028
                        },
                        {
                            "start": 2029,
                            "end": 2148
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 92,
                            "end": 96,
                            "matchedPaperCorpusId": "265351529"
                        },
                        {
                            "start": 96,
                            "end": 99,
                            "matchedPaperCorpusId": "247778841"
                        },
                        {
                            "start": 99,
                            "end": 102,
                            "matchedPaperCorpusId": "255416140"
                        },
                        {
                            "start": 102,
                            "end": 105,
                            "matchedPaperCorpusId": "257232534"
                        },
                        {
                            "start": 105,
                            "end": 108,
                            "matchedPaperCorpusId": "251451612"
                        },
                        {
                            "start": 108,
                            "end": 111,
                            "matchedPaperCorpusId": "251040216"
                        },
                        {
                            "start": 111,
                            "end": 114,
                            "matchedPaperCorpusId": "259951310"
                        },
                        {
                            "start": 114,
                            "end": 117,
                            "matchedPaperCorpusId": "258826110"
                        },
                        {
                            "start": 142,
                            "end": 146,
                            "matchedPaperCorpusId": "254275326"
                        },
                        {
                            "start": 146,
                            "end": 149,
                            "matchedPaperCorpusId": "248228056"
                        },
                        {
                            "start": 234,
                            "end": 238,
                            "matchedPaperCorpusId": "254275326"
                        },
                        {
                            "start": 238,
                            "end": 241,
                            "matchedPaperCorpusId": "248228056"
                        },
                        {
                            "start": 292,
                            "end": 294,
                            "matchedPaperCorpusId": "258762187"
                        },
                        {
                            "start": 736,
                            "end": 738,
                            "matchedPaperCorpusId": "258762187"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76904296875
                }
            ],
            "relevance_judgement": 0.76904296875,
            "relevance_judgment_input_expanded": "# Title: Harmonizing Visual Text Comprehension and Generation\n# Venue: Neural Information Processing Systems\n# Authors: Zhen Zhao, Jingqun Tang, Binghong Wu, Chunhui Lin, Shubo Wei, Hao Liu, Xin Tan, Zhizhong Zhang, Can Huang, Yuan Xie\n## Abstract\nIn this work, we present TextHarmony, a unified and versatile multimodal generative model proficient in comprehending and generating visual text. Simultaneously generating images and texts typically results in performance degradation due to the inherent inconsistency between vision and language modalities. To overcome this challenge, existing approaches resort to modality-specific data for supervised fine-tuning, necessitating distinct model instances. We propose Slide-LoRA, which dynamically aggregates modality-specific and modality-agnostic LoRA experts, partially decoupling the multimodal generation space. Slide-LoRA harmonizes the generation of vision and language within a singular model instance, thereby facilitating a more unified generative process. Additionally, we develop a high-quality image caption dataset, DetailedTextCaps-100K, synthesized with a sophisticated closed-source MLLM to enhance visual text generation capabilities further. Comprehensive experiments across various benchmarks demonstrate the effectiveness of the proposed approach. Empowered by Slide-LoRA, TextHarmony achieves comparable performance to modality-specific fine-tuning results with only a 2% increase in parameters and shows an average improvement of 2.5% in visual text comprehension tasks and 4.0% in visual text generation tasks. Our work delineates the viability of an integrated approach to multimodal generation within the visual text domain, setting a foundation for subsequent inquiries. Code is available at https://github.com/bytedance/TextHarmony.\n## Introduction\nVisual text comprehension and generation tasks such as scene text detection and recognition [70,54,31,66,53,52,24,49], document understanding [55,23], visual question answering (VQA) [26,15,27,30,50], key information extraction (KIE) [55,23], visual text generation, editing, and erasure [57,6,5] are consistently of significant value for both academic research and practical applications. Recently, remarkable advancements have been achieved in visual text comprehension and generation, driven by the evolution of Multimodal Large Language Models (MLLMs) and diffusion models. Foremost text-centric MLLMs [64,20,27,30] utilize a cohesive framework to comprehend text-rich images comprehensively, whereas diffusion-based approaches [57,6,5] introduce innovative modifications to enhance visual text generation capabilities. As depicted in Figure 1, text-centric MLLMs and diffusion models are capable of handling language and vision modalities adeptly, with MLLMs generating texts and diffusion models producing images. However, integrating language and vision generation capabilities within a large multimodal model for visual text scenarios remains unexplored. This paper focuses on the simultaneous manipulation of language and vision generations to further streamline the processing of diverse text-centric multimodal tasks. In the general multimodal domain, some pioneering efforts [48,16,71,56] empower MLLMs with the ability to generate images beyond texts, vastly extending the versatility of multimodal models. Such advancements inspire us to develop a text-centric multimodal generative model. Our foundational model follows these approaches, incorporating a VIT-based image encoder, a text tokenizer, an LLM, a text detokenizer, and a diffusion-based image decoder. \n\nPrevious works [48,71,56] and our pilot experiments (Figure 2) have shown that multimodal generation often leads to a notable decline in performance due to the substantial inconsistency between language and vision modalities in the generation space. Prior studies [48,16,71,56] commonly rely on modality-specific supervised fine-tuning to bolster generative capacities.",
            "reference_string": "[271333958 | Zhao et al. | 2024 | Citations: 26]"
        },
        {
            "title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 34,
            "influential_citation_count": 15,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2259012212",
                    "name": "Ting Jiang"
                },
                {
                    "authorId": "2260342581",
                    "name": "Minghui Song"
                },
                {
                    "authorId": "2260851231",
                    "name": "Zihan Zhang"
                },
                {
                    "authorId": "2146285313",
                    "name": "Haizhen Huang"
                },
                {
                    "authorId": "2066621592",
                    "name": "Weiwei Deng"
                },
                {
                    "authorId": "2247156451",
                    "name": "Feng Sun"
                },
                {
                    "authorId": "2256972722",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2145351849",
                    "name": "Deqing Wang"
                },
                {
                    "authorId": "2162961864",
                    "name": "Fuzhen Zhuang"
                }
            ],
            "abstract": "Multimodal large language models (MLLMs) have shown promising advancements in general visual and language understanding. However, the representation of multimodal information using MLLMs remains largely unexplored. In this work, we introduce a new framework, E5-V, designed to adapt MLLMs for achieving universal multimodal embeddings. Our findings highlight the significant potential of MLLMs in representing multimodal inputs compared to previous approaches. By leveraging MLLMs with prompts, E5-V effectively bridges the modality gap between different types of inputs, demonstrating strong performance in multimodal embeddings even without fine-tuning. We propose a single modality training approach for E5-V, where the model is trained exclusively on text pairs. This method demonstrates significant improvements over traditional multimodal training on image-text pairs, while reducing training costs by approximately 95%. Additionally, this approach eliminates the need for costly multimodal training data collection. Extensive experiments across four types of tasks demonstrate the effectiveness of E5-V. As a universal multimodal model, E5-V not only achieves but often surpasses state-of-the-art performance in each task, despite being trained on a single modality.",
            "corpus_id": 271245054,
            "sentences": [
                {
                    "corpus_id": "271245054",
                    "title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
                    "text": "By designing prompts to project multimodal inputs into the same embedding spaces, we show that MLLMs can represent multimodal inputs correctly even without fine-tuning.\u2022 We introduce a new framework, E5-V, to adapt MLLMs for achieving universal multimodal embeddings.With single modality training on text pairs, E5-v even achieve better multimodal embeddings than image-text pairs.\u2022 Extensive experiments on text-image retrieval and composed image retrieval tasks demonstrate the effectiveness of E5-V in representing multimodal information.E5-V successfully transfers single modality representation capabilities to multimodal embeddings by following task-specific prompts that were not included in the training data.To achieve universal multimodal embeddings, there are several works, such as and UNIIR [WCC + 23], fine-tune CLIP on interlevaed dataset with fusion modal to fuse the visual and language information.There are also some works like VISTA [ZLX + 24] or UniVL-DR [LXL + 22] feed text encoder with CLIP outputs to input visual information.However, it can harm the original text-image retrieval performance of CLIP, and is hard to make text encoder understand the visual information with only contrastive learning, which show poor zero-shot performance on composed image retrieval tasks.\n\nTo achieve universal multimodal embeddings, several works, such as UNIIR [WCC + 23], finetune CLIP with a fusion model to integrate visual and language information.Other works, like VISTA [ZLX + 24] or UniVL-DR [LXL + 22], feed the text embedding models with CLIP outputs to incorporate visual information.However, this approach can harm the original text-image retrieval performance of CLIP and makes it difficult for the text embedding models to understand visual information using only contrastive learning.As a result, these methods show poor zero-shot performance on composed image retrieval tasks.Moreover, these methods require large interleaved training data to achieve universal multimodal embeddings.",
                    "score": 0.5099324632650573,
                    "section_title": "Introduction",
                    "char_start_offset": 3783,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 168,
                            "end": 267
                        },
                        {
                            "start": 267,
                            "end": 381
                        },
                        {
                            "start": 381,
                            "end": 541
                        },
                        {
                            "start": 541,
                            "end": 717
                        },
                        {
                            "start": 717,
                            "end": 916
                        },
                        {
                            "start": 916,
                            "end": 1051
                        },
                        {
                            "start": 1051,
                            "end": 1298
                        },
                        {
                            "start": 1300,
                            "end": 1464
                        },
                        {
                            "start": 1464,
                            "end": 1606
                        },
                        {
                            "start": 1606,
                            "end": 1810
                        },
                        {
                            "start": 1810,
                            "end": 1903
                        },
                        {
                            "start": 1903,
                            "end": 2010
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.765625
                }
            ],
            "relevance_judgement": 0.765625,
            "relevance_judgment_input_expanded": "# Title: E5-V: Universal Embeddings with Multimodal Large Language Models\n# Venue: arXiv.org\n# Authors: Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang\n## Abstract\nMultimodal large language models (MLLMs) have shown promising advancements in general visual and language understanding. However, the representation of multimodal information using MLLMs remains largely unexplored. In this work, we introduce a new framework, E5-V, designed to adapt MLLMs for achieving universal multimodal embeddings. Our findings highlight the significant potential of MLLMs in representing multimodal inputs compared to previous approaches. By leveraging MLLMs with prompts, E5-V effectively bridges the modality gap between different types of inputs, demonstrating strong performance in multimodal embeddings even without fine-tuning. We propose a single modality training approach for E5-V, where the model is trained exclusively on text pairs. This method demonstrates significant improvements over traditional multimodal training on image-text pairs, while reducing training costs by approximately 95%. Additionally, this approach eliminates the need for costly multimodal training data collection. Extensive experiments across four types of tasks demonstrate the effectiveness of E5-V. As a universal multimodal model, E5-V not only achieves but often surpasses state-of-the-art performance in each task, despite being trained on a single modality.\n## Introduction\nBy designing prompts to project multimodal inputs into the same embedding spaces, we show that MLLMs can represent multimodal inputs correctly even without fine-tuning.\u2022 We introduce a new framework, E5-V, to adapt MLLMs for achieving universal multimodal embeddings.With single modality training on text pairs, E5-v even achieve better multimodal embeddings than image-text pairs.\u2022 Extensive experiments on text-image retrieval and composed image retrieval tasks demonstrate the effectiveness of E5-V in representing multimodal information.E5-V successfully transfers single modality representation capabilities to multimodal embeddings by following task-specific prompts that were not included in the training data.To achieve universal multimodal embeddings, there are several works, such as and UNIIR [WCC + 23], fine-tune CLIP on interlevaed dataset with fusion modal to fuse the visual and language information.There are also some works like VISTA [ZLX + 24] or UniVL-DR [LXL + 22] feed text encoder with CLIP outputs to input visual information.However, it can harm the original text-image retrieval performance of CLIP, and is hard to make text encoder understand the visual information with only contrastive learning, which show poor zero-shot performance on composed image retrieval tasks.\n\nTo achieve universal multimodal embeddings, several works, such as UNIIR [WCC + 23], finetune CLIP with a fusion model to integrate visual and language information.Other works, like VISTA [ZLX + 24] or UniVL-DR [LXL + 22], feed the text embedding models with CLIP outputs to incorporate visual information.However, this approach can harm the original text-image retrieval performance of CLIP and makes it difficult for the text embedding models to understand visual information using only contrastive learning.As a result, these methods show poor zero-shot performance on composed image retrieval tasks.Moreover, these methods require large interleaved training data to achieve universal multimodal embeddings.",
            "reference_string": "[271245054 | Jiang et al. | 2024 | Citations: 34]"
        },
        {
            "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 280,
            "citation_count": 548,
            "influential_citation_count": 32,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.04226",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.04226, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2211165440",
                    "name": "Yihan Cao"
                },
                {
                    "authorId": "2211077408",
                    "name": "Siyu Li"
                },
                {
                    "authorId": null,
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2143627170",
                    "name": "Zhiling Yan"
                },
                {
                    "authorId": "31336649",
                    "name": "Yutong Dai"
                },
                {
                    "authorId": "144019071",
                    "name": "Philip S. Yu"
                },
                {
                    "authorId": "46732871",
                    "name": "Lichao Sun"
                }
            ],
            "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.",
            "corpus_id": 257405349,
            "sentences": [
                {
                    "corpus_id": "257405349",
                    "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT",
                    "text": "BLIP [142] combines an encoder and decoder together during pre-training, allowing for multimodal space alignment for both understanding and generation objectives. BLIP is composed of three parts, a unimodal encoder for extracting image and text features, an image-grounded text encoder which accepts image and text features as input, and an image-grounded text decoder, which accepts image features and outputs text. Except for the aligned encoder and decoder structure, the authors also designed several corresponding pre-training tasks to help the model better learn the multimodal dependency. \n\nFrozen deocders. Another way to efficiently perform to-text generation tasks is to freeze the large language model and train an image encoder only, which can also be seen as a way to perform multimodal prompting. Due to the success of prompting and in-context learning in NLP, there has been increased attention towards methods of this nature. This has led people to question whether such methods could be effective in multimodal settings as well. Frozen [143] first introduced incontext learning into vision language tasks. It freezes the language model and only trains the image encoder. The produced image representations will be embeded in the input embeddings of the language model. This method achieves state-of-the-art performance in various zero-shot and few-shot vision language tasks. Later, Alayrac et al. proposed Flamingo [144], which further explored multimodal in-context learning. Flamingo involves a frozen vision encoder and a frozen language encoder to get vision language representations, and utilizes gated cross-attention-dense layer to fuse the image representation into text representation. Recently, [145] proposed a method to realize VL dialogue with frozen language models, enabling the model to generate interleaved multimodal data. This method also freezes input encoders and train text-to-image and image-to-text linear maps to further encode and decode produced embeddings. However, it still remains a question why this kind of prompting based method work in multimodal generation. Some works have also been proposed to answer this question.",
                    "score": 0.5010454843936637,
                    "section_title": "Vision Language Generation.",
                    "char_start_offset": 42059,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 416
                        },
                        {
                            "start": 417,
                            "end": 595
                        },
                        {
                            "start": 598,
                            "end": 614
                        },
                        {
                            "start": 615,
                            "end": 810
                        },
                        {
                            "start": 811,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1045
                        },
                        {
                            "start": 1046,
                            "end": 1122
                        },
                        {
                            "start": 1123,
                            "end": 1187
                        },
                        {
                            "start": 1188,
                            "end": 1285
                        },
                        {
                            "start": 1286,
                            "end": 1392
                        },
                        {
                            "start": 1393,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1712
                        },
                        {
                            "start": 1713,
                            "end": 1858
                        },
                        {
                            "start": 1859,
                            "end": 2002
                        },
                        {
                            "start": 2003,
                            "end": 2110
                        },
                        {
                            "start": 2111,
                            "end": 2170
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 5,
                            "end": 10,
                            "matchedPaperCorpusId": "246411402"
                        },
                        {
                            "start": 1053,
                            "end": 1058,
                            "matchedPaperCorpusId": "235658331"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76318359375
                }
            ],
            "relevance_judgement": 0.76318359375,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT\n# Venue: arXiv.org\n# Authors: Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, Lichao Sun\n## Abstract\nRecently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.\n## Vision Language Generation.\nBLIP [142] combines an encoder and decoder together during pre-training, allowing for multimodal space alignment for both understanding and generation objectives. BLIP is composed of three parts, a unimodal encoder for extracting image and text features, an image-grounded text encoder which accepts image and text features as input, and an image-grounded text decoder, which accepts image features and outputs text. Except for the aligned encoder and decoder structure, the authors also designed several corresponding pre-training tasks to help the model better learn the multimodal dependency. \n\nFrozen deocders. Another way to efficiently perform to-text generation tasks is to freeze the large language model and train an image encoder only, which can also be seen as a way to perform multimodal prompting. Due to the success of prompting and in-context learning in NLP, there has been increased attention towards methods of this nature. This has led people to question whether such methods could be effective in multimodal settings as well. Frozen [143] first introduced incontext learning into vision language tasks. It freezes the language model and only trains the image encoder. The produced image representations will be embeded in the input embeddings of the language model. This method achieves state-of-the-art performance in various zero-shot and few-shot vision language tasks. Later, Alayrac et al. proposed Flamingo [144], which further explored multimodal in-context learning. Flamingo involves a frozen vision encoder and a frozen language encoder to get vision language representations, and utilizes gated cross-attention-dense layer to fuse the image representation into text representation. Recently, [145] proposed a method to realize VL dialogue with frozen language models, enabling the model to generate interleaved multimodal data. This method also freezes input encoders and train text-to-image and image-to-text linear maps to further encode and decode produced embeddings. However, it still remains a question why this kind of prompting based method work in multimodal generation. Some works have also been proposed to answer this question.",
            "reference_string": "[257405349 | Cao et al. | 2023 | Citations: 548]"
        },
        {
            "title": "VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2021,
            "reference_count": 62,
            "citation_count": 67,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2101.12059",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.12059, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48030192",
                    "name": "Xudong Lin"
                },
                {
                    "authorId": "3313330",
                    "name": "Gedas Bertasius"
                },
                {
                    "authorId": "2144536988",
                    "name": "Jue Wang"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "153432684",
                    "name": "Devi Parikh"
                },
                {
                    "authorId": "1732879",
                    "name": "L. Torresani"
                }
            ],
            "abstract": "We present VX2TEXT, a framework for text generation from multimodal inputs consisting of video plus text, speech, or audio. In order to leverage transformer networks, which have been shown to be effective at modeling language, each modality is first converted into a set of language embeddings by a learnable tokenizer. This allows our approach to perform multimodal fusion in the language space, thus eliminating the need for ad-hoc cross-modal fusion modules. To address the non-differentiability of tokenization on continuous inputs (e.g., video or audio), we utilize a relaxation scheme that enables end-to-end training. Furthermore, unlike prior encoder-only models, our network includes an autoregressive decoder to generate open-ended text from the multimodal embeddings fused by the language encoder. This renders our approach fully generative and makes it directly applicable to different \"video+x to text\" problems without the need to design specialized network heads for each task. The proposed framework is not only conceptually simple but also remarkably effective: experiments demonstrate that our approach based on a single architecture outperforms the state-of-the-art on three videobased text-generation tasks\u2014captioning, question answering and audio-visual scene-aware dialog.",
            "corpus_id": 231719672,
            "sentences": [
                {
                    "corpus_id": "231719672",
                    "title": "VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs",
                    "text": "We present VX2TEXT, a framework for text generation from multimodal inputs consisting of video plus text, speech, or audio. In order to leverage transformer networks, which have been shown to be effective at modeling language, each modality is first converted into a set of language embeddings by a learnable tokenizer. This allows our approach to perform multimodal fusion in the language space, thus eliminating the need for ad-hoc cross-modal fusion modules. To address the non-differentiability of tokenization on continuous inputs (e.g., video or audio), we utilize a relaxation scheme that enables end-to-end training. Furthermore, unlike prior encoder-only models, our network includes an autoregressive decoder to generate open-ended text from the multimodal embeddings fused by the language encoder. This renders our approach fully generative and makes it directly applicable to different \"video+x to text\" problems without the need to design specialized network heads for each task. The proposed framework is not only conceptually simple but also remarkably effective: experiments demonstrate that our approach based on a single architecture outperforms the state-of-the-art on three videobased text-generation tasks\u2014captioning, question answering and audio-visual scene-aware dialog.",
                    "score": 0.6028623705668864,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7587890625
                }
            ],
            "relevance_judgement": 0.7587890625,
            "relevance_judgment_input_expanded": "# Title: VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh, L. Torresani\n## Abstract\nWe present VX2TEXT, a framework for text generation from multimodal inputs consisting of video plus text, speech, or audio. In order to leverage transformer networks, which have been shown to be effective at modeling language, each modality is first converted into a set of language embeddings by a learnable tokenizer. This allows our approach to perform multimodal fusion in the language space, thus eliminating the need for ad-hoc cross-modal fusion modules. To address the non-differentiability of tokenization on continuous inputs (e.g., video or audio), we utilize a relaxation scheme that enables end-to-end training. Furthermore, unlike prior encoder-only models, our network includes an autoregressive decoder to generate open-ended text from the multimodal embeddings fused by the language encoder. This renders our approach fully generative and makes it directly applicable to different \"video+x to text\" problems without the need to design specialized network heads for each task. The proposed framework is not only conceptually simple but also remarkably effective: experiments demonstrate that our approach based on a single architecture outperforms the state-of-the-art on three videobased text-generation tasks\u2014captioning, question answering and audio-visual scene-aware dialog.\n",
            "reference_string": "[231719672 | Lin et al. | 2021 | Citations: 67]"
        },
        {
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 229,
            "citation_count": 1819,
            "influential_citation_count": 106,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.10997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2275341478",
                    "name": "Xinyu Gao"
                },
                {
                    "authorId": "2275191447",
                    "name": "Kangxiang Jia"
                },
                {
                    "authorId": "2275530552",
                    "name": "Jinliu Pan"
                },
                {
                    "authorId": "2275171009",
                    "name": "Yuxi Bi"
                },
                {
                    "authorId": "2276187454",
                    "name": "Yi Dai"
                },
                {
                    "authorId": "2275540959",
                    "name": "Jiawei Sun"
                },
                {
                    "authorId": "2258800561",
                    "name": "Qianyu Guo"
                },
                {
                    "authorId": "2291409458",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
            "corpus_id": 266359151,
            "sentences": [
                {
                    "corpus_id": "266359151",
                    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
                    "text": "RAG has transcended its initial text-based questionanswering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains: \n\nImage. RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2 [177] leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zeroshot image-to-text conversions. The \"Visualize Before You Write\" method [178] employs image generation to steer the LM's text generation, showing promise in open-ended text generation tasks. \n\nAudio and Video. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data [179]. UEOP marks a significant advancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text conversion [180]. Additionally, KNN-based attention fusion leverages audio embeddings and semantically related text embeddings to refine ASR, thereby accelerating domain adaptation. \n\nVid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence [181]. \n\nCode. RBPS [182] excels in small-scale learning tasks by retrieving code examples that align with developers' objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion generation and program repair. For structured knowledge, the CoK method [106] first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks.",
                    "score": 0.51028141078864,
                    "section_title": "F. Multi-modal RAG",
                    "char_start_offset": 60334,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 219
                        },
                        {
                            "start": 222,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 481
                        },
                        {
                            "start": 482,
                            "end": 640
                        },
                        {
                            "start": 643,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 950
                        },
                        {
                            "start": 951,
                            "end": 1114
                        },
                        {
                            "start": 1117,
                            "end": 1297
                        },
                        {
                            "start": 1300,
                            "end": 1305
                        },
                        {
                            "start": 1306,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1807
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1291,
                            "end": 1296,
                            "matchedPaperCorpusId": "257232853"
                        },
                        {
                            "start": 1311,
                            "end": 1316,
                            "matchedPaperCorpusId": "259860357"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75732421875
                }
            ],
            "relevance_judgement": 0.75732421875,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang\n## Abstract\nLarge Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\n## F. Multi-modal RAG\nRAG has transcended its initial text-based questionanswering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains: \n\nImage. RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2 [177] leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zeroshot image-to-text conversions. The \"Visualize Before You Write\" method [178] employs image generation to steer the LM's text generation, showing promise in open-ended text generation tasks. \n\nAudio and Video. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data [179]. UEOP marks a significant advancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text conversion [180]. Additionally, KNN-based attention fusion leverages audio embeddings and semantically related text embeddings to refine ASR, thereby accelerating domain adaptation. \n\nVid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence [181]. \n\nCode. RBPS [182] excels in small-scale learning tasks by retrieving code examples that align with developers' objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion generation and program repair. For structured knowledge, the CoK method [106] first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks.",
            "reference_string": "[266359151 | Gao et al. | 2023 | Citations: 1819]"
        },
        {
            "title": "ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 13,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.09760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311429185",
                    "name": "Yixiao Yuan"
                },
                {
                    "authorId": "2275279906",
                    "name": "Yingzhe Peng"
                }
            ],
            "abstract": "The Visual-Dialog Based Emotion Explanation Generation Challenge focuses on generating emotion explanations through visual-dialog interactions in art discussions. Our approach combines state-of-the-art multi-modal models, including Language Model (LM) and Large Vision Language Model (LVLM), to achieve superior performance. By leveraging these models, we outperform existing benchmarks, securing the top rank in the ICCV23 Visual-Dialog Based Emotion Explanation Generation Challenge, which is part of the 5th Workshop On Closing The Loop Between Vision And Language (CLCV) with significant scores in F1 and BLEU metrics. Our method demonstrates exceptional ability in generating accurate emotion explanations, advancing our understanding of emotional impacts in art.",
            "corpus_id": 271213320,
            "sentences": [
                {
                    "corpus_id": "271213320",
                    "title": "ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report",
                    "text": "Multimodal learning in the intersection of visual and linguistic data has seen significant advancements in recent years. This area of research focuses on developing models that can understand and generate content by integrating information from both visual inputs, such as images or videos, and textual descriptions. These models leverage the complementary nature of visual and textual data to perform a wide range of tasks, including but not limited to image captioning, visual question answering, and cross-modal information retrieval. \n\nOne of the key innovations in this field has been the development of models that are trained on vast datasets of image-text pairs. [12,7,8] These models employ contrastive learning techniques to align the representations of images and their corresponding textual descriptions in a shared embedding space. By doing so, they learn to associate specific visual features with relevant linguistic concepts, enabling them to understand complex queries and content with a high degree of accuracy. Moreover, the use of transformer-based [13] architectures has further enhanced the capabilities of these models. Transformers provide a flexible and powerful framework for modeling sequential data, and their adoption in multimodal learning has allowed for the effective integration of visual and textual inputs. This has led to significant improvements in tasks requiring nuanced understanding and generation of content, such as generating descriptive captions for images that accurately reflect their content and context or answering questions based on visual information. \n\nThe emergence of Large Language Models (LLMs) has also marked a significant milestone in the field of artificial intelligence, particularly in natural language processing. Models like GPT [1], BERT [4], and others have demonstrated exceptional abilities in generating and understanding text, significantly advancing the capabilities of AI in understanding human languages. These models have been instrumental in a variety of applications, including text generation, translation, and semantic analysis, showcasing their versatility and powerful computational abilities. However, their primary limitation lies in their text-only nature, confining their applicability to tasks that do not require understanding or generating content in other modalities such as images, speech, and videos. \n\nTo bridge this gap, the research and development of Large Vision Language Models (LVLMs) [9,2,10,15,14] have gained momentum.",
                    "score": 0.5339642430741416,
                    "section_title": "Related Work",
                    "char_start_offset": 2249,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 316
                        },
                        {
                            "start": 317,
                            "end": 537
                        },
                        {
                            "start": 540,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1142
                        },
                        {
                            "start": 1143,
                            "end": 1341
                        },
                        {
                            "start": 1342,
                            "end": 1603
                        },
                        {
                            "start": 1606,
                            "end": 1777
                        },
                        {
                            "start": 1778,
                            "end": 1978
                        },
                        {
                            "start": 1979,
                            "end": 2174
                        },
                        {
                            "start": 2175,
                            "end": 2391
                        },
                        {
                            "start": 2394,
                            "end": 2519
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 677,
                            "end": 679,
                            "matchedPaperCorpusId": "246411402"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75634765625
                }
            ],
            "relevance_judgement": 0.75634765625,
            "relevance_judgment_input_expanded": "# Title: ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report\n# Venue: arXiv.org\n# Authors: Yixiao Yuan, Yingzhe Peng\n## Abstract\nThe Visual-Dialog Based Emotion Explanation Generation Challenge focuses on generating emotion explanations through visual-dialog interactions in art discussions. Our approach combines state-of-the-art multi-modal models, including Language Model (LM) and Large Vision Language Model (LVLM), to achieve superior performance. By leveraging these models, we outperform existing benchmarks, securing the top rank in the ICCV23 Visual-Dialog Based Emotion Explanation Generation Challenge, which is part of the 5th Workshop On Closing The Loop Between Vision And Language (CLCV) with significant scores in F1 and BLEU metrics. Our method demonstrates exceptional ability in generating accurate emotion explanations, advancing our understanding of emotional impacts in art.\n## Related Work\nMultimodal learning in the intersection of visual and linguistic data has seen significant advancements in recent years. This area of research focuses on developing models that can understand and generate content by integrating information from both visual inputs, such as images or videos, and textual descriptions. These models leverage the complementary nature of visual and textual data to perform a wide range of tasks, including but not limited to image captioning, visual question answering, and cross-modal information retrieval. \n\nOne of the key innovations in this field has been the development of models that are trained on vast datasets of image-text pairs. [12,7,8] These models employ contrastive learning techniques to align the representations of images and their corresponding textual descriptions in a shared embedding space. By doing so, they learn to associate specific visual features with relevant linguistic concepts, enabling them to understand complex queries and content with a high degree of accuracy. Moreover, the use of transformer-based [13] architectures has further enhanced the capabilities of these models. Transformers provide a flexible and powerful framework for modeling sequential data, and their adoption in multimodal learning has allowed for the effective integration of visual and textual inputs. This has led to significant improvements in tasks requiring nuanced understanding and generation of content, such as generating descriptive captions for images that accurately reflect their content and context or answering questions based on visual information. \n\nThe emergence of Large Language Models (LLMs) has also marked a significant milestone in the field of artificial intelligence, particularly in natural language processing. Models like GPT [1], BERT [4], and others have demonstrated exceptional abilities in generating and understanding text, significantly advancing the capabilities of AI in understanding human languages. These models have been instrumental in a variety of applications, including text generation, translation, and semantic analysis, showcasing their versatility and powerful computational abilities. However, their primary limitation lies in their text-only nature, confining their applicability to tasks that do not require understanding or generating content in other modalities such as images, speech, and videos. \n\nTo bridge this gap, the research and development of Large Vision Language Models (LVLMs) [9,2,10,15,14] have gained momentum.",
            "reference_string": "[271213320 | Yuan et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Spider: Any-to-Many Multimodal LLM",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 75,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.09439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "15037661",
                    "name": "Jinxiang Lai"
                },
                {
                    "authorId": "2256311901",
                    "name": "Jie Zhang"
                },
                {
                    "authorId": "2330814187",
                    "name": "Jun Liu"
                },
                {
                    "authorId": "2331308958",
                    "name": "Jian Li"
                },
                {
                    "authorId": "2152840238",
                    "name": "Xiaocheng Lu"
                },
                {
                    "authorId": "2286409062",
                    "name": "Song Guo"
                }
            ],
            "abstract": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models (LLMs), enabling the integration of various modalities. However, Any-to-Any MLLMs are limited to generating pairwise modalities 'Text + X' within a single response, such as Text + {Image or Audio or Video}. To address this limitation, we introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG) framework, which can generate an arbitrary combination of modalities 'Text + Xs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our Spider integrates three core components: a Base Model for basic X-to-X (i.e., Any-to-Any) modality processing, an Any-to-Many Instruction Template designed for producing Xs signal prompts, and a novel Efficient Decoders-Controller for controlling multimodal Decoders to generate Xs (many-modal) contents. To train Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset, which facilitates learning the X-to-Xs (i.e., Any-to-Many) capability necessary for AMMG. Ultimately, the well-trained Spider generates a pseudo X-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the potential for AMMG tasks in future research. Overall, this work not only pushes the boundary of multimodal interaction but also provides rich data support for advancing the field. Code: https://github.com/Layjins/Spider",
            "corpus_id": 274023123,
            "sentences": [
                {
                    "corpus_id": "274023123",
                    "title": "Spider: Any-to-Many Multimodal LLM",
                    "text": "To build foundational Multimodal LLMs (MLLMs), researchers align pre-trained encoders from various modalities with the textual space of LLMs, enabling them to process multimodal inputs [2,23,28,30,37,53,73]. For example, Flamingo [2] connects a fixed image encoder to LLMs using cross-attention, while LLaVA [37] links image and word spaces via projection. BLIP-2 [30] uses a Q-Former to translate image queries into LLMs. Similar approaches are applied to videos (e.g., Video-Chat [31], Video-LLaMA [71]) and audios (e.g., SpeechGPT [70]). PandaGPT [54] extends this to six modalities using ImageBind [17]. \n\nHowever, existing MLLMs only perceive multimodal data and cannot generate content in arbitrary modalities. To address this, approaches like Visual-ChatGPT [64], Hug-gingGPT [50], and AudioGPT [22] use LLMs as decisionmakers, incorporating external multimodal encoders and decoders for multimodal input-output. Despite this, discrete text-message-based pipelines can introduce noise and hinder semantic understanding. NExT-GPT [66] overcomes this by learning an end-to-end multimodal input-output LLM, capable of handling any combination of text, image, video, and audio. \n\nHowever, these X-to-X MLLMs are limited to generating pairwise modalities 'Text + X' within a single interaction. In contrast, our proposed X-to-Xs Spider model aims for Any-to-Many Modalities Generation in a single response, supporting arbitrary combinations of a wider range of modalities as shown in Fig. 8, including text, image, audio, video, box, and mask.",
                    "score": 0.5296134886352166,
                    "section_title": "C.2. Large Multimodal Models",
                    "char_start_offset": 23157,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 540
                        },
                        {
                            "start": 541,
                            "end": 607
                        },
                        {
                            "start": 610,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 919
                        },
                        {
                            "start": 920,
                            "end": 1026
                        },
                        {
                            "start": 1027,
                            "end": 1180
                        },
                        {
                            "start": 1183,
                            "end": 1296
                        },
                        {
                            "start": 1297,
                            "end": 1545
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 185,
                            "end": 188,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 194,
                            "end": 197,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 230,
                            "end": 233,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 364,
                            "end": 368,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 602,
                            "end": 606,
                            "matchedPaperCorpusId": "258564264"
                        },
                        {
                            "start": 1036,
                            "end": 1040,
                            "matchedPaperCorpusId": "261696650"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.751953125
                }
            ],
            "relevance_judgement": 0.751953125,
            "relevance_judgment_input_expanded": "# Title: Spider: Any-to-Many Multimodal LLM\n# Venue: arXiv.org\n# Authors: Jinxiang Lai, Jie Zhang, Jun Liu, Jian Li, Xiaocheng Lu, Song Guo\n## Abstract\nMultimodal LLMs (MLLMs) have emerged as an extension of Large Language Models (LLMs), enabling the integration of various modalities. However, Any-to-Any MLLMs are limited to generating pairwise modalities 'Text + X' within a single response, such as Text + {Image or Audio or Video}. To address this limitation, we introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG) framework, which can generate an arbitrary combination of modalities 'Text + Xs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our Spider integrates three core components: a Base Model for basic X-to-X (i.e., Any-to-Any) modality processing, an Any-to-Many Instruction Template designed for producing Xs signal prompts, and a novel Efficient Decoders-Controller for controlling multimodal Decoders to generate Xs (many-modal) contents. To train Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset, which facilitates learning the X-to-Xs (i.e., Any-to-Many) capability necessary for AMMG. Ultimately, the well-trained Spider generates a pseudo X-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the potential for AMMG tasks in future research. Overall, this work not only pushes the boundary of multimodal interaction but also provides rich data support for advancing the field. Code: https://github.com/Layjins/Spider\n## C.2. Large Multimodal Models\nTo build foundational Multimodal LLMs (MLLMs), researchers align pre-trained encoders from various modalities with the textual space of LLMs, enabling them to process multimodal inputs [2,23,28,30,37,53,73]. For example, Flamingo [2] connects a fixed image encoder to LLMs using cross-attention, while LLaVA [37] links image and word spaces via projection. BLIP-2 [30] uses a Q-Former to translate image queries into LLMs. Similar approaches are applied to videos (e.g., Video-Chat [31], Video-LLaMA [71]) and audios (e.g., SpeechGPT [70]). PandaGPT [54] extends this to six modalities using ImageBind [17]. \n\nHowever, existing MLLMs only perceive multimodal data and cannot generate content in arbitrary modalities. To address this, approaches like Visual-ChatGPT [64], Hug-gingGPT [50], and AudioGPT [22] use LLMs as decisionmakers, incorporating external multimodal encoders and decoders for multimodal input-output. Despite this, discrete text-message-based pipelines can introduce noise and hinder semantic understanding. NExT-GPT [66] overcomes this by learning an end-to-end multimodal input-output LLM, capable of handling any combination of text, image, video, and audio. \n\nHowever, these X-to-X MLLMs are limited to generating pairwise modalities 'Text + X' within a single interaction. In contrast, our proposed X-to-Xs Spider model aims for Any-to-Many Modalities Generation in a single response, supporting arbitrary combinations of a wider range of modalities as shown in Fig. 8, including text, image, audio, video, box, and mask.",
            "reference_string": "[274023123 | Lai et al. | 2024 | Citations: 2]"
        },
        {
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 55,
            "citation_count": 37,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.09251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2274151094",
                    "name": "Jinguo Zhu"
                },
                {
                    "authorId": "2258784456",
                    "name": "Xiaohan Ding"
                },
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "51123495",
                    "name": "Yuying Ge"
                },
                {
                    "authorId": "2254048096",
                    "name": "Sijie Zhao"
                },
                {
                    "authorId": "2310758544",
                    "name": "Hengshuang Zhao"
                },
                {
                    "authorId": "2274490604",
                    "name": "Xiaohua Wang"
                },
                {
                    "authorId": "2265579883",
                    "name": "Ying Shan"
                }
            ],
            "abstract": "In this work, we introduce Vision-Language Generative Pre-trained Transformer (VL-GPT), a transformer model proficient at concurrently perceiving and generating visual and linguistic data. VL-GPT achieves a unified pre-training approach for both image and text modalities by employing a straightforward auto-regressive objective, thereby enabling the model to process image and text as seamlessly as a language model processes text. To accomplish this, we initially propose a novel image tokenizer-detokenizer framework for visual data, specifically designed to transform raw images into a sequence of continuous embeddings and reconstruct them accordingly. In combination with the existing text tokenizer and detokenizer, this framework allows for the encoding of interleaved image-text data into a multimodal sequence, which can subsequently be fed into the transformer model. Consequently, VL-GPT can perform large-scale pre-training on multimodal corpora utilizing a unified auto-regressive objective (i.e., next-token prediction). Upon completion of pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance across a diverse range of vision and language understanding and generation tasks, including image captioning, visual question answering, text-to-image generation, and more. Additionally, the pre-trained model retrains in-context learning capabilities when provided with multimodal prompts. We further conduct instruction tuning on our VL-GPT, highlighting its exceptional potential for multimodal assistance. The source code and model weights shall be released.",
            "corpus_id": 266210376,
            "sentences": [
                {
                    "corpus_id": "266210376",
                    "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
                    "text": "Contrary to previous works [9,18,30,48], all embeddings in the multimodal sequence can receive supervision from the auto-regressive loss. During the generation stage, visual embeddings and text tokens can be generated auto-regressively without distinction, and subsequently decoded into raw images and text by the image detokenizer and text detokenizer, respectively. \n\nOwing to the unified modeling, the pre-training of the VL model can be conducted on large-scale image-text pairs and interleaved image-text data. Upon completion of pretraining, the model is capable of perceiving arbitrary multimodal input and generating responses varying in modalities (e.g., text, images or their interleaved contents), allowing it to generalize to a wide range of vision and language understanding and generation tasks in a zero-shot or few-shot manner. Moreover, the pre-trained model exhibits appealing emergent properties for multimodal in-context learning, as it can effectively tackle new unseen tasks when provided with multimodal prompts. The VL generative pre-trained transformer model, referred to as VL-GPT, holds the potential to serve as a powerful foundation model for the multimodal community, similar to the role of GPT family [4,29] in NLP. Our contributions are summarized as follows: \n\n\u2022 We propose an image tokenizer-detokenizer framework to convert images into continuous embeddings and reconstruct them, while exploring effective training methods for this framework. Through efficient training that requires an affordable computational cost, the image tokenizer and detokenizer can effectively retain both semantic information and pixel details of the original image. \u2022 We introduce VL-GPT, a generative pre-trained trans-former model for vision and language (VL) understanding and generation tasks. The model can be pre-trained on large-scale multimodal corpora in a unified autoregressive manner, i.e., predicting the next token in a multimodal sequence containing continuous visual embeddings and discrete text tokens without any discrimination.",
                    "score": 0.5102854253629042,
                    "section_title": "Introduction",
                    "char_start_offset": 3629,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 367
                        },
                        {
                            "start": 370,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1291
                        },
                        {
                            "start": 1294,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1678
                        },
                        {
                            "start": 1679,
                            "end": 1810
                        },
                        {
                            "start": 1811,
                            "end": 2059
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7509765625
                },
                {
                    "corpus_id": "266210376",
                    "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
                    "text": "VL-GPT aims to process the vision and language understanding and generation within a single transformer model in a unified way, similar to GPT handles language tasks. It is capable of perceiving the interleaved multi-modal data and generating content across various modalities. By employing unified modeling, our VL-GPT can conduct autoregressive pre-training on web-scale multimodal corpora, thereby holding the potential to serve as a powerful foundation model in the multimodal research community. \n\nArchitecture As depicted at the bottom of Fig. 1, our VL-GPT comprises five components: a large vision-language transformer model M, an image tokenizer E v , a text tokenizer E t , an image detokenizer D v and a text detokenizer D t . In comparison to a language model, VL-GPT incorporates additional image tokenizer and image detokenizer elements. \n\nGiven any interleaved image-text data, the image tokenizer and the text tokenizer initially encode them into a multimodal sequence. More specifically, the image tokenizer E v converts each image into N continuous visual embeddings x v . Additionally, two special tokens [IMG] and [/IMG] are appended at the beginning and end of the visual embeddings, respectively. The visual embeddings are then combined with the discrete text tokens encoded by the text tokenizer E t to form a interleaved multimodal sequence v = (v 1 , v 2 , . . . , v n ), where v i can be either a discrete text token or a continuous visual embedding. The multimodal sequence v is then fed into the large VL model M for unified auto-regressive modeling. \n\nThe output embedding M(v i ) can be flexibly transformed into a text embedding through a language modeling head for the predefined vocabulary or into a visual embedding with a separate regression head. During training, the selection of the transformed head depends on whether the target for the current embedding is a text token or a visual embedding.",
                    "score": 0.6372625951538424,
                    "section_title": "VL-GPT",
                    "char_start_offset": 13294,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 167,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 500
                        },
                        {
                            "start": 503,
                            "end": 737
                        },
                        {
                            "start": 738,
                            "end": 851
                        },
                        {
                            "start": 854,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1218
                        },
                        {
                            "start": 1219,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1578
                        },
                        {
                            "start": 1581,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 1932
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74853515625
                }
            ],
            "relevance_judgement": 0.7509765625,
            "relevance_judgment_input_expanded": "# Title: VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation\n# Venue: arXiv.org\n# Authors: Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, Ying Shan\n## Abstract\nIn this work, we introduce Vision-Language Generative Pre-trained Transformer (VL-GPT), a transformer model proficient at concurrently perceiving and generating visual and linguistic data. VL-GPT achieves a unified pre-training approach for both image and text modalities by employing a straightforward auto-regressive objective, thereby enabling the model to process image and text as seamlessly as a language model processes text. To accomplish this, we initially propose a novel image tokenizer-detokenizer framework for visual data, specifically designed to transform raw images into a sequence of continuous embeddings and reconstruct them accordingly. In combination with the existing text tokenizer and detokenizer, this framework allows for the encoding of interleaved image-text data into a multimodal sequence, which can subsequently be fed into the transformer model. Consequently, VL-GPT can perform large-scale pre-training on multimodal corpora utilizing a unified auto-regressive objective (i.e., next-token prediction). Upon completion of pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance across a diverse range of vision and language understanding and generation tasks, including image captioning, visual question answering, text-to-image generation, and more. Additionally, the pre-trained model retrains in-context learning capabilities when provided with multimodal prompts. We further conduct instruction tuning on our VL-GPT, highlighting its exceptional potential for multimodal assistance. The source code and model weights shall be released.\n## Introduction\nContrary to previous works [9,18,30,48], all embeddings in the multimodal sequence can receive supervision from the auto-regressive loss. During the generation stage, visual embeddings and text tokens can be generated auto-regressively without distinction, and subsequently decoded into raw images and text by the image detokenizer and text detokenizer, respectively. \n\nOwing to the unified modeling, the pre-training of the VL model can be conducted on large-scale image-text pairs and interleaved image-text data. Upon completion of pretraining, the model is capable of perceiving arbitrary multimodal input and generating responses varying in modalities (e.g., text, images or their interleaved contents), allowing it to generalize to a wide range of vision and language understanding and generation tasks in a zero-shot or few-shot manner. Moreover, the pre-trained model exhibits appealing emergent properties for multimodal in-context learning, as it can effectively tackle new unseen tasks when provided with multimodal prompts. The VL generative pre-trained transformer model, referred to as VL-GPT, holds the potential to serve as a powerful foundation model for the multimodal community, similar to the role of GPT family [4,29] in NLP. Our contributions are summarized as follows: \n\n\u2022 We propose an image tokenizer-detokenizer framework to convert images into continuous embeddings and reconstruct them, while exploring effective training methods for this framework. Through efficient training that requires an affordable computational cost, the image tokenizer and detokenizer can effectively retain both semantic information and pixel details of the original image. \u2022 We introduce VL-GPT, a generative pre-trained trans-former model for vision and language (VL) understanding and generation tasks. The model can be pre-trained on large-scale multimodal corpora in a unified autoregressive manner, i.e., predicting the next token in a multimodal sequence containing continuous visual embeddings and discrete text tokens without any discrimination.\n\n## VL-GPT\nVL-GPT aims to process the vision and language understanding and generation within a single transformer model in a unified way, similar to GPT handles language tasks. It is capable of perceiving the interleaved multi-modal data and generating content across various modalities. By employing unified modeling, our VL-GPT can conduct autoregressive pre-training on web-scale multimodal corpora, thereby holding the potential to serve as a powerful foundation model in the multimodal research community. \n\nArchitecture As depicted at the bottom of Fig. 1, our VL-GPT comprises five components: a large vision-language transformer model M, an image tokenizer E v , a text tokenizer E t , an image detokenizer D v and a text detokenizer D t . In comparison to a language model, VL-GPT incorporates additional image tokenizer and image detokenizer elements. \n\nGiven any interleaved image-text data, the image tokenizer and the text tokenizer initially encode them into a multimodal sequence. More specifically, the image tokenizer E v converts each image into N continuous visual embeddings x v . Additionally, two special tokens [IMG] and [/IMG] are appended at the beginning and end of the visual embeddings, respectively. The visual embeddings are then combined with the discrete text tokens encoded by the text tokenizer E t to form a interleaved multimodal sequence v = (v 1 , v 2 , . . . , v n ), where v i can be either a discrete text token or a continuous visual embedding. The multimodal sequence v is then fed into the large VL model M for unified auto-regressive modeling. \n\nThe output embedding M(v i ) can be flexibly transformed into a text embedding through a language modeling head for the predefined vocabulary or into a visual embedding with a separate regression head. During training, the selection of the transformed head depends on whether the target for the current embedding is a text token or a visual embedding.",
            "reference_string": "[266210376 | Zhu et al. | 2023 | Citations: 37]"
        },
        {
            "title": "Recall: Empowering Multimodal Embedding for Edge Devices",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 70,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15342, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1751449",
                    "name": "Dongqi Cai"
                },
                {
                    "authorId": "2166005800",
                    "name": "Shangguang Wang"
                },
                {
                    "authorId": "2323394790",
                    "name": "Chen Peng"
                },
                {
                    "authorId": "2280326945",
                    "name": "Zeling Zhang"
                },
                {
                    "authorId": "2239403838",
                    "name": "Mengwei Xu"
                }
            ],
            "abstract": "Human memory is inherently prone to forgetting. To address this, multimodal embedding models have been introduced, which transform diverse real-world data into a unified embedding space. These embeddings can be retrieved efficiently, aiding mobile users in recalling past information. However, as model complexity grows, so do its resource demands, leading to reduced throughput and heavy computational requirements that limit mobile device implementation. In this paper, we introduce RECALL, a novel on-device multimodal embedding system optimized for resource-limited mobile environments. RECALL achieves high-throughput, accurate retrieval by generating coarse-grained embeddings and leveraging query-based filtering for refined retrieval. Experimental results demonstrate that RECALL delivers high-quality embeddings with superior throughput, all while operating unobtrusively with minimal memory and energy consumption.",
            "corpus_id": 272831684,
            "sentences": [
                {
                    "corpus_id": "272831684",
                    "title": "Recall: Empowering Multimodal Embedding for Edge Devices",
                    "text": "Unified multimodal embedding Embedding was initially proposed to vectorize text data for understanding similarities between different texts [32]. Large language models use embedding layers to generate text embeddings [12,33]. Similarly, vision, audio, and sensor data can also be transformed into vectorized embeddings [34][35][36]. However, embedding methods focused on a single modality cannot access information across different modalities due to the gap between their embedding spaces. \n\nTo bridge this gap, multimodal embedding models (MEMs) have been developed to unify different modalities into a single embedding space, enhancing the model's ability to understand and bind multimodal inputs. CLIP [10] aligns text and vision by jointly training on image-text pairs, using contrastive learning to map both modalities into a shared space while maintaining their distinction through a dualtower architecture. ImageBind [11] extends this to align six modalities, including text, vision, audio, depth, thermal, and IMU readings. Each modality is processed by a separate encoder, and the embeddings are fused in a multimodal head to generate a unified embedding. ImageBind demonstrates strong zero-shot classification and retrieval performance across these modalities, matching or outperforming singlemodality models. This is achieved through training on largescale multimodal data.",
                    "score": 0.5906713713230757,
                    "section_title": "Background and Motivations 2.1 Multimodal Embedding",
                    "char_start_offset": 9790,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 489
                        },
                        {
                            "start": 492,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1164
                        },
                        {
                            "start": 1165,
                            "end": 1319
                        },
                        {
                            "start": 1320,
                            "end": 1384
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 217,
                            "end": 221,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 327,
                            "end": 331,
                            "matchedPaperCorpusId": "267628000"
                        },
                        {
                            "start": 705,
                            "end": 709,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 924,
                            "end": 928,
                            "matchedPaperCorpusId": "258564264"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75
                }
            ],
            "relevance_judgement": 0.75,
            "relevance_judgment_input_expanded": "# Title: Recall: Empowering Multimodal Embedding for Edge Devices\n# Venue: arXiv.org\n# Authors: Dongqi Cai, Shangguang Wang, Chen Peng, Zeling Zhang, Mengwei Xu\n## Abstract\nHuman memory is inherently prone to forgetting. To address this, multimodal embedding models have been introduced, which transform diverse real-world data into a unified embedding space. These embeddings can be retrieved efficiently, aiding mobile users in recalling past information. However, as model complexity grows, so do its resource demands, leading to reduced throughput and heavy computational requirements that limit mobile device implementation. In this paper, we introduce RECALL, a novel on-device multimodal embedding system optimized for resource-limited mobile environments. RECALL achieves high-throughput, accurate retrieval by generating coarse-grained embeddings and leveraging query-based filtering for refined retrieval. Experimental results demonstrate that RECALL delivers high-quality embeddings with superior throughput, all while operating unobtrusively with minimal memory and energy consumption.\n## Background and Motivations 2.1 Multimodal Embedding\nUnified multimodal embedding Embedding was initially proposed to vectorize text data for understanding similarities between different texts [32]. Large language models use embedding layers to generate text embeddings [12,33]. Similarly, vision, audio, and sensor data can also be transformed into vectorized embeddings [34][35][36]. However, embedding methods focused on a single modality cannot access information across different modalities due to the gap between their embedding spaces. \n\nTo bridge this gap, multimodal embedding models (MEMs) have been developed to unify different modalities into a single embedding space, enhancing the model's ability to understand and bind multimodal inputs. CLIP [10] aligns text and vision by jointly training on image-text pairs, using contrastive learning to map both modalities into a shared space while maintaining their distinction through a dualtower architecture. ImageBind [11] extends this to align six modalities, including text, vision, audio, depth, thermal, and IMU readings. Each modality is processed by a separate encoder, and the embeddings are fused in a multimodal head to generate a unified embedding. ImageBind demonstrates strong zero-shot classification and retrieval performance across these modalities, matching or outperforming singlemodality models. This is achieved through training on largescale multimodal data.",
            "reference_string": "[272831684 | Cai et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Autoregressive Models in Vision: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 241,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.05902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324168511",
                    "name": "Jing Xiong"
                },
                {
                    "authorId": "2330229027",
                    "name": "Gongye Liu"
                },
                {
                    "authorId": "2323103372",
                    "name": "Lun Huang"
                },
                {
                    "authorId": "2277988832",
                    "name": "Chengyue Wu"
                },
                {
                    "authorId": "2137407647",
                    "name": "Taiqiang Wu"
                },
                {
                    "authorId": "2248348669",
                    "name": "Yao Mu"
                },
                {
                    "authorId": "2278219862",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "2290542137",
                    "name": "Hui Shen"
                },
                {
                    "authorId": "2240538849",
                    "name": "Zhongwei Wan"
                },
                {
                    "authorId": "2266440308",
                    "name": "Jinfa Huang"
                },
                {
                    "authorId": "2290488386",
                    "name": "Chaofan Tao"
                },
                {
                    "authorId": "2272708319",
                    "name": "Shen Yan"
                },
                {
                    "authorId": "2290852491",
                    "name": "Huaxiu Yao"
                },
                {
                    "authorId": "2324779503",
                    "name": "Lingpeng Kong"
                },
                {
                    "authorId": "2330266685",
                    "name": "Hongxia Yang"
                },
                {
                    "authorId": "2328615184",
                    "name": "Mi Zhang"
                },
                {
                    "authorId": "2323043345",
                    "name": "Guillermo Sapiro"
                },
                {
                    "authorId": "2322460427",
                    "name": "Jiebo Luo"
                },
                {
                    "authorId": "2311889480",
                    "name": "Ping Luo"
                },
                {
                    "authorId": "2295135094",
                    "name": "Ngai Wong"
                }
            ],
            "abstract": "Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the representation strategy. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multifaceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multimodal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.",
            "corpus_id": 273962846,
            "sentences": [
                {
                    "corpus_id": "273962846",
                    "title": "Autoregressive Models in Vision: A Survey",
                    "text": "To further enhance the efficacy of visual in-context learning, CoTVL (Ge et al., 2023) successfully applies visual chain-of-thought prompt tuning for vision-language modeling, and performs better in tasks that require more reasoning abilities. These models highlight the potential for autoregressive frameworks to serve as foundational architectures in multi-modal applications, promoting a more holistic approach to understanding and generating content across modalities. \n\nThe previously mentioned multimodal tokenization involves encoding images into tokens, allowing Large Language Models (LLMs) to process both visual and language signals in a unified space. Visionto-Language (V2T) Tokenizer (Zhu et al., 2024a) maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) (Lee et al., 2022b) encodes both image and text inputs as tokens, generating coherent multimodal outputs. These approaches improve image-text generation, though they typically predict pixels in a fixed order without considering random generation strategies. \n\nIn summary, the evolution of visual autoregressive models in the realm of multi-modality underscores their growing importance and versatility. From early integrations to sophisticated native architectures, these models continue to push the boundaries of what's possible in generating and understanding multimodal content. Future research directions may focus on improving model efficiency, scalability, and the ability to handle an even broader array of modalities.",
                    "score": 0.5094333739402852,
                    "section_title": "The Framework for Unifying Multimodal Understanding and Generation",
                    "char_start_offset": 88771,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 472
                        },
                        {
                            "start": 475,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1156
                        },
                        {
                            "start": 1159,
                            "end": 1301
                        },
                        {
                            "start": 1302,
                            "end": 1480
                        },
                        {
                            "start": 1481,
                            "end": 1624
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 698,
                            "end": 717,
                            "matchedPaperCorpusId": "268363788"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7490234375
                }
            ],
            "relevance_judgement": 0.7490234375,
            "relevance_judgment_input_expanded": "# Title: Autoregressive Models in Vision: A Survey\n# Venue: arXiv.org\n# Authors: Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, Chaofan Tao, Shen Yan, Huaxiu Yao, Lingpeng Kong, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, Ngai Wong\n## Abstract\nAutoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the representation strategy. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multifaceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multimodal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.\n## The Framework for Unifying Multimodal Understanding and Generation\nTo further enhance the efficacy of visual in-context learning, CoTVL (Ge et al., 2023) successfully applies visual chain-of-thought prompt tuning for vision-language modeling, and performs better in tasks that require more reasoning abilities. These models highlight the potential for autoregressive frameworks to serve as foundational architectures in multi-modal applications, promoting a more holistic approach to understanding and generating content across modalities. \n\nThe previously mentioned multimodal tokenization involves encoding images into tokens, allowing Large Language Models (LLMs) to process both visual and language signals in a unified space. Visionto-Language (V2T) Tokenizer (Zhu et al., 2024a) maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) (Lee et al., 2022b) encodes both image and text inputs as tokens, generating coherent multimodal outputs. These approaches improve image-text generation, though they typically predict pixels in a fixed order without considering random generation strategies. \n\nIn summary, the evolution of visual autoregressive models in the realm of multi-modality underscores their growing importance and versatility. From early integrations to sophisticated native architectures, these models continue to push the boundaries of what's possible in generating and understanding multimodal content. Future research directions may focus on improving model efficiency, scalability, and the ability to handle an even broader array of modalities.",
            "reference_string": "[273962846 | Xiong et al. | 2024 | Citations: 14]"
        },
        {
            "title": "MAGVLT: Masked Generative Vision-and-Language Transformer",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "reference_count": 75,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2303.12208",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.12208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155640909",
                    "name": "Sungwoong Kim"
                },
                {
                    "authorId": "5014906",
                    "name": "DaeJin Jo"
                },
                {
                    "authorId": "2115475688",
                    "name": "Donghoon Lee"
                },
                {
                    "authorId": "2130367436",
                    "name": "Jongmin Kim"
                }
            ],
            "abstract": "While generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask prediction, named MAGVLT, and compare it with an autoregressive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predictions in an iterative refinement, and extended editing capabilities such as image and text infilling. For rigorous training of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to-image, and joint image-and-text mask prediction tasks. Moreover, we devise two additional tasks based on the step-unrolled mask prediction and the selective prediction on the mixture of two image-text pairs. Experimental results on various downstream generation tasks of VL benchmarks show that our MAGVLT outperforms ARGVLT by a large margin even with significant inference speedup. Particularly, MAGVLT achieves competitive results on both zero-shot image-to-text and text-to-image generation tasks from MS-COCO by one moderate-sized model (fewer than 500M parameters) even without the use of monomodal data and networks.",
            "corpus_id": 257663583,
            "sentences": [
                {
                    "corpus_id": "257663583",
                    "title": "MAGVLT: Masked Generative Vision-and-Language Transformer",
                    "text": "While generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask prediction, named MAGVLT, and compare it with an autoregressive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predictions in an iterative refinement, and extended editing capabilities such as image and text infilling. For rigorous training of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to-image, and joint image-and-text mask prediction tasks. Moreover, we devise two additional tasks based on the step-unrolled mask prediction and the selective prediction on the mixture of two image-text pairs. Experimental results on various downstream generation tasks of VL benchmarks show that our MAGVLT outperforms ARGVLT by a large margin even with significant inference speedup. Particularly, MAGVLT achieves competitive results on both zero-shot image-to-text and text-to-image generation tasks from MS-COCO by one moderate-sized model (fewer than 500M parameters) even without the use of monomodal data and networks.",
                    "score": 0.5953513891796037,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.748046875
                }
            ],
            "relevance_judgement": 0.748046875,
            "relevance_judgment_input_expanded": "# Title: MAGVLT: Masked Generative Vision-and-Language Transformer\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Sungwoong Kim, DaeJin Jo, Donghoon Lee, Jongmin Kim\n## Abstract\nWhile generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask prediction, named MAGVLT, and compare it with an autoregressive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predictions in an iterative refinement, and extended editing capabilities such as image and text infilling. For rigorous training of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to-image, and joint image-and-text mask prediction tasks. Moreover, we devise two additional tasks based on the step-unrolled mask prediction and the selective prediction on the mixture of two image-text pairs. Experimental results on various downstream generation tasks of VL benchmarks show that our MAGVLT outperforms ARGVLT by a large margin even with significant inference speedup. Particularly, MAGVLT achieves competitive results on both zero-shot image-to-text and text-to-image generation tasks from MS-COCO by one moderate-sized model (fewer than 500M parameters) even without the use of monomodal data and networks.\n",
            "reference_string": "[257663583 | Kim et al. | 2023 | Citations: 12]"
        },
        {
            "title": "CLIPPO: Image-and-Language Understanding from Pixels Only",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2022,
            "reference_count": 87,
            "citation_count": 49,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2212.08045",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.08045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143902495",
                    "name": "Michael Tschannen"
                },
                {
                    "authorId": "40608942",
                    "name": "Basil Mustafa"
                },
                {
                    "authorId": "2815290",
                    "name": "N. Houlsby"
                }
            ],
            "abstract": "Multimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a pure pixel-based model to perform image, text, and multimodal tasks. Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP-style models, with half the number of parameters and no text-specific tower or embedding. When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Finally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without modifications.",
            "corpus_id": 254685730,
            "sentences": [
                {
                    "corpus_id": "254685730",
                    "title": "CLIPPO: Image-and-Language Understanding from Pixels Only",
                    "text": "In recent years, large-scale multimodal training of Transformer-based models has led to improvements in the state-of-the-art in different domains including vision [2,10,[74][75][76], language [6,11], and audio [5]. In particular, in computer vision and image-language understanding, a single large pretrained model can outperform task-specific expert models [10,74,75]. However, large multimodal models often use modality or dataset-specific encoders and decoders, and accordingly lead to involved protocols. For example, such models frequently involve training different Code and pretrained models are available as part of big vision [4] https://github.com/google-research/big_vision.  Figure 1. CLIP [56] trains separate image and text encoders, each with a modality-specific preprocessing and embedding, on image/alt-text pairs with a contrastive objective. CLIPPO trains a pure pixel-based model with equivalent capabilities by rendering the alt-text as an image, encoding the resulting image pair using a shared vision encoder (in two separate forward passes), and applying same training objective as CLIP. parts of the model in separate phases on their respective datasets, with dataset-specific preprocessing, or transferring different parts in a task-specific manner [75]. Such modality and task-specific components can lead to additional engineering complexity, and poses challenges when introducing new pretraining losses or downstream tasks. Developing a single end-to-end model that can process any modality, or combination of modalities, would be a valuable step for multimodal learning. Here, we focus on images and text. A number of key unifications have accelerated the progress of multimodal learning. First, the Transformer architecture has been shown to work as a universal backbone, performing well on text [6,15], vision [16], audio [5,24,54], and other domains [7,34]. Second, many papers have explored mapping different modalities into a single shared embedding space to simplify the input/output interface [21,22,46,69]",
                    "score": 0.5169612444225791,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 163,
                            "end": 166,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 169,
                            "end": 173,
                            "matchedPaperCorpusId": "249152323"
                        },
                        {
                            "start": 177,
                            "end": 181,
                            "matchedPaperCorpusId": "237291550"
                        },
                        {
                            "start": 192,
                            "end": 195,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 362,
                            "end": 365,
                            "matchedPaperCorpusId": "249152323"
                        },
                        {
                            "start": 702,
                            "end": 706,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 1827,
                            "end": 1830,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.748046875
                }
            ],
            "relevance_judgement": 0.748046875,
            "relevance_judgment_input_expanded": "# Title: CLIPPO: Image-and-Language Understanding from Pixels Only\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Michael Tschannen, Basil Mustafa, N. Houlsby\n## Abstract\nMultimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a pure pixel-based model to perform image, text, and multimodal tasks. Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP-style models, with half the number of parameters and no text-specific tower or embedding. When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Finally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without modifications.\n## Introduction\nIn recent years, large-scale multimodal training of Transformer-based models has led to improvements in the state-of-the-art in different domains including vision [2,10,[74][75][76], language [6,11], and audio [5]. In particular, in computer vision and image-language understanding, a single large pretrained model can outperform task-specific expert models [10,74,75]. However, large multimodal models often use modality or dataset-specific encoders and decoders, and accordingly lead to involved protocols. For example, such models frequently involve training different Code and pretrained models are available as part of big vision [4] https://github.com/google-research/big_vision.  Figure 1. CLIP [56] trains separate image and text encoders, each with a modality-specific preprocessing and embedding, on image/alt-text pairs with a contrastive objective. CLIPPO trains a pure pixel-based model with equivalent capabilities by rendering the alt-text as an image, encoding the resulting image pair using a shared vision encoder (in two separate forward passes), and applying same training objective as CLIP. parts of the model in separate phases on their respective datasets, with dataset-specific preprocessing, or transferring different parts in a task-specific manner [75]. Such modality and task-specific components can lead to additional engineering complexity, and poses challenges when introducing new pretraining losses or downstream tasks. Developing a single end-to-end model that can process any modality, or combination of modalities, would be a valuable step for multimodal learning. Here, we focus on images and text. A number of key unifications have accelerated the progress of multimodal learning. First, the Transformer architecture has been shown to work as a universal backbone, performing well on text [6,15], vision [16], audio [5,24,54], and other domains [7,34]. Second, many papers have explored mapping different modalities into a single shared embedding space to simplify the input/output interface [21,22,46,69]",
            "reference_string": "[254685730 | Tschannen et al. | 2022 | Citations: 49]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "276929408",
            "title": "Recognition-Synergistic Scene Text Editing",
            "text": "In response to the notable progress of large language models in natural language processing [1,2,6], the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models [15], but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,38,40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs [3,13,29] offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation. Inspired by these methods, our approach integrates the multi-modal language model RS-STE, which is specialized in scene text editing.",
            "score": 0.7037648091296218,
            "section_title": "MLLM for Image Generation and Editing",
            "char_start_offset": 6474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1336
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 99,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 583,
                    "end": 586,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "253581838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "274597781",
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "text": "The rise of foundation models [1] trained on vast amounts of text has transformed natural language processing (NLP), \n\nshowing that a single large language model (LLM) [2] can handle many different linguistic tasks [3][4][5]. The rich set of features encoded in LLM embeddings has been then used as an effective prior knowledge both for text-conditional image generation [6][7][8] and image understanding [9][10][11][12][13]. Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings [16] have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture [17][18][19][20]. This unification allows a richer understanding of the visual world, as generative tasks often require insight into the fundamental concepts and relationships within the data [21]. For example, a model that generates images from text descriptions must grasp the semantic content of those images to ensure they faithfully reflect the details and intent of the text [22,23]. As a result, research has rapidly progressed to integrate multiple modalities into a unified framework with increasingly deeper multimodal fusion. \n\nEarly approaches used cross-attention modules between modality-specific encoders [17]. Further advancements highlighted the importance of using a pre-trained LLM backbone combined with lightweight projection layers, often fine-tuning the LLM to refine the multimodal representations [19,24,25]. More recently, research has shown success in training multimodal-output vision-language models (VLMs) from scratch [20,26], achieving performance close to VLMs that only output text on visual understanding tasks.",
            "score": 0.6541430977835287,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1728
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 33,
                    "matchedPaperCorpusId": "237091588"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 374,
                    "end": 377,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 405,
                    "end": 408,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 543,
                    "end": 547,
                    "matchedPaperCorpusId": "263605855"
                },
                {
                    "start": 682,
                    "end": 686,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 1302,
                    "end": 1306,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 1504,
                    "end": 1508,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 1511,
                    "end": 1514,
                    "matchedPaperCorpusId": "261696650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "266210376",
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "text": "VL-GPT aims to process the vision and language understanding and generation within a single transformer model in a unified way, similar to GPT handles language tasks. It is capable of perceiving the interleaved multi-modal data and generating content across various modalities. By employing unified modeling, our VL-GPT can conduct autoregressive pre-training on web-scale multimodal corpora, thereby holding the potential to serve as a powerful foundation model in the multimodal research community. \n\nArchitecture As depicted at the bottom of Fig. 1, our VL-GPT comprises five components: a large vision-language transformer model M, an image tokenizer E v , a text tokenizer E t , an image detokenizer D v and a text detokenizer D t . In comparison to a language model, VL-GPT incorporates additional image tokenizer and image detokenizer elements. \n\nGiven any interleaved image-text data, the image tokenizer and the text tokenizer initially encode them into a multimodal sequence. More specifically, the image tokenizer E v converts each image into N continuous visual embeddings x v . Additionally, two special tokens [IMG] and [/IMG] are appended at the beginning and end of the visual embeddings, respectively. The visual embeddings are then combined with the discrete text tokens encoded by the text tokenizer E t to form a interleaved multimodal sequence v = (v 1 , v 2 , . . . , v n ), where v i can be either a discrete text token or a continuous visual embedding. The multimodal sequence v is then fed into the large VL model M for unified auto-regressive modeling. \n\nThe output embedding M(v i ) can be flexibly transformed into a text embedding through a language modeling head for the predefined vocabulary or into a visual embedding with a separate regression head. During training, the selection of the transformed head depends on whether the target for the current embedding is a text token or a visual embedding.",
            "score": 0.6372625951538424,
            "section_title": "VL-GPT",
            "char_start_offset": 13294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1932
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74853515625
        },
        {
            "corpus_id": "276782523",
            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
            "text": "In this section, we provide an introduction to the base unified generative model, which serves as the foundation for our approach to multimodal understanding and generation. It integrates both auto-regressive and diffusion modeling techniques to achieve joint multimodal understanding and generation [74]. \n\nThe base unified generative model is built upon a pretrained large language model Phi-1.5 [38]. The architecture of the base model is largely inherited from it with minimal modifications to accommodate multimodal input. Specifically, a QK-Norm operation is added to each attention layer [13,70] to enhance training stability. The embedding layer is expanded by adding learnable embeddings for discrete image tokens, enabling the joint encoding of text and image modalities. The final model consists of 24 transformer layers with a total of 1.5 billion parameters. In the proposed DoraCycle, the parameters of the base model are fixed and the LoRA components introduce 4.7 million trainable parameters, accounting for approximately 0.32% of the total model parameters. \n\nThe model tokenizes both text and image data into discrete tokens to create a unified space, maintaining a unified vocabulary. Text data is tokenized using a pre-trained text tokenizer of Phi-1.5 [38]. The codebook size of text tokens is 58498. For images, a quantizer like MAGVIT-v2 [78] is used. This quantizer maintains a codebook of size K = 8, 192 and encodes images at a resolution of 512\u00d7512 into 32 \u00d7 32 discrete tokens. The model utilizes the unified tokenization strategy ensuring that both modalities can be processed consistently, allowing the model to handle multimodal inputs within a shared framework. Besides the text and image tokens, the model also involves different special tokens, like <sot>, <eot>, <soi>, and <soi>, which are used to denote the start and end of the text and image tokens. Among them, there are some special tokens that indicate the task to be executed, where the <mmu> indicates the model should do the understanding task and the <t2i> indicates the model should generate image tokens based on the given text.",
            "score": 0.6334484796902986,
            "section_title": "A. Model Details",
            "char_start_offset": 31031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 595,
                    "end": 599,
                    "matchedPaperCorpusId": "256808367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "259765944",
            "title": "Generative Pretraining in Multimodality",
            "text": "We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.",
            "score": 0.6316621669737311,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66064453125
        },
        {
            "corpus_id": "277501830",
            "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement",
            "text": "\"What I cannot create, I do not understand.\" --Richard Feynman Recent advancements in Large Language Models (LLMs) have significantly enhanced their capability to handle multimodal tasks, particularly by integrating visual inputs into language models. Efforts such as the LLaVA series and the QwenVL series [38,37,1,64] have demonstrated remarkable visual comprehension performance. Meanwhile, the development of text-to-image generation models, such as diffusion-based approaches [53,50,51,54] and more recent autoregressive approaches [52,16,74], has made substantial strides in generating high-fidelity images. These developments have driven the push towards creating unified Multimodal Large Language Models (MLLMs) that seamlessly integrate both visual understanding and generation capabilities. A unified model holds promise not only for advancing task coordination and generalization but also for contributing to the exploration of artificial general intelligence (AGI). By merging understanding and generation capabilities within a single framework, unified models can genuinely grasp the deep relationships between visual and textual information, enabling more intelligent and flexible interactions and task execution in complex real-world scenarios. \n\nTo build such unified models in an autoregressive framework, existing approaches have explored several distinct paradigms. As illustrated in Fig. 2 (a), the earliest models, e.g., Chameleon [59] and Emu3 [65], utilized VQGAN [18] to discretize images, enabling a shared vocabulary for text and vision within an autoregressive (AR) framework. However, even with large-scale training, they lag behind models like LLaVA [38] in visual understanding tasks. To address this issue, works like LaViT [28] and ILLUME [63] (Fig. 2 (b)) learn the semantic codebook from the pretrained text-aligned semantic encoders [78,84], and employ diffusion models to reconstruct images based on semantic tokens. This improves image-text alignment in MLLM pretraining and achieves strong performance in both understanding and generation tasks. However, the lack of texture preservation in vision tokenizers limits their capability in fine-grained image editing.",
            "score": 0.6312456189658229,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 485,
                    "end": 488,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 491,
                    "end": 494,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 1755,
                    "end": 1759,
                    "matchedPaperCorpusId": "263889455"
                },
                {
                    "start": 1868,
                    "end": 1872,
                    "matchedPaperCorpusId": "257767223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "266210376",
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "text": "Driven by the remarkable success of large language models (LLMs) in the field of natural language processing (NLP) [40,41,54], there has been a surge of interest within multimodal community to develop large vision-language (VL) models. One of the promising approaches, exemplified by Flamingo [1], BLIP2 [24], LLAVA [25], have explored how to build large VL models based on powerful pre-trained LLMs. These studies typically adopted a similar architecture: a pre-trained image encoder and an LLM are connected via a trainable connection module, which aligns the image feature and text embeddings, thereby enabling language models to accept images and text as inputs and generate a text sequence. \n\nTo expand the capabilities of generating image in a multimodal context, certain efforts, e.g., Visual ChatGPT [47], attempt to connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which inevitably introduce instability and noise. Alternatively, another line of research achieves it by optimizing models in an end-to-end manner [9,18,23,30,48]. By aligning the output space with the image diffusion models, VL models can not only perceive but also generate images and text. \n\nA crucial characteristic of large language models is autoregressive modeling [31], i.e., predicting next token, which facilitates language understanding and generation in a unified manner. However, in the aforementioned studies, the inconsistency of image embeddings between LLM's input and output sides compels the model to treat input images and generated images differently, resulting in separate modeling for image understanding and generation. Meanwhile, this discrepancy also obstructs the implementation of autoregressive training loss on image embeddings. \n\nIn this study, we introduce VL-GPT, a large visionlanguage generative pre-trained transformer that enables the unified training of both visual and linguistic data using an auto-regressive objective, as depicted in Fig. 1.",
            "score": 0.6279211133770314,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 695
                },
                {
                    "start": 698,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 2002
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7373046875
        },
        {
            "corpus_id": "271333958",
            "title": "Harmonizing Visual Text Comprehension and Generation",
            "text": "Visual text comprehension and generation tasks such as scene text detection and recognition [70,54,31,66,53,52,24,49], document understanding [55,23], visual question answering (VQA) [26,15,27,30,50], key information extraction (KIE) [55,23], visual text generation, editing, and erasure [57,6,5] are consistently of significant value for both academic research and practical applications. Recently, remarkable advancements have been achieved in visual text comprehension and generation, driven by the evolution of Multimodal Large Language Models (MLLMs) and diffusion models. Foremost text-centric MLLMs [64,20,27,30] utilize a cohesive framework to comprehend text-rich images comprehensively, whereas diffusion-based approaches [57,6,5] introduce innovative modifications to enhance visual text generation capabilities. As depicted in Figure 1, text-centric MLLMs and diffusion models are capable of handling language and vision modalities adeptly, with MLLMs generating texts and diffusion models producing images. However, integrating language and vision generation capabilities within a large multimodal model for visual text scenarios remains unexplored. This paper focuses on the simultaneous manipulation of language and vision generations to further streamline the processing of diverse text-centric multimodal tasks. In the general multimodal domain, some pioneering efforts [48,16,71,56] empower MLLMs with the ability to generate images beyond texts, vastly extending the versatility of multimodal models. Such advancements inspire us to develop a text-centric multimodal generative model. Our foundational model follows these approaches, incorporating a VIT-based image encoder, a text tokenizer, an LLM, a text detokenizer, and a diffusion-based image decoder. \n\nPrevious works [48,71,56] and our pilot experiments (Figure 2) have shown that multimodal generation often leads to a notable decline in performance due to the substantial inconsistency between language and vision modalities in the generation space. Prior studies [48,16,71,56] commonly rely on modality-specific supervised fine-tuning to bolster generative capacities.",
            "score": 0.6245975184023917,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1776
                },
                {
                    "start": 1779,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 96,
                    "matchedPaperCorpusId": "265351529"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "247778841"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "255416140"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "257232534"
                },
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "251451612"
                },
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "251040216"
                },
                {
                    "start": 111,
                    "end": 114,
                    "matchedPaperCorpusId": "259951310"
                },
                {
                    "start": 114,
                    "end": 117,
                    "matchedPaperCorpusId": "258826110"
                },
                {
                    "start": 142,
                    "end": 146,
                    "matchedPaperCorpusId": "254275326"
                },
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "248228056"
                },
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "254275326"
                },
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "248228056"
                },
                {
                    "start": 292,
                    "end": 294,
                    "matchedPaperCorpusId": "258762187"
                },
                {
                    "start": 736,
                    "end": 738,
                    "matchedPaperCorpusId": "258762187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "265658856",
            "title": "StoryGPT-V: Large Language Models as Consistent Story Visualizers",
            "text": "Text-to-image synthesis. Numerous works [8][9][10] have demonstrated unprecedented performance on semantic generation. Recently, diffusion-based text-to-image models [39][40][41]43] have shown significant advancements in enhancing image quality and diversity through the utilization of diffusion models. However, these text-to-image approaches primarily concentrate on aligning individualgenerated images grounded on text descriptions and do not take into account the crucial aspects of character and scene consistency across multiple frames in the story visualization task. Additionally, they lack the capability to effectively resolve co-reference issues within a narrative description. \n\nMulti-modal Large Language Models. Large Language Models (LLMs) wield an extensive repository of human knowledge and exhibit impressive reasoning capabilities. \n\nRecent studies [1,5,21,49] utilize pre-trained language models to tackle vision-language tasks, and subsequent studies [6,16,20,52,59,60] further enhance multi-modal abilities by aligning vision models with LLM input space. \n\nIn addition to multi-modal comprehension, several works are dedicated to more challenging multi-modal generation tasks. FROMAGe [19] appends a special retrieval token to LLM and maps the hidden representation of this token into a vector space for retrieving images. Several current works [18,54,57] learn a mapping from hidden embeddings of an LLM represents for additional visual outputs into the input space of a frozen pre-trained text-to-image generation model [41]. In this work, we fed multi-modal LLM with interleaved image and referential text descriptions as input and aligned the output with a character-aware fused embedding from our first-stage Char-LDM, guiding the LLM in implicitly deducing the references. Story Visualization. StoryGAN [22] pioneers the story generation task, which proposes a sequential conditional GAN framework with dual frame and story level discriminators to improve image quality and narrative coherence. DuCoStoryGAN [27] introduces a dual-learning framework that utilizes video captioning to enhance semantic alignment between descriptions and generated images. VLCSto-ryGAN [26] used video captioning for semantic alignment between text and frames.",
            "score": 0.624049848962119,
            "section_title": "Related Work",
            "char_start_offset": 5075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2269
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 43,
                    "matchedPaperCorpusId": "248239727"
                },
                {
                    "start": 43,
                    "end": 46,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 166,
                    "end": 170,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 178,
                    "end": 181,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 868,
                    "end": 871,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 871,
                    "end": 873,
                    "matchedPaperCorpusId": "235351128"
                },
                {
                    "start": 876,
                    "end": 879,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 1544,
                    "end": 1548,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1831,
                    "end": 1835,
                    "matchedPaperCorpusId": "54457433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.666015625
        },
        {
            "corpus_id": "274023126",
            "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
            "text": "Current multimodal generative models can be broadly classified into three distinct categories. The first category includes Any-to-Text (Any Modality to Text) Models, which integrate inputs from multiple modalities, encode them, and project into the word embedding space of the LLM for generating textual output [3]- [5], [27]- [30]. The second category encompasses Any-to-Vision (Any Modality to Vision) Models, which encode inputs across different modalities as conditional information and leverage diffusion models to generate visual outputs [10], [31]- [38]. Thirdly, Any-to-Any (Any Modality to Any Modality) Models perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio [2], [39]- [41]. We summarize the combinations of modalities regarding both inputs and outputs of all categories in TABLE I. Additionally, we provide a comprehensive analysis of their underlying architectures as follows: \n\n\u2022 Any-to-Text Models. Typical models in this category consist of three primary components: an encoder, a pre-trained LLM, and a modality interface that connects them. The modality encoder functions akin to human sensory organs, transforming raw visual or audio data into compact representations. A common approach is to use pre-trained encoders that are already aligned with language data, as seen in CLIP models [43], which facilitate alignment with LLMs. The LLM, often chosen from established pre-trained models like LLaMA [44] and Vicuna [44], serves as the central reasoning unit. These models benefit from extensive pre-training on web corpora, allowing for rich knowledge representation and reasoning capabilities. To bridge the gap between modalities and language, a modality interface is introduced. This interface can either be a learnable projector that directly aligns the encoded modality features with the LLM's input requirements or an expert model that translates non-textual data into language. Overall, Any-to-Text Models utilize a multi-module architecture to effectively integrate multimodal inputs and generate coherent textual outputs. \n\n\u2022 Any-to-Vision Models.",
            "score": 0.6238335735689078,
            "section_title": "A. Multimodal Generative Models",
            "char_start_offset": 8699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "267412182"
                },
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 721,
                    "end": 725,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67333984375
        },
        {
            "corpus_id": "276742446",
            "title": "WeGen: A Unified Model for Interactive Multimodal Generation as We Chat",
            "text": "To evaluate WeGen as a unified framework, 1) we first assess its text-to-image generation capability through quantitative metrics, as this forms the foundation for all visual generation tasks. 2) We then demonstrate the framework's versatility through comprehensive case studies across a wide range of tasks, Detailed quantitative evaluations for specific tasks and more case studies are provided in \u00a74.3 and the supplementary materials. Text-to-Image Generation. In the realm of text-to-image generation, there are two primary technical approaches: one relies on pure diffusion models, while the other leverages multimodal large language models (MLLMs). Our model falls into the latter category, where we have achieved stateof-the-art (SOTA) performance among MLLM-based approaches, achieving an FID score of 9.39 and a CLIP-T score of 0.308, as shown in Table 1. Notably, our model accomplishes this with less training data and reduced computational cost, while also supporting a wide range of tasks within a unified framework. Compared to diffusion-based models, our model's performance is comparable in terms of metrics, but it offers the advantage of supporting multiple tasks and possessing both language and visual understanding capabilities. Case Studies on Diverse Tasks. As shown in Figure 3, we demonstrate WeGen's capabilities across a wide range of tasks, including text-to-image generation, subject-driven generation, condition-based generation (canny, depth, pose), style transfer, super-resolution, inpainting, outpainting, and various editing operations. These qualitative results highlight our model's versatility across diverse visual generation tasks. Detailed quantitative evaluations for specific tasks can be found in \u00a74.3 and the supplementary materials, along with additional case studies.",
            "score": 0.6217960379879532,
            "section_title": "Unified Multi-Task Generation",
            "char_start_offset": 17550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1814
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "273403986",
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "text": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.",
            "score": 0.6180140423542313,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49658203125
        },
        {
            "corpus_id": "272310282",
            "title": "AdaptVision: Dynamic Input Scaling in MLLMs for Versatile Scene Understanding",
            "text": "Using GPT-4, it generates detailed image-text descriptions that are then subjected to manual verification to ensure data quality. With this high-quality dataset, the model employs a two-stage training strategy and significantly outperforms previous methods [3]. Building on these foundational works, subsequent models such as LLaVA-1.5 [13] and MiniGPT-v2 [22], along with other innovative works such as mPLUG-Owl [23], InstructBLIP [24], and Shikra [25], have further advanced the multimodal research field. \n\nThe above methods primarily focus on natural scenes and lack text Optical Character Recognition (OCR) capabilities for environments rich in text, leading to difficulties in handling text-related tasks. This limitation is particularly evident in textheavy scenes, where the performance of these models further declines. To address this issue, Multimodal Large Language Models (MLLMs) optimized specifically for text-rich scenes have been developed.",
            "score": 0.6092264363051514,
            "section_title": "A. MLLMs for Natural Scene Images",
            "char_start_offset": 7265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 958
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "258615266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6748046875
        },
        {
            "corpus_id": "252781022",
            "title": "Visualize Before You Write: Imagination-Guided Open-Ended Text Generation",
            "text": "This work mainly focuses on open-ended text generation, where the search space for the target output is infinite, and the language model would benefit from additional visual imagination distilled from large text-to-image generation models to produce coherent and meaningful content. However, we should note here that despite the commendable performance of text-to-image generation models, there are certain terms and concepts that are inherently challenging to visualize, such as numerical values and abstract philosophical terms. This problem itself is an interesting open research question for all tasks involving text-and-vision. In our current approach, the images are generated offline. In future work, one may explore the integration of text-to-image and image-to-text modules in an end-to-end manner, which may be more suitable for longer text generation that is not covered in this work.\n\nText-to-image generation models currently have a length limit on the input text prompt, which may impede their ability to visualize long text inputs in a single image. Furthermore, as previously discussed, text-to-image models may also encounter difficulties in generating images of complex scenes or situations that are challenging to depict through a single image. Future research could explore the use of multiple images or supplementary videos as visual input in order to provide a more comprehensive representation of the scene or situation in question. The iNLG framework can be easily extended to take video representation by taking longer visual prefixes or iteratively applying visual prefixes at each step.",
            "score": 0.60838279339762,
            "section_title": "Limitations",
            "char_start_offset": 27831,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3515625
        },
        {
            "corpus_id": "273403986",
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "text": "Unifying multimodal understanding and generation capabilities within a single model is a critical milestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu et al., 2024b;Zhu et al., 2023a) in multimodal large language models (MLLMs) have made significant progress in integrating visual reasoning and understanding with natural language interfaces. However, developing a unified framework that excels at both comprehending and generating multimodal content remains a significant challenge in the field of artificial intelligence. \n\nRecent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis. \n\nHowever, existing MLLM-based methods (Sun et al., 2023;2024b) face a common challenge in the trade-off between diversity for text-to-image generation and high controllability for tasks such as image editing. Previous methods mostly rely on single-granular features extracted from a visual encoder and neglect the varying granularity requirements of different tasks. On the one hand, generating diverse images reflecting the real world from text descriptions requires features that encode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image decoder, allowing the diffusion model to generate diverse images that semantically align with the text prompt. On the other hand, tasks demanding precise control over output images, such as image editing and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed visual information for the image decoder. This dichotomy presents a significant challenge for current MLLM-based methods, which typically generate single-granular feature representations for all tasks.",
            "score": 0.6074280164830536,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "255394080",
            "title": "Progressive Text-to-Image Generation",
            "text": "The task of text-to-image generation aims to create natural and consistent images from the input text and has received extensive research interest. Recently, latent autoregressive (AR) generation frameworks have achieved great success in advancing the start-of-the-arts, by learning knowledge and patterns from a large-scale multimodal corpus [35,42,64]. Generally, they treat the task as a form of language modeling and use Transformer-like [59] architectures to learn the relationship between language inputs and visual outputs. A key component of these approaches is the conversion of each image into a sequence of discrete units through the use of a VQ-VAE [57] based image tokenizer, e.g., VQ-GAN [16,63], RQ-VAE [32] and ViT VQ-GAN [63]. Visual tokenization essentially unifies the view of text and images so that both can be treated simply as sequences of discrete tokens and is adaptable to sequenceto-sequence models. To that end, DALL-E [47], CogView [12], RQ-Transformer [32], and Parti [65] employ autoregressive models to learn text-to-image task from a large collection of potentially noisy text-image pairs [5,17,27]. In particular, [62] further expand on this AR over AR modeling approach to support arbitrarily-sized image generation. \n\nAnother research line for text-to-image generation involves diffusion-based methods, such as GLIDE [38], DALL-E 2 [46], stable diffusion [49], RQ-Transformer [32], and Imagen [51]. These models pursue to directly generate images or latent image features with diffusion process [11,22] and produce high-quality images with great aesthetic appeal. Even so, discrete sequence modeling for text-to-image generation remains appealing given extensive prior work on large language models [4] and advances in discretizing other modalities, such as video and audio, as cross-language tokens [3].",
            "score": 0.6063028034150169,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1840
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 1122,
                    "end": 1125,
                    "matchedPaperCorpusId": "231951742"
                },
                {
                    "start": 1128,
                    "end": 1131,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 1391,
                    "end": 1395,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 1531,
                    "end": 1535,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1535,
                    "end": 1538,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66552734375
        },
        {
            "corpus_id": "248240054",
            "title": "Opal: Multimodal Image Generation for News Illustration",
            "text": "Our system adds to a long tradition of work within multimodal authoring tools. [11] and Wordseye [14] were systems that connected language to 3D space, allowing users to use text prompts to generate 3D scenes. [12] allowed users to define 3D shapes based on adjectives and emotive cues, parameterizing creature shapes by qualities like \"cute\" or \"dangerous\". As tools moved away from statistical-based techniques to GANs, a number of text-to-image architectures [39,40,52,53] were proposed. However, many of these architectures were class-conditional, meaning they were constrained to the single-class datasets they were trained upon. \n\nRecently, multimodal systems have found renewed momentum from machine learning advancements in multimodal representation learning. CLIP, one of the latest advancements, demonstrated that images and text pairs can be contrastively learned and optimized such that the image and text embeddings can share a multimodal embedding space. [41] Many of the earliest open-source text-to-image generative frameworks to gain traction used CLIP in a discriminator-like fashion, using it in conjunction with a host of generative models from BigGan to VQGAN. [4,16,17,22] Newer methods such as diffusion models have also increased output quality. [15,19,36,37] DALL\u2022E 2 [42] showed by conditioning on a CLIP text prior, generating CLIP image representations, and decoding those representations either autoregressively or with diffusion, DALL\u2022E 2 could create a text-to-image generator with novel functionalities like image remixing, inpainting, and interpolation. Other state-of-the-art text-to-image architectures have come out in rapid succession. Make-a-Scene offers users control in the form of semantic layout sketches.[1]",
            "score": 0.6055984688669951,
            "section_title": "Multimodality",
            "char_start_offset": 8429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "10046351"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "202783730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58349609375
        },
        {
            "corpus_id": "266210376",
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "text": "In this work, we introduce Vision-Language Generative Pre-trained Transformer (VL-GPT), a transformer model proficient at concurrently perceiving and generating visual and linguistic data. VL-GPT achieves a unified pre-training approach for both image and text modalities by employing a straightforward auto-regressive objective, thereby enabling the model to process image and text as seamlessly as a language model processes text. To accomplish this, we initially propose a novel image tokenizer-detokenizer framework for visual data, specifically designed to transform raw images into a sequence of continuous embeddings and reconstruct them accordingly. In combination with the existing text tokenizer and detokenizer, this framework allows for the encoding of interleaved image-text data into a multimodal sequence, which can subsequently be fed into the transformer model. Consequently, VL-GPT can perform large-scale pre-training on multimodal corpora utilizing a unified auto-regressive objective (i.e., next-token prediction). Upon completion of pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance across a diverse range of vision and language understanding and generation tasks, including image captioning, visual question answering, text-to-image generation, and more. Additionally, the pre-trained model retrains in-context learning capabilities when provided with multimodal prompts. We further conduct instruction tuning on our VL-GPT, highlighting its exceptional potential for multimodal assistance. The source code and model weights shall be released.",
            "score": 0.6047583068423362,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57861328125
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "Unified multimodal models aim to build a single architecture capable of both understanding and generating data across multiple modalities. These models are designed to process diverse forms of input (e.g., text, image, video, audio) and produce outputs in one or more modalities in a unified manner. A typical unified multimodal framework can be abstracted into three core components: modalityspecific encoders that project different input modalities into a representation space; a modality-fusion backbone that integrates information from multiple modalities and enables cross-modal reasoning; and modality-specific decoders that generate output in the desired modality (e.g., text generation or image synthesis). \n\nIn this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation. \n\nIn the following sections, we will delve deeper into each category: Section 3.1 explores diffusion-based models, discussing their unique advantages in terms of generating high-quality images and text from noisy representations. Section 3.2 focuses on autoregressive-based models, detailing how different encoding methods impact their performance in vision-language tasks. Section 3.3 covers fused AR + diffusion models, examining how the combination of these two paradigms can enhance multimodal generation capabilities.",
            "score": 0.6035646075291059,
            "section_title": "UNIFIED MULTIMODAL MODELS FOR UNDER-STANDING AND GENERATION",
            "char_start_offset": 20720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "231719672",
            "title": "VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs",
            "text": "We present VX2TEXT, a framework for text generation from multimodal inputs consisting of video plus text, speech, or audio. In order to leverage transformer networks, which have been shown to be effective at modeling language, each modality is first converted into a set of language embeddings by a learnable tokenizer. This allows our approach to perform multimodal fusion in the language space, thus eliminating the need for ad-hoc cross-modal fusion modules. To address the non-differentiability of tokenization on continuous inputs (e.g., video or audio), we utilize a relaxation scheme that enables end-to-end training. Furthermore, unlike prior encoder-only models, our network includes an autoregressive decoder to generate open-ended text from the multimodal embeddings fused by the language encoder. This renders our approach fully generative and makes it directly applicable to different \"video+x to text\" problems without the need to design specialized network heads for each task. The proposed framework is not only conceptually simple but also remarkably effective: experiments demonstrate that our approach based on a single architecture outperforms the state-of-the-art on three videobased text-generation tasks\u2014captioning, question answering and audio-visual scene-aware dialog.",
            "score": 0.6028623705668864,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7587890625
        },
        {
            "corpus_id": "274860024",
            "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
            "text": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
            "score": 0.6025430145104524,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80908203125
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "Such a configuration facilitates effective multitask pretraining and enhances BLIP's capability for transfer learning, making it a versatile tool in bridging visual and textual modalities.\n\nLLaVA represents an end-to-end trained, large-scale image-text multimodal model that integrates a visual encoder with a language model (LLM) to facilitate a universal understanding of both vision and language.Its primary objective is to effectively leverage the capabilities of pretrained language models and visual models.As depicted in Figure 10, LLaVA exemplifies the single-stream architecture approach.The model selects Vicuna as its language model due to its exceptional performance in language tasks, as evidenced by publicly available checkpoints.For visual input   , LLaVA utilizes the pre-trained CLIP visual encoder ViT-L/14, generating visual features   = (  ).The model emphasizes grid features both before and after the last Transformer layer.Through a simple linear layer, LLaVA connects image features to the word embedding space.Specifically, it employs a trainable projection matrix  to convert   into language embedding tokens   , matching the dimensionality of the word embedding space in the language model.This efficient projection method facilitates rapid experimentation focused on data.The exploration of image-text multimodal model architectures has led to the identification of two principal configurations: single-stream and dual-stream [215,322].The single-stream approach integrates visual and textual data within a unified processing framework, optimizing the use of pre-trained components for enhanced efficiency.In contrast, the dual-stream configuration processes each modality separately before combining them, employing techniques such as contrastive learning and mutual attention mechanisms for a deeper image-text multimodal understanding.These methodologies underscore the field's varied strategies for the integration of visual and textual information, marking significant progress towards developing systems that achieve a comprehensive understanding of the complex interplay between vision and language.",
            "score": 0.6001146072799461,
            "section_title": "[CLS]",
            "char_start_offset": 54974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 190,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 745
                },
                {
                    "start": 745,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1036
                },
                {
                    "start": 1036,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1465
                },
                {
                    "start": 1465,
                    "end": 1635
                },
                {
                    "start": 1635,
                    "end": 1867
                },
                {
                    "start": 1867,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 1455,
                    "end": 1460,
                    "matchedPaperCorpusId": "247762352"
                },
                {
                    "start": 1460,
                    "end": 1464,
                    "matchedPaperCorpusId": "220364511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46435546875
        },
        {
            "corpus_id": "267200040",
            "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
            "text": "Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.",
            "score": 0.5965984751231097,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "257663583",
            "title": "MAGVLT: Masked Generative Vision-and-Language Transformer",
            "text": "While generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask prediction, named MAGVLT, and compare it with an autoregressive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predictions in an iterative refinement, and extended editing capabilities such as image and text infilling. For rigorous training of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to-image, and joint image-and-text mask prediction tasks. Moreover, we devise two additional tasks based on the step-unrolled mask prediction and the selective prediction on the mixture of two image-text pairs. Experimental results on various downstream generation tasks of VL benchmarks show that our MAGVLT outperforms ARGVLT by a large margin even with significant inference speedup. Particularly, MAGVLT achieves competitive results on both zero-shot image-to-text and text-to-image generation tasks from MS-COCO by one moderate-sized model (fewer than 500M parameters) even without the use of monomodal data and networks.",
            "score": 0.5953513891796037,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "262898422",
            "title": "Jointly Training Large Autoregressive Multimodal Models",
            "text": "Autoregressive text-to-image models, as exemplified by works such as Yu et al. (2023;2022), have made remarkable strides in generating highly detailed images, paralleling the achievements of Diffusion Models Nichol et al. (2022); Ramesh et al. (2022); Rombach et al. (2022). These models bear architectural resemblance to Large Language Models (LLMs), yet their training regimen is tailored for paired image-text data. LLMs on the other hand (Brown et al., 2020;Zhang et al., 2022;Touvron et al., 2023) are limited to text-based output, thus lacking multimodal generative capabilities despite their proficiency in textual tasks. The subfield of Multimodal Large Models has emerged in recent years Tsimpoukelli et al. (2021); Alayrac et al. (2022); Li et al. (2022a) in the quest to bring together the disparate strengths of vision and language models. Despite important advances in this direction, these models still predominantly generate one modality, thereby constraining their expressiveness. This study aspires to break this limitation by developing a multimodal model capable of generating integrated text and image outputs. \n\nTo achieve this objective, we conduct a comprehensive empirical investigation into the fusion of two specialized autoregressive, decoder-only, large transformer models, each designed for unique tasks (one for text-to-image and a text only model). We introduce a set of methods under the umbrella of the Joint Autoregressive Mixture (JAM) framework. In building this framework, we take advantage of the inherent architectural compatibility of autoregressive text-to-image models with LLMs, allowing us to do deep model fusion and joint training in ways which would otherwise not be possible. Our modular and data-efficient solution allows for deep, rapid and effective integration of continually evolving large models, using less than 1% of the original pretraining data for both parent models. \n\nOur contributions to this study are twofold.",
            "score": 0.5923218055620884,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 273,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 442,
                    "end": 462,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 697,
                    "end": 723,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 725,
                    "end": 746,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 748,
                    "end": 765,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "272831684",
            "title": "Recall: Empowering Multimodal Embedding for Edge Devices",
            "text": "Unified multimodal embedding Embedding was initially proposed to vectorize text data for understanding similarities between different texts [32]. Large language models use embedding layers to generate text embeddings [12,33]. Similarly, vision, audio, and sensor data can also be transformed into vectorized embeddings [34][35][36]. However, embedding methods focused on a single modality cannot access information across different modalities due to the gap between their embedding spaces. \n\nTo bridge this gap, multimodal embedding models (MEMs) have been developed to unify different modalities into a single embedding space, enhancing the model's ability to understand and bind multimodal inputs. CLIP [10] aligns text and vision by jointly training on image-text pairs, using contrastive learning to map both modalities into a shared space while maintaining their distinction through a dualtower architecture. ImageBind [11] extends this to align six modalities, including text, vision, audio, depth, thermal, and IMU readings. Each modality is processed by a separate encoder, and the embeddings are fused in a multimodal head to generate a unified embedding. ImageBind demonstrates strong zero-shot classification and retrieval performance across these modalities, matching or outperforming singlemodality models. This is achieved through training on largescale multimodal data.",
            "score": 0.5906713713230757,
            "section_title": "Background and Motivations 2.1 Multimodal Embedding",
            "char_start_offset": 9790,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1384
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 221,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "267628000"
                },
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 924,
                    "end": 928,
                    "matchedPaperCorpusId": "258564264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "274656320",
            "title": "Olympus: A Universal Task Router for Computer Vision Tasks",
            "text": "Extensive research [1,17,21,66,70,74,78,81,84,89,93,96] has focused on developing unified multimodal language models proficient in both understanding and generating content. Approaches such as [21,98] integrate continuous embeddings with textual tokens within autoregressive frameworks for image generation. Emu2 [66] combines CLIP ViT [18] image embeddings with text tokens for autoregressive modeling, while Chameleon [71] employs a transformer across diverse modalities with autogressive modeling. Show-o [81] and TransFusion [96] incorporate autoregressive and diffusion modeling within a single transformer. Omni-Gen [79] utilizes a VAE [32] encoder-decoder alongside a transformer to process free-form prompts. Recently, Emu3 [74] trained a unified transformer with next-token prediction across video, image, and text datasets, achieving superior performance on multimodal benchmarks and generation tasks. However, current unified multimodal foundation models predominantly support a narrow range of generative tasks, such as image and video creation or editing, and face significant scalability challenges for broader AI applications. Additionally, their training requires substantial computational resources. To overcome these limitations, we further enhance MLLMs by enabling the seamless integration of domain-specialized models tailored for diverse applications.",
            "score": 0.5886095352775461,
            "section_title": "Unified Vision-Language Foundation Model",
            "char_start_offset": 8218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1373
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 25,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 31,
                    "end": 34,
                    "matchedPaperCorpusId": "258822817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "276929205",
            "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
            "text": "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45]48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model. A crucial question concerning unified multimodal models is whether their understanding and generation capabilities can mutually enhance each other. Some studies [41,45] have provided evidence supporting this phenomenon. However, in contrast to the rich and comprehensive benchmarks for multimodal understanding, T2I benchmarks are often relatively",
            "score": 0.5878907759862659,
            "section_title": "Unified Multimodal Models",
            "char_start_offset": 6123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1083
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
            "score": 0.5870240973458767,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8349609375
        },
        {
            "corpus_id": "276902413",
            "title": "ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability",
            "text": "Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate\"algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io.",
            "score": 0.5858088030072233,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70458984375
        },
        {
            "corpus_id": "265506863",
            "title": "M$^{2}$Chat: Empowering VLM for Multimodal LLM Interleaved Text-Image Generation",
            "text": "Image Generation and Editing. The state-of-the-art generation model has shifted from GAN-based approaches to stable diffusion, as highlighted in the work by [24] and Song [34]. While stable diffusion is renowned for its strong and controllable image generation capabilities, as proposed by SDXL [27], other works have explored the editing problem in image generation by manipulating the input prompts, as seen in the studies by Cao [1] and Hertz [7]. Additionally, Zhang [43] introduced the concept of adding Controlnet to the diffusion model, which enhances the controllability of diffusion-based image generation. \n\nInterleaving Generation. Recent research has explored various approaches to integrate Multimodal Language Models (VLM) with text-image generation tasks. DALLE-3 [25] relies on prompts for generation without image conditions, while Emu [38] fine-tunes VLM for multimodal context generation but requires substantial computational Figure 2. Illustration of M 2 Chat, which features a generation pipeline that processes both image and text inputs, harnessing the capabilities of LLaMA-AdapterV2 [4] and SDXL [27] to craft high-fidelity image-text pairs. Our system excels in three key areas: Text-to-Image (T2I) generation, Storytelling, and Multimodal dialogue. Image generation occurs as VLM forward propagation yields hidden embeddings, which are then utilized to train the M 3 Adapter-distinguished by its minimal trainable parameters. \n\nresources. NextGPT [41] aligns audio, text, and image modalities using adapters but only aligns output text embeddings. DreamLLM [2] generates images and text simultaneously but also requires significant computational resources. MiniDALLE3 [11] proposes a two-stage generation method using a chat editing model, and SEED-LLaMA [5,6] aligns LLaMA and generation models with discrete vision tokens. Additionally, chat editing models for 3D models, such as 3D-GPT [36], show promise in this area. Moreover, there are also a lot of explorations of multi-modality generation [9,13,29,39,44].",
            "score": 0.5842920655571047,
            "section_title": "VLM Downstream Tasks",
            "char_start_offset": 5174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "231979499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "277349538",
            "title": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
            "text": "Driven by the aforementioned challenges, we present Lumina-Image 2.0, a unified and efficient T2I generative framework that comprises four key components: (1) a Unified Next-DiT model for generating images that faithfully aligned with the text input, (2) a Unified Captioner (UniCap) for producing high-quality text-image pairs, and a series of specific designs for (3) efficient training and (4) efficient inference. Specifically, to address architectural limitations, our Unified Next-DiT model utilizes a joint self-attention mechanism, enabling our model to process both textual and visual tokens in a fully end-to-end manner, similar to decoder-only transformers in recent large language models [23,24,25,26]. This design facilitates seamless multimodal interaction, allowing for the integration of additional multimodal tokens or specific prompt templates to extend the model's capabilities without modifying the core architecture. In response to the scarcity of high-quality textual descriptions in paired text-image data, we introduce Unified Captioner (UniCap), a unified captioning system specifically designed for T2I generation. UniCap excels at precisely understanding complex scenes, and generating comprehensive and coherent multilingual descriptions. Leveraging these capabilities, we employ UniCap to create multi-granularity, multi-dimensional textual descriptions that better align with the images. Furthermore, our experiments reveal that when combining the unified Next-DiT and UniCap for training, the text-to-image attention in transformer blocks dynamically adjusts its capacity based on the length of textual embeddings, behaving similarly to a dynamic feed-forward network. This observation motivates us to further enhance model capacity and performance by increasing the richness of textual descriptions without introducing additional parameters. Furthermore, both training and inference efficiency are crucial for model development and deployment. To perform efficient training, Lumina-Image 2.0 employs a multi-stage progressive training strategy with hierarchical high-quality data. The multi-domain system prompts and an auxiliary loss are further utilized to learn domain-specific knowledge and preserve low-frequency features, respectively.",
            "score": 0.5839302498512426,
            "section_title": "Introduction",
            "char_start_offset": 1672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.484130859375
        },
        {
            "corpus_id": "270067594",
            "title": "Multi-modal Generation via Cross-Modal In-Context Learning",
            "text": "The advancement of large language models (LLMs) [31,34] trained on extensive textual corpora has enabled remarkable adaptability across various modalities.\n\nEarlier works demonstrated the effectiveness of grounding text-only LLMs to images for vision-and-language tasks [9,20,27,46,57], as well as in embodied settings for robotics [2,14] and beyond.These methods leverage the capabilities of LLMs that are trained on large scale text-only data, while keeping the LLM weights frozen.In this work, we tackle the problem of generating novel images with lengthy text descriptions or complex sequence of text prompts by leveraging the capabilities of both LLMs [54] and diffusion models [37,38].\n\noutputs given lengthy multimodal inputs.To this end, we introduce a novel Cross-Modal Refinement Module to enable learning the cross-modal dependencies between text and image in the LLMs embedding space during training.This module aids the pre-trained LLM to explicitly learn the correspondence between text and image tokens using cross-attentions.By leveraging the refinement module, the model gains semantic understanding of the scene based on the input prompt sequence.Moreover, to enhance the fine grained details in the output, we incorporate a contextual object grounding module.Utilizing the in-context learning [5,29], we predict bounding boxes of the objects present in the prompt while maintaining the temporal consistency of the prompt sequence.Thereby, we collectively solve the problem of the object present in the scene and their count.\n\nExtensive quantitative and qualitative experiments are conducted on two datasets: Visual Story Generation (VIST) [19] and Visual Dialogue Context (VisDial) [12].Our MGCC performs favorably against text-to-image generation methods and state-of-the-art GILL [22].When handling multimodal context in VIST dataset, our MGCC outperforms the state-of-the-art approach in terms of both CLIP Similarity from 0.641 to 0.652 and LPIPS score from 0.693 to 0.679.",
            "score": 0.5814067958095838,
            "section_title": "Introduction",
            "char_start_offset": 2094,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 157,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 483
                },
                {
                    "start": 483,
                    "end": 691
                },
                {
                    "start": 693,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1165
                },
                {
                    "start": 1165,
                    "end": 1278
                },
                {
                    "start": 1278,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1543
                },
                {
                    "start": 1545,
                    "end": 1706
                },
                {
                    "start": 1706,
                    "end": 1806
                },
                {
                    "start": 1806,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 687,
                    "end": 690,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1312,
                    "end": 1315,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 1701,
                    "end": 1705,
                    "matchedPaperCorpusId": "1820614"
                },
                {
                    "start": 1801,
                    "end": 1805,
                    "matchedPaperCorpusId": "258959284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61083984375
        },
        {
            "corpus_id": "265608774",
            "title": "Effectively Fine-tune to Improve Large Multimodal Models for Radiology Report Generation",
            "text": "The last few years have witnessed the rise of both uni-and multi-modal foundational models [4]. These models [5,28,32], pretrained on millions or even billions of data, demonstrate impressive zero-shot capabilities and can often be easily adapted for downstream tasks via fine-tuning. More recently, LLMs have shown increasingly astonishing generation capabilities as they scale up to billions of parameters. However, it is challenging for vision-language generative models to keep up with the scale as collecting paired image-text data for training is naturally much harder. Thus, many recent works have focused on combining pretrained vision models and large language models for multimodal generation tasks. In this work, we followed a popular framework of combing pretrained vision models and large language models by using a mapping network to project visual features to langauge model's text embedding space as soft visual prompts [25,13,23,18].",
            "score": 0.5794190059065056,
            "section_title": "Large Multimodal Models",
            "char_start_offset": 2933,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 950
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6171875
        },
        {
            "corpus_id": "231719672",
            "title": "VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs",
            "text": "Our goal is to design a unified framework that can generate open-ended text from video and accompanying modalities, e.g., audio, speech, or dialog history. We are specifically interested in tasks such as video captioning, question answering and audio-visual scene-aware dialog.\n\nFormally, let x = {x 1 , x 2 , ..., x M } be a multimodal input sample, where x m denotes the m-th modality. We specify the task that we want our model to address using a special task token t \u2208 {Answer, Caption, Dialog, ...}. Our goal is then to train a model F(t, x 1 , x 2 , ..., x M ; W) that generates a sequence of text tokens y = [y 1 , y 2 , ..., y N ] representing the output for task t. W denotes the trainable parameters. Depending on the task, our generated text may be in the form of answers, questions, interactive responses in a dialog, or captions.\n\nAt a high-level, our approach can be summarized in three steps. First, we leverage pretrained modality-specific classifiers to obtain most probable category predictions for each modality. We then embed the textual names of the predicted categories into a semantic language space via our proposed differentiable tokenization scheme, which enables end-to-end training of the whole system including the modality-specific classifiers. Finally, we employ a generative encoder-decoder language model [40] for mapping the embedding vectors from the multiple modalities into freeform text. This allows us to reformulate different \"video+x to text\" problems as a single sequence-to-sequence task. We now present each of these steps in more detail.",
            "score": 0.5771774755341499,
            "section_title": "Technical Approach",
            "char_start_offset": 6062,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "A unified model capable of both understanding and generating multimodal content holds immense potential: it could generate images based on complex instructions, reason about visual data, and visualize multimodal analyses through generated outputs. The unveiling of GPT-4o's enhanced capabilities [27] in March 2025 has further highlighted this potential, sparking widespread interest in unification. \n\nHowever, designing such a unified framework presents significant challenges. It requires integrating the strengths of autoregressive models for reasoning and text generation with the robustness of diffusion-based models for highquality image synthesis. Key questions remain unresolved, state of unified multimodal models (as illustrated in Fig. 1), thereby benefiting future research endeavors, we present this survey. We begin by introducing the foundational concepts and recent advancements in both multimodal understanding and image generation, covering both autoregressive and diffusion-based paradigms. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. Within the autoregressive and hybrid categories, we further classify models based on their image tokenization strategies, reflecting the diversity of approaches in this area. \n\nBeyond architecture, we assemble datasets and benchmarks tailored for training and evaluating unified multimodal models. These resources span multimodal understanding, text-to-image generation, image editing, and other relevant tasks, providing a foundation for future exploration. Finally, we discuss the key challenges facing this nascent field, including efficient tokenization strategy, data construction, model evaluation, etc. Tackling these challenges will be crucial for advancing the capabilities and scalability of unified multimodal models. \n\nIn the community, there exist excellent surveys on large language models [40], [41], multimodal understanding [42], [43], [44], and image generation [45], [46], while our work focuses specifically on the integration of understanding and generation tasks. Readers are encouraged to consult these complementary surveys for a broader perspective on related topics. We aim to inspire further research in this rapidly evolving field and provide a valuable reference for the community.",
            "score": 0.576975105131979,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1950
                },
                {
                    "start": 1953,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2314
                },
                {
                    "start": 2315,
                    "end": 2432
                }
            ],
            "ref_mentions": [
                {
                    "start": 2032,
                    "end": 2036,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 2063,
                    "end": 2067,
                    "matchedPaperCorpusId": "271750640"
                },
                {
                    "start": 2069,
                    "end": 2073,
                    "matchedPaperCorpusId": "265308931"
                },
                {
                    "start": 2075,
                    "end": 2079,
                    "matchedPaperCorpusId": "259243718"
                },
                {
                    "start": 2102,
                    "end": 2106,
                    "matchedPaperCorpusId": "252199918"
                },
                {
                    "start": 2108,
                    "end": 2112,
                    "matchedPaperCorpusId": "265821110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77880859375
        },
        {
            "corpus_id": "266573844",
            "title": "MIVC: Multiple Instance Visual Component for Visual-Language Models",
            "text": "By incorporating visual models with the capabilities of pre-trained Large Language Models (LLMs), multimodal LLMs have demonstrated dramatic improvements in various tasks, such as visual question answering (VQA), captioning, and etc.The majority of recent multimodal LLMs [8,18,20] share a similar framework by utilizing separate vision and text towers to independently encode the two modalities first.The encoded single modality representations are then fused together, e.g. by projecting image representation via a single or multiple projection layers, or by directly concatenating, and then fed into LLMs.Depending on how image representation embedding is generated, it can be further categorized into two types: first, image patch embedding based vision tower in Figure 1 (a), which is generally composed of a single visual foundation model encoder (e.g.ViT [9]) and utilizes the generated image patch embedding directly as visual representation; and second, image patch and query embedding based vision tower in Figure 1 (c), which sequentially combines a vision foundation model's encoder and a query embedding module (such as Q-former in BLIP-2 [18] or the Perceiver Resampler as in Flamingo [1]).\n\nOne constraint to such a framework is the lack of capability to process multiple image inputs per request, when all images contribute and correspond to a single label.These multiple images, also referred to as multiple instances, typically carry complementary information; therefore they are more informative than a single instance for the corresponding task and shouldn't be ignored.Such applications are not rare in industry and other scientific areas, including utilizing multiple product images corresponding to a single product for e-commerce-related classification, caption generation, product information inference, synthesizing multiple X-ray images for medical diagnosis [14], and geological simulation from multiple underground mapping [19], among others.To the best of our knowledge, all current visual-language models only consider a single image instance as input.Although it can be adapted to multiple image instances, this is largely achieved through customization at the input stage, either by taking only one single image as input or by concatenating multiple raw images into a single image.",
            "score": 0.5769615619516877,
            "section_title": "Architecture Overview",
            "char_start_offset": 9436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1204
                },
                {
                    "start": 1206,
                    "end": 1373
                },
                {
                    "start": 1373,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1971
                },
                {
                    "start": 1971,
                    "end": 2083
                },
                {
                    "start": 2083,
                    "end": 2314
                }
            ],
            "ref_mentions": [
                {
                    "start": 1199,
                    "end": 1202,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1886,
                    "end": 1890,
                    "matchedPaperCorpusId": "3537028"
                },
                {
                    "start": 1952,
                    "end": 1956,
                    "matchedPaperCorpusId": "67161442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63427734375
        },
        {
            "corpus_id": "269354784",
            "title": "RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery",
            "text": "Vision-language models represent a category of models that integrate computer vision and NLP techniques to achieve a comprehensive understanding of visual and textual data.The field of research in this area has witnessed significant advancements, encompassing a wide range of techniques and architectures that aim to fuse vision and language, thereby enhancing performance and capabilities across various tasks.\n\nOne approach adopted by these models involves learning generic multi-modal representations from a large corpus of image-text pairs.Models such as CLIP [46] and ALIGN [47] employ dual encoders trained with a contrastive objective on extensive imagetext datasets, enabling them to model the interaction between modalities.However, these models do not natively support multi-modal generation tasks (e.g., image captioning and VQA).Other models, like BLIB [44] and CoCa [45], combine contrastive and generative pre-training to support both cross-modal alignment and multi-modal generation tasks.\n\nAnother paradigm of vision-language models are the generalized models that are trained to jointly perform multiple tasks without task-specific fine-tuning.For instance, UniT [48] unifies different tasks in a single model by designing a decoder with per-task query embedding and task-specific output heads.OFA [49] formulates different visionlanguage tasks as sequence-to-sequence tasks and trains a transformer model on them without task-specific layers.GIT [50] is a multi-tasking transformer-based model that conditions the decoder on vision inputs and text to generate the output text.Flamingo [51] extends sequence-to-sequence models to support interleaved image, video and text inputs for generating the output text.\n\nBuilding upon the success of GPT models, GPT-4 [14] has exhibited impressive capabilities in engaging in multi-modal dialogues with humans.The open-source project LLaVA [15] aims to replicate this performance by aligning visual representations with the input space of the LLM.By leveraging the original self-attention mechanism within the LLM, LLaVA enables effective processing of visual information alongside textual input.Multimodal-GPT [52] utilizes gated cross-attention layers to facilitate seamless interactions between images and text.",
            "score": 0.5756578271059138,
            "section_title": "Vision-Language Models in General Computer Vision",
            "char_start_offset": 13494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 411
                },
                {
                    "start": 413,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 1004
                },
                {
                    "start": 1006,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1311
                },
                {
                    "start": 1311,
                    "end": 1460
                },
                {
                    "start": 1460,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1727
                },
                {
                    "start": 1729,
                    "end": 1868
                },
                {
                    "start": 1868,
                    "end": 2005
                },
                {
                    "start": 2005,
                    "end": 2154
                },
                {
                    "start": 2154,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "221279805"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "226602850"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "249926846",
            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
            "text": "People are generally able to conjure rich and detailed scenes through descriptions expressed in written or spoken language. Supporting the ability to generate images based on such descriptions can potentially unlock creative applications in many areas of life, including the arts, design, and multimedia content creation. Recent research on text-to-image generation, e.g., DALL-E [2] and CogView [3], has made significant progress in generating high-fidelity images and demonstrating generalization capabilities to unseen combinations of objects and concepts. Both treat the task as a form of language modeling, from textual descriptions into visual words, and use modern sequenceto-sequence architectures like Transformers [4] to learn the relationship between language inputs and visual outputs. A key component of these approaches is the conversion of each image into a sequence of discrete units through the use of an image tokenizer such as dVAE [5] or VQ-VAE [6]. Visual tokenization essentially unifies the view of text and images so that both can be treated simply as sequences of discrete tokens-and thus amenable to sequence-to-sequence models. To that end, DALL-E and CogView employed decoder-only language models, similar to GPT [7], to learn from a large collection of potentially noisy text-image pairs [8,9]. Make-A-Scene [10] further expands on this two-stage modeling approach to support both text and scene-guided image generation. \n\nA different line of research with considerable momentum involves diffusion-based text-to-image models, such as GLIDE [11] and concurrent works DALL-E 2 [12] (a.k.a., unCLIP) and Imagen [13]. These models eschew the use of discrete image tokens in favor of diffusion models [14,15] to directly generate images. These models improve zero-shot Fr\u00e9chet Inception Distance (FID) scores on MS-COCO [16] and produce images of markedly higher-quality and greater aesthetic appeal compared to previous work.",
            "score": 0.5722156110562944,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1449
                },
                {
                    "start": 1452,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 396,
                    "end": 399,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 1317,
                    "end": 1320,
                    "matchedPaperCorpusId": "231951742"
                },
                {
                    "start": 1320,
                    "end": 1322,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 1725,
                    "end": 1729,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1729,
                    "end": 1732,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56,4,29,33,31,35], to embodied settings for robotics [3,18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. \n\nIn this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4,2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special FROMAGe Retrieval only multimodal language model. Image outputs limited to the candidate retrieval set.",
            "score": 0.5714366515278524,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1869
                }
            ],
            "ref_mentions": [
                {
                    "start": 1180,
                    "end": 1184,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1328,
                    "end": 1332,
                    "matchedPaperCorpusId": "51876975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.673828125
        },
        {
            "corpus_id": "270391793",
            "title": "VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks",
            "text": "Conventional MLLMs. With the advancement of large language models (LLMs) [145,146,21,215,171,37,172,13,170,9,103,7,59,43,22], multimodal large language models (MLLMs) have also gained significant momentum recently. Notable commercial models include GPT-4V [2], Gemini series [169,150], Claude-3 [10], and Qwen-VL-Max [14], known for their outstanding performance. \n\nEarly open-source MLLMs like InstructBLIP [42], LLaVA [107] and MiniGPT-4 [223] fine-tune on instruction-following datasets. InternVL [34,33] series models align a large-scale vision encoder with LLMs and perform comparably to commercial models. Efficient MLLMs [100,228,38] have also studied. However, these models only can output text, restricting their applications. \n\nExtension of MLLMs' Text Output. To extend MLLMs to downstream tasks, models like Kosmo-2 [140], Shikra [27], VisionLLM [182], Ferret [201,212], and All-Seeing V2 [180] achieve this using specially-designed tokens or encoding coordinates as text tokens. Despite these advancements, using LLMs solely as visual decoders falls short of resolving the fine-grained visual context needed for precise detection and segmentation. The other line of works focus on broadening the modality scope. AnyGPT [210] builds a multimodal text-centric dataset for any-to-any multimodal generation (text, image, speech, music) with sequence modeling. Chameleon [168] uses fully token-based representations for both texts and images, capable of understanding and generating interleaved imagetext sequences. CM3leon [5,205] are autoregressive models for text-to-image and image-to-text tasks. All these works could unify image understanding and generation in one network. Our model can support more vision and vision-language tasks. \n\nMLLMs w/ Downstream Tools.",
            "score": 0.5700337038219535,
            "section_title": "Multimodal Large Language Model",
            "char_start_offset": 5416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 420,
                    "end": 425,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 440,
                    "end": 445,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 858,
                    "end": 863,
                    "matchedPaperCorpusId": "258762579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "274280479",
            "title": "UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing",
            "text": "Two main challenges need to be solved for building such a unified multimodal LLM framework. The first challenge is creating a unified representation space across 3D poses and texts, enabling the unification of diverse pose-relevant tasks. Existing work [19] processes 3D poses and texts differently, encoding 3D poses as continuous high-level features while tokenizing linguistic texts into discrete token sequences. This non-unified processing incurs an extra burden on LLMs to model interactions between 3D poses and texts, hindering the unifying of pose comprehension, generation and editing. The second challenge lies in achieving fine-grained pose perception within the visual branch of the multimodal framework. Most MLLMs [4,19,44,66] employ CLIP [53] as their visual branch. While CLIP's visual encoder aligns well with the text embedding space through image-text contrastive learning, it struggles to capture detailed pixel-level information, such as keypoints and parsing maps, due to the global supervision provided by image captions. This limitation constrains MLLM's capabilities in fine-grained pose comprehension and generation. \n\nTo address these challenges, we propose UniPose, a uniform multimodal framework for human pose comprehension, generation and editing, which harnesses the powerful language generation abilities of LLMs to unify various pose-relevant tasks (Tab. 1). UniPose comprises three tires. Firstly, UniPose is equipped with a pose tokenizer for processing 3D poses and texts uniformly. Inspired by the obser-vation that human poses exhibit a semantic coupling similar to language [32,47,69], we treat 3D pose as a specific language. Akin to language, the pose tokenizer compresses raw 3D pose into a sequence of discrete semantic tokens. By encoding both 3D pose and language within a shared vocabulary, we build a unified representation space across 3D poses and texts, which enables LLMs to be easily adapted to handle pose comprehension, generation, and editing.",
            "score": 0.5666437197294711,
            "section_title": "Introduction",
            "char_start_offset": 3462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "269330276"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "269330276"
                },
                {
                    "start": 735,
                    "end": 738,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 754,
                    "end": 758,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1615,
                    "end": 1619,
                    "matchedPaperCorpusId": "259262201"
                },
                {
                    "start": 1619,
                    "end": 1622,
                    "matchedPaperCorpusId": "270063983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.379638671875
        },
        {
            "corpus_id": "266573642",
            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
            "text": "integrating multiple modalities. Overall, the architecture enables the integration of diverse modalities into a large language model for multimodal understanding and response generation. \n\nPandaGPT is a model that integrates multimodal encoders from ImageBind and language models from Vicuna to perform instruction-following tasks across six modalities: image/video, text, audio, depth, thermal, and IMU. It demonstrates the ability to connect information from different modalities and compose their semantics naturally, enabling tasks such as image description generation, story writing inspired by videos, and answering questions about audios. PandaGPT's training on aligned image-text pairs allows it to display emergent cross-modal capabilities for data other than image and text, paving the way for holistic understanding of inputs across different modalities. \n\nLAMM introduces the Language-Assisted multimodal (LAMM) dataset, framework, and benchmark, aiming to facilitate the and evaluation of multimodal large language models (MLLMs). The main contributions include the comprehensive dataset and benchmark covering a wide range of vision tasks for 2D and 3D vision, a detailed methodology for constructing multimodal instruction tuning datasets, and a primary MLLM training framework optimized for modality extension. Additionally, the paper provides baseline models, extensive experimental observations, and analysis to accelerate future research in the field of multimodal language models. \n\nLLaVAR As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework. \n\nOtterHD introduces OtterHD-8B model, which addresses the limitations of fixed-resolution inputs in Large Multimodal Models (LMMs). It leverages the Fuyu-8B ar-Fig. 28: Illustration of SEED-LLaMA [137].",
            "score": 0.5657981395533265,
            "section_title": "Visual Assistant",
            "char_start_offset": 68635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1500
                },
                {
                    "start": 1503,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2111
                },
                {
                    "start": 2114,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2277
                },
                {
                    "start": 2278,
                    "end": 2315
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "272694523",
            "title": "OmniGen: Unified Image Generation",
            "text": "Generative Foundation Models. The GPT series [46,53] have demonstrated that language models can learn numerous 2 https://huggingface.co/openai/clip-vit-large-patch14-336 tasks via training on a large-scale dataset. Beyond language, multi-modal large language models [11,37] have been proposed to integrate vision and language capabilities. However, they lack the capability to generate images. Some works propose integrating LLMs with diffusion models to equip LLMs with image generation capability [18,60,61,69]. Others use discrete tokens to support both image and text generation simultaneously [41,62,71]. They focus on multi-modal generation with limited image-generation capability. Concurrent works such as TransFusion [79] and Show-O [71] unify diffusion and autoregressive methods into a single model, generating text autoregressively and images through diffusion. Nonetheless, like existing diffusion models, these works focus on a limited range of image generation tasks, primarily text-to-image generation, and cannot cover more complex and various visual generation tasks. The construction of a universal foundation model for image generation remains unclear and has not been fully explored. \n\nDiffusion Model. Recent advancements in diffusion models have been remarkable, with notable contributions from the Stable Diffusion (SD) series [12,51,55], DALL-E [54], and Imagen [25]. These models are predominantly designed for text-to-image generation tasks. Many efforts have been made to extend the capabilities of diffusion models, such as ControlNet [75], T2I-Adapter [43], StyleShot [15]. Instruct-Pix2Pix [3], InstructSD [48] and EMU-edit [58] explore performing general image editing tasks through instructions. However, these methods are task-specific, extending the capabilities of SD by modifying the model architecture. In contrast, OmniGen is a model that natively supports various image-generation tasks, and no longer requires any preprocessing steps or assistance from other models. There is some work exploring the unification of computer vision (CV) tasks [2,14,19,65].",
            "score": 0.5632561006288439,
            "section_title": "Related Work",
            "char_start_offset": 26526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1204
                },
                {
                    "start": 1207,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2096
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 503,
                    "end": 506,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "265506621"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "266573555"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1358,
                    "end": 1361,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1564,
                    "end": 1568,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 1655,
                    "end": 1659,
                    "matchedPaperCorpusId": "265221391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5205078125
        },
        {
            "corpus_id": "273374871",
            "title": "CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning",
            "text": "Existing caption rewriting works (e.g., VeCLIP (Lai et al., 2024)) rely on a multimodal captioning model to generate captions that are short sentences containing visual concepts. Image captions can be very descriptive but often only cover the most salient object of the scene, they are coarse-grained in structure (whole sentence or paragraph), and are hard to modify. Our key distinction is that VTM produces a comprehensive list of metadata information that describes the visual concepts in an image as completely as possible. \n\nLanguage Model. Large language models (LLMs) have exhibited strong instruction-following capabilities. The goal of an LLM in CtrlSynth is to take an input textual instruction on how to generate a synthetic text that meets the requirements specified in the instruction. CtrlSynth employs the reasoning and composition capability of LLMs to recombine the visual image tags in the task instruction and compose new synthetic texts. The instruction for an LLM consists of three parts (Figure 3): (i) task template that specifies the details of the text synthesis task, (ii) task content that contains the actual visual tags (phrases) and an optional caption paired with the image, and (iii) task constraint that describes the style and formatting of the output text. Users can also apply custom policies over the instructions to guide the text synthesis process. \n\nText-to-Image Model. Text-to-image models generate novel and diverse image samples based on different input text prompts. CtrlSynth applies an image controller to account for the user-specified control policies and accordingly, updates the input text instructions from the previous step (i.e., language model). These updated instructions are then fed to text-to-image models for generating the image as an output. In our experiments, we use StableDiffusion models for text-to-image generation. \n\nText and Image Controllers. The controller in CtrlSynth is a function that takes an input text and transforms it into a specific text instruction for the LLM or text-to-image model. \n\nThe text controller accepts the visual tags of an image and a user-defined policy along with an optional original text as input and produces instructions to control the generation of synthetic text.",
            "score": 0.5625013888705405,
            "section_title": "KEY COMPONENTS",
            "char_start_offset": 11182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2068
                },
                {
                    "start": 2071,
                    "end": 2269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "277065986",
            "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating human-like text. Advanced transformer-based architectures such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and T5 (Raffel et al., 2020) capture contextual relationships within text, enabling tasks like summarization, question answering, and code generation. However, LLMs process only textual data, limiting their utility in domains requiring multimodal understanding, such as image or video analysis. Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation.",
            "score": 0.5612261349265448,
            "section_title": "Addressing the Problem",
            "char_start_offset": 4189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 253,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "248721934",
            "title": "Deep learning and synthetic media",
            "text": "increasingly easy to guide image generation directly with text. DALL-E, a new multimodal Transformer model trained on a dataset of text-image pairs, is capable of generating plausible images in a variety of styles simply from a text description of the desired output . DALL-E's outputs can exhibit complex compositional structure corresponding to that of the input text sequences, such as \"An armchair in the shape of an avocado\", \"a small red block sitting on a large green block\", or \"an emoji of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants\". DALL-E has been developed jointly with another multimodal model, called CLIP, capable of producing a natural language caption for any input image . Using CLIP to steer the generation process, it is also possible to produce images with GANs from natural language descriptions of the desired output (Galatolo et al., 2021;Patashnik et al., 2021;see Fig. 5).\n\nIf these trends continue-and there is no reason for them to slow down significantly as hardware improvement and architectural breakthroughs continue to spur larger and more efficient models, it is only a matter of time before DL algorithms allow us to generate high-resolution stylized or photorealistic samples of arbitrary scenes that are consistently indistinguishable from human-made outputs. In the domain of static visual media, that goal is already within sight for medium to large resolutions (around 1024x1024 pixels at the time of writing). The domain of dynamic visual media presents a significantly greater challenge, as the spatiotemporal consistency of the scene needs to be taken into account. Nonetheless, it is plausible that we will be able to synthesize realistic and coherent video scenes at relatively high resolution in the short to medium term, beyond mere face swapping in existing videos.",
            "score": 0.5610835016009372,
            "section_title": "Totally synthetic DLSAM",
            "char_start_offset": 21091,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 875,
                    "end": 898,
                    "matchedPaperCorpusId": "231749958"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "The development of large-scale vision-and-language models is significantly impacting a wide range of fields like automated dialogue systems and digital content creation. With the surge in research and development in this domain, the current state-of-the-art Large Language Models (LLMs) (OpenAI, 2023;Chiang et al., 2023;Ouyang et al., 2022) and vision-and-language models such as (Wu et al., 2023a;Li et al., 2023c;Tsimpoukelli et al., 2021;Alayrac et al., 2022) fall short in generating coherent multimodal outputs. This limitation becomes particularly evident in tasks that demand an integrated handling of vision and language, essential for the next generation Large Language Models (LLMs). \n\nOur work, as illustrated in Fig. 1, seeks to address these shortcomings by enhancing the integration of text and image generation in LLMs. The challenges in developing a multimodal LLM capable of interleaved vision and language generation are manifold. First, LLMs typically lack mechanisms to directly produce images, prompting us to introduce \"generative vokens\" that bridge the gap between textual and visual feature spaces. Second, the constraint of data scarcity, especially in visionand-language tasks (Sharma et al., 2018) lacking extensive detailed descriptions of images (Huang et al., 2016), is countered by our unique description-free training approach. Third, maintaining both image-text and image-image consistency poses a significant challenge, which we address through dual-loss strategies. Finally, as we push forward the boundaries with LLMs, the large memory requirements urge us to devise more efficient end-to-end strategies and create an efficient training pipeline accessible for the community, especially in downstream tasks. \n\nSpecifically, to overcome these challenges, we present MiniGPT-5, a novel approach for interleaved vision-and-language generation. By combing the Stable Diffusion with LLMs through special visual tokens (Tan & Bansal, 2020) -\"generative vokens\", we develop a new approach for multimodal generation. Our two-stage training methodology emphasizes a description-free foundational phase, enabling effective model training even with limited caption-grounded images.",
            "score": 0.5603334556258666,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 341,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 416,
                    "end": 442,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 442,
                    "end": 463,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1205,
                    "end": 1226,
                    "matchedPaperCorpusId": "51876975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5390625
        },
        {
            "corpus_id": "273850262",
            "title": "REEDIT: Multimodal Exemplar-Based Image Editing",
            "text": "Diffusion Models. Prior to diffusion models, GANs [13,59,63] were the de-facto generative models used for (conditional) image synthesis and editing. However, training GAN networks is prone to instability and mode collapse, among many issues. Recently, large-scale text-to-image generative models [8, 9, 39-41, 43, 58] have benefitted from superior model architectures [50] and large-scale training data available on the internet. Of particular interest is diffusion models [17,35,37,40,41,43,46], that are trained to denoise random gaussian noise resulting in high-fidelity and highly diverse images. These models are typically trained on millions of text-image pairs. In this work, we use a pretrained Stable Diffusion [41] model which operates in the latent space instead of the image pixel space. \n\nMultimodal Vision-Language Models (VLMs). Multimodal VLMs [25, 27-29, 38, 45] have the remarkable capability to understand and process both texts and images. Two particularly useful works fall in the scope of this paper: CLIP [38] and LLaVA [27][28][29]. CLIP represents both images and texts in a shared embedding space. It was trained on 400M image-text pairs in a contrastive manner -maximizing the similarity between related image-text embeddings, while minimizing the similarity between unrelated image-text embeddings. LLaVA combines a visual encoder with Vicuna [5] to provide powerful language and visual comprehension capabilities. It has impressive capacity to follow user instructions based on visual cues. \n\nText-based Image Editing. Diffusion models, with their impressive generative capabilites, have also been adapted for image editing [7,10,19,23,24,33,36,42,49,56,60,62]. Multimodal models like CLIP [38], and cross-attention mechanisms [50] have enabled conditioning a diffusion model to directly edit an image with a text input [2,35]. SDEdit [32] takes an image as input along with a user guide, and subsequently denoises it using SDE prior to increase its realism.",
            "score": 0.5601558972570391,
            "section_title": "Related Work",
            "char_start_offset": 3691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 57,
                    "matchedPaperCorpusId": "1277217"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "14924561"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1043,
                    "end": 1047,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1662,
                    "end": 1665,
                    "matchedPaperCorpusId": "244909410"
                },
                {
                    "start": 1668,
                    "end": 1671,
                    "matchedPaperCorpusId": "253581838"
                },
                {
                    "start": 1671,
                    "end": 1674,
                    "matchedPaperCorpusId": "256616002"
                },
                {
                    "start": 1674,
                    "end": 1677,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "253801961"
                },
                {
                    "start": 1683,
                    "end": 1686,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1686,
                    "end": 1689,
                    "matchedPaperCorpusId": "254408758"
                },
                {
                    "start": 1719,
                    "end": 1723,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1849,
                    "end": 1852,
                    "matchedPaperCorpusId": "244714366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "271709458",
            "title": "REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language Models",
            "text": "Generative vision-language models [36,44] represent a significant step towards developing multimodal systems that bridge the gap between computer vision and natural language processing. Text-to-image (T2I) models [5,38] convert text prompts to high-quality images, while multimodal large language models (MLLMs) [25,48] process images as inputs, and generate rich and coherent natural language outputs in response. As a result, these models have found diverse applications in robotics [45], image editing [17], image-to-image translation [31], and more. However, recent studies [20] and benchmarks such as Fig. 1: Text-to-Image models struggle to generate images that faithfully represent the spatial relationships mentioned in the input prompt. We develop REVISION, an efficient rendering pipeline that enables a training-free and guidance-based mechanism to address this shortcoming. Our method results in improvements in spatial reasoning for T2I models for three dimensional relationships demonstrated by consistently higher scores on VISOR and T2I-CompBench benchmarks. \n\nDALL-Eval [8], VISOR [15], and T2I-CompBench [18] have found that generative vision-language models suffer from a common mode of failure -their inability to correctly reason over spatial relationships. We postulate that the lack of spatial understanding in generative visionlanguage models is a result of the lack of guidance from image-text datasets. Compared to T2I models, graphics rendering tools such as Blender allow deterministic and accurate object placement, but are limited by their lower visual detail and photorealism and do not have intuitive workflows such as T2I models where users can generate images by simply typing a sentence. To get the best of both worlds, in this work, we develop REVISION, a Blender-based image rendering pipeline which enables the synthesis of images with 101 3-dimensional object (assets), 11 spatial relationships, diverse backgrounds, camera perspectives, and lighting conditions. REVISION parses an input text prompt into assets and relationships and synthesizes the scene using Blender to exactly match the input prompt in terms of both objects and their spatial arrangement.",
            "score": 0.5573640850286258,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1074
                },
                {
                    "start": 1077,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 38,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 312,
                    "end": 316,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "251252882"
                },
                {
                    "start": 538,
                    "end": 542,
                    "matchedPaperCorpusId": "256616002"
                },
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "264805345"
                },
                {
                    "start": 1087,
                    "end": 1090,
                    "matchedPaperCorpusId": "253510037"
                },
                {
                    "start": 1122,
                    "end": 1126,
                    "matchedPaperCorpusId": "259847295"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454345703125
        },
        {
            "corpus_id": "255096283",
            "title": "Do DALL-E and Flamingo Understand Each Other?",
            "text": "In this work, we investigated mutual understanding between multimodal image-to-text models and text-to-image models through image-text-image and text-image-text reconstruction tasks. We found that the reconstruction quality of text-to-image and image-to-text models can be utilized to evaluate the quality of the text or image generation. Specifically, the best textual description for an image is one that leads to a better reconstruction of the input image. The best image representation of text input is one that leads to a better recovery of the original text. Leveraging these findings, we proposed a novel framework for finetuning the image captioning and image generation models. We demonstrated enhanced performance of our models on both tasks. Our work advocates further exploring multimodal communication between text-to-image and image-to-text models. Finally, our work demonstrated the value of symbolic sentences to convey information: Image content can effectively be compressed into a sentence, and a sentence can be reconstructed as an image. This latter step can be considered a form of grounded cognition or embodiment. \n\nwith sampling steps of 50 and a guidance scale of 7.5. The output image size is 512. The number of minimal words is set to 5 and the maximum number of words is set to 30. We set p=0.9 in the Top-p strategy. Top-p sampling, which is also referred to as nucleus sampling, is a text generation method that samples words from a set of candidates whose cumulative probability exceeds a specified threshold p. By varying p, we achieve a trade-off between the diversity and accuracy of the generated text. Broadly speaking, larger values of p result in more diverse captions, whereas smaller values of p lead to less variable yet more accurate captions for an input image.",
            "score": 0.5569828140124031,
            "section_title": "Conclusion",
            "char_start_offset": 32531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1805
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2269287109375
        },
        {
            "corpus_id": "266210376",
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "text": "Multimodal Pre-training in the Pre-LLM Era. Prior research efforts primarily concentrated on model architecture to facilitate the fusion and interaction of cross-model data [6,50,52]. The success of transformers in language models [42] and ViT [10] inspired the development of unified multi-modal modeling [27,44]. Although images and language can be processed by a unified model with shared parameters, they often have distinct training objectives. It is worth mentioning that the BEiT series [2,45] successfully adapted the masked language modeling objective from BERT [8] to vision and multimodal pre-training. \n\nMultimodal Pre-training in the LLM Era. Building upon pre-trained large language models (LLMs) [33,40,41,54], recent studies have effectively developed multimodal language models capable of processing image and text inputs to generate text outputs [1, 22,24,25,56]. Another challenge for large multimodal models is generating multimodal content beyond language. Several efforts, such as Visual ChatGPT [47] and HuggingGPT [38], have achieved this by connecting LLMs with other generation tools within an LLM integration framework, e.g., LangChain. However, these systems exhibit instability and limited room for further optimization. To enable LLMs to generate images with optimization, M-VADER [46] aligns the semantic consistence between an LLM and a diffusion decoder by training them on image-text pair data. GILL [18] achieves more complex interleaved image-text generation by mapping the embedding spaces of the LLM to text-to-image generation models. NExT-GPT [48] extends this concept to additional modalities, such as audio and video. DreamLLM [9] facilitates passing the differential gradient from image diffusion models to language models, enabling the generation of free-form interleaved content. Following similar methods, Kosmos-G [30] enhances the fidelity of generated images in context through a compositional instruction tuning task. \n\nIn contrast to our VL-GPT, these studies mainly focus on  leveraging existing LLMs and exploring the integration of current image encoders and image generation models into LLMs.",
            "score": 0.5568189081418484,
            "section_title": "Related Work",
            "char_start_offset": 6155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 43
                },
                {
                    "start": 44,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 313,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 711,
                    "end": 715,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.732421875
        },
        {
            "corpus_id": "265220926",
            "title": "Chain of Images for Intuitively Reasoning",
            "text": "The monumental advancements in large language models within the domain of natural language processing have spurred a surge in efforts to adapt these successes to other modalities. Unlike these language models, multimodal models are designed to accept or produce input in various modalities beyond just natural language. These models incorporate additional modules to understand and generate content in formats such as images, voice, or video. \n\nThe underlying principle of this work involves integrating the pre-trained language model with another pre-train model for a specific modality, such as images. For instance, Li et al. (2023) proposes to train a lightweight Transformer to align the image representation from an image encoder with the textual representation from an LLM. Drawing inspiration from the instruction tuning techniques used in pure language models, Liu et al. (2023a) employs data generated by GPT-4 to enhance question-answering performance. Alayrac et al. (2022) achieves closer integration of image and text data by incorporating image information into the LLM through cross attention layer instead of embedding. Moreover, Girdhar et al. (2023) proposes a joint embedding across six different modalities, including images, text, audio, depth, terminal, and IMU data, without the need for training on the last four modalities. This is achieved by leveraging the zero-shot capacity of the large image-text models. The above-mentioned models do not possess image-generation capabilities. To make up for this shortcoming, Dong et al. (2023) realize image generation by combining the model proposed by Liu et al. (2023a) with the Stable diffusion (Rombach et al., 2022). Building on this, Wu et al. (2023) further integrates it with the model from Girdhar et al. (2023), utilizing its ability to align multiple modalities to generate and accept multiple modalities. Very recently, GPT-4 (OpenAI, 2023) has demonstrated its astounding ability to understand and generate in the modalities of images and voice. In the examples it showcased, the GPT-4 has already reached a level close to human capabilities, adding both confidence and pressure to the research in this field.",
            "score": 0.5561857393985821,
            "section_title": "MULTIMODEL LARGE LANGUAGE MODEL",
            "char_start_offset": 25133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 964,
                    "end": 985,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1147,
                    "end": 1168,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 1666,
                    "end": 1688,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1767,
                    "end": 1788,
                    "matchedPaperCorpusId": "258564264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "274992513",
            "title": "TextMatch: Enhancing Image-Text Consistency Through Multimodal Optimization",
            "text": "Text-to-image generative models excel in creating images from text but struggle with ensuring alignment and consistency between outputs and prompts. This paper introduces TextMatch, a novel framework that leverages multimodal optimization to address image-text discrepancies in text-to-image (T2I) generation and editing. TextMatch employs a scoring strategy powered by large language models (LLMs) and visual question-answering (VQA) models to evaluate semantic consistency between prompts and generated images. By integrating multimodal in-context learning and chain of thought reasoning, our method dynamically refines prompts through iterative optimization. This process ensures that the generated images better capture user intent of, resulting in higher fidelity and relevance. Extensive experiments demonstrate that TextMatch significantly improves text-image consistency across multiple benchmarks, establishing a reliable framework for advancing the capabilities of text-to-image generative models. Our code is available at https://anonymous.4open.science/r/TextMatch-F55C/.",
            "score": 0.5552453717782482,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.583984375
        },
        {
            "corpus_id": "252781022",
            "title": "Visualize Before You Write: Imagination-Guided Open-Ended Text Generation",
            "text": "Open-ended text generation is a task that provides an input context, and asks the model to generate a piece of text that is consistent with the context. This work mainly focused on introducing machine-rendered images to assist LM in performing open-ended text generation. More specifically, given the context x i , we first use a text-to-image generator to illustrate an image I i that depicts the input context. The LM is prompted with image I i as the visual prefix along with the text context x i , and will incorporate the multimodal input to generate the output text\u0177 i . Figure 2 provides an overview of our iNLG framework, which mainly involves two modules. The first module is a text-to-image generator that takes in the input context and illustrates a descriptive image, which we also refer to as the machine imagination. The second module is a visuallyguided language model that utilizes the machine imagination as a source of input and also a supervision that encourages the LM to generate text that is semantically similar to the visual information.",
            "score": 0.5550169300964594,
            "section_title": "Overview",
            "char_start_offset": 7513,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49072265625
        },
        {
            "corpus_id": "278394475",
            "title": "Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation",
            "text": "Visual understanding and generation have traditionally been handled by separate models: vision language models [49,50] for understanding and diffusion models [20,25,28,69] for generation. Recently, efforts to unify the two tasks into a single foundation model have gained momentum, driven by the potential to produce higher quality, more consistent multimodal content [2,85]. \n\nUnified Framework A central challenge in building unified models is the misalignment between text and visual modalities. State-of-the-art vision-language models (VLMs) are typically built on GPT-style large language models (LLMs) [7,63,64], and trained via next-token prediction [4,35,81]. In contrast, high-performance visual generation models are based on diffusion models [18,32]. Recent efforts attempted to unify visual generation within an auto-regressive (AR) framework by using next-token prediction. For example, the Emu series [72,73] consider images as continuous feature vectors concatenated with text embeddings, allowing a language model [77] to generate both text tokens and image vectors autoregressively. Similarly, Chameleon [74] replaces continuous image features with discrete visual tokens. However, AR methods often struggle to generate high-quality images [8,39,75], compared to diffusion-based approaches. \n\nOther approaches embed diffusion modules directly into an LLM backbone. For example, recent approaches such as Show-O [88] and TransFusion [96], are able to generate text token-by-token, and synthesize an images through a diffusion process, by sharing a transformer backbone [60,78]. This can improve the performance of visual generation, but might sacrifice the understanding capability, due to the conflicts of two tasks learning with shared parameters [44,45]. In Mogao, we adopt the MMDiT architecture [18], which decouples two tasks by using separate text and visual parameters to reduce such cross-modal conflicts. \n\nUnified Representation Another challenge lies in the differing requirements of visual representations for understanding and generation [55,62]. Features optimized for understanding may be suboptimal for generation, making visual tokenizer design critical.",
            "score": 0.5543814683037417,
            "section_title": "Related Work",
            "char_start_offset": 3738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1307
                },
                {
                    "start": 1310,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1930
                },
                {
                    "start": 1933,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 115,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 368,
                    "end": 371,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 608,
                    "end": 611,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1260,
                    "end": 1263,
                    "matchedPaperCorpusId": "270560593"
                },
                {
                    "start": 1263,
                    "end": 1266,
                    "matchedPaperCorpusId": "268876071"
                },
                {
                    "start": 1585,
                    "end": 1589,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 1816,
                    "end": 1820,
                    "matchedPaperCorpusId": "268247980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.634765625
        },
        {
            "corpus_id": "274596974",
            "title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
            "text": "In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining -- over four times fewer than what is typically needed -- while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing.",
            "score": 0.5535403328764172,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "265456168",
            "title": "GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions",
            "text": "The instruction RoI is extracted for the editing regions from both the input 3D scene G input and the text instruction T provided by the user. To achieve this, we employ a multimodal model M M M in conjunction with the large language model assistant M LLM . The first step is the scene description generation M Desc , which aims to get the scene description T scene from 3D Gaussians G input : \n\nThe process of the scene description generation M Desc is shown in Fig. 3. By leveraging the technique of differentiable splatting as shown in Eq. 1, a set of 2D image samples {I sample } are generated and then inputted into a multimodal model M M M to generate corresponding text descriptions {T sample }: \n\nwhere P M M is a prompt, such as \"What is the content of the image\", for multimodal model M M M to get precise description. Subsequently, these descriptions {T sample } are fed into a large language model M LLM , which is specifically instructed by a prompt P merge to merge descriptions of diverse views into one detailed scene description T scene : \n\nAfter that, the scene description T scene and the user instruction T are combined with a predefined tem-plate T template : \"Text description: T scene Edit Instruction: T Answer:\" to form the user message T user =T template (T scene , T ). The LLM model M LLM is used to extract the instruction RoI T RoI from user message T user with a new prompt P extract :",
            "score": 0.5529126347481268,
            "section_title": "RoI Extraction of Text Instruction",
            "char_start_offset": 9523,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1055
                },
                {
                    "start": 1058,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1416
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3525390625
        },
        {
            "corpus_id": "271218251",
            "title": "Visualization Literacy of Multimodal Large Language Models: A Comparative Study",
            "text": "Recently, models that accept multiple modalities as input are gaining increasing popularity.From the early work, e.g., CLIP [26], that jointly embedded image and text to models designed for input in one modality and output in another, such as text-to-image [28,27] and image-to-text [37] models.In the context of this work, we focus on multi-modal large language models (LLM) [36] that can understand both image and text inputs for text generation.LLMs are often referred to as foundation models due to their extensive capabilities and scale.They often possess a wide range of knowledge, allowing them to understand implicit context and common sense.\n\nSince humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35].Since the initial introduction of GPT4-V, many state-of-the-art MLLMs have been introduced such as the recent update to GPT4-o from OpenAI, multimodal support for Claude 3 [3] from Anthropic, and Gemini family from Google.On the open weight model side, many attempts have been made to develop similar capabilities, such as LLAVA [18], and CogVLM [30].",
            "score": 0.5513403845684024,
            "section_title": "Multimodal Vision Language Models",
            "char_start_offset": 8099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 92,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 650
                },
                {
                    "start": 652,
                    "end": 885
                },
                {
                    "start": 885,
                    "end": 1107
                },
                {
                    "start": 1107,
                    "end": 1236
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1214,
                    "end": 1218,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "271974843",
            "title": "Pixels to Prose: Understanding the art of Image Captioning",
            "text": "Unified Architecture -Normal approaches to combining vision and language tasks typically involve creating separate architectures and objectives for each task. However, there's a growing interest in developing a unified framework that can handle various tasks within a single architecture. \n\nVarious vision-language tasks can be accomplished by unifying several image and text related tasks as a single objective, such as text-generation or language modeling, the authors in [14] introduce VL-T5 and VL-BART which unifies various image-text tasks with a unified text-generation objective, they use pre trained language models for text generation and eliminate the need for task-specific hand crafted architectures and objectives. \n\nThey created a label text by encoding textual and visual inputs using a transformer encoder-decoder architecture. The decoder attends to prior tokens and the encoder outputs to predict the probability of the future text tokens, while the encoder integrates text and visual embeddings to create joint representations. \n\nWith input text and image taken into account, the model parameters are trained to minimize the negative log-likelihood of label text tokens [14]. This process seeks to generate text that maintains coherence and meaning irrespective of the input format-image, audio, or video. The focus lies on generating text that accurately reflects the information conveyed by the various input modalities, maintaining relevance and coherence throughout the generated content [14]. \n\nFrozen -this method uses the pre-trained large language model for various multi-modal vision language tasks. The architecture of frozen consists of a visual encoder which takes raw image as input, the vision encoder's outputs are linearly mapped and reshaped to generate a sequence of continuous embeddings,so that these visual representations can be fed into the pre-trained language models.Frozen is different than other models in a way that to apply this method to various vision language tasks does not need updating the weights of the language transformer as activations move through the model, the system becomes more adept at handling tasks that involve both vision and language, gradually adapting and enhancing its performance on multi-modal challenges. Only the visual encoder's parameters are updated during training.",
            "score": 0.5512144948049086,
            "section_title": "Case Studies of various VLP models:",
            "char_start_offset": 78152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 2282
                },
                {
                    "start": 2283,
                    "end": 2348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "273403986",
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "text": "The rapid advancement of large language models (LLMs) has catalyzed significant progress in multimodal large language models (MLLMs) for multimodal understanding tasks Dai et al. (2023); Li et al. (2024b); Zhang et al. (2023a); Chen et al. (2024); Lin et al. (2024); Zhang et al. (2024b); Li et al. (2024a). Pioneering works such as LLaVA (Liu et al., 2024b) and MiniGPT-4 (Zhu et al., 2023a) have demonstrated remarkable performance across diverse image understanding tasks, including visual question answering (VQA), visual reasoning, optical character recognition (OCR), and object grounding. These approaches typically employ visual encoders, such as the CLIP encoder (Radford et al., 2021), to extract continuous image features, which are then projected into the LLM's embedding space for subsequent tasks. While successfully unifying various image understanding tasks within a single model, these methods mostly adhere to a multimodal-input, text-output paradigm. Consequently, they excel at text-based responses to visual inputs but cannot generate multimodal outputs beyond text, limiting their applicability in tasks requiring visual content generation.",
            "score": 0.5501752015248307,
            "section_title": "MULTIMODAL UNDERSTANDING",
            "char_start_offset": 6299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1162
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 248,
                    "end": 265,
                    "matchedPaperCorpusId": "266174746"
                },
                {
                    "start": 339,
                    "end": 358,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 672,
                    "end": 694,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7607421875
        },
        {
            "corpus_id": "256662241",
            "title": "Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models",
            "text": "The progress in image synthesis using large-scale textto-image models has garnered significant attention due to their unprecedented synthetic capacity. Text-guided image synthesis methods such as VQGAN-CLIP [7] and CLIPguided Diffusion [6], utilize the superior performance of text-to-image alignment in the CLIP [35] latent spaces. \n\nParti [54], Make-a-scene [12], and DALL-E 1 [39] have employed large auto-regressive architectures, while Imagen [43], DALL-E 2 [38], and Stable Diffusion [42] have utilized diffusion-based architectures. MUSE [5] employs a masked transformer architecture and learns to predict masked tokens using the embeddings projected from Large Language Models (LLM) and image tokens. The overwhelming performance of these models has sparked ongoing research efforts to harness their power for image editing. \n\nWhile prior frameworks have mainly focused on the generation of individual, high-quality images, real-world applications place an equal emphasis on coherency within a series of generated images. For example, in the context of an AI-generated storybook, it is crucial for the main character to maintain a consistent appearance throughout the book. Therefore, we address the challenge of not only locally editing an image, but also applying these techniques to a practical real-world application: the generation of a training-free storybook that maintains the coherency of the main character. \n\nOne prior attempt at this is Long Stable Diffusion [58], which generates a book (both plot and corresponding illustrations) using a combination of LLM and text-image models. However, their use of a LLM is limited to generating illustration ideas, which can lead to irrelevant images if directly used as text conditioning or require significant manual modification of prompts. Another pioneering work in the field of story synthesis is StoryGAN [24], a conditional GAN [29] trained to generate visual sequences corresponding to text descriptions of a story. Subsequently, StoryDALL-E [28] proposed the adaptation of pre-trained text-to-image transformers to generate visual sequences that continue a given text-based story, introducing a new task of story continuation.",
            "score": 0.5498040650319359,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 210,
                    "matchedPaperCorpusId": "248239727"
                },
                {
                    "start": 313,
                    "end": 317,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 490,
                    "end": 494,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1872,
                    "end": 1876,
                    "matchedPaperCorpusId": "54457433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "270514555",
            "title": "Automatic, Meta and Human Evaluation for Multimodal Summarization with Multimodal Output",
            "text": "Among the above models, only the HAN model could output both texts and images.Other models could only generate text-only summaries.To have multimodal outputs, we further design different algorithms to select the images from the source image set.Given a multimodal input, there is a reference summary image set (with N images).We edit this reference set with the following methods:",
            "score": 0.549669598938225,
            "section_title": "RQ2: Meta-Evaluation",
            "char_start_offset": 10915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 78,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 245
                },
                {
                    "start": 245,
                    "end": 326
                },
                {
                    "start": 326,
                    "end": 380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41748046875
        },
        {
            "corpus_id": "274982061",
            "title": "AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues",
            "text": "There has been a surge of Multimodal Large Language Models (MM-LLMs) [26,28,31,32,34,41,44,47,48] that are capable of processing multimodal input. BLIP-2 [28] introduces a Q-Former module to bridge the modality gap between the visual and text. Llava [31,32] aligns the visual and text modality by utilizing a large-scale multimodal instruction-following dataset on image QA, image description, and complex reasoning. Video-LlaMA [51] and Video-ChatGPT [34] process the visual and audio content in a video to engage in a conversation describing the visual scenes and actions in a spatio-temporal context. Hug-gingGPT [41] conducts task planning to connect various AI models upon receiving a user request and NextGPT [48] builds an end-to-end system that is capable of understanding and generating audio, visual, and text modality. However, they primarily concentrate on multimodal grounding tasks. They excel in integrating multimodal data for content reasoning but do not leverage this information to enhance interaction with the human user. \n\nRecently, a few multimodal large language models [4,18,21] tailored for emotion-aware dialogue system attempted to incorporate emotional context. FaceChat [4] employs a pre-trained facial emotion classifier [42] to predict textual emotion labels from the user's facial expression and incorporate this label into the LLM's template prompt. It also needs an ASR module to transcribe speech into text, and utilizes the zero-shot capabilities of a pre-trained text LLM for response generation. SMES [18] utilizes a pretrained Video-Llama [51] to extract emotion labels of each time step via prompting. These emotional labels are then combined with text inputs and fed into an LLM for response generation. EmpathyEar [21] utilizes an ImageBind [22] to extract multimodal embeddings, which are also combined with text embeddings to be fed into an LLM.",
            "score": 0.5494359107518667,
            "section_title": "Multimodal Large Language Models",
            "char_start_offset": 5846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 79,
                    "end": 82,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "257833781"
                },
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "257833781"
                },
                {
                    "start": 1251,
                    "end": 1255,
                    "matchedPaperCorpusId": "2814088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62890625
        },
        {
            "corpus_id": "274130618",
            "title": "SoK: Unifying Cybersecurity and Cybersafety of Multimodal Foundation Models with an Information Theory Approach",
            "text": "In this section, we first explore how multimodal learning has evolved from unimodal learning and then introduce several typical MFMs. From unimodal to multimodal learning. Multimodal learning extends beyond traditional unimodal learning by integrating data from various modalities like text, images, and audio. In unimodal learning (Figure 2(a)), the model maps input features to output labels within a discrete feature space, focusing on patterns within one data type. It extracts features, converts them into vectors, and then predicts labels based on these vectors. In contrast, multimodal learning (Figure 2(b)) involves mapping and aligning continuous feature spaces across different modalities, such as pairing images with text. Instead of directly predicting labels, multimodal models create unified representations by linking feature spaces from different data types. This allows for more complex tasks as the model can work with more diverse information. Existing MFMs. Within the image/text multimodal space, there are four typical classes of multimodal foundation models (MFMs): feature alignment models, text-to-image generation models, audio-to-text generation models, and models that take both text and image inputs, as summarized in Table    that uses a dual-encoder framework to process images and text separately. It trains on large image-text pairs using contrastive learning [17] to create a shared representation space, excelling in tasks like zero-shot classification and image retrieval. \u2022 Text-to-image generation models (T2I). DALL\u2022E [18] and Stable Diffusion [3] are prominent models in this category, generating images by embedding text prompts into models. These models typically leverage feature alignment techniques, such as CLIP, to convert text into feature vectors that serve as conditional guidance for image generation, enabling the model to synthesize visuals corresponding to the input text. \u2022 Text generation from text and/or image inputs (I/T2T).",
            "score": 0.5494113517601659,
            "section_title": "Multimodal Foundation Models",
            "char_start_offset": 6865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 1394,
                    "end": 1398,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "256503775",
            "title": "SceneScape: Text-Driven Consistent Scene Generation",
            "text": "Related to our task is 3D-aware image synthesize, which typically involves incorporating a 3D representation explicitly into a generative model, e.g., a NeRF representation [8,9,14,33,47], or neural surface renderer [35]. These methods have shown impressive results on structured domains (e.g., cars or faces), but deploying them for diverse scenes is extremely challenging and often requires additional guidance [11], or heavy supervision, e.g., large-scale data that contains accurate camera trajectories, which can be obtained reliably mainly for synthetic data [4]. We aim for a much more diverse, flexible, and lightweight generation of arbitrary scenes. \n\nText-to-video generation and editing. Our work follows a broad line of works in the field of text-tovideo synthesis and editing. There has been great progress in developing text-to-video generative models by expanding architectures to the temporal domain and learning priors from large-scale video datasets [e.g. 13,16,30,48]. Nevertheless, these models are still in infancy, and are lagging behind image models in terms of quality (resolution and video length). On the other side of the spectrum, a surge of methods proposes to tune or directly leverage a 2D text-image model for video editing tasks [e.g., 3,6,37,56,62]. We also follow this approach in conjunction with generating an explicit 3D scene representation. \n\nText-to-3D Generation and Editing. Recently, language-vision pre-trained models [38,46], and differentiable renderers [28,32] have been utilized for text-driven 3D content generation. For example, CLIP-based losses have been used for the text-driven stylization of a given surface mesh [31] or for synthesizing a NeRF from an input text [21]. Taking this approach further, Poole et al. [36] distills the generative prior of a pre-trained text-to-image diffusion model by encouraging the optimized NeRF to render images that are in the distribution of the diffusion model [46].",
            "score": 0.5490192138506577,
            "section_title": "Related Work",
            "char_start_offset": 2844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1381
                },
                {
                    "start": 1384,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1960
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "227151657"
                },
                {
                    "start": 184,
                    "end": 187,
                    "matchedPaperCorpusId": "220364071"
                },
                {
                    "start": 216,
                    "end": 220,
                    "matchedPaperCorpusId": "245353683"
                },
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "232478819"
                },
                {
                    "start": 981,
                    "end": 984,
                    "matchedPaperCorpusId": "257532642"
                },
                {
                    "start": 1464,
                    "end": 1468,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1468,
                    "end": 1471,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1502,
                    "end": 1506,
                    "matchedPaperCorpusId": "102484000"
                },
                {
                    "start": 1670,
                    "end": 1674,
                    "matchedPaperCorpusId": "244908764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "277940398",
            "title": "Compile Scene Graphs with Reinforcement Learning",
            "text": "Scene graphs, as structured visual representations, have gained increasing attention in many vision applications, such as robot manipulation [44,41], robot navigation [7,23,37], and medical image / video analysis [20,24], etc. To generate scene graphs from an image, traditional Scene Graph Generation (SGG) models [10,34,14,38,29,2,15,11,5,40,4] decouple the task into two subtasks, i.e., object detection and visual relationship recognition, and directly maximize the likelihood of the ground-truth labels given the image (Fig. 1 (a)). Essentially, these models tend to overfit the distribution of annotated datasets; consequently, they struggle to handle long-tail distributions and are prone to generating biased scene graphs (e.g., all predicted relationships are head classes like \"on\" and \"of\"). \n\nWhile traditional SGG models rely on manual annotated datasets and struggle to generalize to new domains, recent advances in large languge models (LLMs) offer a new paragdim. LLM4SGG [12] utilizes an LLM to extract relationship triplets from captions using both original and paraphrased text, while GPT4SGG [3] employs an LLM to synthesize scene graphs from dense region captions. Additionally, Li [17] generates scene graphs via image-to-text generation using vision-language models (VLMs). These weakly supervised methods demonstrate potential for generating scene graphs with little or no human annotation but suffer from accuracy issues in the generated results. Despite these advancements, existing methods typically employ text-only LLMs or rely on intermediate captions as input, which do not fully leverage the rich visual context. In contrast, multimodal Preprint. Under review. (a) M-LLM with SFT is optimized token by token (here, w i refers to a token). large language models (M-LLMs) which integrate both visual and linguistic modalities offer the potential for more direct and holistic scene understanding. By processing visual information alongside natural language prompts, M-LLMs can generate scene graphs in an end-to-end manner.",
            "score": 0.5487046549152704,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 802
                },
                {
                    "start": 805,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "229156165"
                },
                {
                    "start": 145,
                    "end": 148,
                    "matchedPaperCorpusId": "277314071"
                },
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "263134620"
                },
                {
                    "start": 170,
                    "end": 173,
                    "matchedPaperCorpusId": "268819732"
                },
                {
                    "start": 173,
                    "end": 176,
                    "matchedPaperCorpusId": "273233851"
                },
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "252408633"
                },
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "247596915"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "16414666"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "1780254"
                },
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "21277943"
                },
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "4379400"
                },
                {
                    "start": 328,
                    "end": 331,
                    "matchedPaperCorpusId": "54459095"
                },
                {
                    "start": 331,
                    "end": 333,
                    "matchedPaperCorpusId": "72941015"
                },
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "245502596"
                },
                {
                    "start": 339,
                    "end": 341,
                    "matchedPaperCorpusId": "246294640"
                },
                {
                    "start": 341,
                    "end": 344,
                    "matchedPaperCorpusId": "260976241"
                },
                {
                    "start": 344,
                    "end": 346,
                    "matchedPaperCorpusId": "265294726"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "268819202"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56005859375
        },
        {
            "corpus_id": "239011704",
            "title": "Unifying Multimodal Transformer for Bi-directional Image and Text Generation",
            "text": "As a result of developing multimodal processing technology, networks, and devices, the number of existing multimodal industrial applications (especially mobile applications) is rapidly increasing. This trend has encouraged the development of techniques to support multimodal interaction in a unified system. Bi-directional image and text generation is a technique that automatically translates across real scene images and natural language descriptions. It has a broad range of industrial applications. For example, a product description-and-picture generation system is an important application for consumers to search and preview products. Specifically, consumers could easily get a text query when the system receives a picture of one or several products. Meanwhile, the users could say \"show me the picture of a room with wooden furniture and purple curtains. \" and receive an illustrating picture by the system to preview this collocation. As these applications are equipped in mobile devices, unifying multimodal interactions in a single model will be a better choice to optimize storage utilization compared to two separate models.\n\nDespite the great benefit of a unified framework for image and text generation, the bi-directional generation tasks are conducted separately with task-specific architectures traditionally. As shown in Figure 1 (1), a typical image-to-text generator consists of a visual encoder (e.g., CNN) to embed visual information, and a caption decoder (e.g., LSTM) to generate captions [2]; while predominant text-to-image generators adopt Generative Adversarial Nets (GANs) [11] framework based on CNN architectures [53]. To support multimodal interaction, Huang et al. jointly trains an LSTM-based image-to-text generator and a GAN-based text-to-image generator in a framework [17]. However, task-specific architectures are still needed, which introduces expensive design efforts.\n\nTo alleviate above hassles, in this paper, we propose to unify image-to-text and text-to-image generation tasks in one framework. In this framework, we adopt Transformer-based architecture since it supports simple and task-agnostic designs, and exhibits strong performance in image or text generative models [27,36]. We formulate both tasks as sequence generation tasks, where an image and a text are represented",
            "score": 0.5482833281796646,
            "section_title": "INTRODUCTION",
            "char_start_offset": 65,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1515,
                    "end": 1518,
                    "matchedPaperCorpusId": "3753452"
                },
                {
                    "start": 1604,
                    "end": 1608,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1646,
                    "end": 1650,
                    "matchedPaperCorpusId": "91183909"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "29154127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6708984375
        },
        {
            "corpus_id": "275213056",
            "title": "Hierarchical Vision-Language Alignment for Text-to-Image Generation via Diffusion Models",
            "text": "Text-to-image generation has garnered significant attention in recent years due to its potential to bridge the gap between natural language understanding and high-quality image synthesis. With advancements in deep learning, particularly in the integration of Large Vision-Language Models (LVLMs), the field has seen remarkable progress in generating semantically aligned and visually coherent images based on textual prompts. LVLMs, such as CLIP and Flamingo, have demonstrated exceptional capabilities in understanding and encoding multimodal information, making them promising candidates for enhancing text-to-image generation tasks [1], [2]. However, while these models bring significant strengths, effectively leveraging them to generate complex, text-rich, and contextually accurate images remains an open challenge. \n\nOne major challenge in this domain lies in achieving finegrained alignment between the input text and the generated image, especially in scenarios with multi-object layouts or intricate text elements. Existing approaches often fail to capture the hierarchical and compositional nature of text descriptions, resulting in misaligned or incoherent outputs. Additionally, the computational cost of training large-scale LVLMs alongside generative models is prohibitively high, limiting their practical adoption [3]. Moreover, while techniques like diffusion models have shown promise in improving image quality, their integration with LVLMs for handling textual complexity remains underexplored. These challenges highlight the need for a more efficient and semantically robust framework to address the shortcomings of current methods. \n\nMotivated by these challenges, we propose a novel framework, Vision-Language Aligned Diffusion (VLAD), which leverages a dual-stream alignment strategy to enhance the semantic mapping between textual prompts and visual outputs. Our approach introduces a contrastive alignment mechanism for fine-tuning pretrained LVLMs, ensuring that the embeddings of textual and visual features are tightly coupled. Additionally, we design a hierarchical representation module, called the Contextual Composition Module (CCM), which decomposes complex textual prompts into global and local semantic structures. By integrating these components with a multi-stage diffusion model, VLAD ensures that the generated images accurately reflect the provided textual descriptions while maintaining high visual quality.",
            "score": 0.5479227959174358,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2250
                },
                {
                    "start": 2251,
                    "end": 2449
                }
            ],
            "ref_mentions": [
                {
                    "start": 635,
                    "end": 638,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 640,
                    "end": 643,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "273025657",
            "title": "Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks",
            "text": "Multimodal Large Language Models (MLLMs). Many approaches have been proposed for building MLLMs, leveraging different architectural designs. A widely adopted approach is the decoder-only architecture, exemplified by LLaVA [41], Emu2 [61], and Intern-VL [9]. These models typically incorporated a visual encoder to encode images, a vision-language connector to project visual features into the language feature space, and a language model that processes both visual and textual information jointly. Another line of work employed cross-attention architectures where encoded image features are integrated with textual tokens via crossattention layers, as seen in Flamingo [1], OpenFlamingo [3] and CogVLM [66]. Such a design allows models to retain the benefits of a fully intact language model but introduces new parameters to manage the visual-textual interplay. \n\nText-rich MLLMs. Text-rich images are traditionally processed in pipelines [18,60], where an OCR module first recognized text from the image, followed by processing through a language model. To improve efficiency and avoid error propagation, with the advent of MLLMs, end-to-end approaches become more popular recently. For instance, LLaVAR [75] utilized a dataset of 400K instances with OCR-enhanced text to outperform LLaVA on various textrich VQA tasks. Subsequent models such as UReader [70], TextMonkey [44], and Mplug-DocOwl-1.5 [16] recognized the importance of high-resolution encoding for accurate text comprehension, so they adopted strategies that cropped single images into multiple sub-images to preserve the original resolution during visual encoding. However, these approaches are primarily trained on single-image data, and struggle to generalize effectively to multi-image scenarios. Furthermore, the straightforward partitioning technique encounters challenges with multi-image inputs, as the sequence length rapidly increases with the number of images.",
            "score": 0.5458814575260669,
            "section_title": "Related Work",
            "char_start_offset": 5450,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "265034288"
                },
                {
                    "start": 943,
                    "end": 946,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "263829804"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "277741023",
            "title": "Have we unified image generation and understanding yet? An empirical study of GPT-4o's image generation ability",
            "text": "Text-to-Image (T2I) Models Text-to-image (T2I) generation has advanced through specialized models prioritizing visual fidelity and multimodal architectures unifying vision-language capabilities. Specialized approaches evolved from autoregressive frameworks (Chen et al., 2020;Fan et al., 2024) to diffusion-based methods (Ho et al., 2020;Rombach et al., 2022), where latent diffusion models (LDMs) like Stable Diffusion (Podell et al., 2023) balance efficiency and quality through iterative denoising in compressed latent spaces. Recent unified multimodal systems (Team, 2024;Jin et al., 2023;Ge et al., 2024) extend large language models (LLMs) to visual generation via diffusion heads or autoregressive decoders, with architectures like Transfusion (Zhou et al., 2024) demonstrating bidirectional diffusion-language integration for contextual alignment. Despite progress, both paradigms face challenges: specialized models lack dynamic reasoning beyond fixed text embeddings, while unified architectures struggle with inconsistent knowledge grounding (Wu et al., 2024;Ma et al., 2024), limiting their semantic synthesis capabilities. \n\nT2I Evaluations Evaluating text-to-image (T2I) models necessitates benchmarks that assess both technical fidelity and semantic alignment with real-world knowledge. Early efforts like GenEval (Ghosh et al., 2024) established foundational metrics for basic instruction compliance and photorealism, while Reason-Edit (Huang et al., 2024) introduced instruction-guided editing tasks to evaluate fine-grained manipulation capabilities, albeit prioritizing syntactic edits (e.g., object replacement) over semantic reasoning. The WISE benchmark (Niu et al., 2025) addresses these limitations by pioneering world knowledge-informed evaluation, introducing 1,000 structured prompts across 25 subdomains (e.g., cultural common sense, natural science) and the WiScore metric to assess knowledge-image alignment beyond superficial text-pixel matching.",
            "score": 0.5445347241723371,
            "section_title": "Related Work",
            "char_start_offset": 2749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1135
                },
                {
                    "start": 1138,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1977
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 293,
                    "matchedPaperCorpusId": "273404213"
                },
                {
                    "start": 321,
                    "end": 338,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1329,
                    "end": 1349,
                    "matchedPaperCorpusId": "264288728"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "258833320",
            "title": "i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data",
            "text": "The convergence of text, visual, and audio data is a key step towards human-like artificial intelligence, however the current Vision-Language-Speech landscape is dominated by encoder-only models which lack generative abilities. We propose closing this gap with i-Code V2, the first model capable of generating natural language from any combination of Vision, Language, and Speech data. i-Code V2 is an integrative system that leverages state-of-the-art single-modality encoders, combining their outputs with a new modality-fusing encoder in order to flexibly project combinations of modalities into a shared representational space. Next, language tokens are generated from these representations via an autoregressive decoder. The whole framework is pretrained end-to-end on a large collection of dual- and single-modality datasets using a novel text completion objective that can be generalized across arbitrary combinations of modalities. i-Code V2 matches or outperforms state-of-the-art single- and dual-modality baselines on 7 multimodal tasks, demonstrating the power of generative multimodal pretraining across a diversity of tasks and signals.",
            "score": 0.5437683591514479,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "Q-former is trained in two stages: fixing the image encoder to learn representations of visual-language consistency, and fixing the language large-scale model to enhance the generation capability from vision to language.InstructBLIP [42] proposes a paradigm of instruction fine-tuning based on BLIP-2, using additional instructions to extract more useful visual features.mPLUG-2 [43] is capable of handling various modal inputs such as text, images, and videos.It has achieved leading or comparable performance in more than 30 multimodal and unimodal tasks.Models such as BEIT3 [44] and X-FM [45] undergo joint pre-training on both image and text data, enabling them to learn joint representations of images and text.These models can handle not only tasks that involve only text or images but also complex tasks that require simultaneous processing of both text and images, such as generating image descriptions and answering questions related to images.LLaVA [46] fine-tunes on the generated visual-language instruction data end-to-end, validating the effectiveness of using generated data for instruction-tuning in LMM, and providing practical techniques for constructing general instructions that follow visual agents.OFA [47] unifies the understanding and generation tasks of multimodal and single-modal data into a single simple Seq2Seq generative framework.It undergoes pretraining and fine-tuning using task instructions, without introducing additional task-specific layers.GPT-4 [48]takes this to the next level, boasting a total parameter count of 1.8 trillion, a significant increase compared to GPT-3's 100 billion parameters.This massive parameter scale equips GPT-4 with potent computational capabilities and potential in natural language understanding and generation tasks.It enhances its contextual comprehension, allowing it to handle more complex textual information and possess advanced semantic analysis and knowledge reasoning abilities.MiniGPT-4 [49] fixes the parameters of the language model and the visual model, and then uses only the projection layer to align the language decoder with LLaMA and the visual encoder with BLIP-2.",
            "score": 0.5433817949616963,
            "section_title": "Recent Image-text Multimodal Models",
            "char_start_offset": 19080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 557
                },
                {
                    "start": 557,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1221
                },
                {
                    "start": 1221,
                    "end": 1363
                },
                {
                    "start": 1363,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1637
                },
                {
                    "start": 1637,
                    "end": 1787
                },
                {
                    "start": 1787,
                    "end": 1957
                },
                {
                    "start": 1957,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "250373772"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "246634906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "265466568",
            "title": "Efficient Multimodal Diffusion Models Using Joint Data Infilling with Partially Shared U-Net",
            "text": "Diffusion models [25] have emerged as a potent framework for generating data across diverse domains such as language and vision. They have found broad applications across various domains, enabling high-quality and detail-oriented conditional data generation, as well as cross-modality data generation such as text-to-image [3,9,18,21], text-to-video [10], etc. A case in point is Stable Diffusion [21] which has demonstrated human-like text-to-image generation capabilities, showcasing the robustness and versatility of diffusion models. In addition to continuous domain, diffusion models have also shown promising futures in discrete data spaces such as language generation [8,11,14,26]. \n\nThe above-mentioned models can only generate data in a single data modality, a major discrepancy from human intelligence. For example, a human painter can use language to describe the scene they see, and when a text description of a scene is seen, they can reconstruct the visual scene either in imagination or through painting. In the pursuit of a more powerful generative system, it is necessary to develop architectures that have the ability to generate multimodal data given only partial data modality. To offer this ability, diffusion model architectures have recently been extended to accommodate cross-modal and multimodal generation scenarios [2,27,30]. These works enable any-to-any generation with bidirectional conditioning and simultaneous generation of multiple modalities. They utilize different techniques to bring data from different domains into a shared embedding space. Versatile Diffusion (VD) [30] aligns separate modality flows using shared global model layers. Conditional sampling is achieved by injecting the context and generation modality pairs in a shared context layer. Composable Diffusion (CoDi) [27] selects text as the \"bridging\" modality, leveraging its availability in multimodal paired data. It then trains separate encoders for each modality to align with the text space. Unidiffuser [2] concatenates both images and text into a shared continuous latent space, and learns the joint diffusion of both modalities which may not lead to optimal learning. For a more thorough discussion of related work, please refer to the Appendix.",
            "score": 0.5433312048901344,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 331,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 331,
                    "end": 334,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "259765944",
            "title": "Generative Pretraining in Multimodality",
            "text": "The unified auto-regressive modeling of different modalities endows Emu with a powerful ability to serve as a multimodal generalist that can perform any types of completion in a multimodal sequence, i.e., accepting multimodal sequence as input, and outputting signals across vision and language modalities. For example, given two examples as the prompt, Emu automatically infers and completes the corresponding task given a new input, as shown in the second block of Figure 1. \n\nSpecifically, given a multimodal context, if the expected output format is text, Emu will use the language modeling head to generate discrete text tokens. If the desired output is image, we will append a [IMG] token at the end of the input sequence, then Emu will autoregressively generate N visual embeddings that will then be sent to the visual decoder for decoding into a real-world image.",
            "score": 0.5412009145673128,
            "section_title": "GENERALIST INTERFACE",
            "char_start_offset": 8407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 871
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3916015625
        },
        {
            "corpus_id": "265658988",
            "title": "UPOCR: Towards Unified Pixel-Level OCR Interface",
            "text": "The emergence of Transformer [18,72] breaks the boundary between different modalities [10,26,28,29,45,58,96], fostering a broad variety of generalist models [75,88,101]. One category of these models unifies both the input and output as sequences and bridges them with a sequenceto-sequence learning framework. Pix2Seq [12] pioneered in unifying the output vocabulary of natural language and spatial coordinates and demonstrated effectiveness in object detection. Subsequently, Pix2Seq v2 [13] simultaneously tackled multiple tasks with the guidance of specific prompts. Furthermore, OFA [76] discretized both the input and output as token sequences, accomplishing various unimodal and cross-modal vision-language tasks. Similarly, Unified-IO [49] extended the framework to a wider range of tasks and modalities. With the rise of LLMs [9,56,69], numerous studies [1,38,42,91,94,100] connected pretrained ViTs and LLMs for generalist models with stronger reasoning and robustness. Following an image-to-image translation pipeline, MAE-VQGAN [5] treated diverse tasks as inpainting problems. Painter [79] further investigated visual in-context learning which allows it to adapt to unseen tasks. \n\nIn the OCR field, several generalist models have been studied following sequence-to-sequence paradigms. Donut [33] pioneered in employing a prompt-based sequence generation framework for document understanding tasks. UDOP [68] further unified the representation of image, text, and layout modalities. Moreover, Nougat [7] and Kosmos-2.5 [51] focused on translating document images into markup languages, encapsulating multiple document OCR tasks into a single sequence. Based on large multimodal models with pretrained ViTs and LLMs, recent approaches [22,89,90] augment them using OCR-related data and fine-grained visual perception. However, these approaches primarily focused on document scenarios and failed to generate pixels. Moreover, some of them [33,68] still required dataset-specific finetuning.",
            "score": 0.5409464999194958,
            "section_title": "Generalist Model",
            "char_start_offset": 7297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 33,
                    "end": 36,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "261049745"
                },
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "236635379"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "255416140"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "245144274"
                },
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "249152323"
                },
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "251105279"
                },
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "244799261"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "237593074"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "249674493"
                },
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 742,
                    "end": 746,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 862,
                    "end": 865,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 865,
                    "end": 868,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1097,
                    "end": 1101,
                    "matchedPaperCorpusId": "254246343"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "250924870"
                },
                {
                    "start": 1416,
                    "end": 1420,
                    "matchedPaperCorpusId": "254275326"
                },
                {
                    "start": 1949,
                    "end": 1953,
                    "matchedPaperCorpusId": "250924870"
                },
                {
                    "start": 1953,
                    "end": 1956,
                    "matchedPaperCorpusId": "254275326"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55712890625
        },
        {
            "corpus_id": "278207820",
            "title": "Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields",
            "text": "With the rapid advancement of AIGC foundation models, text-to-image (T2I) generation has seen remarkable improvements. Models such as Stable Diffusion 3 [14], Flux [28], and DALL\u2022E 3 [7] have demonstrated the ability to produce high-resolution, semantically rich, and stylistically diverse images from natural language descriptions. In particular, GPT-4o [43] showcases unprecedented multimodal capabilities, including accurate text rendering, detailed scene synthesis, and image editing guided by both textual and visual inputs. Therefore, when the prompt length is fixed (i.e., the compression cost remains constant), the specific content of the prompt plays a crucial role in determining the quality of the generated image. This raises a crucial question: how can we obtain text prompts that not only produce high perceptual quality but also preserve consistency with the original image? \n\nTo achieve this, as illustrated in Fig. 2, we introduce a structural raster-scan prompt engineering approach. This prompt is carefully designed to guide multimodal large language models (MLLMs) in preserving the structural layout and maintaining multi-level visual consistency with the original image. Specifically, the prompt instructs the model to describe visual elements in a top-to-bottom, left-to-right sequence, emphasizing the position, shape, and appearance of objects. To further enhance consistency, we consider six dimensions that guide the model to elaborate on image content more precisely: feature correspondence captures local visual details such as textures and edges; geometric consistency describes spatial layout and object alignment; photometric properties refer to lighting, shadows, and color tones; stylistic alignment specifies the overall visual style (e.g., photo, painting, anime); semantic coherence ensures object completeness and plausibility; and structural integrity maintains the global composition. This comprehensive approach ensures that the spatial arrangement and stylistic attributes of visual elements are faithfully captured. \n\nAdditionally, the prompt provides tailored instructions for different image types. By enforcing a concise formatlimiting responses to a fixed number of words (e.g., 30 words) or fewer, it encourages clear, content-rich descriptions. This structurald methodology effectively supports high-fidelity image reconstruction within the generative compression framework.",
            "score": 0.5395602497358092,
            "section_title": "Structural Raster-scan Prompt",
            "char_start_offset": 9864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2060
                },
                {
                    "start": 2063,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2295
                },
                {
                    "start": 2296,
                    "end": 2425
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "264403242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64208984375
        },
        {
            "corpus_id": "267200040",
            "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
            "text": "In this paper, we propose UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts comprising freeform interleaved vision-language inputs. Unlike traditional text-only prompts, multimodal prompts encompass various combinations of image entities and textual elements, as demonstrated in Figure 1. UNIMO-G is designed to faithfully reproduce all image entities, render textual content, and follow the instructions in multimodal prompts. Specifically, we leverage the perception capabilities of Multimodal Large Language Models (MLLMs) to encode multimodal prompts into a unified visionlanguage semantic space. Subsequently, a conditional diffusion network generates images from these encoded representations. \n\nTo train UNIMO-G efficiently, we implement a two-phase strategy. Initially, the model undergoes pre-training on a large-scale dataset of text-image pairs, enhancing its proficiency in conditional image generation. This is followed by a phase of instruction tuning with multimodal prompts, learns to generate images that align with the detailed specifications provided in these prompts. A carefully designed data processing pipeline, incorporating language grounding and image segmentation, is employed to construct these multimodal prompts. This approach enables UNIMO-G to harness rich features from the MLLM encoder to generate images faithfully reproducing the contents across various contexts. \n\nUNIMO-G exhibits a comprehensive capability for controllable image generation, excelling not only in text-to-image synthesis but also in zero-shot subject-driven generation. It adeptly produces highfidelity images from multimodal prompts, even those containing multiple image entities. To assess its performance, we conducted evaluations in both text-to-image and subject-driven generation contexts using the MS-COCO (Lin et al., 2014) and DreamBench (Ruiz et al., 2023) datasets, respectively. The results consistently highlight UNIMO-G's superior performance in these scenarios. Additionally, recognizing DreamBench's focus on single-subject generation, we introduce Multi-Bench, a new benchmark featuring images with multiple entities. The evaluation on MultiBench confirms UNIMO-G's effectiveness in zero-shot multi-entity subject-driven generation.",
            "score": 0.5395419042024179,
            "section_title": "Introduction",
            "char_start_offset": 1856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 1866,
                    "end": 1884,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 1900,
                    "end": 1919,
                    "matchedPaperCorpusId": "251800180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "276422421",
            "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
            "text": "Multimodal Large Language Models (MLLMs) [6,20,22,31] have made significant progress in recent years, allowing the comprehension and generation of diverse data modalities including text and images. However, in real-world * Equal contribution. scenarios, there exists a pervasive graph-structured relationships between texts and images. Such graph-structured relationship can be described as \"Multimodal Attributed Graphs\" (MMAGs) [17,27,40], where nodes are associated with image and text information. For example, the artwork graph [25] is composed of nodes that include images (pictures) and text (titles), with edges representing shared genres and authorship. This structure uniquely represents each artwork in relation to thousands of others within the graph, providing a context that extends beyond simple language descriptions or image references. While MLLMs have demonstrated outstanding comprehension and generation capability for text and image data, it is questionable how they could utilize the structural information on MMAGs. \n\nIn this context, we formulate the problem of multimodal content generation on MMAGs which tasks MLLMs with producing both a textual description and an accompanying image for a new node based on the graph connectivity and node attributes. This task focuses on generating text-image pairs for a node from MMAGs, reflecting a wide range of practical applications. For example, generating an image and a text for a product node linked to others through copurchase edges in an e-commerce MMAG is equivalent to recommending [5,24] potential future products to users. Likewise, creating an image and a title for a virtual artwork node in the art MMAG is comparable to creating virtual artwork [7,14] that reflects the subtle styles of various artists and genres. \n\nHowever, directly adopting MLLMs on MMAGs for multimodal content generation presents several challenges: (1) Graph Size Explosion: Although MMAGs provide sub-arXiv:2502.11925v1 [cs.AI] 17 Feb 2025 stantial context for image and text generation, inputting the entire local subgraph structure to a model is impractical due to the exponential increase in size with additional hops, leading to excessively long context sequences.",
            "score": 0.5384727010061363,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 47,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 47,
                    "end": 50,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 50,
                    "end": 53,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "195848773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61181640625
        },
        {
            "corpus_id": "260887336",
            "title": "Large Language Models and Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",
            "text": "The new Gemini 1.53 developed by Google delivers dramatically enhanced performance with a more efficient architecture compared to its previous version (Team et al., 2023b). \n\nHowever, a majority of FMs are typically engineered for either understanding-based or generation-based tasks. To address this issue, BLIP (Li et al., 2022), a new Vision-Language Pre-training (VLP) framework, is introduced for both vision-language understanding and generation tasks. To effectively utilize the noisy web data by bootstrapping the captions, a captioner is employed to generate synthetic captions, subsequently, a filter is adopted to remove the noisy ones. BLIP achieves state-of-the-art results across various vision-language tasks, such as image-text retrieval, image captioning, and VQA. Similarly, OpenFlamingo (Alayrac et al., 2022) is trained on large-scale multimodal web corpora with arbitrarily interleaved text and images for a variety of image and video tasks via few-shot learning, such as image captioning and multiple-choice VQA (Alayrac et al., 2022). KOSMOS-1 (Huang et al., 2023), a Multimodal Large Language Model (MLLM), is subsequently introduced to process multimodal inputs, adhere to instructions, and perform in-context learning for language tasks and multimodal tasks, such as multimodal dialogue, image captioning, VQA, and image recognition with descriptions. Impressively, KOSMOS-1 demonstrates exemplary performance across various settings, such as zero-shot, few-shot, and multimodal chain-of-thought prompting, on various tasks without necessitating any gradient updates or fine-tuning. Built on previous versions, GPT-4 (OpenAI, 2023), a largescale and multimodal model is introduced by OpenAI to accept both image and text inputs and produce text outputs. \n\nThe future of MFMs is set to be exciting, with further advancements anticipated in several areas.",
            "score": 0.5382523375155859,
            "section_title": "Multimodal foundation models (MFMs)",
            "char_start_offset": 25234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 330,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68505859375
        },
        {
            "corpus_id": "270688372",
            "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
            "text": "Large Multimodal Models for Interleaved Generation The advent of large multimodal models (LMMs) (Koh et al., 2023;Sun et al., 2023a) has significantly advanced the field of interleaved textand-image generation. Previous models such as DALL-E (Ramesh et al., 2021) and Stable Diffusion (Podell et al., 2023) have demonstrated impressive capabilities in generating high-quality images conditioned on textual descriptions. However, previous focus has predominantly been on unidirectional generation tasks, either from text to image or image to text, without considering the interleaved generation scenarios where text and images are seamlessly integrated within the same output. Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023;Sun et al., 2023b;Dong et al., 2024;Tian et al., 2024;Zhan et al., 2024;Chen et al., 2023). These models leverage advanced architectures and training techniques to enhance their ability to produce coherent and contextually relevant interleaved content. Despite these advancements, the evaluation of such models remains an underexplored area, with most evaluations still relying on separate assessments of text and image quality or simplistic reference-based metrics. Our proposed INTERLEAVEDBENCH benchmark aims to bridge this gap by providing a holistic evaluation framework tailored specifically for interleaved text-and-image generation.",
            "score": 0.5382066672284536,
            "section_title": "Related Work",
            "char_start_offset": 5358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1482
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 114,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 242,
                    "end": 263,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 824,
                    "end": 842,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 860,
                    "end": 878,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 896,
                    "end": 914,
                    "matchedPaperCorpusId": "267750101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78125
        },
        {
            "corpus_id": "266162752",
            "title": "4M: Massively Multimodal Masked Modeling",
            "text": "4M is a generalist framework for training multimodal and multitask models that not only perform many key vision tasks out of the box, but also demonstrate strong transfer results to a wide range of downstream tasks. 4M's in-painting and any-to-any generation capabilities enable it to perform a wide range of multimodal generative and expressive editing tasks -all using a single model. In the following, we discuss some limitations of our approach and potential future work. \n\nAdditional modalities. While 4M already includes a number of modalities, from semantics-based and geometry-based to text-based, bringing additional modalities could vastly improve its usability. For example, training on features extracted from a large language model has been shown to significantly boost text-to-image generation capabilities [97,123,18]. Introducing modalities like edges, sketches, or human poses has the potential to greatly improve the expressiveness [130] of 4M, but it may also be expanded to videos or multi-view imagery to unlock spatial-temporal generation and editing capabilities. We anticipate 4M to conveniently extend to such new modalities. \n\nTokenizer quality. 4M can benefit from better tokenizers, both in terms of generation and transfer results, but there are limits to the amount of information that can be encoded in tokenized patches. \n\nOperating on tokens that cover a smaller image region, or operating on higher-resolution images may improve image quality, but is expected to be computationally more expensive. Generally, improvements along this direction are expected to directly boost the performance of 4M. \n\nDataset size and quality. While our binding dataset choice of CC12M is standard, 4M can benefit from training on significantly larger datasets [99,14,39]. Web-scraped image-text datasets contain many low-quality images, as well as captions that are not related to the image content. Fine-tuning 4M on a more curated dataset like LAION-Aesthetics V2 [99], or tuning the model using reinforcement learning [83] could significantly improve generation quality and diversity. We leave this for the future.",
            "score": 0.5371068850159665,
            "section_title": "Conclusion and Limitations",
            "char_start_offset": 31406,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1630
                },
                {
                    "start": 1633,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 825,
                    "end": 829,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 829,
                    "end": 832,
                    "matchedPaperCorpusId": "255372955"
                },
                {
                    "start": 950,
                    "end": 955,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1776,
                    "end": 1780,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 1982,
                    "end": 1986,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 2037,
                    "end": 2041,
                    "matchedPaperCorpusId": "256900646"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66552734375
        },
        {
            "corpus_id": "263835060",
            "title": "Multimodal Graph Learning for Generative Tasks",
            "text": "End-to-End Multimodal Learning: While many discriminative multimodal models [14,22] have also been developed, we primarily consider related work on generative multimodal models, as this is most closely related with our approach. Several recent approaches tackle multimodal learning by building upon the Transformer [32] architecture. Multimodal extensions typically use either full self-attention over modalities concatenated across the sequence dimension [3,28] or a crossmodal attention layer [30]. Self-supervised multimodal pretraining methods train these architectures from large-scale unlabeled multimodal data before transferring them to downstream multimodal tasks via fine-tuning [9,19]. These methods perform end-to-end pre-training, incurring extremely high computation costs, especially as model parameters increase [17]. Moreover, this framework is relatively inflexible for end-to-end pre-trained models to leverage readily available unimodal pre-trained models, such as text-only LMs or pretrained vision models. \n\nMultimodal Learning with Frozen Image Encoders and Large Language Models: Recently, various vision-language models have been proposed to leverage off-the-shelf pre-trained models and keep them frozen during pretrainig [1,17,16]. To input visual information directly to a frozen text-only LLM, a key challenge is to align visual features to the text space. Motivated by Frozen [31], which finetunes a visual encoder to map images into the hidden space of a text-only LLM, Blip-2 [17] and GILL [16] finetune separate image mapping networks whose inputs are precomputed by frozen image encoders and outputs are directly used as soft prompts to LLMs. On the other hand, Flamingo [1] inserts new cross-attention layers into the LLM to inject visual features and pre-trains the new layers on image-text pairs. Note that all these methods primarily focus on processing interleaved image and text inputs to generate text outputs.",
            "score": 0.5364696458518261,
            "section_title": "Related Work",
            "char_start_offset": 6104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1027
                },
                {
                    "start": 1030,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1951
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 80,
                    "end": 83,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 456,
                    "end": 459,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "173990158"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "231740629"
                },
                {
                    "start": 692,
                    "end": 695,
                    "matchedPaperCorpusId": "259275338"
                },
                {
                    "start": 1248,
                    "end": 1251,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1406,
                    "end": 1410,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 1705,
                    "end": 1708,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "261582721",
            "title": "InstructDiffusion: A Generalist Modeling Interface for Vision Tasks",
            "text": "The longstanding goal in artificial intelligence research is to develop a universal model that can solve any task. We present a brief overview of recent efforts towards this goal. Vision Language Foundation Models. The vast amount of easily accessible web-scale image-text pairs has spurred innovative research in vision language foundation models [15,35,39,45,66,75,97]. The pioneering works, CLIP [57] and ALIGN [26], use contrastive loss to align image-text pairs in a shared cross-modal embedding space, demonstrating strong generalization. Subsequent efforts extend them to a broader spectrum, such as the image-textlabel space proposed in UniCL [88] and a wider range of tasks as well as modalities supported in Florence [94] and INTERN [62]. However, these methods lack the ability to generate language, which limits their use in open-ended tasks like captioning or visual question answering. \n\nRecently, the success of large language models like GPT series [7,53,55,56], PaLM [4,12], and LLaMA [68], has been attracting a lot of interest [24,38,64,69,73,74,79] in enhancing these models with visual capabilities. Mostly, these models cast a variety of open-ended vision tasks as text prediction problems, mapping visual input content to language semantics. BEIT3 [77] unifies the pretraining task in a masked modeling manner. CoCa [92] and BLIP [36,37] unify contrastive and generative learning. Flamingo [2] accepts interleaved visual data and text as input and generates text in an open-ended manner. LLaVA [43] exploits visual instruction tuning by converting image-text pairs into an instruction-following format. GLIP v2 [95] and Kosmos v2 [54] leverage grounded image-text pairs to unlock the grounding capability of multimodal large language models. Our work sets itself apart by attempting to formulate vision tasks, such as segmentation and keypoint detection, into an instruction-following framework. This is challenging due to the unclear instructions and the diverse continuous output formats in these tasks. Vision Generalist Models.",
            "score": 0.5360358906341622,
            "section_title": "Related Work",
            "char_start_offset": 4250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "261138"
                },
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "215754208"
                },
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "9059202"
                },
                {
                    "start": 367,
                    "end": 370,
                    "matchedPaperCorpusId": "222125307"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 414,
                    "end": 418,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "248006101"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 974,
                    "end": 977,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "244920947"
                },
                {
                    "start": 1053,
                    "end": 1056,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 1056,
                    "end": 1059,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 1059,
                    "end": 1062,
                    "matchedPaperCorpusId": "252283985"
                },
                {
                    "start": 1353,
                    "end": 1357,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 1413,
                    "end": 1416,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1634,
                    "end": 1638,
                    "matchedPaperCorpusId": "249626342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.576171875
        },
        {
            "corpus_id": "276742029",
            "title": "Enhancing Monocular 3D Scene Completion with Diffusion Model",
            "text": "As one of the deep generative methods, diffusion-based models are widely adopted in many real-life applications such as image synthesis [23], video generation [10], and dense visual prediction [13]. Stable Diffusion [19,23] as Latent Diffusion Models (LDMs), shows great success on the high-resolution image synthesis task. Apart from image generation, diffusion models have also gained great success in image inpainting, which is an image editing technique that can edit or complete a given image by user requirements that has the form of text description, mask, or layout [12]. Ledits++ [2] allows users to provide a mask that indicates the target region to edit, combined with certain text that describes the desired content to generate of that region. This editing pipeline aligned with our goal of completing a specific image region. In our work, we use a pre-trained Stable Diffusion Figure 3. The pipeline of Flash3D [28]. Flash3D uses a ResNet encoder that extracts features from both the RGB image and its depth map estimated with a pre-trained monocular depth estimation model. They are subsequently processed by two decoders, which together output all 3D Gaussian parameters eventually. Image reproduced from [28]. \n\nmodel to generate multi-view images guided by mask and text description. \n\nVision language model. Vision Language Model (VLM) captures correlations between images and text, enabling application to downstream vision-language tasks without fine-tuning [34]. CLIP [20], a pioneering VLM, employs contrastive learning between text and image embeddings, each extracted by respective encoders. Its objective is to maximize similarity within matched text-image pairs while minimizing similarity across non-matching pairs. Despite CLIP's success in matching text-image pairs, it does not address image-to-text generation. CoCa [33] addresses this by proposing an image-to-caption framework that decouples the encoder and decoder, facilitating text generation from images. In CoCa, the image and text are initially encoded separately, then cross-attended in a multimodal decoder to generate captions.",
            "score": 0.5360342461776131,
            "section_title": "Related Work",
            "char_start_offset": 4347,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 140,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "248006185"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "257833921"
                },
                {
                    "start": 220,
                    "end": 223,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 589,
                    "end": 592,
                    "matchedPaperCorpusId": "265466786"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "257913547"
                },
                {
                    "start": 1489,
                    "end": 1493,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1847,
                    "end": 1851,
                    "matchedPaperCorpusId": "248512473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64990234375
        },
        {
            "corpus_id": "270619856",
            "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
            "text": "6 and Eq. 7.\n\nto the input text and uses them as contextual input to the language model [Tan and Bansal, 2020, Lu et al., 2022, Wang et al., 2023].Similarly, Tang et al. [2021] employs a knowledge distillation approach to fuse visual knowledge.Other works [Zhang et al., 2023, Guo et al., 2023, Li et al., 2023b] distill visual knowledge from multimodal embedding methods such as CLIP [Radford et al., 2021] into text-only language models.Another set of works utilizes pre-trained text-to-image generative models.In the context of diffusion-based text-to-image models, Z-LaVi [Yang et al., 2022] leverage generated visuals that match possible label predictions of a given text-only language model.Our method, instead, considers visuals that match the input text.LiVE [Tang et al., 2023] introduces a vision-text plug-and-play vision-text fusion layer, which is inserted within transformed blocks of pre-trained language models (LMs).iNLG [Zhu et al., 2023] uses generated images as additional visual supervision to guide the language model in text generation, where the visual input is provided as an additional input to the LM in the form of a visual prefix.Unlike LiVE and iNGL, which integrate visual knowledge as input to the LM or as an integrated layer, we, instead, use the output of an unmodified pre-trained LLM together with an encoding of a generated image, using a late-fusion layer.This enables our model to focus on the input text, while also using visual information if needed.Second, instead of using a specialized attention-like mechanism or a mapping network, our work aggregates scores simply by averaging predictions made using different generated images, obtained from variations of the input text.This enables our method to use a diverse set of predictions obtained using diverse visual \"experts\" and gauge its final prediction towards the more confident predictions.\n\nMultiple Generations Agreement.",
            "score": 0.5357451281990845,
            "section_title": "Related Work",
            "char_start_offset": 4435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 14,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 244
                },
                {
                    "start": 244,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 933
                },
                {
                    "start": 933,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1719
                },
                {
                    "start": 1719,
                    "end": 1889
                },
                {
                    "start": 1891,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 176,
                    "matchedPaperCorpusId": "235743020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405029296875
        },
        {
            "corpus_id": "274023123",
            "title": "Spider: Any-to-Many Multimodal LLM",
            "text": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models (LLMs), enabling the integration of various modalities. However, Any-to-Any MLLMs are limited to generating pairwise modalities 'Text + X' within a single response, such as Text + {Image or Audio or Video}. To address this limitation, we introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG) framework, which can generate an arbitrary combination of modalities 'Text + Xs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our Spider integrates three core components: a Base Model for basic X-to-X (i.e., Any-to-Any) modality processing, an Any-to-Many Instruction Template designed for producing Xs signal prompts, and a novel Efficient Decoders-Controller for controlling multimodal Decoders to generate Xs (many-modal) contents. To train Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset, which facilitates learning the X-to-Xs (i.e., Any-to-Many) capability necessary for AMMG. Ultimately, the well-trained Spider generates a pseudo X-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the potential for AMMG tasks in future research. Overall, this work not only pushes the boundary of multimodal interaction but also provides rich data support for advancing the field. Code: https://github.com/Layjins/Spider",
            "score": 0.5356779718149276,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56201171875
        },
        {
            "corpus_id": "270063324",
            "title": "TIE: Revolutionizing Text-based Image Editing for Complex-Prompt Following and High-Fidelity Editing",
            "text": "Text-guided Image Generation.Text-to-Image (T2I) generation has gained popularity and become one of the most hot topics currently.Early T2I methods are based on GANs [34; 41], which trained a generator and a discriminator in the adversarial process.Another stream of T2I methods follow an auto-regressive pipeline [25; 6; 36], mainly predicting image tokens sequentially by employing Transformers.More recently, Diffusion models [22; 28; 29; 24] have made great progress compared to GANs and auto-regressive methods due to their ability to generate highly realistic and more diverse images.GLIDE [22] is the first T2I framework based on the Diffusion Model, which replaced the original class label with text.Imagen [28] follows GLIDE and adopts a pretrained and frozen large language model as the text encoder.LDM [26] compresses images into low-dimensional latent space representations, effectively reducing computational complexity.Stable Diffusion [29] is a milestone work which scaled up based on LDM [26].DALLE-2 [24] takes CLIP [23] as the text encoder and generates images from the CLIP latent space.The progress in diffusion models has also stimulated their applications in text-to-image editing.For instance, Prompt-to-Prompt [11] facilitates image editing by altering words in the original prompts and incorporating cross-attention maps during the diffusion process.DiffEdit [4] emphasizes areas of an input image that should be modified based on a text query to assist editing.InstructPix2Pix [3] provides a large-scale editing dataset composed of instruction-based samples.HQ-Edit [15] further finetunes InstructPix2Pix, enhancing the performance of cutting-edge text-to-image editing.\n\nMulti-modal LLM based Understanding and Generation.In light of the rapid advancement and impressive performance of recent multimodal large language models, numerous studies have begun to explore multimodal large language model based understanding and generation models.These methods can be broadly categorized into two types: embedding-based and text-based approaches.",
            "score": 0.5356560851243894,
            "section_title": "Related works",
            "char_start_offset": 8297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 29,
                    "end": 130
                },
                {
                    "start": 130,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 397
                },
                {
                    "start": 397,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 810
                },
                {
                    "start": 810,
                    "end": 934
                },
                {
                    "start": 934,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1107
                },
                {
                    "start": 1107,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1376
                },
                {
                    "start": 1376,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1585
                },
                {
                    "start": 1585,
                    "end": 1697
                },
                {
                    "start": 1699,
                    "end": 1750
                },
                {
                    "start": 1750,
                    "end": 1968
                },
                {
                    "start": 1968,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 715,
                    "end": 719,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 814,
                    "end": 818,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1005,
                    "end": 1009,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1034,
                    "end": 1038,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1504,
                    "end": 1507,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "271924334",
            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
            "text": "This motivates a research question: can one single transformer handle both multimodal understanding and generation? \n\nVery recently, Chameleon (Team, 2024) demonstrated this is possible. Specifically, Chameleon enables an early fusion of different modalities to generate both text and image tokens through the same manner of autoregressive modeling. While it is reasonable to model text tokens autoregressively (Touvron et al., 2023;Liu et al., 2024c), it is less clear whether it is better to model im- age/video patches (or pixels) autoregressively as well. An apparent and significant bottleneck of autoregressively predicting an image is the large number of sampling steps required due to its causal attention, particularly when dealing with images/videos in higher resolution. Further, (continuous) diffusion models (Podell et al., 2023;Esser et al., 2024) have exhibited superior capabilities in visual generation than autoregressive ones and are in full attention. This motivates us to ponder: can such one single transformer involve both autoregressive and diffusion modeling? Here we envision a new paradigm that text is represented as discrete tokens and modeled autoregressively same with large language models (LLMs), and continuous image pixels are modeled using denoising diffusion. However, it is non-trivial to integrate these two distinct techniques into one single network due to the significant differences between discrete text tokens and continuous image representations. Another challenge lies in the fact that existing state-of-the-art diffusion models typically rely on two distinct models, i.e., a text encoder to encode text conditional information and a denoising network to predict noise. \n\nTo this end, we present a novel unified model, i.e., Show-o, capable of addressing both multimodal understanding and generation tasks simultaneously with mixed autoregressive and diffusion modeling (as shown in Fig. 2). Specifically, Show-o is built upon a pre-trained LLM and inherits the autoregressive modeling capability for text-based reasoning.",
            "score": 0.5352091432119452,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 842,
                    "end": 861,
                    "matchedPaperCorpusId": "268247980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "263671826",
            "title": "Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency",
            "text": "Current vision-language generative models rely on expansive corpora of paired image-text data to attain optimal performance and generalization capabilities. However, automatically collecting such data (e.g. via large-scale web scraping) leads to low quality and poor image-text correlation, while human annotation is more accurate but requires significant manual effort and expense. We introduce $\\textbf{ITIT}$ ($\\textbf{I}$n$\\textbf{T}$egrating $\\textbf{I}$mage $\\textbf{T}$ext): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data. ITIT is comprised of a joint image-text encoder with disjoint image and text decoders that enable bidirectional image-to-text and text-to-image generation in a single framework. During training, ITIT leverages a small set of paired image-text data to ensure its output matches the input reasonably well in both directions. Simultaneously, the model is also trained on much larger datasets containing only images or texts. This is achieved by enforcing cycle consistency between the original unpaired samples and the cycle-generated counterparts. For instance, it generates a caption for a given input image and then uses the caption to create an output image, and enforces similarity between the input and output images. Our experiments show that ITIT with unpaired datasets exhibits similar scaling behavior as using high-quality paired data. We demonstrate image generation and captioning performance on par with state-of-the-art text-to-image and image-to-text models with orders of magnitude fewer (only 3M) paired image-text data.",
            "score": 0.535000522054613,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464599609375
        },
        {
            "corpus_id": "267897370",
            "title": "LLMBind: A Unified Modality-Task Integration Framework",
            "text": "Overview. The framework of LLMBind is illustrated in Fig. 3. It primarily consists of a vision transformer [46] as the image encoder, a large language model called Vicuna [8], a SAM-like mask decoder as well as image, audio, and video diffusion models [52,57,58] for generation and editing tasks. \n\nVisual Encoder and Tokenizer. We prioritize the processing of inputs from two primary modalities: images and text. For image encoding, we employ the ViT-L/14 [46] to extract image features. We first reshape the image  \u2208 R  \u00d7 \u00d7 into a sequence of flattened 2D patches \n\nis the resulting number of image patches. The LLM uses constant latent vector size  through all of its layers, so we use two MLP layers to map the image patches to  dimensions. Similarly, text is processed through a text embedding function , which projects it into a sequence of text tokens \n\n, where  is the length of this text token sequence. Task-Specific Tokens. Different multimodal tasks often require distinct network structures, making it difficult to consolidate them within a shared LLM. To overcome this challenge, we expand the vocabulary of the LLM with specially designed, learnable task-specific tokens. As shown in Fig. 4 , these tokens can be classified into two categories: Task-Prompt Tokens and Semantic-Embedding Tokens. The first category includes tokens similar to standard text tokens, such as <gen> and </gen> for image generation, <vid_cap> Image-Editing Model  Modality Task Integration. We seamlessly integrate various modality-task models in our framework using task-specific text prompts and token embeddings generated by LLM. For understanding tasks, we directly output text. In the generation and editing scenarios, we feed text prompts into the corresponding task model. Specifically, we use Stable Diffusion [52] for image generation, CoDi [58] for audio generation, ModelScopeT2V [61] for video generation, Instruct-pix2pix [5] for image editing, thus ensuring high-quality, modality-specific output.",
            "score": 0.5345969725575223,
            "section_title": "LLMBind",
            "char_start_offset": 9775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 111,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 457,
                    "end": 461,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "254408789",
            "title": "Learning Video Representations from Large Language Models",
            "text": "Vision-language representation learning maps visual and textual embeddings into a common space using metriclearning techniques [21,73]. Recently, different pretext tasks are proposed to learn a finer-grained association between visual and textual modality, e.g. masked language modeling (MLM) [10,41,62] and captioning [16,80]. Another line of research focuses on scaling up both models and pre-training data. For instance, CLIP [49] is pre-trained on 400M image-text pairs with a contrastive loss (InfoNCE [48,59]) while CoCa [80] unifies contrastive and generative approaches with a single foundation model. Similar trends are also witnessed in the videotext domain [36,64,88]. However, collecting high-quality video-text data is more difficult than image-text. Therefore, efforts are made to learn from uncurated videos with machine-generated audio transcripts via contrastive learning [44,77,82] or unsupervised alignment [25] while other works focus on either adapting well-performing image-text models to videos [32,40,47,78], or curriculum learning from a single frame to multiple frames [4]. In contrast, our approach leverages language models to generate temporally dense textual supervision on long-form videos. Generative Visual Language Models (VLM) were first used for image/video captioning using recurrent networks [17,68] and Transformer-based architectures [42,56]. More recently, generative VLMs have unified multiple vision tasks [11,89] by training multi-modal Transformers on visual-text pairs [30,81]. Meanwhile, generative VLMs also excel at multimodal tasks via zero-shot or few-shot prompting [1,65,83] by leveraging multi-billion-parameter LLMs pre-trained on massive text corpus [7,28,50]. In our work, we demonstrate that generative VLMs can narrate long videos and the resulting video-text data benefits video-language representation learning. Large-scale multimodal video datasets are crucial for 6587 video understanding tasks but are hard to collect. Conventional video-text",
            "score": 0.5344872612939846,
            "section_title": "Related Work",
            "char_start_offset": 5131,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 131,
                    "matchedPaperCorpusId": "261138"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 300,
                    "end": 303,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "219573658"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "911406"
                },
                {
                    "start": 668,
                    "end": 672,
                    "matchedPaperCorpusId": "231880022"
                },
                {
                    "start": 672,
                    "end": 675,
                    "matchedPaperCorpusId": "102483628"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "219617394"
                },
                {
                    "start": 889,
                    "end": 893,
                    "matchedPaperCorpusId": "209370497"
                },
                {
                    "start": 893,
                    "end": 896,
                    "matchedPaperCorpusId": "238215257"
                },
                {
                    "start": 896,
                    "end": 899,
                    "matchedPaperCorpusId": "235352775"
                },
                {
                    "start": 926,
                    "end": 930,
                    "matchedPaperCorpusId": "247996793"
                },
                {
                    "start": 1018,
                    "end": 1022,
                    "matchedPaperCorpusId": "244954623"
                },
                {
                    "start": 1022,
                    "end": 1025,
                    "matchedPaperCorpusId": "251403197"
                },
                {
                    "start": 1025,
                    "end": 1028,
                    "matchedPaperCorpusId": "251320177"
                },
                {
                    "start": 1028,
                    "end": 1031,
                    "matchedPaperCorpusId": "265040160"
                },
                {
                    "start": 1095,
                    "end": 1098,
                    "matchedPaperCorpusId": "232478955"
                },
                {
                    "start": 1330,
                    "end": 1334,
                    "matchedPaperCorpusId": "5736847"
                },
                {
                    "start": 1334,
                    "end": 1337,
                    "matchedPaperCorpusId": "1169492"
                },
                {
                    "start": 1378,
                    "end": 1381,
                    "matchedPaperCorpusId": "246063824"
                },
                {
                    "start": 1449,
                    "end": 1453,
                    "matchedPaperCorpusId": "231802355"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "244527510"
                },
                {
                    "start": 1618,
                    "end": 1621,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 1624,
                    "end": 1627,
                    "matchedPaperCorpusId": "247922520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55859375
        },
        {
            "corpus_id": "271213320",
            "title": "ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report",
            "text": "Multimodal learning in the intersection of visual and linguistic data has seen significant advancements in recent years. This area of research focuses on developing models that can understand and generate content by integrating information from both visual inputs, such as images or videos, and textual descriptions. These models leverage the complementary nature of visual and textual data to perform a wide range of tasks, including but not limited to image captioning, visual question answering, and cross-modal information retrieval. \n\nOne of the key innovations in this field has been the development of models that are trained on vast datasets of image-text pairs. [12,7,8] These models employ contrastive learning techniques to align the representations of images and their corresponding textual descriptions in a shared embedding space. By doing so, they learn to associate specific visual features with relevant linguistic concepts, enabling them to understand complex queries and content with a high degree of accuracy. Moreover, the use of transformer-based [13] architectures has further enhanced the capabilities of these models. Transformers provide a flexible and powerful framework for modeling sequential data, and their adoption in multimodal learning has allowed for the effective integration of visual and textual inputs. This has led to significant improvements in tasks requiring nuanced understanding and generation of content, such as generating descriptive captions for images that accurately reflect their content and context or answering questions based on visual information. \n\nThe emergence of Large Language Models (LLMs) has also marked a significant milestone in the field of artificial intelligence, particularly in natural language processing. Models like GPT [1], BERT [4], and others have demonstrated exceptional abilities in generating and understanding text, significantly advancing the capabilities of AI in understanding human languages. These models have been instrumental in a variety of applications, including text generation, translation, and semantic analysis, showcasing their versatility and powerful computational abilities. However, their primary limitation lies in their text-only nature, confining their applicability to tasks that do not require understanding or generating content in other modalities such as images, speech, and videos. \n\nTo bridge this gap, the research and development of Large Vision Language Models (LVLMs) [9,2,10,15,14] have gained momentum.",
            "score": 0.5339642430741416,
            "section_title": "Related Work",
            "char_start_offset": 2249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2391
                },
                {
                    "start": 2394,
                    "end": 2519
                }
            ],
            "ref_mentions": [
                {
                    "start": 677,
                    "end": 679,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "266551693",
            "title": "From text to multimodal: a survey of adversarial example generation in question answering systems",
            "text": "In [61], VX2TEXT is proposed to generate text from multimodal inputs such as video, text, speech, or audio. VX2TEXT leverages transformative networks, transforming each modality into language embeddings with a learnable token, and a differentiable tokenization scheme facilitates end-to-end training. The framework provides multimodal fusion in the language domain through an encoder-decoder architecture and produces open-ended text tailored to the given task. Xie et al. focused on generating questions based on a given image and target answer, called conditional VQG [62]. They proposed a model called MOAG, which consists of four modules. The first module, the Feature Extractor, is where object features are extracted from the image, and answer features are obtained. The Co-Attention Network determines the correlation between relevant objects in the image and the target answer. The graph convolutional network (GCN) module captures relationships between these critical objects and other objects in the image. Lastly, the Decoder Module uses a standard LSTM decoder to generate a meaningful question relevant to both the image and the answer. \n\nIn [63], Li et al. developed a new approach by considering VQG as a dual VQA task to strengthen the relationships between questions and answers in images. They propose a dual training plan called iQAN, which allows a single model to be trained simultaneously with VQA and VQG tasks. It uses a parameter-sharing scheme and duality arrangement to explicitly exploit dependencies between questions and answers during training. Mostafazadeh et al. introduced a novel approach to QG using images as context [64]. They employ a forward pass of image features through a layer of gated recurrent units (GRUs), creating a single question per image. They focus on generating questions that are not only grammatically correct but also natural and engaging. \n\nWang et al. introduced Video QG [65]. Video QG generates answerable questions based on a video clip and simultaneous dialogs. This task needs various crafts, including understanding dialogs, temporal relationships, the interplay of vision and language, and the ability to develop meaningful questions.",
            "score": 0.5329880316166887,
            "section_title": "Encoder-decoder models",
            "char_start_offset": 29780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 7,
                    "matchedPaperCorpusId": "231719672"
                },
                {
                    "start": 570,
                    "end": 574,
                    "matchedPaperCorpusId": "239012092"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "3184591"
                },
                {
                    "start": 1932,
                    "end": 1936,
                    "matchedPaperCorpusId": "195833622"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "267200040",
            "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
            "text": "Multimodal Large Language Models (MLLMs) have significantly broadened the capabilities of language models to process various modalities (Liu et al., 2023a;Li et al., 2021Li et al., , 2022a;;Wang et al., 2023;Driess et al., 2023). These models inherently facilitate interleaved vision-language input, effectively handling multiple images. Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder. The key component of KOSMOS-G is an AlignerNet, which is trained solely on textual data to align the output embedding space of the frozen SDv1.5 U-Net with the MLLM. In contrast, our approach centers on training the U-Net model end-to-end specifically for multimodal diffusion, significantly enhancing both the faithfulness and relevance of generated images in multimodal contexts. Differing from alignmentbased approaches, our two-stage training strategy markedly improves the model's proficiency in following multimodal instructions, particularly in complex multi-entity scenarios.",
            "score": 0.5324202213532938,
            "section_title": "Generating with Multi-modal Language Models",
            "char_start_offset": 14730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 170,
                    "matchedPaperCorpusId": "229924402"
                },
                {
                    "start": 170,
                    "end": 190,
                    "matchedPaperCorpusId": "247519008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.798828125
        },
        {
            "corpus_id": "233296879",
            "title": "Towards Open-World Text-Guided Face Image Generation and Manipulation",
            "text": "The first category produces images from texts directly by one generator and one discriminator. For example, Reed et al. [6] propose to use conditional GANs to generate plausible images from given text descriptions. Tao et al. [30] propose a simplified backbone that generates high-resolution images directly by Wasserstein distance and fuses the text information into visual feature maps to improve the image quality and text-image consistency. Despite the plainness and conciseness, the one-stage models produce dissatisfied results in terms of both photo-realism and text-relevance in some cases. Thus, another thread of research focuses on multi-stage processing. Zhang et al. [31] stack two GANs to generate high-resolution images from text descriptions through a sketch-refinement process. They further propose a three-stage architecture [32] that stacks multiple generators and discriminators, where multi-scale images are generated progressively in a course-to-fine manner. Xu et al. [5] improve the work of [32] from two aspects. First, they introduce attention mechanisms to explore fine-grained text and image representations. Second, they propose a Deep Attentional Multimodal Similarity Model (DAMSM) to compute the similarity between the generated image and the sentence. The subsequent studies basically follow the framework of [5] and have proposed several variants by introducing different mechanisms like attention [33] or memory writing gate [34]. However, the multi-stage frameworks produce results that look like a simple combination of visual attributes from different image scales. A concurrent zeroshot text-to-image system called DALL-E [35] is trained on large-scale unconstrained image-text pairs and can generate arbitrary classes of images from open-world descriptions. Despite the ability to generate arbitrary classes of images from open-world descriptions, the quality and resolution of its results are often limited compared with the state-of-theart text-to-image methods that focus on certain classes [36]. \n\nText-Guided Image Manipulation. Text-guided image manipulation is similar to text-to-image generation in terms of producing results that contain desired visual attributes. Vision-language Representation Learning. One key of textguided image synthesis is to match visual attributes with corresponding words.",
            "score": 0.5317697402839667,
            "section_title": "RELATED WORK",
            "char_start_offset": 10615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2039
                },
                {
                    "start": 2042,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2213
                },
                {
                    "start": 2214,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2348
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 123,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 680,
                    "end": 684,
                    "matchedPaperCorpusId": "1277217"
                },
                {
                    "start": 843,
                    "end": 847,
                    "matchedPaperCorpusId": "3646966"
                },
                {
                    "start": 991,
                    "end": 994,
                    "matchedPaperCorpusId": "8858625"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "3646966"
                },
                {
                    "start": 1342,
                    "end": 1345,
                    "matchedPaperCorpusId": "8858625"
                },
                {
                    "start": 1460,
                    "end": 1464,
                    "matchedPaperCorpusId": "91183909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53515625
        },
        {
            "corpus_id": "227736593",
            "title": "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption",
            "text": "The Vision-language tasks incorporating scene text [7,18,49,46], e.g., Text-VQA [49,8,40,56] and Text-Caption [46], pose new challenges to vision-language models of reading and understanding scene text in image context. Extended from Visual Question Answering (VQA) [6], Text-VQA aims to answer questions by understanding the\n\nText-Caption: In between the clock and the big screen at the San Francisco Giants field, is a large banner advertisement for Coors light. 18. 2% 11.0% 23.4% Figure 1. (a) Text-VQA and Text-Caption tasks aim at reading and understanding scene text in images for question answering and image caption generation, respectively. We highlight the scene textrelated words in bold. (b) By explicitly incorporating scene text in pre-training, Text-Aware Pre-training (TAP) significantly outperforms both the non-TAP baseline and previous state of the art on multiple tasks (bars shown in red and blue colors, respectively). scene text in the image-question context. Text-Caption seeks to generate an image caption [54,4] that describes both the visual and scene text information in the image, as shown in Figure 1 (a). These tasks have many potential applications, including robotics [5], document understanding [40], assisting visually-impaired people [7,18], etc.\n\nA typical Text-VQA/Text-Caption framework consists of 1) a feature encoder for each single modality (text word, visual object, and scene text), 2) a multi-modal fusion module, and 3) a decoding module for prediction generation. Previous studies [49,17,16,20,25,46,55] improve the model's performance by designing stronger network architectures. Among them, LoRRA [49] added an OCR attention branch for scene text encoding to a VQA model [24]. M4C [20,46] proposed a transformer-based multi-modal fusion module [52] and a multi-step multichoice decoding module. Despite the effective network design, most previous models are optimized with a sole ob-je",
            "score": 0.5316920386197663,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 54,
                    "matchedPaperCorpusId": "52804681"
                },
                {
                    "start": 54,
                    "end": 57,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 84,
                    "end": 86,
                    "matchedPaperCorpusId": "173188651"
                },
                {
                    "start": 86,
                    "end": 89,
                    "matchedPaperCorpusId": "209413409"
                },
                {
                    "start": 89,
                    "end": 92,
                    "matchedPaperCorpusId": "211258929"
                },
                {
                    "start": 266,
                    "end": 269,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 465,
                    "end": 467,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 1036,
                    "end": 1038,
                    "matchedPaperCorpusId": "3753452"
                },
                {
                    "start": 1202,
                    "end": 1205,
                    "matchedPaperCorpusId": "4673790"
                },
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "209413409"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "matchedPaperCorpusId": "52804681"
                },
                {
                    "start": 1274,
                    "end": 1277,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 1530,
                    "end": 1534,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1534,
                    "end": 1537,
                    "matchedPaperCorpusId": "214727709"
                },
                {
                    "start": 1540,
                    "end": 1543,
                    "matchedPaperCorpusId": "208006464"
                },
                {
                    "start": 1543,
                    "end": 1546,
                    "matchedPaperCorpusId": "220714231"
                },
                {
                    "start": 1549,
                    "end": 1552,
                    "matchedPaperCorpusId": "222278686"
                },
                {
                    "start": 1648,
                    "end": 1652,
                    "matchedPaperCorpusId": "85553602"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46142578125
        },
        {
            "corpus_id": "54457433",
            "title": "StoryGAN: A Sequential Conditional GAN for Story Visualization",
            "text": "Variational AutoEncoders (VAEs) [21], Generative Adversarial Nets (GANs) [8], and flow-based generative models [6,7]) have been widely applied to a wide range of generation tasks including text-to-image generation, video gen-eration, style transfer, and image editing. Story visualization falls into this broad categorization of generative tasks, but has several distinct aspects. \n\nVery relevant for story visualization task is conditional text-to-image transformation [26,15,36,33], which can now generate high-resolution realistic images [34,32]. A key task in text-to-image generation is to understand longer and more complex input texts. For example, this has been explored in dialogue-to-image generation, where the input is a complete dialogue session rather than a single sentence [27]. Another related task is that of textual image editing, which edits an input image according to a textual editing query [3,28]. This task requires consistency between the original image and the output image. Finally, there is the task of placing pre-specified images and objects in a picture from a text description [18]. This task also relates text to a consistent image, but does not require a full image generation procedure. \n\nA second closely related task to story visualization is that of video generation, especially that of text-to-video [22,11] or image-to-video generation [1,29,30]. Existing approaches only generate short video clips [11,4,10] without scene changes. The biggest challenge in video generation is how to ensure a smooth motion transition across successive video frames. To this end, researchers disentangle dynamic and static features for motion and background, respectively [30,22,29,5]. In our modeling of story visualization, the whole story sets the static features and each input sentence encodes dynamic features. However, there are several differences: (i) conditional video generation has only one input, while our task has a sequential, evolving input; and (ii) the motion in video clips is continuous in a single scene, while the generated images that visualizes a story are discrete and often represent different scenes.",
            "score": 0.5309777620365352,
            "section_title": "Related Work",
            "char_start_offset": 4416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "233404466"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "7577075"
                },
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "1277217"
                },
                {
                    "start": 914,
                    "end": 917,
                    "matchedPaperCorpusId": "28316672"
                },
                {
                    "start": 1340,
                    "end": 1344,
                    "matchedPaperCorpusId": "8672818"
                },
                {
                    "start": 1383,
                    "end": 1386,
                    "matchedPaperCorpusId": "9933254"
                },
                {
                    "start": 1444,
                    "end": 1446,
                    "matchedPaperCorpusId": "3663219"
                },
                {
                    "start": 1446,
                    "end": 1449,
                    "matchedPaperCorpusId": "4622697"
                },
                {
                    "start": 1696,
                    "end": 1700,
                    "matchedPaperCorpusId": "9933254"
                },
                {
                    "start": 1700,
                    "end": 1703,
                    "matchedPaperCorpusId": "8672818"
                },
                {
                    "start": 1706,
                    "end": 1708,
                    "matchedPaperCorpusId": "13349803"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.469970703125
        },
        {
            "corpus_id": "274269212",
            "title": "Multimodal robot-assisted English writing guidance and error correction with reinforcement learning",
            "text": "By optimizing this loss function, the model is more likely to generate text that is highly relevant to the context and adheres to linguistic rules, especially in scenarios requiring correction and assistance in English writing. This improvement makes the model more targeted during the generation phase, enhancing the quality and practicality of the generated text. On the image encoder side, a pre-trained VGG19 model is used as the base. VGG19 extracts image features through its multi-layer convolutional structure, which are then input into the ALBEF framework for alignment and fusion with text features. To ensure that the image encoder can effectively adapt to the multimodal tasks in this paper, VGG19 retains its original feature extraction capabilities during training while further optimizing to make its feature representation more accurate and representative. Through this process, the image encoder provides high-quality image feature inputs for multimodal tasks, ensuring that the model is efficient and accurate in handling multimodal data. Ultimately, the text generator and image encoder work together within the ALBEF framework to optimize the processing of multimodal tasks, thereby improving the overall performance of the model. \n\nALBEF as a foundational framework: While ALBEF serves as the base framework for aligning and fusing multimodal information, our model introduces crucial modifications, especially in how we handle the text generation and correction tasks. ALBEF primarily focuses on alignment and fusion of visual and textual information. In contrast, our contribution lies in developing an enhanced text editing mechanism that leverages this multimodal alignment for more effective and contextually appropriate English writing guidance. \n\nNovel text editing framework with improved loss functions: One of our key contributions is the development of a unified framework that is compatible with various loss function configurations. We designed this framework to support more advanced learning objectives by incorporating convex functions into the loss formulation. The introduction of convex-based composite loss functions offers significant advantages, particularly for error correction and language assistance tasks, where highprecision outputs are essential. This allows the model to better focus on generating high-probability target outputs, resulting in more natural, contextually accurate text generation, which is crucial for English writing guidance and error correction.",
            "score": 0.5308954685118773,
            "section_title": "Methodology . Overview of our network",
            "char_start_offset": 18414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2296
                },
                {
                    "start": 2297,
                    "end": 2515
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "276107395",
            "title": "End-to-end Training for Text-to-Image Synthesis using Dual-Text Embeddings",
            "text": "Text-to-Image (T2I) synthesis is a challenging task that requires modeling complex interactions between two modalities ( i.e., text and image). A common framework adopted in recent state-of-the-art approaches to achieving such multimodal interactions is to bootstrap the learning process with pre-trained image-aligned text embeddings trained using contrastive loss. Furthermore, these embeddings are typically trained generically and reused across various synthesis models. In contrast, we explore an approach to learning text embeddings specifically tailored to the T2I synthesis network, trained in an end-to-end fashion. Further, we combine generative and contrastive training and use two embeddings, one optimized to enhance the photo-realism of the generated images, and the other seeking to capture text-to-image alignment. A comprehensive set of experiments on three text-to-image benchmark datasets (Oxford-102, Caltech-UCSD, and MS-COCO) reveal that having two separate embeddings gives better results than using a shared one and that such an approach performs favourably in comparison with methods that use text representations from a pre-trained text encoder trained using a discriminative approach. Finally, we demonstrate that such learned embeddings can be used in other contexts as well, such as text-to-image manipulation.",
            "score": 0.5308747361810879,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "277955233",
            "title": "PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline",
            "text": "Multimodal models have long sought to bridge heterogeneous modalities by learning unified semantic representations [43]. Early CLIP-based multimodal models [40] pioneered the dual-encoder architecture (e.g., ViT for images, BERT for text) and employed contrastive loss [6] to align cross-modal embeddings in shared semantic spaces. The advent of large multimodal models (LMM) marks a paradigm shift that extends beyond representation alignment to encompass multimodal generation capabilities. This evolution is driven by the integration of large-scale transformerbased architectures, such as large language models (LLMs), Vision Transformers (ViT), and diffusion transformers (DiT). LMMs integrate encoders with the backbone model using modality adapters, as illustrated in Fig. 1. The backbone model subsequently generates output representations either through The distribution of text tokens per image in OBELICS [27], LAION-2B [41], and ScienceQA [31] image datasets, and text tokens per second in ShareGPT4Video [5], InternVid [50], and MMTrail-2M [7] video datasets. The Y-axis shows normalized data proportions. \n\n(c-d) Computational requirements across 100 data batches for VLM [33] and T2V models after data packing. The X-axis represents the data batch index sorted in ascending order by total computational cost, while the Y-axis indicates floating-point operations measured in TFLOPs. \n\nautoregressive processes or diffusion mechanisms. These representations are then converted into human-perceivable formats, including text, audio, and images, via specialized modality decoders. The scalable and generative architecture of LMM significantly broadens the spectrum of supported tasks compared to CLIP-based models. For instance, in an LMM (Fig. 2), users can interleave text, images, and video within a single query and iteratively refine their queries by appending follow-up questions to create multi-turn dialogs [10,22,35].",
            "score": 0.5306678683203221,
            "section_title": "Large Multimodal Models",
            "char_start_offset": 6761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 272,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 930,
                    "end": 934,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "271571434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "257427461",
            "title": "Scaling up GANs for Text-to-Image Synthesis",
            "text": "Text-to-image synthesis. Generating a realistic image given a text description, explored by early works [42,85], is a challenging task. A common approach is text-conditional GANs [55,56,67,71,76,83] on specific domains [68] and datasets with a closed-world assumption [41]. With the development of diffusion models [13,21], autoregressive (AR) transformers [10], and large-scale language encoders [50,52], text-to-image synthesis has shown remarkable improvement on an open-world of arbitrary text descriptions. GLIDE [46], DALL\u00b7E 2 [53], and Imagen [59] are representative diffusion models that show realistic outputs with the aid of a pretrained language encoder [50,52]. AR models, such as DALL\u00b7E [54], Make-A-Scene [16], CogView [14,15], and Parti [73] also achieve amazing results. While these models exhibit unprecedented image synthesis ability, they require time-consuming iterative processes to achieve high-quality image sampling.\n\nGAN-based image synthesis. GANs [17] have been one of the primary families of generative models for natural image synthesis. As the sampling quality and diversity of GANs improve [31-34, 36, 51, 60], GANs have been deployed to various computer vision and graphics applications, such as text-to-image synthesis [55], image-toimage translation [23,27,37,47,48,82], and image editing [1,6,49,81]. Notably, StyleGAN-family models [32,34] have shown impressive ability in image synthesis tasks for single-category domains [1,25,49,70,84]. Other works have explored class-conditional GANs [5,29,62,74,79] on datasets with a fixed set of object categories.\n\nIn this paper, we change the data regimes from singleor multi-categories datasets to extremely data-rich situations. We make the first expedition toward training a largescale GAN for text-to-image generation on a vast amount of web-crawled text and image pairs,",
            "score": 0.5297108670530271,
            "section_title": "Related Works",
            "char_start_offset": 4929,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "9996719"
                },
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "263889849"
                },
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "1515901"
                },
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "247778989"
                },
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "1277217"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 401,
                    "end": 404,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 518,
                    "end": 522,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 665,
                    "end": 669,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 669,
                    "end": 672,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 974,
                    "end": 978,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 1284,
                    "end": 1288,
                    "matchedPaperCorpusId": "4883312"
                },
                {
                    "start": 1288,
                    "end": 1291,
                    "matchedPaperCorpusId": "6200260"
                },
                {
                    "start": 1291,
                    "end": 1294,
                    "matchedPaperCorpusId": "260444154"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "matchedPaperCorpusId": "220871180"
                },
                {
                    "start": 1297,
                    "end": 1300,
                    "matchedPaperCorpusId": "81981856"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "233404466"
                },
                {
                    "start": 1323,
                    "end": 1326,
                    "matchedPaperCorpusId": "102350964"
                },
                {
                    "start": 1326,
                    "end": 1328,
                    "matchedPaperCorpusId": "13890001"
                },
                {
                    "start": 1328,
                    "end": 1331,
                    "matchedPaperCorpusId": "232428282"
                },
                {
                    "start": 1331,
                    "end": 1334,
                    "matchedPaperCorpusId": "14924561"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "235606261"
                },
                {
                    "start": 1372,
                    "end": 1375,
                    "matchedPaperCorpusId": "209202273"
                },
                {
                    "start": 1459,
                    "end": 1462,
                    "matchedPaperCorpusId": "102350964"
                },
                {
                    "start": 1462,
                    "end": 1465,
                    "matchedPaperCorpusId": "214802845"
                },
                {
                    "start": 1465,
                    "end": 1468,
                    "matchedPaperCorpusId": "232428282"
                },
                {
                    "start": 1525,
                    "end": 1528,
                    "matchedPaperCorpusId": "52889459"
                },
                {
                    "start": 1528,
                    "end": 1531,
                    "matchedPaperCorpusId": "240353909"
                },
                {
                    "start": 1531,
                    "end": 1534,
                    "matchedPaperCorpusId": "246441861"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38037109375
        },
        {
            "corpus_id": "253581213",
            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
            "text": "Composing large pretrained models Recent work has shown that large pretrained models can be combined to solve multimodal tasks that no one model can perform alone, such as image captioning and visual question answering (tasks that require the knowledge of both a large language model and a text-image model). Techniques for combining pretrained models include joint finetuning on a new task [4,34,41,68], communication through prompting [63,70], composing probability distributions of energybased models [11,38], guiding one model with feedback from another [62], and iterative optimization [35]. Our method is similar to prior work in that it leverages the complementary abilities of two pretrained models-GPT-3 [7]) and Stable Diffusion [52]-but differs in that we use these models to generate paired multi-modal training data. \n\nDiffusion-based generative models Recent advances in diffusion models [60] have enabled state-of-the-art image synthesis [10,18,19,54,56,61] as well as generative models of other modalities such as video [21,59], audio [31], text [36] and network parameters [46]. Recent text-toimage diffusion models [42,49,52,55] have shown to generate realistic images from arbitrary text captions. \n\nGenerative models for image editing Image editing models traditionally targeted a single editing task such as style transfer [15,16] or translation between image domains [22,24,37,43,72]. Numerous editing approaches invert [1][2][3]12] or encode [8,51,64] images into a latent space (e.g., StyleGAN [26,27]) where they can be edited by manipulating latent vectors. Recent models have leveraged CLIP [48] embeddings to guide image editing using text [5,9,14,29,32,42,45,71]. We compare with one of these methods, Text2Live [6], an editing method that optimizes for an additive image layer that maximizes a CLIP similarity objective.",
            "score": 0.5296723590387198,
            "section_title": "Prior work",
            "char_start_offset": 1447,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 504,
                    "end": 508,
                    "matchedPaperCorpusId": "214223619"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 953,
                    "end": 957,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 960,
                    "end": 963,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 963,
                    "end": 966,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "233241040"
                },
                {
                    "start": 969,
                    "end": 972,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 1140,
                    "end": 1143,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1348,
                    "end": 1351,
                    "matchedPaperCorpusId": "206593710"
                },
                {
                    "start": 1389,
                    "end": 1393,
                    "matchedPaperCorpusId": "4883312"
                },
                {
                    "start": 1393,
                    "end": 1396,
                    "matchedPaperCorpusId": "6200260"
                },
                {
                    "start": 1396,
                    "end": 1399,
                    "matchedPaperCorpusId": "146120584"
                },
                {
                    "start": 1399,
                    "end": 1402,
                    "matchedPaperCorpusId": "233231542"
                },
                {
                    "start": 1402,
                    "end": 1405,
                    "matchedPaperCorpusId": "233404466"
                },
                {
                    "start": 1442,
                    "end": 1445,
                    "matchedPaperCorpusId": "102350964"
                },
                {
                    "start": 1445,
                    "end": 1448,
                    "matchedPaperCorpusId": "208290975"
                },
                {
                    "start": 1448,
                    "end": 1451,
                    "matchedPaperCorpusId": "244729249"
                },
                {
                    "start": 1451,
                    "end": 1454,
                    "matchedPaperCorpusId": "248524853"
                },
                {
                    "start": 1465,
                    "end": 1468,
                    "matchedPaperCorpusId": "232269768"
                },
                {
                    "start": 1468,
                    "end": 1471,
                    "matchedPaperCorpusId": "220936362"
                },
                {
                    "start": 1471,
                    "end": 1474,
                    "matchedPaperCorpusId": "231802331"
                },
                {
                    "start": 1518,
                    "end": 1522,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 1522,
                    "end": 1525,
                    "matchedPaperCorpusId": "209202273"
                },
                {
                    "start": 1618,
                    "end": 1622,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1668,
                    "end": 1671,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 1671,
                    "end": 1673,
                    "matchedPaperCorpusId": "248239727"
                },
                {
                    "start": 1676,
                    "end": 1679,
                    "matchedPaperCorpusId": "244909410"
                },
                {
                    "start": 1679,
                    "end": 1682,
                    "matchedPaperCorpusId": "244773443"
                },
                {
                    "start": 1685,
                    "end": 1688,
                    "matchedPaperCorpusId": "232428282"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6162109375
        },
        {
            "corpus_id": "274023123",
            "title": "Spider: Any-to-Many Multimodal LLM",
            "text": "To build foundational Multimodal LLMs (MLLMs), researchers align pre-trained encoders from various modalities with the textual space of LLMs, enabling them to process multimodal inputs [2,23,28,30,37,53,73]. For example, Flamingo [2] connects a fixed image encoder to LLMs using cross-attention, while LLaVA [37] links image and word spaces via projection. BLIP-2 [30] uses a Q-Former to translate image queries into LLMs. Similar approaches are applied to videos (e.g., Video-Chat [31], Video-LLaMA [71]) and audios (e.g., SpeechGPT [70]). PandaGPT [54] extends this to six modalities using ImageBind [17]. \n\nHowever, existing MLLMs only perceive multimodal data and cannot generate content in arbitrary modalities. To address this, approaches like Visual-ChatGPT [64], Hug-gingGPT [50], and AudioGPT [22] use LLMs as decisionmakers, incorporating external multimodal encoders and decoders for multimodal input-output. Despite this, discrete text-message-based pipelines can introduce noise and hinder semantic understanding. NExT-GPT [66] overcomes this by learning an end-to-end multimodal input-output LLM, capable of handling any combination of text, image, video, and audio. \n\nHowever, these X-to-X MLLMs are limited to generating pairwise modalities 'Text + X' within a single interaction. In contrast, our proposed X-to-Xs Spider model aims for Any-to-Many Modalities Generation in a single response, supporting arbitrary combinations of a wider range of modalities as shown in Fig. 8, including text, image, audio, video, box, and mask.",
            "score": 0.5296134886352166,
            "section_title": "C.2. Large Multimodal Models",
            "char_start_offset": 23157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1545
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 188,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 194,
                    "end": 197,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 602,
                    "end": 606,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 1036,
                    "end": 1040,
                    "matchedPaperCorpusId": "261696650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "277741397",
            "title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering",
            "text": "It leverages a direct token-level injection mechanism, wherein visual embeddings are inserted into designated positions in the textual input. This design enables seamless multimodal fusion without the need for intermediary projection layers or modality-specific adapters, thereby preserving efficiency and reducing complexity. \n\nGiven the architectural heterogeneity between the vision and language components, effective cross-modal integration requires dedicated pretraining. To this end, a three-stage arXiv:2504.08269v1 [cs.CV] 11 Apr 2025 pretraining framework is proposed to progressively enhance multimodal capabilities. The first stage aligns visual embeddings with the frozen embedding space of the language model using instruction-following image-caption tasks. The second stage performs joint optimization using semantically enriched image-text pairs to refine fine-grained alignment. The final stage focuses on visual question answering, during which the language model is trained to perform multimodal reasoning while the vision encoder remains fixed. This progressive alignment strategy enhances the model's ability to integrate and reason over multimodal information, ultimately improving its performance in downstream tasks. \n\nBuilding on the pretrained VLMT backbone, a two-stage MMQA framework is constructed. The first stage employs a multimodal reranker that scores candidate documents based on their relevance to a given question, and applies top-k strategy in conjunction with relative threshold to retrieve informative contexts. The second stage uses a multimodal question answering model that generates answer based on the selected content. Both modules operate in a fully multimodal fashion, sharing the VLMT backbone and benefiting from its robust alignment and reasoning capabilities. \n\nExtensive experiments on the MultimodalQA and WebQA datasets demonstrate that the proposed framework achieves state-of-the-art performance, significantly outperforming existing methods in both retrieval and answer generation. These results validate the effectiveness of the proposed architecture and training strategy, establishing VLMT as a robust solution for complex multimodal reasoning tasks. The main contributions of this work are summarized as follows:",
            "score": 0.5292296072831109,
            "section_title": "body",
            "char_start_offset": 2274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1810
                },
                {
                    "start": 1813,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2210
                },
                {
                    "start": 2211,
                    "end": 2273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "267897956",
            "title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends",
            "text": "In recent epochs, the domain of large language models (LLMs) for natural language processing has witnessed a precipitous progression. Prototypes such as BERT, GPT-3, and PaLM have demonstrated exceptional aptitude in language apprehension and generation, courtesy of self-supervised pretraining on voluminous text corpora [85]. As LLMs expand in magnitude and range, incorporating additional modalities beyond text is a burgeoning field of study. Multimodal LLMs strive to amalgamate language, vision, and other sensory inputs within a singular model architecture. They hold the potential to attain a more holistic understanding of the world by concurrently learning representations across diverse data types [96]. A significant hurdle is the effective fusion of the strengths of CNNs for computer vision and transformer architectures for language modeling. \n\nOne strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks. Self-supervised pretraining continues to be vital for multimodal LLMs to learn effective joint representations before downstream task tuning. Contrastive learning objectives that predict associations between modalities have proven highly effective [99]. Models pre-trained on large datasets of image-text pairs have demonstrated robust zeroshot transfer performance on multimodal tasks. \n\nAs multimodal LLMs increase in scale, the efficient combination of diverse convolution types and attention mechanisms will be crucial. Compact CNN architectures could help to reduce the cost of computing. Sparse attention and memory compression techniques can assist with scalability.",
            "score": 0.5291483572215102,
            "section_title": "H. Evolution of Language Models and Multimodal LLMs",
            "char_start_offset": 60107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2114
                },
                {
                    "start": 2117,
                    "end": 2251
                },
                {
                    "start": 2252,
                    "end": 2321
                },
                {
                    "start": 2322,
                    "end": 2401
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "265466543",
            "title": "GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation",
            "text": "Multimodal Language Models Numerous studies have developed multimodal language models that can handle visual inputs and text outputs, or vice versa, such as [6,58]. With these advancements of LLMs, some researches have focused on learning a joint embedding space for multiple modalities, as demonstrated in [16,42]. Others have combined pre-trained single-modality models to showcase impressive zero-shot capabilities [1,26]. More recently, there has been a growing interest in enabling multi-modal LLMs to follow instructions, as shown in [12,59,69]. To facilitate research in this area, [57] introduced MultiInstruct, the first multi-modal instruction tuning benchmark dataset covering a wide range of tasks and categories. \n\nAdditionally, [33] explored multi-modal instruction-tuning using machine-generated data, while [36] fine-tuned all model parameters to allow the textual LLM to process four modalities. Large Language Models Large language models (LLMs) commonly refer to as Transformer-based language models with billions of parameters [50] and have revolutionized the research paradigm in natural language processing community [13,49]. Furthermore, recent works have demonstrated that supervised fine-tuning, also known as instruction-tuning, can effectively improve the zero-shot performance of these LLMs [11,48]. Zhao et al. [66] present a comprehensive survey on the research of LLMs. These prompts are then used to create videos from the Text-to-Video Model Gallery. Icons of a snowflake and a flame visually distinguish between the non-trainable and trainable parameters within the system. The two bullet points highlight GPT4Video's dual capabilities in understanding and generating video content. \n\nText-to-Image/Video Generation Text-to-image/video generation refers to the task of producing realistic images or videos based on natural language descriptions. One of the earliest approaches to this task was the use of conditional GANs [43]. Since then, various techniques have been developed to improve the quality of the generated images [39]. Compared to text-to-image generation, textto-video generation is relative new and still remains challenging.",
            "score": 0.5286527855795332,
            "section_title": "RELATED WORK",
            "char_start_offset": 5722,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "257663916"
                },
                {
                    "start": 160,
                    "end": 163,
                    "matchedPaperCorpusId": "249712016"
                },
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1139,
                    "end": 1143,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 1956,
                    "end": 1960,
                    "matchedPaperCorpusId": "1563370"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "273482179",
            "title": "ViConsFormer: Constituting Meaningful Phrases of Scene Texts using Transformer-based Method in Vietnamese Text-based Visual Question Answering",
            "text": "To evaluate the effectiveness of our proposed Vi-ConsFormer on the Vietnamese Text-based VQA dataset, we compared this method with the following baselines: \n\n\u2022 M4C (Hu et al., 2019): M4C is the first vision-language learning task that was constructed based on the Transformer architecture (Vaswani et al., 2017). Its multimodal backbone is BERT (Devlin et al., 2019). M4C approaches the text-based VQA task by sequentially generating tokens to form the answers. Tokens of answers can be obtained from the vocabulary or copied from the scene texts available in the images using the Pointer Network (Hu et al., 2019) module. \n\n\u2022 LaTr (Biten et al., 2021): This is the first method that integrated spatial information of scene texts into the multimodal backbone. \n\nThey encoded the coordinates of the bounding boxes into 4-dimensional vector space, then projected them directly to the latent space of the multimodal backbone and added them to the features of scene texts. Unlike M4C, LaTr proposed using T5 (Raffel et al., 2019) as its multimodal backbone and using a subword tokenizer to encode scene texts. Scene texts in the images that are not available in the vocabulary of the T5 pre-trained model are subsegmented into sequences of chunks. Hence, instead of copying scene texts from images via a particular module, it learns how to form out-of-vocabulary scene texts from respective subwords. \n\n\u2022 PreSTU (Kil et al., 2022): Instead of modeling the relation among scene texts via their spatial relations, PreSTU pre-trained the T5 backbone to approximate the distribution of the scene texts. The particular technique of PreSTU differs from other methods in that they sort the scene texts in left-right topbottom orders. \n\n\u2022 SaL (Fang et al., 2023): SaL proposed integrating the labels of objects and tokens of scene texts to their respective visual features. These labels and tokens are embedded by the embedding layer of the T5 backbone to yield the textual meaning of the objects and scene texts.",
            "score": 0.5282284538988167,
            "section_title": "Baselines",
            "char_start_offset": 18937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 181,
                    "matchedPaperCorpusId": "208006464"
                },
                {
                    "start": 289,
                    "end": 311,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 345,
                    "end": 366,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 597,
                    "end": 614,
                    "matchedPaperCorpusId": "208006464"
                },
                {
                    "start": 1004,
                    "end": 1025,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1408,
                    "end": 1426,
                    "matchedPaperCorpusId": "252211798"
                },
                {
                    "start": 1731,
                    "end": 1750,
                    "matchedPaperCorpusId": "261395684"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "276961544",
            "title": "OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer",
            "text": "The perception part of OVTR builds on Zhang et al. (2022a), incorporating visual-language modality fusion in both the encoder and decoder. To efficiently conduct multimodal interaction and learn generalization ability, the decoder adopts a dual-branch structure, consisting of the object feature alignment (OFA) branch and the category text interaction (CTI) branch. \n\nGenerating Image and Text Embeddings. To obtain the text modality input, we feed the text and prompts (Du et al., 2022) into the CLIP (Radford et al., 2021) text encoder to generate text embeddings. Simultaneously, we use ground truth boxes to generate image embeddings via the CLIP image encoder and combine embeddings of the same category into a single representation. \n\nUnlike the time-consuming preprocessing (Gu et al., 2021;Du et al., 2022) that generates numerous image embeddings from proposals, which are produced by an additional RPN-based detector and partially contain novel categories (unseen during training), our approach is simpler. We only need image embeddings for the closed-set categories. The embeddings are prepared offline. \n\nFeature Pre-fusion and Enhancement. In the encoder, inspired by multi-modal detectors such as GLIP (Li et al., 2022a) and Grounding DINO (Liu et al., 2023), we integrated image-to-text and textto-image cross-attention modules for feature fusion, which enhance image and text representations, preparing them for interaction in the decoder. Since the encoder outputs preliminary content features that may introduce misguidance for the decoder, we follow the approach of DINO-DETR (Zhang et al., 2022a) by generating the content parts of our queries through learnable initialization, while the position parts are derived from the reference points produced by E img , the outputs of the encoder, through sin-cos positional encoding. \n\nDual-Branch Structure.",
            "score": 0.5281673911881073,
            "section_title": "LEVERAGING ALIGNED QUERIES FOR SEARCH IN CROSS-ATTENTION",
            "char_start_offset": 10991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 471,
                    "end": 488,
                    "matchedPaperCorpusId": "247778949"
                },
                {
                    "start": 503,
                    "end": 525,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 799,
                    "end": 815,
                    "matchedPaperCorpusId": "247778949"
                },
                {
                    "start": 1217,
                    "end": 1235,
                    "matchedPaperCorpusId": "244920947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "270285703",
            "title": "Understanding Information Storage and Transfer in Multi-modal Large Language Models",
            "text": "Multimodal Large Language Models.. We consider a MLLM to be a model that takes an image and text as input, and generates a text output [2].Over the last year, such models have made tremendous advances in tasks like VQA and image captioning, including BLIP [15], BLIP-2 [16], Instruct-BLIP [7], LLaVA [18,19], Flamingo [3] and multi-modal Phi-2 (from the Bunny repo) [11].These MLLMs can broadly be categorized into two families based on how their visual information is integrated into the language model: (i) by embedding the vision encoder's output into each layer of the language model with a cross-attention layer (e.g., Flamingo, BLIP) or, (ii) by mapping the vision encoder's output into \"visual tokens\" in the language model's input space (i.e.alongside the text tokens) via a projection layer (e.g., LLaVA, Bunny).Both families are widely used, however, the projection layer family has recently shown stronger performance on popular benchmark [18,19,11].We, therefore, focus our study of information storage and transfer on this model family.",
            "score": 0.5274788155158483,
            "section_title": "Related Works",
            "char_start_offset": 4494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1049
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 304,
                    "end": 307,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64453125
        },
        {
            "corpus_id": "273507447",
            "title": "The Scene Language: Representing Scenes with Programs, Words, and Embeddings",
            "text": "Texts provide an easy user interface for scene generation as shown in Sec. 6.1, and images allow users to specify further intentions that could be hard to describe with natural language alone. Integrating neural embeddings increases the expressivity of the Scene Language and allows it to encode visual details from image prompts, and as a result, enhances our method's controllability in image-prompted scene generation and editing tasks. Generation. Fig. 9 shows results on the task of generating 3D scenes consistent with the structure and content from a user-provided image, termed as the \"inverse semantics\" task in GraphDreamer [7]. We report the LPIPS [41] score, i.e., the AlexNet [21] feature dissimilarity, between an input image and an output rendering, averaged over 3 scenes with 20 viewpoints each in Tab. 4. Embeddings allow our representation to encode visual details from inputs, such as the Coke can texture in Fig. 9, which is not captured in scene graphs from GraphDreamer using per-object natural language descriptions. This is reflected quantitatively in Tab. 4. Editing. Our representation applies to image-prompted editing tasks such as style transfer (Fig. 1). Edits can target partial or full scenes as shown in Fig. 8(a) and (b), respectively, by updating embeddings for corresponding entities.",
            "score": 0.5274344207977009,
            "section_title": "Image-Prompted Generation and Editing",
            "char_start_offset": 24412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1321
                }
            ],
            "ref_mentions": [
                {
                    "start": 634,
                    "end": 637,
                    "matchedPaperCorpusId": "265551966"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "4766599"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24658203125
        },
        {
            "corpus_id": "248239727",
            "title": "VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance",
            "text": "Generating and editing images from open domain text prompts is a challenging task that heretofore has required expensive and specially trained models. We demonstrate a novel methodology for both tasks which is capable of producing images of high visual quality from text prompts of significant semantic complexity without any training by using a multimodal encoder to guide image generations. We demonstrate on a variety of tasks how using CLIP [37] to guide VQGAN [11] produces higher visual quality outputs than prior, less flexible approaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being trained for the tasks presented. Our code is available in a public repository.",
            "score": 0.5273698569343396,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5693359375
        },
        {
            "corpus_id": "253581213",
            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
            "text": "We combine the abilities of two large-scale pretrained models that operate on different modalities-a large language model [7] and a text-to-image model [52]-to generate a multi-modal training dataset containing text editing instructions and the corresponding images before and after the edit. In the following two sections, we describe in detail the two steps of this process. In Section 3.1.1, we describe the process of fine-tuning GPT-3 [7] to generate a collection of text edits: given a prompt describing an image, produce a text instruction describing a change to be made and a prompt describing the image after that change (Figure 2a). Then, in Section 3.1.2, we describe the process of converting the two text prompts (i.e., before and after the edit) into a pair of corresponding images using a text-toimage model [52] (Figure 2b).",
            "score": 0.527088268116562,
            "section_title": "Generating a Multi-modal Training Dataset",
            "char_start_offset": 6796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 840
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 156,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 823,
                    "end": 827,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "260886966",
            "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models",
            "text": "Large text-to-image models are mainly divided into two categories: autoregressive models and diffusion models. \n\nEarly works, such as DALLE [13], CogView [14,15] and Make-A-Scene [16], are autoregressive models. For the autoregressive model, an image tokenizer like VQ-VAE [17] is used to convert an image to tokens, then an autoregressive transformer [18] conditioned on text tokens is trained to predict image tokens. However, autoregressive models often require large parameters and computing resources to generate high-quality images, as seen in Parti [19]. \n\nRecently, diffusion models (DMs) [20,21,22,23] [24], a large transformer language model pretrained on text-only data, as the text encoder of diffusion model. Re-Imagen [25] uses retrieved information to improve the fidelity of generated images for rare or unseen entities. SD is built on the latent diffusion model [4], which operates on the latent space instead of pixel space, enabling SD to generate high-resolution images with only a diffusion model. To improve text alignment, eDiff-I was designed with an ensemble of text-to-image diffusion models, utilizing multiple conditions, including T5 text, CLIP text, and CLIP image embeddings. Versatile Diffusion [26] presents a unified multi-flow diffusion framework to support text-to-image, image-to-text, and variations within a single model. To achieve controllable image synthesis, Composer [27] presents a joint fine-tuning strategy with various conditions on a pretrained diffusion model conditioned on image embedding. RAPHAEL introduces a mixture-of-experts (MoEs) strategy [28,29] into the text-conditional image diffusion model to enhance image quality and aesthetic appeal. \n\nAn attractive feature of DALL-E 2 is that it can also use image prompt to generate image variations. Hence, there are also some works to explore to support image prompt for the text-to-image diffusion models conditioned only on text.",
            "score": 0.5270477086566185,
            "section_title": "Text-to-Image Diffusion Models",
            "char_start_offset": 6864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 113,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "248476190"
                },
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 597,
                    "end": 601,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 607,
                    "end": 610,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 611,
                    "end": 615,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 879,
                    "end": 882,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1602,
                    "end": 1605,
                    "matchedPaperCorpusId": "231573431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44921875
        },
        {
            "corpus_id": "258865230",
            "title": "Visual Programming for Text-to-Image Generation and Evaluation",
            "text": "Text-to-image generation models. In the T2I generation task, models generate images from text. Early deep learning models used the Generative Adversarial Networks (GAN) [38] framework for this task [39; 40; 41; 31]. More recently, multimodal language models [42; 22] and diffusion models [43; 44; 21; 45] have gained popularity. Recent advances in multimodal language models such as Parti [46] and MUSE [47], and diffusion models like Stable Diffusion [21], UnCLIP [23], and Imagen [24], have demonstrated a high level of photorealism in zero-shot image generation. \n\nBridging text-to-image generation with layouts. One line of research decomposes the T2I generation task into two stages: text-to-layout generation and layout-to-image generation [25; 26; 27; 28]. However, the previous approaches focus on a set of predefined object classes by training a new layout predictor module from scratch and therefore cannot place new objects unseen during training. In contrast, our VPGEN uses an LM to handle layout generation by generating objects/counts/positions in text, allowing flexible adaptation of pretrained LMs that can understand diverse region descriptions. \n\nLanguage models with visual modules. Although large language models (LLMs) have shown a broad range of commonsense knowledge, most of them are trained only on text corpus and cannot understand image inputs to tackle vision-and-language (VL) tasks. Thus, recent works explore tackling VL tasks by solving sub-tasks with external visual modules and combining their outputs to obtain the final response [9; 10; 11; 12; 13; 14; 15]. The visual sub-tasks include describing images as text, finding image regions relevant to the text, editing images with text guidance, and obtaining answers from a VQA model. However, existing work focuses on converting visual inputs into text format so that LLMs can understand them. Our work is the first work using visual programming for interpretable and explainable T2I generation and evaluation. \n\nEvaluation of text-to-image generation models.",
            "score": 0.5270159142303799,
            "section_title": "Related Works",
            "char_start_offset": 6589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1997
                },
                {
                    "start": 2000,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 389,
                    "end": 393,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "268363925",
            "title": "Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller",
            "text": "Do you want to read a vivid story book begin with your predetermined beginning?How about changing the ending of the comic to another possible plot (bad ending to happy ending)?Rendering such application scenes is a general storytelling task that generates cohesive narratives represented as language and image, conditioned on a few images.The storytelling task is challenging for it requires tying disparate moments and synthesizing a coherent multimodal story across time so that they seamlessly blend into a complete story.Previous storytelling methods [Park and Kim, 2015;Huang, Ting-Hao K. and Ferraro, Francis et al., 2016;Xu et al., 2021] employ the vision-to-text model to generate stories based on semantic concepts of visual features and event correlations among images.Despite elaborate designs of these models for alignment in either the whole story or specific objects, the result of the storytelling task is frustrating due to poor narrative and diversity.\n\nSuch poor storytelling results can be attributed to two aspects.First, they record events in multiple images by brief descriptions based on restricted visual elements, disregarding rich interactive information between elements and the expended imagination.Current multi-modal LLM, such as LLaVa [Liu et al., 2023], consists of an image encoder with strong image understanding ability and an LLM decoder with powerful text description ability that can describe visual objects with few oversights.However, due to the lack of multiimage understanding ability, LLaVa struggles with generating sequential consistent stories.Secondly, current single-modal story generation pipelines limit the diverse expression of storytelling.MiniGPT-5 [Zheng et al., 2023] can generate results in both text and pictures but produces poor expressiveness and consistency.In order to immerse ourselves in a story, we usually prefer interesting plots, supplemented by consistent visual imagery.\n\nFirstly, the storytelling ability of existing models is restricted by current low-quality training data.However, urgently required high-quality data previously relied heavily on extensive manual annotation, which is difficult for long story annotation.To this end, we propose a new pipeline, called Large Language model assisted Multimodal Storyteller (LLaMS).In the pipeline, we propose a unique sequential data quality enhancement strategy to rewrite the story automatically.",
            "score": 0.5266977697971971,
            "section_title": "Introduction",
            "char_start_offset": 2480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 79,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 969
                },
                {
                    "start": 971,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1466
                },
                {
                    "start": 1466,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1820
                },
                {
                    "start": 1820,
                    "end": 1941
                },
                {
                    "start": 1943,
                    "end": 2047
                },
                {
                    "start": 2047,
                    "end": 2195
                },
                {
                    "start": 2195,
                    "end": 2303
                },
                {
                    "start": 2303,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 575,
                    "matchedPaperCorpusId": "2838785"
                },
                {
                    "start": 628,
                    "end": 644,
                    "matchedPaperCorpusId": "235306396"
                },
                {
                    "start": 1266,
                    "end": 1284,
                    "matchedPaperCorpusId": "2838785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3642578125
        },
        {
            "corpus_id": "272911245",
            "title": "MIO: A Foundation Model on Multimodal Tokens",
            "text": "In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.",
            "score": 0.52601233650111,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59228515625
        },
        {
            "corpus_id": "268201520",
            "title": "HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation",
            "text": "Text-to-Shape synthesis [5,8,27,32,37] involves the task of generating high-quality and faithful shapes given a text prompt and holds significant promise for a wide range of applications including augmented/virtual reality and design, offering the potential for automated, diverse, and cost- effective 3D content. Unlike image-based media [1,19,61], natural language provides a more direct means of expression. However, effectively marrying the realms of 3D geometry and natural language is challenging, leading to no established standard for text-guided 3D shape generation. We argue that hierarchy, preordering elements of a set in increasing complexity is fundamental to linking geometry and language as illustrated in Fig. 1. 3D shapes inherently exhibit compositionality [34,45], possessing hierarchical part-to-whole relationships [33]. On the other hand, language exhibits a hierarchical tree-like syntactic structure [9,13,58], rooted in inter-word relationships. Recognizing these parallel hierarchical natures requires rethinking text-to-shape correspondence also within a similar hierarchical framework. For example, a general prompt like \"a chair\" can correspond to thousands of 3D shapes. In contrast, a more detailed description like \"a wooden chair with armrests and four legs\" narrows down the possible shapes to those with specific attributes. Fully embracing and leveraging such hierarchical nuances can significantly improve the fidelity and specificity of generated shapes, making strides in the field of text-to-shape synthesis. \n\nExisting text-to-shape methods can be divided into two categories depending on the data type they handle: one for paired text-shape data [5,8,15,26,29,32,48] and the other for unpaired data [22,27,37,40,42,43,51,53]. Some methods [22,27,37,40,51,53] for unpaired data generate intermediary images, and then transform these images into 3D shapes by 3D generative models, like NeRFs [27,53]. Others [42,43] leverage a shared text-image embedding space, generating 3D shapes by an image-3D generator. These methods do not directly learn the text-shape representation.",
            "score": 0.5256151212843048,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1549
                },
                {
                    "start": 1552,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 27,
                    "matchedPaperCorpusId": "4707877"
                },
                {
                    "start": 27,
                    "end": 29,
                    "matchedPaperCorpusId": "254408516"
                },
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "253708074"
                },
                {
                    "start": 32,
                    "end": 35,
                    "matchedPaperCorpusId": "247518895"
                },
                {
                    "start": 339,
                    "end": 342,
                    "matchedPaperCorpusId": "253708307"
                },
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "257805232"
                },
                {
                    "start": 345,
                    "end": 348,
                    "matchedPaperCorpusId": "254125457"
                },
                {
                    "start": 776,
                    "end": 780,
                    "matchedPaperCorpusId": "244714384"
                },
                {
                    "start": 837,
                    "end": 841,
                    "matchedPaperCorpusId": "252407422"
                },
                {
                    "start": 928,
                    "end": 931,
                    "matchedPaperCorpusId": "6380915"
                },
                {
                    "start": 931,
                    "end": 934,
                    "matchedPaperCorpusId": "174802532"
                },
                {
                    "start": 1689,
                    "end": 1692,
                    "matchedPaperCorpusId": "4707877"
                },
                {
                    "start": 1692,
                    "end": 1694,
                    "matchedPaperCorpusId": "254408516"
                },
                {
                    "start": 1694,
                    "end": 1697,
                    "matchedPaperCorpusId": "250643915"
                },
                {
                    "start": 1697,
                    "end": 1700,
                    "matchedPaperCorpusId": "254366593"
                },
                {
                    "start": 1700,
                    "end": 1703,
                    "matchedPaperCorpusId": "247761914"
                },
                {
                    "start": 1703,
                    "end": 1706,
                    "matchedPaperCorpusId": "247518895"
                },
                {
                    "start": 1706,
                    "end": 1709,
                    "matchedPaperCorpusId": "267026771"
                },
                {
                    "start": 1742,
                    "end": 1746,
                    "matchedPaperCorpusId": "257921697"
                },
                {
                    "start": 1746,
                    "end": 1749,
                    "matchedPaperCorpusId": "253708074"
                },
                {
                    "start": 1752,
                    "end": 1755,
                    "matchedPaperCorpusId": "257687714"
                },
                {
                    "start": 1755,
                    "end": 1758,
                    "matchedPaperCorpusId": "238408362"
                },
                {
                    "start": 1758,
                    "end": 1761,
                    "matchedPaperCorpusId": "258170536"
                },
                {
                    "start": 1761,
                    "end": 1764,
                    "matchedPaperCorpusId": "254125253"
                },
                {
                    "start": 1764,
                    "end": 1767,
                    "matchedPaperCorpusId": "255340806"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "257921697"
                },
                {
                    "start": 1786,
                    "end": 1789,
                    "matchedPaperCorpusId": "253708074"
                },
                {
                    "start": 1792,
                    "end": 1795,
                    "matchedPaperCorpusId": "257687714"
                },
                {
                    "start": 1795,
                    "end": 1798,
                    "matchedPaperCorpusId": "254125253"
                },
                {
                    "start": 1798,
                    "end": 1801,
                    "matchedPaperCorpusId": "255340806"
                },
                {
                    "start": 1933,
                    "end": 1937,
                    "matchedPaperCorpusId": "253708074"
                },
                {
                    "start": 1937,
                    "end": 1940,
                    "matchedPaperCorpusId": "255340806"
                },
                {
                    "start": 1949,
                    "end": 1953,
                    "matchedPaperCorpusId": "238408362"
                },
                {
                    "start": 1953,
                    "end": 1956,
                    "matchedPaperCorpusId": "258170536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360595703125
        },
        {
            "corpus_id": "271946807",
            "title": "Foundational Model for Electron Micrograph Analysis: Instruction-Tuning Small-Scale Language-and-Vision Assistant for Enterprise Adoption",
            "text": "Our novel encoder-decoder multimodal framework efficiently processes and aligns images and text to generate textual responses to questions across image captioning and open-ended VQA tasks. Key components of MAEMI for the zero-shot image captioning task are illustrated in Figure 2. The multimodal model, MAEMI, integrates visual processing and language modeling for answering questions about specific image features. It includes: (a) The vision encoder, using a vision transformer (Dosovitskiy et al., 2020), analyzes the microscopic images by splitting them into patches and using self-attention mechanism to capture beyond pair-wise patch relationships. This allows for understanding the global context and highlighting relevant visual regions and relationships. A <cls> token attends to and aggregates information from all patches, resulting in a higher-level visual semantic representation to capture the overall context or summary of the input image. (b)The text encoder, crucial for analyzing end-user questions, takes as input an interleaved multimodal prompt. We insert <image> token in the prompt at the image location and append an <Encode> token to facilitate multimodal integration, with its output embedding symbolizing the fused representation. The text encoder leverages instruction-tuned Llama-2-7b, a pretrained language model, to capture language nuances and context. The language-only model is customized using parameter-efficient fine tuning technique, enhancing its ability to interpret end-user questions. Both the vision and language-only unimodal encoders synergize to interpret end-user questions and analyze visual input for generating answers consistent with the visual context. (c) It utilizes a multi-layered structure with multiple blocks, alternating between self-attention and gated cross-attention blocks. This design facilitates complex interactions between visual and textual modalities. By extracting and refining information from both modalities at each level, the framework progressively builds a comprehensive understanding, enabling coherent and contextually relevant answers to the end-user questions. Gated cross-attention blocks integrate visual features with textual features.",
            "score": 0.5254024349583781,
            "section_title": "Introduction",
            "char_start_offset": 8846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2220
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39599609375
        },
        {
            "corpus_id": "256459691",
            "title": "Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications",
            "text": "BLIP-2 [98], an improved visual-language model based on comprehensive pretraining strategy, achieves state-of-the-art performance across numerous vision-language tasks. BLIP-2 exhibits a wide range of zero-shot image-to-text abilities, which include visual knowledge reasoning, visual common sense reasoning, visual conversation, and personalized image-to-text generation. \n\nLike its predecessor, BLIP-2 uses the same evaluation datasets and continues to implement the Captioning and Filtering (CapFilt) method. The UNITER [82] is a pretraining approach conducted at a large scale over four benchmark datasets. Joint embedding from both modalities is learned to perform heterogeneous downstream tasks. MLM, MRM (Masked Region Modeling), ITM (Image-Text Matching), and WRA (Word-Region Alignment) are employed as a pretraining task. Additionally, the pretraining task achieves global image text alignment using Conditional masking. Optimal Transport is the author's second concept for the WRA task to improve the alignment between images and words. Hu et al. [108] proposed a unified transformer for multitasking based on multimodal learning at a time. The proposed architecture uses a specialized encoder for each modality and a shared decoder for every task. DETR [87] is used for visual features encoding and BERT [35] performs the textual feature encoding. Contrastive learning is employed on multimodal data by Akbari et al. [109] to train a transformer encoder that processes audio, text, and video simultaneously. \n\nWang et al. [110] proposed the One For All (OFA) method, which unifies tasks and modalities via a sequence-tosequence framework based on Unified vocabulary (for all kinds of the modality). OFA represents data of different modalities in a unified space that discretizes images and text to form a unified output vocabulary. They presented three qualities that a unifying model should support to maintain multitasking for any modality: (1) Task-Agnostic: handcraft instruction-based learning is utilized to achieve this property.",
            "score": 0.5244769466266627,
            "section_title": "Transformer-Based Architectures. Recent work on large scale pretraining has inspired the research community to",
            "char_start_offset": 24156,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 523,
                    "end": 527,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1058,
                    "end": 1063,
                    "matchedPaperCorpusId": "237204499"
                },
                {
                    "start": 1265,
                    "end": 1269,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 1316,
                    "end": 1320,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1429,
                    "end": 1434,
                    "matchedPaperCorpusId": "233346984"
                },
                {
                    "start": 1534,
                    "end": 1539,
                    "matchedPaperCorpusId": "246634906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.771484375
        },
        {
            "corpus_id": "258170444",
            "title": "AutoSplice: A Text-prompt Manipulated Image Dataset for Media Forensics",
            "text": "Recent advancements in attention-based transformer and diffusion models have significantly improved text-to-image generation in the past two years. Several large-scale language-image models have been developed. The DALL-E model [54], proposed by OpenAI in 2021, uses an autoregressive transformer to achieve high-quality image generation on the MS-COCO dataset [40] without using any training labels. Other models, such as CogView [17], Parti [70], and Make-A-Scene [22], have also trained autoregressive transformer models on text and image tokens for text-toimage generation. In 2022, an updated version of DALL-E, DALL-E2 [53], was developed using a diffusion model with CLIP image embeddings, making it computationally more efficient and able to produce higher-quality and more diverse samples. Other models, such as GLIDE [50], Stable-Diffusion [55], and Imagen [57], have also used diffusion models to improve text-to-image synthesis. Inspired by these powerful LLI synthesis models, several studies have developed text-guided image editing models, including DiffEdit [10], Prompt-to-prompt [25], Null-text Inversion [47], Imagic [34], and Muse [5]. These models apply local semantic editing to an image given a text input (with the desired edit) and an optional scene layout (segmentation map). However, their optimization tends to maximize the similarity to the original image while maintaining the ability to perform meaningful editing on local regions. This kind of entire synthesis can be easily identified if seen in training data [9]. To create more challenging fake media, we utilize the DALL-E2 model with high-quality local image editing techniques, which can generate text-guided pixels only in erased image regions.",
            "score": 0.5243859668444079,
            "section_title": "LLI Synthesis Models",
            "char_start_offset": 4295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1733
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 365,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 431,
                    "end": 435,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 466,
                    "end": 470,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 850,
                    "end": 854,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6162109375
        },
        {
            "corpus_id": "270391793",
            "title": "VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks",
            "text": "Multimodal large language models (MLLMs) [8,97,223,107,105,140,14,169,34,33] have recently made significant progress, demonstrating outstanding performance across various visionlanguage tasks, even in scenarios requiring complex understanding and reasoning. However, a notable limitation is that current MLLM outputs are in text form, which significantly constrains their capacity to represent structured or visual information. Some researchers [140,182,181,180] have expanded the text-based output formats of MLLMs to better align with downstream tasks. While these efforts have shown promise, they have not fully addressed practical needs such as dense object detection, pose estimation, and image generation. \n\nTo overcome this limitation, a line of research [116,187,166,111,117,47] enhances the capabilities of MLLMs by transmitting task information to tools via text messages, as illustrated in Figure 1(a). Despite these advances, these text-based methods are restricted by the information that text can convey. They are not end-to-end, and the feedback gradient from the tools cannot be relayed back to the MLLM. This limitation has spurred another research direction [89,148,193,44,83,164]  The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a \"super link\" technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks. \n\nthat employs learnable embeddings as intermediaries to connect MLLM with one specific task decoder (see Figure 1(b)). However, the naive embedding connection is difficult to scale to multi-task scenarios. A routing mechanism is needed to ensure the correct selection of tools, and the issue of task conflicts [224] arising from joint multi-task training is also a problem that needs to be considered. Therefore, developing an end-to-end MLLM generalist for various vision and vision-language tasks beyond text output remains a significant challenge. \n\nGiven these challenges, developing an end-to-end generalist MLLM requires a more effective information transmission method than conventional text messages and naive embeddings.",
            "score": 0.5241797282234775,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 47,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 47,
                    "end": 51,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 51,
                    "end": 55,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 450,
                    "end": 454,
                    "matchedPaperCorpusId": "258762579"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "260438589"
                },
                {
                    "start": 771,
                    "end": 775,
                    "matchedPaperCorpusId": "257505358"
                },
                {
                    "start": 1188,
                    "end": 1191,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 1191,
                    "end": 1194,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 1194,
                    "end": 1198,
                    "matchedPaperCorpusId": "259765944"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "254535649",
            "title": "Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis",
            "text": "Text-to-Image Synthesis (T2I) is to generate natural and faithful images given a text prompt as input. Recently, there has been a significant advancement in the quality of generated images by extremely large-scale vision-language models, such as DALL-E 2 (Ramesh et al., 2022), Imagen (Saharia et al., 2022), and Parti (Yu et al., 2022). In particular, Stable Diffusion (Rombach et al., 2022) is the state-of-the-art open-source implementation showing superior evaluation metric gains after training over billions of text-image pairs. \n\nIn addition to generating high-fidelity images, the ability to compose multiple objects into a coherent scene is also essential. Given a text prompt from the user end, T2I models need to generate an image that contains all necessary visual concepts as mentioned in the text. Achieving such ability requires the model to understand both the full prompt and individual linguistic concepts from the prompt. As a result, the model should be able to combine multiple concepts and generate novel objects that have never been included in the training data. In this work, we mainly focus on improving the compositionality of the generation process, as it is essential to achieve controllable and generalized text-to-image synthesis with multiple objects in a complex scene. \n\nAttribute binding is a critical compositionality challenge (Ramesh et al., 2022;Saharia et al., 2022) to existing large-scale diffusion-based models. Despite the improvements in generating multiple objects in the same scene, existing models still fail when given a prompt such as \"a brown bench in front of a white building\" (see Fig. 1). The output images contains \"a white bench\" and \"a brown building\" instead, potentially due to strong training set bias or imprecise language understanding. From a practical perspective, explaining and solving such a two-object binding challenge is a primary step to understanding more complex prompts with multiple objects. Therefore, how to bind the attributes A red car and a white sheep. \n\nA blue backpack and a brown elephant A brown bench sits in front of an old white building",
            "score": 0.5236942470733741,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1302
                },
                {
                    "start": 1305,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2034
                },
                {
                    "start": 2037,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 392,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "239011704",
            "title": "Unifying Multimodal Transformer for Bi-directional Image and Text Generation",
            "text": "Our model mainly consists of a unified multimodal Transformer [45], which has a multi-layer architecture and each layer mainly consists of multi-head self-attention and position-wise fully connected feedforward networks. We adopt LXMERT [43] as our Transformerbased architecture following X-LXMERT [6] for a direct comparison, since our proposal is mainly based on feature representation and training mechanism but not on the specific model. LXMERT is a cross-modality Transformer consisting of an object-relation encoder, a language encoder and a cross-modality encoder. We omit an exhaustive background description of the model architecture and refer readers to [6,43,45] for additional details.\n\nTo enable both image-to-text and text-to-image generation tasks, we formulate both tasks as sequence generation tasks. Specifically, we firstly pre-process image-and-text pairs into sequences of image tokens and text tokens. Then the Transformer accepts sequences with masked tokens as input, and maps the input to a contextualized representation via attention networks. Lastly, an image or text linear transformation classifier projects the contextualized representation to predicted target tokens. In our model, we share the parameters of most Transformer modules, except for the last classifiers since image and text vocabularies have different sizes and semantics. For text-to-image generation, we use an additional GAN-based image generator to convert the size of 8 \u00d7 8 image token predictions to a 256 \u00d7 256 resolution visible image.\n\nText representation is a combination of the position embedding and the word embedding, where the position refers to the index of the word within the caption, and the word embedding is initialized from pre-trained models like BERT [8] or LXMERT [43].",
            "score": 0.5235133754947741,
            "section_title": "Unified Multimodal Framework",
            "char_start_offset": 9005,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 237,
                    "end": 241,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 667,
                    "end": 670,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 670,
                    "end": 673,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1770,
                    "end": 1773,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "201103729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "265019039",
            "title": "LLM-driven multimodal target volume contouring in radiation oncology",
            "text": "For the image encoder/decoder and the large language model (LLM), we employed the 3D Residual U-Net 39 and the pre-trained Llama2-7B-chat 10 model, respectively. For the interactive alignment modules, we utilized the two-way transformer modules of SAM 16 . We further propose detailed multimodal AI framework as illustrated in Supplementary Fig. 5. We introduce three key components: (a) text prompt tuning, (b) multimodal interactive alignment, and (c) CTV delineation. \n\n(a) Text Prompt Tuning To efficiently fine-tune the large language model (LLM), we introduce N -text prompts V = {v n | N n=1 } as illustrated in Supplementary Fig. 5(a), where each v n \u2208 R M \u00d7D consists of M vectors with the dimension D, which is same embedding dimension as the LLM. \n\nThese learnable vectors are randomly initialized, and then consistently prepended to each of tokenized clinical data, which denoted as [TEXT] tokens. We additionally append a token, denoted as [SEG], which is intended to attend to all the aforementioned vectors and tokens. Here, the final prompted text input t can be formulated as follows: \n\nThen, using the prompted text input t, the frozen LLM results the context embeddings g \u2208 R N \u00d7D as output embeddings as for the inputted [SEG] token.",
            "score": 0.5234575159736978,
            "section_title": "Details of Datasets.",
            "char_start_offset": 34376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 102,
                    "matchedPaperCorpusId": "2164893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423095703125
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "One major direction in unified multimodal understanding and generation models adopts autoregressive (AR) architectures, where both vision and language tokens are typically serialized and modeled sequentially. In these models, a backbone Transformer, typically adapted from large language models (LLMs) such as LLaMA family [1], [2], [133], Vicuna [58], Gemma series [134], [135], [136], and Qwen series [5], [6], [9], [10], serves as the unified modality-fusion module to autoregressively predict multimodal outputs. \n\nTo integrate visual information into the AR framework, as shown in Fig. 5, existing methods propose different strategies for image tokenization during modality encoding. These approaches can be broadly categorized into four types: pixel-based, semantic-based, learnable query-based, hybrid-based encoding methods. 1) Pixel-based Encoding. As shown in Fig. 5 (b-1), pixelbased encoding typically refers to the representation of images as continuous or discrete tokens obtained from pretrained autoencoders supervised purely by image reconstruction, such as VQGAN-like models [32], [137], [138], [139]. These encoders compress the high-dimensional pixel space into a compact latent space, where each spatial patch corresponds to an image token. In unified multimodal autoregressive models, image tokens serialized from such encoders are processed analogously to text tokens, allowing both modalities to be modeled within a single sequence. \n\nRecent works have adopted and enhanced pixel-based tokenization with various encoder designs. LWM [29] employs a VQGAN tokenizer [32] to encode images into discrete latent codes without requiring semantic supervision. It proposes a multimodal world modeling framework, wherein visual and textual tokens are serialized together for unified autoregressive modeling. By learning world dynamics purely through reconstruction-based visual tokens and textual descriptions, LWM demonstrates that largescale multimodal generation is feasible without specialized semantic tokenization.",
            "score": 0.5234393517505439,
            "section_title": "Auto-Regressive Models",
            "char_start_offset": 27036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 1093,
                    "end": 1097,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 1099,
                    "end": 1104,
                    "matchedPaperCorpusId": "252367709"
                },
                {
                    "start": 1106,
                    "end": 1111,
                    "matchedPaperCorpusId": "263671912"
                },
                {
                    "start": 1113,
                    "end": 1118,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 1588,
                    "end": 1592,
                    "matchedPaperCorpusId": "229297973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "273969615",
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "text": "Multimodal RAG technology is an extension of the traditional RAG model, designed to enhance the performance of generative tasks by incorporating information from multiple data modalities [118]. Unlike the traditional RAG, which processes only textual data, multimodal RAG can handle not only text but also other modalities such as images, audio, and video. It is capable of extracting information from various modalities and integrating this information to generate richer and more accurate outputs. In multimodal RAG systems, embeddings for various data types, such as text and images, are generated through modality-specific encoders [119]. These encoders share a unified embedding space, which is also employed for encoding the query. \n\nThe latest advancements in RAG in the image domain have led to significant improvements [120]. RA-CM3 [121] enhances both text-to-image and image-to-text generation by combining the CLIP retriever and the CM3 Transformer generator, achieving a performance boost while reducing computational costs by over 30%. Mortaheb et al. introduced a reranking mechanism based on a relevance score model [122], improving context selection during retrieval and reducing hallucinations, thereby enhancing the quality of generated responses. Yu et al.'s VisRAG [123] framework bypasses the text parsing stage to directly process multi-modal documents containing both text and images, achieving substantial improvements in multi-modal tasks. Bonomo and Bianco's Visual RAG [124] expands the visual knowledge of large MLLMs without the need for fine-tuning by dynamically retrieving relevant examples, offering high computational efficiency. Riedler and Langer's work on multimodal inputs for industrial applications demonstrates that integrating both images and text in RAG systems significantly improves performance [125]. \n\nRAG has also made significant progress in the video domain, driving innovations in long video comprehension. Yongdong Luo et al. propose Video-RAG [126], which enhances long video understanding by integrating visually-aligned auxiliary texts into large video-language models, surpassing models like Gemini1.5-Pro and GPT-4o.",
            "score": 0.523344515976994,
            "section_title": "D. Multimodal RAG",
            "char_start_offset": 70659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78125
        },
        {
            "corpus_id": "271218686",
            "title": "How Control Information Influences Multilingual Text Image Generation and Editing?",
            "text": "We propose a novel diffusion-based network for multilingual text generation and editing, which demonstrates robust performance.Our network excels at generating high-quality scene images with text.However, the framework based on latent diffusion presents certain limitations.1) The VAE employed in latent diffusion restricts the performance of fine-grained texture generation, particularly for complex texts.Because the diffusion process operates in the latent feature space, the VAE decoder struggles to generate small or complex texts.Consequently, our method is unable to generate such images.This issue can be addressed by generating each local sub-region separately.\n\n2) The text condition controls the content of the generated image.However, the CLIP text encoder has limited ability to comprehend text, resulting in performance limitations.To resolve this issue, we can pre-train the diffusion model with a large language model serving as the text condition encoder.",
            "score": 0.5228055050408309,
            "section_title": "Limitations",
            "char_start_offset": 22562,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 196
                },
                {
                    "start": 196,
                    "end": 274
                },
                {
                    "start": 274,
                    "end": 407
                },
                {
                    "start": 407,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 595
                },
                {
                    "start": 595,
                    "end": 670
                },
                {
                    "start": 672,
                    "end": 738
                },
                {
                    "start": 738,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327880859375
        },
        {
            "corpus_id": "266210376",
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "text": "In this study, we introduce VL-GPT, a large visionlanguage generative pre-trained transformer that enables the unified training of both visual and linguistic data using an auto-regressive objective, as depicted in Fig. 1. To achieve this, we propose an image tokenizer-detokenizer framework for the conversion between raw image pixels and continuous visual embeddings, analogous to the role of the text tokenization [19,43] in language models. The framework comprises an image tokenizer and an image detokenizer, where the tokenizer encodes raw images into a sequence of continuous visual embeddings, and the detokenizer decodes the continuous embeddings into pixel space. To obtain visual continuous embeddings that are rich in both image details and semantic information, we employ the image embeddings and their corresponding caption embeddings extracted by pre-trained encoders (i.e., CLIP [32]) as the supervision for training of the framework. Furthermore, the efficiency of the framework training is enhanced through weight initialization from pre-trained image encoders and high-quality image diffusion models. \n\nBy employing the image tokenizer-detokenizer framework, visual embeddings can achieve consistency on both the input and output sides of the transformer model. Consequently, interleaved image-text data can be trained in a unified auto-regressive manner. Specifically, the image tokenizer and the existing text tokenizer (i.e., BPE tokenizer [43]) first convert the image and text into a multimodal sequence consisting of interleaved continuous visual embeddings and discrete text tokens. The transformer can then be trained to predict the next embedding or token in this multimodal sequence, employing mean squared error (MSE) loss for continuous visual embeddings and crossentropy loss for discrete text tokens. Contrary to previous works [9,18,30,48], all embeddings in the multimodal sequence can receive supervision from the auto-regressive loss.",
            "score": 0.5226262537663925,
            "section_title": "Introduction",
            "char_start_offset": 1796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 420,
                    "end": 423,
                    "matchedPaperCorpusId": "202539075"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "matchedPaperCorpusId": "202539075"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7294921875
        },
        {
            "corpus_id": "239011704",
            "title": "Unifying Multimodal Transformer for Bi-directional Image and Text Generation",
            "text": "We study the joint learning of image-to-text and text-to-image generations, which are naturally bi-directional tasks. Typical existing works design two separate task-specific models for each task, which impose expensive design efforts. In this work, we propose a unified image-and-text generative framework based on a single multimodal model to jointly study the bi-directional tasks. We adopt Transformer as our unified architecture for its strong performance and task-agnostic design. Specifically, we formulate both tasks as sequence generation tasks, where we represent images and text as unified sequences of tokens, and the Transformer learns multimodal interactions to generate sequences. We further propose two-level granularity feature representations and sequence-level training to improve the Transformer-based unified framework. Experiments show that our approach significantly improves previous Transformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for text-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for fine-tuned image-to-text generation on the MS-COCO dataset. Our code is available online.",
            "score": 0.5223071190491124,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "249461933",
            "title": "Decentralized, not Dehumanized in the Metaverse: Bringing Utility to NFTs through Multimodal Interaction",
            "text": "Many multimodal used for generative art creation [32] and artistic analyses [15] and text-and-image approaches combining AI process have appeared in many artistic cases. DALL-E3 , one of those tools, aims to generate images based on text descriptions (See Fig. 1). In contrast, CLIP4 (contrastive language image pre-training) aims to associate text and images more robustly than current AI models. OpenAI builds both. The essence of this type of multimodal framework for text-to-image synthesis is pre-train the textual information on a large-scale image on the Internet. It corresponds to the image's content expressed in response and then outputs an image result that fuses with the textual information and the original image. The user can change some text input parameters to see how the output is affected, such as the number of iterations and the adjectives of various attributes. \n\nThe usages of these sophisticated models provide us with unprecedented analysis and creativity, especially in artistic creation and interaction, and many studies have demonstrated their potential [33]. The Portrait of Edmund Bellamy created by AI already sells at high prices [36,37], and generated artwork by multimodal may have more commercial value because the input has multiple datasets having better interaction with users. It also indirectly reveals its artistic value. Therefore, integrating DALL\u2022E, an artificial intelligence generation method with a text-to-image process, as an input method can enable these interactive co-creation methods of NFTs.",
            "score": 0.5218662505702233,
            "section_title": "Multimodality in Arts",
            "char_start_offset": 5779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1547
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "237513697"
                },
                {
                    "start": 1164,
                    "end": 1168,
                    "matchedPaperCorpusId": "215817586"
                },
                {
                    "start": 1168,
                    "end": 1171,
                    "matchedPaperCorpusId": "219764620"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333984375
        },
        {
            "corpus_id": "272693949",
            "title": "NVLM: Open Frontier-Class Multimodal LLMs",
            "text": "Large language models (LLMs) [11] have laid the foundation for the rapid progress in AI recently. Since the introduction of ChatGPT [104], LLMs have revolutionized the text domain and are becoming universal task solvers for natural language processing, math and coding problems. Simultaneously, multimodal LLMs (MLLMs) [4; 107], which bridge the physical world with language models, have gained significant traction. The release of GPT-4V [107] has sparked a competitive race in the development of proprietary multimodal LLMs for vision-language intelligence [35; 108; 5; 6; 153; 154; 122]. However, the model architectures, training data, and methods used to build these proprietary models remain undisclosed, preventing the research community from building upon them. \n\nA notable feature of leading proprietary multimodal LLMs is their exceptional performance on both multimodal and text-only tasks, a quality we refer to as production-grade multimodality [108; 35; 36]. For example, GPT-4o is a single neural network trained end-to-end on text and images, achieving stateof-the-art results in both text-only and vision-language tasks [110]. This unified approach simplifies deployment by eliminating the need to route different input modalities to separate LLMs, offering users a seamless experience for switching between modalities without losing text or multimodal context. [19; 18; 111], and Llama 3-V [82]. The most common architectures used to build these multimodal LLMs are the decoder-only architecture (e.g., LLaVA [79] and InternVL [18]), which processes image tokens within the LLM self-attention layers, and the cross-attention-based architecture (e.g., Flamingo [4] and Llama 3-V [82]), which handles image tokens through LLM cross-attention layers. \n\nHowever, the previous studies of multimodal LLMs have several limitations:",
            "score": 0.5217162544598901,
            "section_title": "Introduction",
            "char_start_offset": 3183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1842
                }
            ],
            "ref_mentions": [
                {
                    "start": 1678,
                    "end": 1681,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "274860024",
            "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
            "text": "Over the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023;Koh et al., 2024;Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and Unified-IO (Lu et al., 2022(Lu et al., , 2024) ) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities. However, these models typically train from scratch, demanding significant computational resources to achieve proficiency across all modalities. The computational cost of mastering even a single modality is substantial-training a state-of-the-art text-only large language models (LLMs) like Llama-3 (Dubey et al., 2024) requires training over 15 trillion tokens. \n\nGiven these computational demands, we investigate an alternative paradigm that reuses and adapts existing pretrained LLMs (Ge et al., 2023;Sun et al., 2023;Wu et al., 2024b). We address a fundamental research question: How to preserve the text-only performance of pretrained LLMs while equipping them with visual understanding and generation abilities? Our experiments show that naive finetuning of pretrained text-only LLMs on multimodal data leads to significant degradation of their language processing capabilities. \n\nTo address this challenge, we introduce LMFusion, a framework that enhances a pretrained text-only LLM, Llama-3 (Dubey et al., 2024) with multimodal capabilities by building upon the recipe of Transfusion (Zhou et al., 2024). Drawing from recent and parallel work on modality separation (Shen et al., 2023;Chen et al., 2023;Liang et al., 2024;Liu et al., 2024a), LMFusion integrates the original Llama modules pretrained for language processing while introducing additional dedicated transformer modules for visual understanding and generation tasks.",
            "score": 0.5216649653138572,
            "section_title": "Introduction",
            "char_start_offset": 2915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 828
                },
                {
                    "start": 831,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1903
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 218,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 323,
                    "end": 339,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 339,
                    "end": 360,
                    "matchedPaperCorpusId": "266573555"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78369140625
        },
        {
            "corpus_id": "274981694",
            "title": "SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization",
            "text": "Subsequently, CLIP generates a feature representation of the input image, which is a sequence of 768-dimensional visual tokens, representing the content. \n\nTo integrate this visual representation with the lan-guage model, we project the 768-dimensional visual tokens to a higher-dimensional space (R 4096 ) through a visual adapter. Inspired by MiniGPT-v2 [71], we designed the adapter with two Linear layers and the GELU activation function [23], which serves as an essential bridge between the vision backbone and the language model. After that, the output of the visual adapter is concatenated with audio embeddings produced by the audio adapter, creating a multimodal feature representation. During the model development process, we observed that by concatenating multiple encoders, not limited by either pair of images and text or images and audio, we can create a robust, versatile model that is capable of handling diverse input types. \n\nLarge Language Model: At the core of SilVar is a language model, responsible for generating text and bounding boxes by processing information from the audio and visual encoders. To this end, we decided to use LLama 3.1-8B [17], a novel open-source model, which was trained on a diverse range of text-related tasks. LLama serves as the foundational model, allowing us to effectively transfer its pre-trained knowl-edge into domain-specific tasks such as localization where understanding both verbal and visual inputs. By combining visual and audio tokens, we provide a diverse set of token embeddings for the language model, requiring it to process and generate a final representation of textual tokens. After processing these tokens, the language model outputs tokens that are untokenized to form natural language text, which is the model's response to the input prompt. The integration of these components enables SilVar to generate human-quality text responses by leveraging the complementary strengths of each modality, making it a robust system for multimodal instruction and interaction.",
            "score": 0.521540884462993,
            "section_title": "Architecture",
            "char_start_offset": 18008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2037
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43310546875
        },
        {
            "corpus_id": "272910689",
            "title": "CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches",
            "text": "Vision Language Pre-Training. The great success of large language models (LLMs) such as BERT [11] and GPT [2] has drawn significant attention within the field of natural language processing and code generation domain. \n\nCodeT5+ [27] is a family of open-source encoder-decoder LLMs for code tasks, ranging from 220 million to 16 Billion parameters. Beyond single modality LLMs, multimodal foundation models have also exhibited enhanced performance in various vision-and-language tasks [6,12,20]. Depending on downstream tasks, Various pre-training objectives have also been proposed like image-text contrastive learning [20], image-text matching [12], and (masked) language modeling [12]. \n\nMost recently, developing domain-specific multimodality LLMs has gained momentum such as MedBLIP [4] and TableGPT [33]. CAD Sketch Generation 2D engineering sketches are fundamental in CAD for designing and manufacturing mechanical parts. The recent availability of extensive engineering sketch datasets has facilitated the development of generative models that enhance traditional CAD workflows. The SketchGraphs [22] dataset, featuring 15 million parametric CAD sketches with detailed geometric constraint graphs, showcases primitive interrelations. The paper also introduces a baseline model using autoregressive message-passing networks for constraint graph generation, but it lacks primitive coordinate output, relying solely on constraint graphs and the Onshape solver for sketch configuration, thus limiting its utility in CAD autocompletion tasks. Ganin et al. [8] proposed another large-scale CAD dataset containing over 4.7M parametric sketches from the Onshape public repository. However, limited by sketch format, this dataset has not been widely researched in the realm of CAD generative models. \n\nAnother line of work for generative modeling of CAD sketches is based on Transformer [23,[28][29][30]. Vitruvion [23] models both primitives and constraints, but its training on Transformer does not inherently include geometric knowledge. Furthermore, Vitruvion's major limitation lies in its lack of quantitative comparison in terms of CAD generation quality.",
            "score": 0.5213093945045892,
            "section_title": "Related Work",
            "char_start_offset": 5673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 217
                },
                {
                    "start": 220,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 97,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 106,
                    "end": 109,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 487,
                    "end": 490,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 490,
                    "end": 493,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 682,
                    "end": 686,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 1886,
                    "end": 1890,
                    "matchedPaperCorpusId": "238215827"
                },
                {
                    "start": 1890,
                    "end": 1894,
                    "matchedPaperCorpusId": "233307490"
                },
                {
                    "start": 1894,
                    "end": 1898,
                    "matchedPaperCorpusId": "234789948"
                },
                {
                    "start": 1914,
                    "end": 1918,
                    "matchedPaperCorpusId": "238215827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "267547631",
            "title": "Real-world robot applications of foundation models: a review",
            "text": "While individual language or image processing has limited capabilities, combining vision and language allows us to build diverse foundation models. These Vision-Language Models (VLMs) are trained with the Internet-scale massive datasets as well as LLMs. For instance, CLIP [28] is trained on 400 million image-text pair datasets from the Internet. When classifying VLMs from the network input and output, they mainly fall into the following four categories. \n\nVision + Language \u2192 Latent involves converting images and text into latent space vectors, which allows one to calculate the similarity between them for downstream tasks such as retrieval. Prominent models include CLIP [28] and GLIP [57]. While CLIP [28] computes correspondences between a single image-text pair, GLIP [57] can calculate correspondences between multiple regions of an image with their text descriptions. \n\nVision + Language \u2192 Language enables tasks like image captioning (IC), visual question answering (VQA), and visual entailment (VE). Prominent models include BLIP2 [58], Flamingo [59], OFA [60], and Unified-IO [61]. BLIP2 is dedicated to IC and VQA, whereas OFA and Unified-IO serve as a foundation model capable of VQA, IC, Visual Grounding (VG), Text-to-Image Generation (TIG), and more within a single network. Flamingo supports in-context learning for both vision and language. GPT-4 Vision (GPT-4V) [27] can analyze images that users input into the model and generate the answer as texts, which can apply to IC, VWA, and VE. Extending GPT-4V, GPT-4o (omni) is a multimodal model that can reason across audio, vision, and text. \n\nVision + Language \u2192 Vision facilitates tasks like image editing and image generation through language. Prominent models include Stable Diffusion [62], DALL-E [63], and the previously mentioned OFA [60] and Unified-IO [61]. They incorporate Transformer-based architectures and diffusion architectures into the models. \n\nVision + Language \u2192 Recognition enables tasks like semantic segmentation, instance segmentation, and bounding box extraction from images and language.",
            "score": 0.5210289204351707,
            "section_title": "Foundation Models for Vision and Language",
            "char_start_offset": 11191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 678,
                    "end": 682,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1060,
                    "end": 1064,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1070,
                    "end": 1074,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 1091,
                    "end": 1095,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 1760,
                    "end": 1764,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1812,
                    "end": 1816,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "249848272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "278394510",
            "title": "GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing",
            "text": "Text editing is a task that, given a selected text region, replaces existing text content with new user inputs. The most straightforward approach involves a two-stage process: text removal followed by font-matched text insertion. While this approach may suffice for images with printed texts, it requires substantial expertise in font recognition to maintain style consistency. Moreover, for text in natural scenes (i.e. scene texts), this approach faces great challenges: even with perfect font identification, achieving visual coherence between the inserted text and the scene remains infeasible due to complex environmental factors such as perspective distortions, lighting conditions, and surface properties. \n\nThis challenge motivates the task of scene text editing, a special task within image editing domain that aims to modify texts in images while preserving the original style and maintaining visual coherence with the surrounding environment. Advances in this field benefits both everyday users and professional designers, particularly when they deal with typefaces that are either difficult to identify or impossible to replicate through conventional means. \n\nToward this end, diffusion-based [4] scene text editing methodologies [1,2,6,25] have demonstrated significant potential to meet these demanding requirements. Dif-fUTE [1], a specialized scene text editing framework built upon diffusion models, exemplifies this approach. In contrast to text-to-image generation approaches that utilize text encoders such as CLIP [14] for extracting text embeddings from descriptive languages to enable language-driven generation, the framework employs features extracted by a pretrained OCR model from rendered text lines as the glyph guidance for scene text editing. While DiffUTE achieves robust style preservation through its conditional inpainting formulation and demonstrates effective text editing capabilities for scenes with simple printed text, it shows significant limitations in text legibility when handling complex glyph structures such as Chinese characters (Figure 5). These limitations stem from the insufficient representational capacity of its OCR feature utilization in encoding intricate glyph structures. \n\nTo address the challenge of creating robust glyph representations for achieving precise scene text editing results, we propose GlyphMastero, a novel trainable glyph encoder that generates fine-grained glyph guidance for scene text editing.",
            "score": 0.5204843573359041,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2231
                },
                {
                    "start": 2234,
                    "end": 2473
                }
            ],
            "ref_mentions": [
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1242,
                    "end": 1245,
                    "matchedPaperCorpusId": "258762543"
                },
                {
                    "start": 1245,
                    "end": 1247,
                    "matchedPaperCorpusId": "258762187"
                },
                {
                    "start": 1249,
                    "end": 1252,
                    "matchedPaperCorpusId": "265034144"
                },
                {
                    "start": 1340,
                    "end": 1343,
                    "matchedPaperCorpusId": "258762543"
                },
                {
                    "start": 1535,
                    "end": 1539,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25244140625
        },
        {
            "corpus_id": "274023123",
            "title": "Spider: Any-to-Many Multimodal LLM",
            "text": "Large Language Models (LLMs) such as Vicuna [9], LLaMA [58], ChatGPT [42], and GPT-4 [1] have demonstrated human-level proficiency in language understanding and generation. However, as the demand for more complex, real-world applications grew, the need for integrating LLMs with multiple types of input and output modalities (e.g., text, images, audio, video) became apparent. This evolution has led to the rise of Multimodal LLMs (MLLMs), which extend LLMs' capabilities by incorporating multimodal perception modules [2,23,28,30,37,53,54,66,73]. \n\nThe development of MLLMs marks a significant ad-vancement in enabling comprehensive understanding and generation across various modalities. Initially, models like LLaVA1.5 [36] and MiniGPT-4 [73] were capable of processing only two modalities: text and images. Further innovations saw the rise of models like PandaGPT [54], OneLLM [19], Gemini [57], and NExT-GPT [66], which expanded support to four modalities, incorporating text, image, audio, and video into their multimodal frameworks. \n\nHowever, as depicted in Fig. 1(a), these X-to-X (Anyto-Any) MLLMs are restricted to generate pairwise modalities 'Text + X' within a single interaction, such as 'Text + Image' or 'Text + Audio'. For example, when a user asks to generate an image of a dog, the model responds with image output. In subsequent interaction, to get an audio of the dog's bark, the user needs to give a new instruction. These MLLMs based on Multi-Round Dialogue Generation paradigm, require several rounds of user instructions and do not allow for a seamless integration of multiple modalities within a single interaction. Each pair of modalities is handled independently, resulting in a fragmented user experience where the responses feel disjointed rather than cohesive. Another example is in Fig. 7 of Appendix B.",
            "score": 0.5203828096075522,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "257532815"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 528,
                    "end": 531,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "265709786"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "261696650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69775390625
        },
        {
            "corpus_id": "271329151",
            "title": "SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models",
            "text": "Image Large Language Models. Significant advances have been observed in the development of multimodal large language models (LLMs) (Achiam et al., 2023;Team et al., 2023;McKinzie et al., 2024;Abdin et al., 2024;Liu et al., 2024). As a pioneer work, Flamingo (Alayrac et al., 2022) accepts arbitrarily interleaved visual and text data as inputs and generates text in an open-ended manner. BLIP-2 (Li et al., 2023b) uses pre-trained visual and text models, and bridges the domain gap with the proposed Q-Former. LLaVA(-v1.5/NeXT) (Liu et al., 2023b;a;2024) achieves remarkable performance by leveraging a simple linear connector or an MLP between visual and text models and designing an efficient instruction following data pipeline assisted with GPT. More recently, MM1 (McKinzie et al., 2024) conducts comprehensive ablation studies on model components and data choices, and offers valuable insights for understanding Image LLMs. There are also efforts to ingest other modalities. Ferret (You et al., 2023;Zhang et al., 2024a) focuses on the box/shape modality and enhances a model's language grounding capability at any granularity. 3D-LLM (Hong et al., 2023) enables open-ended question answering in 3D by injecting 3D representations into an LLM. 4M (Mizrahi et al., 2023;Bachmann et al., 2024) presents a general any (modality) to any (modality) framework with strong out-of-box perceptional and generative capabilities. \n\nVideo Large Language Models. With the rapid development of LLMs (Achiam et al., 2023;Team et al., 2023;Chiang et al., 2023;Touvron et al., 2023a;b), there is increasing interest in generalist video models that can perform a wide range of video tasks.",
            "score": 0.5202781184727046,
            "section_title": "RELATED WORK",
            "char_start_offset": 4150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1677
                }
            ],
            "ref_mentions": [
                {
                    "start": 395,
                    "end": 413,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 528,
                    "end": 547,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.638671875
        },
        {
            "corpus_id": "267897370",
            "title": "LLMBind: A Unified Modality-Task Integration Framework",
            "text": "In the multi-modal domain, the dependence of various models on specific input formats leads to user confusion and hinders progress. To address this challenge, we introduce \\textbf{LLMBind}, a novel framework designed to unify a diverse array of multi-modal tasks. By harnessing a Mixture-of-Experts (MoE) Large Language Model (LLM), LLMBind processes multi-modal inputs and generates task-specific tokens, enabling the invocation of corresponding models to accomplish tasks. This unique approach empowers LLMBind to interpret inputs and generate outputs across various modalities, including image, text, video, and audio. Furthermore, we have constructed an interaction dataset comprising 400k instructions, which unlocks the ability of LLMBind for interactive visual generation and editing tasks. Extensive experimentation demonstrates that LLMBind achieves very superior performance across diverse tasks and outperforms existing models in user evaluations conducted in real-world scenarios. Moreover, the adaptability of LLMBind allows for seamless integration with the latest models and extension to new modality tasks, highlighting its potential to serve as a unified AI agent for modeling universal modalities.",
            "score": 0.5202586582618887,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58544921875
        },
        {
            "corpus_id": "271064936",
            "title": "VIMI: Grounding Video Generation through Multi-modal Instruction",
            "text": "A <image_2> is sitting on the beach.\n\nA <image_2> is sitting on the <image_3>.During the pretraining stage, we first construct a large-scale multimodal prompt dataset by employing a large-scale retrieval method to pair multimodal in-context examples with the given text prompts.The retrieved contexts from a web-scale corpus provide a rich multimodal in-context environment for model training.With these paired datasets, we can either pretrain a multimodal video generator from scratch or fine-tune an existing textto-video generator.After this stage, the model gains the capability to understand both text-only and multimodal inputs for video generation.This establishes a foundation model for grounded video generation, capable of integrating diverse modalities into cohesive video outputs.\n\nIn the second stage, we fine-tune the model from the first stage on various video generation tasks, incorporating multimodal instructions.This finetuning process further refines the model's ability to handle diverse inputs and tasks, ensuring it can seamlessly integrate multimodal information.After this two-stage training process, VIMI demonstrates enhanced multimodal understanding capabilities, producing contextually rich and personalized videos grounded in the provided inputs.This makes the model highly versatile and effective for a wide range of video generation applications.\n\nIn summary, our main contributions include:\n\n\u2022 Novel Dataset Construction: We are the first to use retrieval methods to build large-scale multimodal dataasets for video pretraining.\n\n\u2022 Retrieval Augmented Video Pretraining:\n\nWe propose a novel retrieval-augmented pretraining framework specifically designed for grounded video generation.Our pretraining framework enables video generators to receive multi-modal prompts, instead of textonly prompts.\n\n\u2022 Instruction Tuning for Video Generation:\n\nWe introduce instruction tuning for video generation, unifying three distinct video generation tasks within a single, cohesive instruction framework.This innovative approach allows the model to flexibly handle various video generation tasks based on specific instructions.",
            "score": 0.5188710589891462,
            "section_title": "Generated Videos",
            "char_start_offset": 2474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 38,
                    "end": 78
                },
                {
                    "start": 78,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 792
                },
                {
                    "start": 794,
                    "end": 932
                },
                {
                    "start": 932,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1277
                },
                {
                    "start": 1277,
                    "end": 1379
                },
                {
                    "start": 1381,
                    "end": 1424
                },
                {
                    "start": 1426,
                    "end": 1562
                },
                {
                    "start": 1564,
                    "end": 1604
                },
                {
                    "start": 1606,
                    "end": 1719
                },
                {
                    "start": 1719,
                    "end": 1830
                },
                {
                    "start": 1832,
                    "end": 1874
                },
                {
                    "start": 1876,
                    "end": 2025
                },
                {
                    "start": 2025,
                    "end": 2148
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.389404296875
        },
        {
            "corpus_id": "270094821",
            "title": "Multi-Modal Generative Embedding Model",
            "text": "The Multi-Modal Generative Embedding Model (MM-GEM) presents a unified approach to multimodal learning by integrating generative and embedding objectives within a single Large Language Model (LLM).Our experiments demonstrate that these two objectives do not significantly conflict with each other.MM-GEM achieves competitive performance across a range of tasks, including cross-modal retrieval, zero-shot classification, and image captioning.A key contribution is the PoolAggregator, enhancing the model's ability to handle fine-grained tasks.Additionally, MM-GEM's advanced text module significantly improves performance on long-form text retrieval, showcasing the benefits of leveraging a robust LLM for text processing.\n\nMM-GEM represents a significant step towards unified multi-modal models, yet there are still many subsequent potential directions: 1) We mainly focus on image captioning for generative tasks in this work, the performance impact of adding plain language data needs to be further investigated.2) MM-GEM enables LLMs generate discriminative outputs besides language tokens, this may benefit multi-modal large language model by retrieving or grounding visual information efficiently.Further investigation into these aspects will be explored in future work.",
            "score": 0.5187408305999397,
            "section_title": "Conclusion",
            "char_start_offset": 25786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 722
                },
                {
                    "start": 724,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1276
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "257622925",
            "title": "HIVE: Harnessing Human Feedback for Instructional Visual Editing",
            "text": "As shown in Fig. 3, the reward model takes in an input image c I , a text instruction c E , and an edited image x and outputs a scalar value. Inspired by the recent work on the vision-language model, especially BLIP [28], we employ a visual transformer [13] as our image encoder and an image-grounded text encoder as the multimodal encoder for images and text. Finally, we set a linear layer on top of the image-grounded text encoder to map the multimodal embedding to a scalar value. \n\n(1) Visual transformer. We encode both the input image c I and edited image x with the same visual transformer. Then we obtain the joint image embedding by concatenating the two image embeddings vit(c I ), vit(x). (2) Image-grounded text encoder. The image-grounded text encoder is a multimodal encoder that inserts one additional cross-attention layer between the self-attention layer and the feed-forward network for each transformer block of BERT [11]. The additional cross-attention layer incorporates visual information into the text model. The output embedding of the image-grounded text encoder is used as the multimodal representation of the (c I , c E , x) triplet. \n\nWe gather a dataset comprising 3,634 images for the purpose of ranking. For each image, we generate five variant edited images, and ask an annotator to rank images from best to worst. Additionally, we ask annotators to indicate if any of the following scenarios apply: (1) all edited images are edited but none of them follow the instruction; (2) all edited images are visually the same as the original image; (3) all images are edited beyond the scope of instruction; (4) edited images have harmful content containing sex, violence, porn, etc; and (5) all edited images look similar to each other. We compare training reward models by filtering some/all of these options. \n\nWe note that a considerable portion of the collected data falls under at least one of the aforementioned categories, indicating that even for humans, ranking these images is challenging.",
            "score": 0.518739804311754,
            "section_title": "B.2. Human Feedback Rewards Learning",
            "char_start_offset": 26376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 2025
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2802734375
        },
        {
            "corpus_id": "272694532",
            "title": "MM2Latent: Text-to-facial image generation and editing in GANs with multimodal assistance",
            "text": "Training multimodal image generation models typically requires a large-scale dataset comprising image-text pairs. However, labelling text descriptions generally is both time-intensive and costly. Our framework leverages the visual and language alignment in FaRL to avoid being limited to the image-text pairs dataset. Specifically, our work requires the knowledge of the ground truth w of the input f text and f mask . By sampling the Z-space of the StyleGAN, images and its ground truth w are generated. Then a third-party facial parsing method [74] is applied to the images to generate facial masks for training (facial sketch is generated by OpenCV [7]). We use FaRL image encoder to get the image embeddings f img . We use mask/sketch encoder to extract f mask /f sketch . After generating the f \u2032 text from f img by the Pesudo text embedding generator, we now have the input f \u2032 text and f mask and their ground truth w for training the framework.",
            "score": 0.5186302448900572,
            "section_title": "Training the whole framework",
            "char_start_offset": 15145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 952
                }
            ],
            "ref_mentions": [
                {
                    "start": 546,
                    "end": 550,
                    "matchedPaperCorpusId": "244908314"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302734375
        },
        {
            "corpus_id": "277103675",
            "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
            "text": "Large Language Models (LLMs) have recently advanced from text-centric architectures, such as BERT [11] and GPT [35], toward multimodal systems capable of understanding and generating content across different modalities. GPT-3 [5] and PaLM [9] show that scaling language models leads to emergent capabilities, while Flamingo [2] further demonstrates that incorporating visual inputs facilitates unified multimodal reasoning. This trend toward unified vision-language model-using a single model for diverse tasks of visual understanding and generation-has promising potential for leveraging knowledge and reasoning abilities that transfer across different vision and language tasks, ultimately facilitating more robust and generalizable multimodal representation and modeling capabilities. \n\nMotivated by the advantages and strong scaling properties of autoregressive models, coupled with their simplicity, we investigate a pure autoregressive framework for unified visual generation and understanding, without the limitations introduced by vector quantization (VQ). In this paper, we introduce UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,21] for image generation and a pre-trained SigLIP [59] image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities.",
            "score": 0.5181928472540605,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2436
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1830,
                    "end": 1833,
                    "matchedPaperCorpusId": "270560593"
                },
                {
                    "start": 1880,
                    "end": 1884,
                    "matchedPaperCorpusId": "257767223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8583984375
        },
        {
            "corpus_id": "277501830",
            "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement",
            "text": "Additionally, in contrast to VQGAN-based models (Fig. 2 (a)) that purely supervised by image reconstruction, these semantic encoders facilitate image-text alignment as demonstrated in ILLUME [63]. Thus, incorporating a pre-text-aligned semantic encoder is essential for a unified foundation model. \n\nImage texture preservation. The quality of image reconstruction of vision tokenizers is essential for handling editing tasks, which determines the up-bound of unified models to maintain consistency of unchanged regions in images. Therefore, not only semantic information but also texture preservation is required as a crucial consideration in vision tokenizer design choices. \n\nNo information loss for image input. While the vision tokenizer is the key to enabling unified autoregressive image-text generation, it inevitably introduces information loss during the quantization process. To this end, using continuous features before the quantizer of vision tokenizer as visual input for LLM serves as a more suitable choice to guarantee fine-grained multimodal understanding capability. \n\nUnified image input and output representation. The decoupled mechanism in Janus series [67,11], i.e., semantic representation for visual input while pixel representation for visual output, inevitably hinders the model's ability to accurately interpret and further modify its own visual outputs in multiround steps. Hence, a unified representation for visual input and output is necessary to further support image-text interleaved generation, multi-turn dialogues and chain-of-thought reasoning. \n\nUnified text and image head. In a single autoregressive framework, a unified output head for both image and text is preferable, as it not only simplifies infrastructure design but also enhances crossmodal interactions. In contrast, the requirement of a separate image head in Fig. 2 (c) introduces challenges in modality switching during generation. For example, as we need special line separator tokens interleaved with visual tokens to represent an image in different resolutions, how can the model seamlessly transition between text and image heads during inference? To avoid such complexity, a unified head offers a more effective and elegant solution.",
            "score": 0.5178269055503311,
            "section_title": "Introduction",
            "char_start_offset": 4386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 297
                },
                {
                    "start": 300,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2154
                },
                {
                    "start": 2155,
                    "end": 2241
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "261557127",
            "title": "RSDiff: Remote Sensing Image Generation from Text Using Diffusion Model",
            "text": "Language models [23,24] are trained on a corpus consisting solely of text, which is notably bigger in size compared to paired image-text data. As a result, these models are exposed to a vast and diverse distribution of textual information. The size of these models is typically larger than the text encoders seen in existing image-text models. Therefore, it is logical to investigate both categories of text encoders for the task of text-to-image conversion. Pretrained text encoder T5 [24,25] is utilized in our approach as it uses a unified framework where all tasks, both generation and classification, are treated as text-to-text problems. Furthermore, T5 exhibits a hybrid nature, being trained to produce either single words or multiple words for a single mask. This flexibility enables the model to effectively learn the intricate structure of language. To ensure simplicity, the weights of the text encoder are held constant. The process of freezing offers numerous benefits, including the ability to do offline computation of embeddings. This feature leads to minimal computational requirements and memory use for training the text-to-image model. It is evident that increasing the size of the text encoder enhances the quality of text-to-image generation [22].",
            "score": 0.517515985290641,
            "section_title": "Pretrained text encoders",
            "char_start_offset": 8491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1270
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 486,
                    "end": 490,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1265,
                    "end": 1269,
                    "matchedPaperCorpusId": "248986576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39404296875
        },
        {
            "corpus_id": "269983697",
            "title": "From Text to Pixel: Advancing Long-Context Understanding in MLLMs",
            "text": "The rapid progress in Multimodal Large Language Models (MLLMs) has significantly advanced their ability to process and understand complex visual and textual information. However, the integration of multiple images and extensive textual contexts remains a challenge due to the inherent limitation of the models' capacity to handle long input sequences efficiently. In this paper, we introduce SEEKER, a multimodal large language model designed to tackle this issue. SEEKER aims to optimize the compact encoding of long text by compressing the text sequence into the visual pixel space via images, enabling the model to handle long text within a fixed token-length budget efficiently. Our empirical experiments on six long-context multimodal tasks demonstrate that SEEKER can leverage fewer image tokens to convey the same amount of textual information compared with the OCR-based approach, and is more efficient in understanding long-form multimodal input and generating long-form textual output, outperforming all existing proprietary and open-source MLLMs by large margins.",
            "score": 0.5174345503168025,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "265302359",
            "title": "Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report",
            "text": "This approach is highly effective for capturing the context and meaning of each token in the input text, enabling the model to better understand the overall meaning of the text. When processing an input text T , the text encoder generates a sequence of embeddings w cls , w 1 , ..., w N , where w cls represents the embedding of the [CLS] token, and the remaining w i represent the embeddings of the individual input tokens. This sequence of embeddings is then passed to the multimodal encoder to be combined with the visual embeddings generated by the vision encoder. \n\nMultimodal Encoder The multimodal encoder is a complex module that plays a critical role in enabling the model to learn the relationships between the visual and textual inputs. To achieve this, it incorporates an additional cross-attention (CA) layer that sits between the self-attention (SA) layer and the feed-forward network (FFN) for each transformer block of the text encoder. By doing so, the model can attend to both the textual and visual inputs and build better representations of the image-text pair. To create a multimodal representation of the image-text pair, the text input is modified by appending a taskspecific [Encode] token at the end of the sequence, which is then fed into the multimodal encoder. The output embedding of this token is used as the final representation of the image-text pair. The embedding layers, CA layers, and FFN share similar functionality between encoding and decoding tasks, which means that they can be shared to improve training efficiency and benefit from multi-task learning. Additionally, the cross-attention layer introduces another set of attention weights to the model, which requires additional computation and increases the number of parameters to be learned. However, this additional complexity is necessary to enable the model to learn Figure 3: The overall architecture of our model, which comprises a vision encoder responsible for processing visual data, a language encoder that focuses on comprehending textual information, and a multimodal encoder that combines the input from both the vision and language encoders to fuse comprehensive representations.",
            "score": 0.5171948816293226,
            "section_title": "Retrieval System",
            "char_start_offset": 15648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 2185
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482666015625
        },
        {
            "corpus_id": "262898422",
            "title": "Jointly Training Large Autoregressive Multimodal Models",
            "text": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
            "score": 0.5171846090662012,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "254685730",
            "title": "CLIPPO: Image-and-Language Understanding from Pixels Only",
            "text": "In recent years, large-scale multimodal training of Transformer-based models has led to improvements in the state-of-the-art in different domains including vision [2,10,[74][75][76], language [6,11], and audio [5]. In particular, in computer vision and image-language understanding, a single large pretrained model can outperform task-specific expert models [10,74,75]. However, large multimodal models often use modality or dataset-specific encoders and decoders, and accordingly lead to involved protocols. For example, such models frequently involve training different Code and pretrained models are available as part of big vision [4] https://github.com/google-research/big_vision.  Figure 1. CLIP [56] trains separate image and text encoders, each with a modality-specific preprocessing and embedding, on image/alt-text pairs with a contrastive objective. CLIPPO trains a pure pixel-based model with equivalent capabilities by rendering the alt-text as an image, encoding the resulting image pair using a shared vision encoder (in two separate forward passes), and applying same training objective as CLIP. parts of the model in separate phases on their respective datasets, with dataset-specific preprocessing, or transferring different parts in a task-specific manner [75]. Such modality and task-specific components can lead to additional engineering complexity, and poses challenges when introducing new pretraining losses or downstream tasks. Developing a single end-to-end model that can process any modality, or combination of modalities, would be a valuable step for multimodal learning. Here, we focus on images and text. A number of key unifications have accelerated the progress of multimodal learning. First, the Transformer architecture has been shown to work as a universal backbone, performing well on text [6,15], vision [16], audio [5,24,54], and other domains [7,34]. Second, many papers have explored mapping different modalities into a single shared embedding space to simplify the input/output interface [21,22,46,69]",
            "score": 0.5169612444225791,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 166,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 169,
                    "end": 173,
                    "matchedPaperCorpusId": "249152323"
                },
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "237291550"
                },
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "249152323"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1827,
                    "end": 1830,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "265050698",
            "title": "LRM: Large Reconstruction Model for Single Image to 3D",
            "text": "In this paper, we propose LRM, the first large transformer-based framework to learn an expressive 3D prior from a million 3D data to reconstruct objects from single images. LRM is very efficient in training and inference; it is a fully-differentiable network that can be trained end-to-end with simple image reconstruction losses and only takes five seconds to render a high-fidelity 3D shape, thus enabling a wide range of real-world applications. In the era of large-scale learning, we hope our idea can inspire future research to explore data-driven 3D large reconstruction models that generalize well to arbitrary in-the-wild images. \n\nFuture Directions In addition to addressing the limitations mentioned in Sec. 4.3.2, we suggest two future directions of our research; (1) Scaling up the model and training data: with the simplest transformer-based design and minimal regularization, LRM can be easily scaled to a larger and more capable network, including but not limited to applying a larger image encoder, adding more attention layers to the image-to-triplane decoder, and increasing the resolution of triplane representations. On the other hand, LRM only requires multi-view images for supervision, hence a wide range of 3D, video, and image datasets can be exploited in training. We expect both approaches to be promising in improving the model's generalization ability and the quality of reconstruction. \n\n(2) Extension to multimodal 3D generative models: LRM model builds a pathway for generating novel 3D shapes from language by leveraging a text-to-image generation model to first create 2D images. But more interestingly, we suggest the learned expressive triplane representations could be applied to directly bridge language descriptions and 3D to enable efficient text-to-3D generation and editing (e.g., via latent diffusion (Rombach et al., 2022)). We will explore these ideas in our future research.",
            "score": 0.516958746326581,
            "section_title": "CONCLUSION",
            "char_start_offset": 25847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1920
                }
            ],
            "ref_mentions": [
                {
                    "start": 1844,
                    "end": 1866,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "270765005",
            "title": "MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data",
            "text": "Text-to-image generative AI produces detailed images from simple text prompts [1,2,3,4,5,6,7]. However, text is not always sufficient to describe user intent. One potential improvement is multimodal prompting which allows users to specify both text and reference images. We bootstrap a multimodal prompt dataset from existing text-image data, and we train our image generation model with multimodal understanding, MUMU, by replacing the text encoder, CLIP [8], of a diffusion model [9,10,11,12,13,14], SDXL [6], with a vision-language model [15,16,17,18,19,20,21], Idefics2 [19]. \n\nWe construct a multimodal training set bootstrapped from text-image data. We use open vocabulary object detection to extract image crops corresponding to words in the image captions [22]. The image crops are then placed before their corresponding words in the text prompt, see Figure 3 for an example. Our data is mostly synthetically generated from SDXL with some added high quality publicly available data. SDXL conditions on text via cross-attention [23] on CLIP hidden states. We replace the CLIP hidden states with those of a minorly modified Idefics2. Idefics2 is composed of a vision transformer [24] for embedding image inputs, a perceiver transformer for pooling image embeddings to a fixed sequence length, and a large vision-language model transformer. For MUMU, we remove Idefics2's perceiver transformer to use a larger number of tokens per image. We find that removing the perceiver and using more tokens improves image quality with image quality saturating at approximately 1, 000 tokens per image. We also add a small non-causal \"adapter\" transformer on top of Idefics2's hidden states [25]. Figure 2 shows the full architecture. MUMU is further trained from base SDXL and Idefics2 on a single 8xH100 GPU node for approximately 300,000 steps or 6 days. \n\nMUMU can directly place conditioning images into the generated image.",
            "score": 0.5169517297886383,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1850
                },
                {
                    "start": 1853,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 81,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 85,
                    "end": 87,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 456,
                    "end": 459,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 497,
                    "end": 500,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 545,
                    "end": 548,
                    "matchedPaperCorpusId": "235351128"
                },
                {
                    "start": 548,
                    "end": 551,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 557,
                    "end": 560,
                    "matchedPaperCorpusId": "259287020"
                },
                {
                    "start": 1035,
                    "end": 1039,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390869140625
        },
        {
            "corpus_id": "271924334",
            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
            "text": "Given that the embedding of image tokens is newly initialized, it necessitates large-scale pre-training to align for multimodal understanding and generation. Besides, Show-o eliminates the text encoder to extract text embeddings for text-to-image generation, which poses a significant challenge for achieving effective alignment between text and image content within one single transformer. To this end, we employ a three-stage approach to progressively and effectively train Show-o: \n\ni) Image Token Embedding and Pixel Dependency Learning: We employ RefinedWeb (Penedo et al., 2023) dataset to train Show-o to maintain the language modeling ability. Meanwhile, ImageNet-1K dataset (Deng et al., 2009) and image-text pairs are adopted to train Show-o for classconditional image generation and image captioning, respectively. Here, we directly leverage the class names from ImageNet-1K as textual inputs for learning class-conditional image generation. This stage primarily involves the learning of new learnable embeddings for discrete image tokens, pixel dependency for image generation, and alignment between image and text for image captioning. \n\nii) Image-Text Alignment for Multimodal Understanding and Generation: Building upon the pre-trained weights, we proceed to involve training of text-to-image generation on the image-text data instead of the ImageNet-1K. This stage mainly focuses on image and text alignment for both image captioning and text-to-image generation. \n\niii) High-Quality Data Fine-tuning: Lastly, we further refine the pre-trained Show-o model by incorporating filtered high-quality image-text pairs for text-to-image generation and instructional data for multimodal understanding and mixed-modality generation.",
            "score": 0.5162091383641558,
            "section_title": "TRAINING PIPELINE",
            "char_start_offset": 24402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1740
                }
            ],
            "ref_mentions": [
                {
                    "start": 683,
                    "end": 702,
                    "matchedPaperCorpusId": "57246310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "276928576",
            "title": "Language-Depth Navigated Thermal and Visible Image Fusion",
            "text": "With advancements in network architectures and largescale datasets, visual-language models have gained attention in generative modeling. CLIP [24], which aligns imagetext pairs using contrastive loss through two neural network encoders, demonstrates strong feature extraction and zeroshot recognition capabilities. Supported by CLIP, various text-driven image generation and processing methods have emerged. Style-CLIP [23] integrates textual prompts with StyleGAN [12], enabling image modifications through text. Beyond GANs, text-conditioned diffusion models like Diffu-sionCLIP [13] and Stable Diffusion [27] have expanded the possibilities for text-guided image generation by combining diffusion models with text encoders and attention mechanisms. These approaches facilitate interactive multimodal fusion and control, offering more flexible image creation and editing options. TextIF [45] proposed a text-guided image fusion framework to address the limitations of existing methods in complex scenes, enabling fusion through interactive text inputs. [41] used CLIP to create a language-driven fusion model that links embedding vectors for fusion objectives and image features, along with a language-driven loss function for image fusion. However, the textual descriptions in these methods are often overly simplistic. To enhance fusion accuracy, we extract enriched textual descriptions from visible and infrared images with their predicted depth, using them as priors to guide the fusion process.",
            "score": 0.515872892246823,
            "section_title": "C. language-Vision Models",
            "char_start_offset": 6252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1502
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 146,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 419,
                    "end": 423,
                    "matchedPaperCorpusId": "232428282"
                },
                {
                    "start": 465,
                    "end": 469,
                    "matchedPaperCorpusId": "209202273"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "244909410"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 889,
                    "end": 893,
                    "matchedPaperCorpusId": "268680542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66162109375
        },
        {
            "corpus_id": "273821495",
            "title": "TypeScore: A Text Fidelity Metric for Text-to-Image Generative Models",
            "text": "Image Generation with Embeded Text Advances in image generation models have significantly improved the quality of synthetic images. Models such as DALL-E 3 [Ramesh et al., 2021], Stable Diffusion 3 [Rombach et al., 2022], ideogram [Liu et al., 2023], and MidJourney [MidJourney, 2022] have shown remarkable progress in producing diverse and high-quality visuals. Despite their success and rapid improvement, these models still encounter challenges in generating text with high fidelity, often producing text with typos, repeated or missing characters, and extraneous glyphs. Many methods have been proposed to improve the fidelity of embedded text in the generation. TextDiffuser [Chen et al., 2023b] addresses these issues by using a two-stage process: first, a Transformer model generates the layout of keywords from text prompts, and then diffusion models generate images conditioned on these layouts. TextDiffuser-2 [Chen et al., 2023c] further enhances text rendering by integrating large language models for layout planning and text encoding, enabling more flexible and diverse text generation. AnyText [Tuo et al., 2023] takes a different approach by focusing on multilingual visual text generation and editing, leveraging a diffusion pipeline to first mask the image and then employ an Optical Character Recognition (OCR) model to encode stroke data as embeddings to generate texts that can integrate with the background. However, these models typically involve multiple components, and generating text with both high fidelity and aesthetic and natural style remains challenging, as high-fidelity text generation frequently sacrifices rendering quality and artistic value. \n\nText Fidelity Evaluation Metrics and Datasets Traditional text fidelity metrics such as CIDEr [Vedantam et al., 2015], SPICE [Anderson et al., 2016], and BLEU [Papineni et al., 2002a] have been widely used for evaluating image captions. CIDEr focuses on consensus in large datasets, SPICE uses scene graph structures for more detailed semantic evaluation, and BLEU measures n-gram precision against reference texts. While these metrics have been foundational, they sometimes fall short in capturing the holistic meaning.",
            "score": 0.5157477818037207,
            "section_title": "Related Work",
            "char_start_offset": 5642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 220,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 920,
                    "end": 940,
                    "matchedPaperCorpusId": "265466475"
                },
                {
                    "start": 1777,
                    "end": 1800,
                    "matchedPaperCorpusId": "9026666"
                },
                {
                    "start": 1808,
                    "end": 1831,
                    "matchedPaperCorpusId": "11933981"
                },
                {
                    "start": 1842,
                    "end": 1866,
                    "matchedPaperCorpusId": "11080756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "271310517",
            "title": "Visual Text Generation in the Wild",
            "text": "(3) Random selection of text attributes results in text pixels that exhibit obvious artifacts. \n\nRecently, diffusion models have demonstrated state-of-the-art fidelity and diversity in image generation, and there have been some cutting-edge works [5,6,55,61] exploring their capabilities in visual text generation. As shown in Fig. 2 (b), diffusion-based visual text generation generally generates images directly through prompt text or additionally combines text-related priors, such as text layout, content, etc. Owing to the semantic alignment ability of prompt text, the foreground texts and background images are more integrated than rendering-based methods. However, existing methods demonstrate limited diversity in terms of text region and text scale, as they exhibit a strong propensity to generate text in constrained regions [5] and encounter difficulties in generating smaller-scale characters [55] following the VAE encoding and latent diffusion paradigm. Furthermore, their utility is compromised by inaccurate text annotations, as the generated texts often fail to align with the regions indicated in the conditions and superfluous texts may also be generated outside of the specified regions. \n\nIn this paper, we explore visual text generation in real-world scenes and propose a new method called SceneVTG. As shown in Fig. 2 (c), SceneVTG follows a two-stage paradigm and consists of a Text Region and Content Generator (TRCG) and a Local Visual Text Renderer (LVTR). In TRCG, we leverage the visual reasoning ability of Multimodal Large Language Models (MLLMs) to identify suitable text regions on the background image across multiple scales and recommend contents that are contextually coherent and visually fitting. Additionally, TRCG can generate curved text regions and provide accurate text annotations. In LVTR, we utilize a local conditional diffusion model rather than relying on latent features of the whole image, enabling text generation at arbitrary scales. Specifically, based on the output of TRCG, LVTR constructs image-level and embedding-level conditions to generate local text regions with consistent backgrounds and then embed them into the entire image to finally achieve realistic scene text image generation.",
            "score": 0.515400127056649,
            "section_title": "Introduction",
            "char_start_offset": 2125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 97,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2248
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 252,
                    "matchedPaperCorpusId": "258762187"
                },
                {
                    "start": 255,
                    "end": 258,
                    "matchedPaperCorpusId": "258960586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.533203125
        },
        {
            "corpus_id": "258762543",
            "title": "DiffUTE: Universal Text Editing Diffusion Model",
            "text": "Typically, a cropped text region with the target style is needed as the reference image. Such methods then transfer a rendered text in the desired spelling to match the reference image's style and the source image's background. However, the two major challenges for scene text editing remains. (i) These methods are currently constrained to editing English and fail to accurately generate complex text style (e.g., Chinese). (ii) The process of cropping, transferring style and blending results in less natural-looking outcomes. End-to-end pipelines are needed for the consistency and harmony. \n\nTo address the above issues, we present DiffUTE, a general diffusion model designed to tackle high-quality multilingual text editing tasks. DiffUTE utilizes character glyphs and text locations in source images as auxiliary information to provide better control during character generation. As shown in Figure 1, our model can generate very realistic text. The generated text is intelligently matched to the most contextually appropriate text style and seamlessly integrated with the background while maintaining high quality. \n\nThe major contribution of this paper is the universal text edit diffusion model proposed to edit scene text images. DiffUTE possesses obvious advantages over existing methods in several folds: \n\n1. We present DiffUTE, a novel universal text editing diffusion model that can edit any text in any image. DiffUTE generates high-quality text through fine-grained control of glyph and position information. DiffUTE is capable of seamlessly integrating various styles of text characters into the image context, resulting in realistic and visually pleasing outputs. \n\n2. We design a self-supervised learning framework that enables the model to be trained with large amounts of scene text images. The framework allows the model to learn from the data without annotation, making it a highly efficient and scalable solution for scene text editing. \n\n3. We conduct extensive experiments to evaluate the performance of DiffUTE. Our method performs favorably over prior arts for text image editing, as measured by quantitative metrics and visualization.",
            "score": 0.5153425403159198,
            "section_title": "Introduction",
            "char_start_offset": 2137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1961
                },
                {
                    "start": 1964,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2164
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "276249583",
            "title": "A Generative Framework for Bidirectional Image-Report Understanding in Chest Radiography",
            "text": "MAViLT is based on a pre-trained large language model (LLM) extended to multimodal settings. It adopts a generative architecture that processes and outputs both textual and visual data. The key components of MAViLT include: \n\n\u2022 Tokenization Module: VQ-GAN-based tokenization is used to encode images into discrete latent tokens, which are integrated with textual tokens in a shared embedding space. \n\n\u2022 Multimodal Embedding Layer: The LLM's embedding table is expanded to accommodate image tokens, allowing joint modeling of text and visual data. \n\n\u2022 Decoder: The decoder generates sequences for the specified modality (text or image) in an autoregressive manner. Formally, given an input x, which consists of visual inputs x v (e.g., CXR images) and textual inputs x t (e.g., radiology reports), the goal is to model the joint distribution: \n\ndepending on the specific task. The model generates outputs y by maximizing the conditional likelihood: \u0177 = arg max y P (y|x). \n\n(2)",
            "score": 0.5150736837544448,
            "section_title": "A. Model Overview",
            "char_start_offset": 10688,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 976
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80908203125
        },
        {
            "corpus_id": "272987668",
            "title": "From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models",
            "text": "By keeping the image encoder static, the model minimizes the computational burden on frame embedding. \n\nInspired by Flamingo [121], OpenFlamingo [134] is a framework that generates autoregressive vision-language models utilizing CLIP for image encoding and open-source language models for decoding. It handles interleaved sequences of images and text tokens, facilitating their interaction through dense cross-attention modules. Furthermore, the framework embeds images utilizing a trainable Perceiver resampler. OpenFlamingo undergoes training using a combination of image-text pairs and interleaved image-text sequences. \n\nOtter [135], an in-context instruction-tuned multimodal model, derives from OpenFlamingo [134], a large-scale, interleaved image-text pretraining foundation. It integrates linguistic and visual inputs to augment perception, reasoning, and planning functions. Designed for contextual instruction adherence, Otter excels in multi-turn conversational reasoning and acts as an egocentric visual assistant, leveraging video and image sequences for indoor navigation and event planning. \n\nGato [136] is designed to train on diverse data types such as images, text, observations, and actions. The information is then converted into sequential tokens to effectively handle this multi-modal data. During deployment, Gato utilizes these tokens to generate contextual responses or actions. By employing a joint visual-language model, Gato is capable of understanding and generating responses using both text and visual data, enabling it to handle tasks that involve the comprehension of both visual and textual information. \n\nThe enhanced PaLI-X [137] model represents an advancement of the original PaLI, emphasizing Optical Character Recognition (OCR) capabilities with a substantial visual component. This model utilizes a more complex, twostage training process wherein only the augmented visual encoder [138] remains unchanged in the initial stage. In the subsequent stage, potential augmentations to the visual component correspond with model progression and escalated image resolution parameters. The PaLI-X model employs an extensive UL2 [139] encoder-decoder infrastructure comprising 50 equivalent layers and effectively integrates visual and token embeddings.",
            "score": 0.5147931839274973,
            "section_title": "Multimodal Pre-training Foundation Models",
            "char_start_offset": 41534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 130,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1922,
                    "end": 1927,
                    "matchedPaperCorpusId": "256808367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "273098417",
            "title": "Enhancing Screen Time Identification in Children with a Multi-View Vision Language Model and Screen Time Tracker",
            "text": "In the Feature Synthesis layer, text embeddings were normalized and concatenated to ensure the brevity of image captions. These concatenated embeddings were then processed through a linear alignment layer, ensuring dimensional consistency with the embeddings used in the Swin Transformer model. In particular, the text embeddings extracted from this phase serve as the annotation for the following text generation model. We adopt the MiniLM for processing the text input for two reasons: MiniLM maps input texts of varying lengths into fixed-length embeddings, enhancing the robustness and scalability of the proposed framework. Moreover, by using the embeddings as input, the proposed framework can focus on high-level semantic information from the text. \n\nAlignment layer. The alignment layer is a fullyconnected layer which projects embeddings of multimodal data to the LLMs' input space. The fused multi-modal data by concatenation is sent to the alignment layer. Then processed embeddings are sent to the LLM for screen identification. The alignment layer has trainable parameters that can be updated during the training process, in order to achieve the optimal projection of the multi-modal data to the LLM's input space. \n\nText Generation (Llama). In this study, we used a large language model, Llama2 \u2212 7B [40], to efficiently produce fast scene description regarding the contents in the Multi-View images. We used one set of fully connected layers, which served as a soft prompt, to align the visual features with the large language model. Similarly, another set of alignment layers connected the text embeddings with the large language model. The training of text generation model was an optimization process for alignment layers, where \u03b8 v and \u03b8 t corresponded to the weights in alignment for vision model and the weights in alignment layers for text mode. The loss function for the report generation task was jointly optimized by the combination of loss terms from each task:",
            "score": 0.5147498401726387,
            "section_title": "Vision Language Model",
            "char_start_offset": 17630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1987
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408935546875
        },
        {
            "corpus_id": "237259955",
            "title": "Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling",
            "text": "The Text-VQA task aims at reading and understanding the text captured in the images to answer visual questions. Although the importance of involving scene texts in visual question answering tasks was originally emphasized by [2], due to the lack of available large-scale datasets, early development of question answering tasks related to embedded text understanding was limited in narrow domains such as bar-charts [14] or diagrams [17,18]. The first large-scale open-domain dataset of the Text-VQA task, TextVQA, was introduced by [28], followed by several similar works including STVQA [3], OCR-VQA [24] and EST-VQA [31].\n\nRecent studies [28,12,15,33,7,8,22,11] have proposed several models and network architectures for the Text-VQA task. Introduced together with the TextVQA dataset, the LoRRA [28] model is built upon Pythia [13] with an OCR module added to detect and recognize the scene texts. M4C [12] first models Text-VQA as a multimodal task and uses a multimodal transformer to fuse different features over a joint embedding space. Also, it attaches a pointer network that can dynamically copy words from OCR systems. SA-M4C [15] added spatial information between objects, OCR, and question tokens to implicitly capture their relationships based on M4C to get further improvement. TAP [33] proposed to pretrain the model on several auxiliary tasks such as masked language modeling (MLM) and relative position prediction (RPP). It also leverages additional large-scale OCR datasets to enhance its ability to capture the contextualized information of scene text.\n\nAlthough some previous works reported better results obtained by purely changing the OCR system [15,33], they either don't fully realize the potential of modeling the text modality with information from the visual modality or use expensive large-scale OCR data for pretraining. In this paper, we look deeper into other solutions to ground and refine features in the textual modality within existing datasets to facilitate the understanding of scene text in the multimodal fusion process. Figure 2 demonstrates the LOGOS",
            "score": 0.5146978884727909,
            "section_title": "Related Works",
            "char_start_offset": 3626,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "52804681"
                },
                {
                    "start": 415,
                    "end": 419,
                    "matchedPaperCorpusId": "4445015"
                },
                {
                    "start": 432,
                    "end": 436,
                    "matchedPaperCorpusId": "2682274"
                },
                {
                    "start": 436,
                    "end": 439,
                    "matchedPaperCorpusId": "1310550"
                },
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 588,
                    "end": 591,
                    "matchedPaperCorpusId": "173188651"
                },
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "209413409"
                },
                {
                    "start": 618,
                    "end": 622,
                    "matchedPaperCorpusId": "211258929"
                },
                {
                    "start": 640,
                    "end": 644,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 644,
                    "end": 647,
                    "matchedPaperCorpusId": "208006464"
                },
                {
                    "start": 655,
                    "end": 657,
                    "matchedPaperCorpusId": "214727709"
                },
                {
                    "start": 657,
                    "end": 660,
                    "matchedPaperCorpusId": "222278468"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 905,
                    "end": 909,
                    "matchedPaperCorpusId": "208006464"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475341796875
        },
        {
            "corpus_id": "257364842",
            "title": "PaLM-E: An Embodied Multimodal Language Model",
            "text": "General vision-language modeling. Building on successes in large language (Brown et al., 2020;Devlin et al., 2018) and vision (Dosovitskiy et al., 2020) models, recent years have seen a growing interest in large vision-language models (VLMs) (Li et al., 2019;Lu et al., 2019;Hao et al., 2022;Gan et al., 2022). Unlike their predecessors, VLMs are capable of simultaneously understanding both images and text, and can be applied to tasks such as visual question answering (Zhou et al., 2020;Zellers et al., 2021b), captioning (Hu et al., 2022), optical character recognition (Li et al., 2021), and object detection (Chen et al., 2021b). The methods by which images are integrated varies. For example, Alayrac et al. (2022) augments pretrained language models with a mechanism to directly attend to a single context image. In contrast, PaLM-E represents images and text as \"multimodal sentences\" of latent vectors, allowing it to process multiple images in a flexible way within any part of a sentence. More closely related to our work is Frozen (Tsimpoukelli et al., 2021) where vision encoder parameters are optimized via backpropagation through a frozen LLM (Lu et al., 2021). Inspired by this work, we investigate the design in a broader scope by introducing alternative input modalities (e.g. neural scene representations), and our proposed approach empirically outperforms Frozen by more than 45% on the VQAv2 benchmark. More importantly we demonstrate that PaLM-E is applicable not only to perceptual but also embodied tasks. \n\nActions-output models.",
            "score": 0.5145833890375909,
            "section_title": "Related Work",
            "char_start_offset": 7176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1555
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 94,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 471,
                    "end": 490,
                    "matchedPaperCorpusId": "202734445"
                },
                {
                    "start": 525,
                    "end": 542,
                    "matchedPaperCorpusId": "244527510"
                },
                {
                    "start": 1044,
                    "end": 1071,
                    "matchedPaperCorpusId": "235658331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "275920966",
            "title": "Turn That Frown Upside Down: FaceID Customization via Cross-Training Data",
            "text": "Text-to-image refers to the process of generating images from textual descriptions using pre-trained image generation models [24,6,7,23,25,27,13]. These models are trained to understand the relationship between textual input and visual content, enabling them to create images that match the given description. Thanks to the success of the transformer model [33], most early text-to-image approaches can be broken down into two stages: (1) using an image encoder, such as DARN [10], PixelCNN [31], Pix-elVAE [11], or VQ-VAE [32], to convert an image into several tokens; and (2) training the model to predict these image tokens based on the provided text input within the transformer framework [33]. Recently, diffusion models [28,29,18,5,23,27,25,2,13] have emerged as the new state-of-the-art approach for image generation, offering innovative solutions for the text-to-image task. In this approach, the text prompt is first encoded into embeddings using a pre-trained language model such as T5 [22] or CLIP [21], and then these encoded embeddings are used to guide the diffusion process, resulting in the generation of highquality images. For example, GLIDE [18] employs a cascaded diffusion architecture with CLIP [21] as the text encoder to condition on natural language descriptions, facilitating both image generation and editing. Imagen [27] adopts T5 [22], a generic large language model pre-trained on text-only corpora, as the text encoder of diffusion models, to further enhance the text understanding.",
            "score": 0.5139139387084635,
            "section_title": "Text-to-image Models",
            "char_start_offset": 5002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1513
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 129,
                    "end": 131,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 131,
                    "end": 133,
                    "matchedPaperCorpusId": "248476190"
                },
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "14989939"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5830078125
        },
        {
            "corpus_id": "271310477",
            "title": "Urban Visual Appeal According to ChatGPT: Contrasting AI and Human Insights",
            "text": "The advent of Large Language Models (LLMs) such as GPT-3 (28) and BERT (29) has not only revolutionised AI with sophisticated text generation and understanding, but also democratised interactions with AI technologies (30).These models enable users to execute commands, optimise and fine-tune AI responses, and engage in nuanced interactions without requiring deep AI expertise.This suite of LLMs illustrates the leap towards intuitive, accessible technology, transforming user interactions across various domains.This capability is particularly crucial for our study, allowing for the customisation of analysis criteria.Nonetheless, the application of LLMs is limited, because it lacks the ability to process visual media, essential for assessing urban visual appeal.\n\nThe emergence of MLLMs like GPT-4 (31) and Kosmos-2.5 (32,33), which integrate the textual interaction capability of LLMs with image analysis, presents a novel solution.Early studies in the field were dedicated to understanding and generating text based on multimodal inputs, focusing on how models interpret the relationship between visual elements and text.This research area benefited greatly from projects like BLIP-2 (Bootstrapping Language-Image Pre-training-2) (34), CLIP (Contrastive Language-Image Pretraining) (35), and LLaVA (Large Language-and-Vision Assistant) (36).As the field evolved, the scope of MLLMs broadened to include generating outputs specific to various modalities.These models emphasise the use of modality encoders, LLM backbones, and modality generators to process and generate multimodal content efficiently (37).\n\nNotable in this development is the flexibility in input representation, allowing for seamless integration of several data types into the LLM framework.\n\nThis advancement provides new options for applications requiring comprehensive analysis of both visual and textual information, such as an urban area's visual assessment, that can potentially benefit spatial planning.Some early studies have explored this field, including the study by Jongwiriyanurak et al. (38), which used LLaVA by prompting six questions to gather information on various factors considered critical in assessing motorcycle crash risks.",
            "score": 0.5137854474917413,
            "section_title": "AI in Sentiment and Multimodal Analysis",
            "char_start_offset": 5160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 767
                },
                {
                    "start": 769,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1460
                },
                {
                    "start": 1460,
                    "end": 1612
                },
                {
                    "start": 1614,
                    "end": 1765
                },
                {
                    "start": 1767,
                    "end": 1984
                },
                {
                    "start": 1984,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 217,
                    "end": 221,
                    "matchedPaperCorpusId": "258520642"
                },
                {
                    "start": 823,
                    "end": 827,
                    "matchedPaperCorpusId": "257219775"
                },
                {
                    "start": 1289,
                    "end": 1293,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1343,
                    "end": 1347,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "262825203",
            "title": "VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning",
            "text": "Recent text-to-video (T2V) generation methods have seen significant advancements. However, the majority of these works focus on producing short video clips of a single event (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules. This prompts an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which includes the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency groupings of the entities. Next, guided by this video plan, our video generator, named Layout2Vid, has explicit control over spatial layouts and can maintain temporal consistency of entities across multiple scenes, while being trained only with image-level annotations. Our experiments demonstrate that our proposed VideoDirectorGPT framework substantially improves layout and movement control in both single- and multi-scene video generation and can generate multi-scene videos with consistency, while achieving competitive performance with SOTAs in open-domain single-scene T2V generation. Detailed ablation studies, including dynamic adjustment of layout control strength with an LLM and video generation with user-provided images, confirm the effectiveness of each component of our framework and its future potential.",
            "score": 0.5137836324458982,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.442138671875
        },
        {
            "corpus_id": "273346517",
            "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
            "text": "\"True evaluation lies in the seamless interweaving of diverse modalities.\" \n\nMultimodal learning has made remarkable progress with the development of Large Vision-Language Models (LVLMs) (Liu et al., 2023a;Zhu et al., 2023;Dai et al., 2023), which are capable of handling diverse tasks that involve both images and text. Despite their advancements, most of these models are limited to multimodal tasks for text generation, such as visual question answering (VQA) and image captioning, which do not fully reflect the potential of multimodal capacity. To broaden their application, interleaved text-and-image generation has emerged as a critical area of research (Liu et al., 2024). It requires models to generate images and text in any sequence, thereby enhancing the versatility and effectiveness of multimodal systems. It opens up possibilities for various complex applications, such as multi-step inference (Lu et al., 2024;Kazemi et al., 2024), multimodal situational analysis (Yang et al., 2021), and visual storytelling (Huang et al., 2016). \n\nWhile recent LVLMs are evolving to support interleaved text-and-image generation (Team, 2024;Xie et al., 2024;Chern et al., 2024;Zhou et al., 2024), a comprehensive evaluation benchmark is still falling behind due to the following two challenges: Figure 1: Typical samples from the MMIE Benchmark showcase its support for multiple image inputs and outputs, with ground truth provided for every query. MMIE evaluates models across diverse fields, ensuring a comprehensive evaluation of their capabilities. \n\n\u2022 Difficulty in Constructing Modality-Coherent Benchmarks. The first challenge lies in the difficulty of constructing modality-aligned multimodal datasets, where both the input and output contain images and text.",
            "score": 0.5135492427720594,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 77,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 909,
                    "end": 926,
                    "matchedPaperCorpusId": "264491155"
                },
                {
                    "start": 1025,
                    "end": 1045,
                    "matchedPaperCorpusId": "2574224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "271097376",
            "title": "SEED-Story: Multimodal Long Story Generation with Large Language Model",
            "text": "Interleaved image-text data is ubiquitous on the internet, characterized by multiple images interspersed with text.In recent years, there has been a surge of interest in generating interleaved image-text content [52,16,1,11,49], driven by the remarkable advancements in image generation [43,26,9,36,54] and open-form text generation [53,48,59].This has given rise to Multimodal Story Generation, an intriguing and valuable task that involves the generation of narrative texts and vivid images in an interleaved manner.This task goes beyond traditional text-based storytelling by fusing text and visuals to enrich narratives, creating a more immersive storytelling experience where visuals and text dynamically interact and complement each other.\n\nHowever, multimodal story generation poses significant challenges due to the complexity of the inputs and the high demands on the quality of the outputs.Firstly, this task necessitates a thorough comprehension of interleaved data, where text is not only abstract and narrative in nature, but also deeply intertwined with complex images.The model must be adept at deciphering the intricate relationships between images and texts to maintain a coherent narrative flow.Secondly, this task requires the generation of not only a plausible text plot, but also visually captivating images that are consistent in characters and styles.The model should be capable of achieving coherence in the generation of both text and visuals, ensuring an engaging storytelling output.\n\nRecently, Multimodal Large Language Models (MLLMs) [23,60,39,5,28,58,26,22] have showcased powerful comprehension abilities in understanding multimodal data, which makes them ideally suited for interleaved image-text content in multimodal stories.Consequently, we introduce SEED-Story, a novel approach that builds upon the MLLM to harness its comprehension strength, while further equipping it with the capability to generate coherent images align with the narrative texts.\n\nSpecifically, following previous work [45,16], we utilize a pre-trained image tokenizer and detokenizer, which can decode realistic images with SD-XL [41] by taking the features of a pre-trained ViT as input.",
            "score": 0.5133203469338304,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 518
                },
                {
                    "start": 518,
                    "end": 745
                },
                {
                    "start": 747,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 1083
                },
                {
                    "start": 1083,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1510
                },
                {
                    "start": 1512,
                    "end": 1759
                },
                {
                    "start": 1759,
                    "end": 1986
                },
                {
                    "start": 1988,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "232428282"
                },
                {
                    "start": 1563,
                    "end": 1567,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "matchedPaperCorpusId": "259287020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "272146286",
            "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
            "text": "Recent developments in large language models have progressed rapidly, with models becoming increasingly powerful, such as off-the-shelf Llama 3.1 [meta, 2024], Mixtral [mixtral, 2024], Qwen-2 [Yang et al., 2024a], and the well-known GPT-4. As an extension of their capabilities, language models are beginning to master understanding other modalities, exemplified by LLaVA [Liu et al., 2024], Qwen2-Audio [Chu et al., 2024] and Video-llama [Zhang et al., 2023b]. Despite their strength in specific tasks, a significant gap remains that hinders further integration of large language models into daily application: real-time voice interaction capability. GPT-4o [openai, 2024], introduced by OpenAI, is the first model to feature real-time multimodal speech interaction capabilities. It can understand and engage with vision, audio, and text while enabling real-time speech conversations, although it remains closed-source. Other models typically adopt two approaches to incorporate speech capabilities. The first is a cascade method, where the language model generates text, followed by a text-to-speech (TTS) model for audio synthesis. This approach introduces significant latency due to the time required for text generation, severely impacting user experience. The second, an end-to-end method like SpeechGPT [Zhang et al., 2023a], generates text before continuing to generate audio. However, this still requires waiting for text generation. Large language models need real end-to-end speech output capabilities to provide real-time feedback. \n\nEnhancing models with speech output capabilities is a challenging task, primarily due to four factors: \n\n(1) Complexity of Audio Reasoning: Our experiments indicate that direct training for audio modality reasoning is highly challenging, often resulting in incoherent outputs from the model. (2) Model Complexity: Incorporating additional modules for speech input and output increases the overall complexity. (3) Difficulty in Modality Alignment: The reasoning abilities developed for text are difficult to transfer to the audio domain.",
            "score": 0.513254833551572,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 390,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "259108357"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "267523393",
            "title": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding",
            "text": "Our model architecture as shown in Figure 1 is inspired by the architecture of the PaLI family of models [Chen et al., 2022;Chen et al., 2023a;Chen et al., 2023b], which is composed of a multimodal encoder block with a vision encoder like ViT [Dosovitskiy et al., 2020] and a mT5 [Xue et al., 2020;Raffel et al., 2020] language encoder consuming image and text inputs, followed by an autoregressive decoder. The input image is transformed into a sequence of embeddings by the vision encoder and these embeddings are concatenated with the input text embeddings and fed into the mT5 language encoder. The output of this encoder is passed to the decoder to generate the text output. This generic formulation enables us to use the same model architecture to solve a variety of vision and multimodal tasks that can be recast as a text+image (input) to text (output) problem. Compared to the text input, the image embeddings constitute a significant portion of the input length to the multimodal encoder. \n\nWe further extend PaLI's encoder-decoder architecture to accept various image patching patterns. The original PaLI architecture only accepts a fixed grid pattern of patches for processing the input images. However, the data we encounter in screen-related domains spans a wide variety of resolutions and aspect ratios. To have a single model to work across all screen shapes, it is necessary to use a patching strategy which can work well with images of various shapes. To this end, we borrow a technique introduced in Pix2Struct [ Lee et al., 2023], which allows us to have image patches with arbitrary grid shapes based on the input image shape and a pre-defined maximum number of patches, as shown in Figure 1. This enables us to accommodate input images of various formats and aspect ratios without the need for padding or stretching the image to a fixed shape, making our model more polyvalent to handle both mobile (i.e. portrait) and desktop (i.e. landscape) image formats.",
            "score": 0.5132434251043166,
            "section_title": "Architecture",
            "char_start_offset": 6505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 124,
                    "matchedPaperCorpusId": "231698419"
                },
                {
                    "start": 143,
                    "end": 162,
                    "matchedPaperCorpusId": "6623010"
                },
                {
                    "start": 298,
                    "end": 318,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1532,
                    "end": 1549,
                    "matchedPaperCorpusId": "252762394"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "274514993",
            "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators",
            "text": "Image Tokenizer. We use a VQGAN [15] from Chameleon [64] as image tokenizer. It encodes a 512 \u00d7 512 image into 32 \u00d7 32 discrete tokens in a codebook of size 8192, which are flattened into a 1024 image token for LLM. In LLMs such as Llama [69] and Gemma [65], the raw text input will also be tokenized into numbers (discrete IDs) representing the positions of the words in the vocabulary by the BPE [58] tokenizer. For example, \"a small dog\" is tokenized into [235250,2301,5929] in Gemma, the vocabulary size is 256,000. Therefore, we only need to extend the entire vocabulary size to 256, 000 + 8192, where the former for text and the latter for image. In this way, both vision and language are represented as sequences of discrete IDs, whether for input or output. After that, IDs index the input embeddings through an embedding-layer, thus completing the unification of the embedding space. Correspondingly, we extend the original LM head by 8192 dimensions to enable the model to predict both text and image tokens within the same embedding space. \n\nArchitecture. As illustrated in Fig. 2, Liquid can be constructed based on any existing LLMs. In this paper, we employ the GEMMA-7B [65] as base model of Liquid to validate its multimodal understanding, image generation capabilities, and performance on text-only tasks after augmenting it with the ability to understand and generate images. To further investigate whether a trade-off exists when accommodating visual generation tasks and text generation tasks within the same LLM space, as well as to explore the scaling performance of different sized models, we conduct same training on Llama-3 1B [14], GEMMA-2 series [66] at scales of 2B, 9B, and Qwen2.5 series [23] at scales of 0.5B, 7B, 32B, thereby observing their distinct performances. We refrain from altering any structures within the LLMs to facilitate continued training directly from pre-trained weights.",
            "score": 0.5127937115290587,
            "section_title": "Preliminaries",
            "char_start_offset": 6427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "1114678"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "268537142",
            "title": "Text\u2010to\u20103D Shape Generation",
            "text": "1 Introduction\n\nText to 3D shape generation methods can revolutionize 3D content creation by allowing anyone to generate 3D content based on a simple text description.It is no wonder that there has been an explosion of interest in this research direction.Recent advances in generative models for text and images [RDN*22;SCS*22] enabled by largescale language and vision-language models, as well as advances in learned 3D representations and 3D generative models have acted as catalysts for progress in text to 3D shape generation.\n\nAt the same time as this rapid progress, a number of open research challenges are coming into focus.There is currently a spar-sity of available 3D data paired with natural language text descriptions, making it infeasible to rely purely on direct supervision from data pairs in both domains.Moreover, current text to 3D generation methods do not afford natural editability of the generated outputs in an intuitive way based on user inputs.Generation of larger-scale 3D outputs representing compositions of objects into natural scenes also remains challenging.Lastly, the complexity of underlying 3D generative models coupled with the complexity of the optimization problem when avoiding reliance on paired text and 3D data lead to a challenging learning problem with significant compute and training time requirements.In 3DUT, data is limited to only 3D shapes, so methods in this group leverage the aligned text-3D embedding space of CLIP encoders allowing conditioning on rendered images during training and text prompts during inference.In No3D, no data is available so methods rely on pretrained guidance models such as CLIP and T2I diffusion models to optimize similarity and distillation respectively with differentiable 3D representations.Finally, Hybrid3D methods often combine techniques from NO3D with priors learned from 3D assets to train 3D-aware T2I models or to enable text-to-image-to-3D using pretrained large text-to-image models.\n\nThere have been some recent related surveys addressing 3D generative models [CRW*20; SPX*22] and text-conditioned generation [CG23;LZW*23].The former group of surveys on 3D generative models addresses what is typically one component of a complete text-to-3D shape system.",
            "score": 0.5127426078181266,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 16,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 530
                },
                {
                    "start": 532,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1090
                },
                {
                    "start": 1090,
                    "end": 1349
                },
                {
                    "start": 1349,
                    "end": 1571
                },
                {
                    "start": 1571,
                    "end": 1777
                },
                {
                    "start": 1777,
                    "end": 1979
                },
                {
                    "start": 1981,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25927734375
        },
        {
            "corpus_id": "273010220",
            "title": "PERCEPTION AND EVALUATION OF TEXT-TO-IMAGE GENERATIVE AI MODELS: A COMPARATIVE STUDY OF DALL-E, GOOGLE IMAGEN, GROK, AND STABLE DIFFUSION",
            "text": "The research landscape in text-to-image generation is still relatively nascent, with a limited number of works exploring this emerging field. While interest in text-to-image synthesis has been growing, particularly in recent years, the volume of literature remains relatively modest compared to more established areas of machine learning and computer vision. In the context of image tuning, et al. Xu et al. (2023) introduced UniTune, an innovative approach to image editing that accepts any image along with a textual description of the desired edit. It is capable of executing the modification while preserving the original image's quality. Unlike other methods, UniTune doesn't rely on additional inputs like masks or sketches and can handle multiple edits without retraining. In evaluations against another similar potential model, SDEdit, UniTune demonstrated a clear advantage, with a 72% preference over SDEdit's 28%. These results indicate that while both methods excel when edits are minor, UniTune outperforms significantly in scenarios requiring substantial pixel alterations, such as object duplication, movement, or resizing. Xu et al. (2023) extended the original single-flow diffusion pipeline into a versatile multi-task multimodal network called Versatile Diffusion (VD), capable of handling various tasks such as text-to-image and image-to-text conversions within a single unified model. VD comprises three key components: a diffuser that operates within a multi-flow multimodal framework, variational autoencoders (VAEs) for converting data samples into latent representations, and context encoders for embedding contextual information. In comparison to existing models like CogView, LAFITE, GLIDE, and Make-a-Scene, VD demonstrates superior FID performance, with a score of 11.21 \u00b10.03 compared to 11.10 \u00b10.09. Elarabawy et al. (2022) developed a method for featuring multiple easily adjustable hyperparameters for enabling a diverse array of real image edits. This method, termed as optimization-free and zero fine-tuning, relies on text-based semantic instructions for flexible editing. Unlike approaches generating numerous outputs and relying on additional mechanisms for filtering, this method allows for systematic modulation of target edits.",
            "score": 0.5126702532379932,
            "section_title": "Literature Review",
            "char_start_offset": 4852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 414,
                    "matchedPaperCorpusId": "253523371"
                },
                {
                    "start": 1139,
                    "end": 1155,
                    "matchedPaperCorpusId": "253523371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39501953125
        },
        {
            "corpus_id": "276961121",
            "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing",
            "text": "It requires a new formulation and collecting training data in this new format. Second, existing diffusion-based models cannot leverage explicit language reasoning chains during visual generation. We need to design a framework supporting end-to-end language reasoning and visual generation. \n\nTo address the first challenge, we formulate GoT as a multimodal reasoning chain that integrates semantic and spatial analyses to enhance image generation and editing tasks. For visual generation, GoT provides precise control over object layout, relationships, and attributes, while for editing, it leverages semantic and spatial understanding to decompose user requests into coherent grounding and modification steps. We utilize advanced MLLMs and LLMs to construct complex annotation pipelines, which capture semantic-spatial interactions across diverse visual contexts. We assembled extensive datasets comprising 8.4M images for text-to-image generation (from Laion-Aesthetics [39], JourneyDB [41], and FLUX [21]) and 920K examples for image editing (from OmniEdit [48] and SEED-Edit-Multiturn [12]). This computationally intensive effort produced the first large-scale dataset of reasoning chains for image generation and editing. \n\nTo tackle the second challenge of architecture design supporting reasoning and generation, we construct a unified end-to-end framework. Our GoT framework integrates the reasoning capabilities of MLLMs with the high-fidelity generation qualities of diffusion models. The proposed framework leverages an MLLM to generate reasoning steps and visual tokens, providing explicit guidance that incorporates semantic relationships and spatial configurations. This guidance flows into our novel Semantic-Spatial Guidance Module (SSGM), which conditions the diffusion process to ensure that generated images are closely guided by the reasoning process. This design supports end-to-end training and inference for visual generation and editing guided by explicit reasoning chains. \n\nBy effectively integrating reasoning into visual generation, our GoT framework demonstrates significant improvements in both text-to-image generation quality and image editing accuracy. Additionally, GoT enables interactive generation, allowing users to control the generated image by directly modifying the explicit reasoning process according to their preferences. These advantages represent a substantial advancement in reasoning-guided visual synthesis.",
            "score": 0.5125234819964142,
            "section_title": "Introduction",
            "char_start_offset": 3613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1997
                },
                {
                    "start": 2000,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2366
                },
                {
                    "start": 2367,
                    "end": 2457
                }
            ],
            "ref_mentions": [
                {
                    "start": 988,
                    "end": 992,
                    "matchedPaperCorpusId": "259316541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7041015625
        },
        {
            "corpus_id": "270094821",
            "title": "Multi-Modal Generative Embedding Model",
            "text": "However, the lack of joint training with the visual modality results in a performance deficiency.\n\nTo explore the minimalism of multi-modal paradigms, we propose Multi-Modal Generative Embedding Model (MM-GEM) in this paper.MM-GEM is an end-to-end optimized model that combines two paradigms by encapsulating the generative and embedding objectives in the same language model.Specifically, for embedding, we align the image features with the sentence embeddings derived from the last token.Concurrently, for the generative task, we concatenate the image features with the word embeddings of the language model to execute the captioning process.Notably, both objectives leverage a shared forward path within the language model.To boost efficiency and enable the ability of fine-grained embedding and generation, we propose a PoolAggregator to represent an image by the feature map, instead of a global feature on [CLS] token [12,40].\n\nExperimental results demonstrate the superiority of MM-GEM.MM-GEM instantiated from ViT-Large and TinyLlama [53] achieves comparable results with OpenCLIP [19] on image-text retrieval benchmarks such as COCO [31] and Flickr30K [39], and zero-shot image classification benchmark ICinW [29].Meanwhile, MM-GEM shows competitive performance on image captioning benchmarks such as COCO [31] and NoCaps [1].Additionally, qualitative results show that MM-GEM can generate region-level image captions and fine-grained text-to-image retrieval without further training or modification.Besides, the advanced text module in MM-GEM brings better ability of text understanding.MM-GEM achieves over 5% higher Recall@1 for long text and image retrieval, compared to CLIP.\n\nOur contributions are summarized as follows:\n\n1. We propose a Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and embedding objectives are encapsulated to achieve unification.",
            "score": 0.5116140785393246,
            "section_title": "Introduction",
            "char_start_offset": 2036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 99,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 490
                },
                {
                    "start": 490,
                    "end": 644
                },
                {
                    "start": 644,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 932
                },
                {
                    "start": 934,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1509
                },
                {
                    "start": 1509,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1689
                },
                {
                    "start": 1691,
                    "end": 1735
                },
                {
                    "start": 1737,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 924,
                    "end": 928,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 928,
                    "end": 931,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1142,
                    "end": 1146,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 1161,
                    "end": 1165,
                    "matchedPaperCorpusId": "6941275"
                },
                {
                    "start": 1218,
                    "end": 1222,
                    "matchedPaperCorpusId": "248239757"
                },
                {
                    "start": 1315,
                    "end": 1319,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 1331,
                    "end": 1334,
                    "matchedPaperCorpusId": "56517630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.399658203125
        },
        {
            "corpus_id": "276929408",
            "title": "Recognition-Synergistic Scene Text Editing",
            "text": "The aim of scene text editing is to edit text image I A to synthesize image I B by altering the text content T A into the desired content T B while retaining the image style of I A . Our proposed RS-STE for this task is able to conduct text recognition and editing within a unified framework, resulting in a straightforward pipeline. As shown in Figure 3, it comprises Input Tokenizer, Multi-modal Parallel Decoder, and Image Detokenizer. \n\nGiven the target text content T B and a reference image I A , Input Tokenizer encodes them into text embeddings and image embeddings respectively, and outputs a cascaded embedding sequence. Then Multi-modal Parallel Decoder performs scene text editing in the feature space and predicts the tokens of T \u2032 A and I \u2032 B in a parallel manner. Lastly, Image Detokenizer generates target image I \u2032 B from decoded image tokens D i I B . While the generated I \u2032 B contains different text content from I A , their stylistic components including background and typeface are required to be completely identical. \n\nOur RS-STE is optimized in two learning stages. It is first trained on a large corpus of synthesized data with paired I A and I B to endow it with the basic capability of scene text editing. Then in the second stage, it is further optimized on unpaired real-world data (without ground-truth) using our specially designed cyclic self-supervised finetuning strategy, which substantially improves its robustness and generalizability towards real-world data. We will first describe the model structure of RS-STE in Section 3.2, and then elaborate on the training strategy in Section 3.3.",
            "score": 0.511588480146739,
            "section_title": "Overview",
            "char_start_offset": 7831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1626
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5595703125
        },
        {
            "corpus_id": "271719739",
            "title": "One Framework to Rule Them All: Unifying Multimodal Tasks with LLM Neural-Tuning",
            "text": "The overall pipeline of our proposed neural tuning framework is illustrated in Figure 1. The inputs from the different modalities are tokenized and processed concurrently, emulating the ability of humans to seamlessly handle multiple tasks. We also introduce a novel sparse task network into pretrained models, which functions as either the Sparse Distributed Representation (SDR) or the Sparse Coding Hypothesis (SCH), through which only a subset of neurons are activated, tailored to the specific task at hand. This paradigm enables the model to take both image and text inputs, generating task-specific outputs such as multi-instance referring segmentation (<OBJ>) and image generation (<GEN>). \n\nSpecifically, for a multimodal input consisting of images and sentences, we first embed the text into \n\n, where L t denotes the length of the text. Next, we use a pretrained vision encoder to extract features from the image, resulting in I img = {I l img } L i l=1 where L i representing the number of image patches. To integrate the multimodal input, we concatenate the text embeddings and image features (when the task requires visual input) to form the final input for the pretrained large language model (LLM): I = [I img ; I txt ]. In this scheme, the textual and visual modalities  )V with the Query(Q), Key(K), and Value(V) derived from the concatenated multimodal input I. Unlike prior works that employ cross-attention for modality interaction [6,15], where Q, K, and Vare derived from different modalities, our self-attention approach offers several advantages. Not only does it simplify the architecture by avoiding explicit cross-modal mappings, but it also enables the model to compute both inter-and intra-modal relationships. This dual interaction within the self-attention framework enhances the model's ability to understand and fuse multimodal information more effectively. \n\nIn the output stage of the LLM, we introduce new task-specific tokens alongside the original textual tokens to handle multimodal tasks.",
            "score": 0.5115079633044368,
            "section_title": "All-in-token Multimodal Paradigm",
            "char_start_offset": 12224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 1453,
                    "end": 1456,
                    "matchedPaperCorpusId": "260860453"
                },
                {
                    "start": 1456,
                    "end": 1459,
                    "matchedPaperCorpusId": "244909191"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52734375
        },
        {
            "corpus_id": "273025657",
            "title": "Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks",
            "text": "Multimodal large language models (MLLMs) have revolutionized vision-language tasks, driving advancements in a variety of areas such as image captioning and object detection [67,72,74]. These improvements extend to applications involving text-rich images where text serves as the primary visual element guiding image comprehension, such as visual document understanding [51] and scene text recognition [59]. Traditional OCR-based pipelines in these text-rich visual scenarios are being replaced by end-to-end approaches that directly encode intertwined multimodal inputs [63,69,75], leading to improved efficiency and accuracy in handling text-rich images. Despite these advancements, the majority of existing open-source MLLMs, like LLaVAR [75] and mPlug-DocOwl-1.5 [16], have primarily focused on optimizing performance for text-rich single-image tasks. This focus inherently limits their applicability in many real-world scenarios, where tasks often involve multiple inter-connected images. For instance, multi-page visual document understanding requires integrating information spread across different pages to capture the logical flow across the whole document [25,64]. To understand presentation slides, grasping the overarching narrative requires understanding multiple slides with unique but interrelated content [62]. These vision-language tasks on multiple text-rich images require advanced capabilities that go beyond merely recognizing text and visuals within a single image; they involve understanding and reasoning about relationships and logical flows across multiple visual inputs. While some models -such as OpenFlamingo [3], VILA [38], Idefics2 [28] -have made strides toward supporting multi-image inputs, they mainly focus on scenarios with natural images but fall short in understanding sequences of text-rich images with interrelated textual and visual information. We plot the performance of representatives of the aforementioned models in Figure 1. Upon examining their training data and model architecture, we identified two primary limitations. \n\nFirst, there is a scarcity of high-quality instruction tuning datasets on text-rich multi-image scenarios.",
            "score": 0.5113876607718273,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2069
                },
                {
                    "start": 2072,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "257038341"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "258959011"
                },
                {
                    "start": 180,
                    "end": 183,
                    "matchedPaperCorpusId": "257913547"
                },
                {
                    "start": 369,
                    "end": 373,
                    "matchedPaperCorpusId": "220280200"
                },
                {
                    "start": 401,
                    "end": 405,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1165,
                    "end": 1169,
                    "matchedPaperCorpusId": "258685443"
                },
                {
                    "start": 1320,
                    "end": 1324,
                    "matchedPaperCorpusId": "255749397"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "276961000",
            "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
            "text": "We propose a novel framework, CINEMA, for coherent multi-subject video generation, leveraging the powerful comprehending capabilities of MLLMs. Utilizing MM-DiT for video generation, CINEMA integrates multimodal conditional information through three key modules, i.e., multimodal large language model, semantic alignment network, and visual entity encoding. An overview of our framework is provided in Figure 2. The MLLM is employed to encode  multimodal conditions (e.g., images and text) into unified feature representations. Additionally, the semantic alignment network AlignerNet is introduced to bridge the gap between this unified representation space and the original DiT conditioning feature space. Lastly, the visual entity encoding is designed to capture fine-grained entity-level visual attributes. The architecture and functionality of each module are elaborated in the following subsection. \n\nMultimodal large language model. At the core of CIN-EMA lies the utilization of an MLLM. Specifically, Qwen2-VL [47] is adopted to generate unified representations from arbitrary reference images, and text prompts. These unified representations are then fed into the MM-DiT backbone, where multimodal joint attention mechanism facilitates interactive fusion of the encoded tokens. In addition, we employ an instruction tuning approach, incorporating a specialized instruction template to guide the MLLM for more accurate and contextually relevant encoding results. The specific instruction is as shown in Figure 3. \n\nSemantic alignment network. We build CINEMA on the basis of pre-trained, open-sourced video generative model model, CogVideoX [53], which is largely pre-trained on text/image-to-video task, encoding text prompts with a large language model [33]. To address the semantic representation gap across LLM and MLLM, we propose to replace T5 encoder with MLLM by introducing AlignerNet module. \n\nOur AlignerNet is a transformer-based network. In particular, AlignerNet maps the hidden states generated by the MLLM onto the feature space of the T5 text encoder, ensuring that the resulting multimodal semantic features encompassing both visual and textual information are wellaligned with the feature space of the T5 encoder.",
            "score": 0.5112751628016483,
            "section_title": "Video Generation via MLLM-Based Guidance",
            "char_start_offset": 12902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1909
                },
                {
                    "start": 1912,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 1763,
                    "end": 1767,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "270870119",
            "title": "Unveiling Glitches: A Deep Dive into Image Encoding Bugs within CLIP",
            "text": "Evaluation of multimodal models: Existing research (1) tackles failures in NLP (factual consistency, bias) and CV (adversarial examples) often through manual curation or specific failure modes.Multimodal evaluation focuses on output quality, not systematic errors.The paper addresses the gap by introducing MultiMon, which automatically identifies systematic failures in multimodal systems with language models.MultiMon leverages language models to uncover generalizable error patterns, offering a more scalable and comprehensive evaluation approach.\n\nText-guided multimodal models: In this work, we analyze the shortcomings of text-guided multimodal generation models.These models can create various outputs, including images, videos, and 3D scenes, based on textual descriptions (2).Commonly, these models employ a vision-language model (VLM) to encode the text input.VLMs establish a shared embedding space for both text and image data (3).Subsequently, a guided diffusion process is used to generate the final output (4).\n\nVulnerability of vision and language models: Several studies (5) have explored robustness in vision and language models.For instance, prior work demonstrated that Transformers trained with Masked Language Modeling exhibit limited sensitivity to word order (6).This suggests BERT's success might rely more on co-occurrence statistics than true syntactic and semantic understanding (7).Existing benchmarks evaluate robustness against image corruptions, style changes, and viewpoints, primarily focusing on unimodal tasks (vision-only or language-only) (8).Our work differs by investigating CAB, a cross-modal phenomenon requiring both image and language data.While studies have explored compositional generalization and probed visuo-linguistic abilities with datasets like Winoground, models like CLIP (9) struggle on these tasks.This highlights the need for models to go beyond basic compositional understanding and incorporate reasoning and object detection.Our work complements these findings by revealing CLIP's brittleness to CAB, a previously unexplored vulnerability.\n\nPeculiarities of CLIP: Recent works highlight limitations in the compositional reasoning abilities of state-of-the-art image generation models, including DALL-E 2 (9).",
            "score": 0.5111414617164335,
            "section_title": "Related Work",
            "char_start_offset": 2491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 550
                },
                {
                    "start": 552,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 785
                },
                {
                    "start": 785,
                    "end": 870
                },
                {
                    "start": 870,
                    "end": 943
                },
                {
                    "start": 943,
                    "end": 1025
                },
                {
                    "start": 1027,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1287
                },
                {
                    "start": 1287,
                    "end": 1411
                },
                {
                    "start": 1411,
                    "end": 1581
                },
                {
                    "start": 1581,
                    "end": 1684
                },
                {
                    "start": 1684,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 1985
                },
                {
                    "start": 1985,
                    "end": 2099
                },
                {
                    "start": 2101,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 781,
                    "end": 784,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1283,
                    "end": 1286,
                    "matchedPaperCorpusId": "233231592"
                },
                {
                    "start": 1407,
                    "end": 1410,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41748046875
        },
        {
            "corpus_id": "271534005",
            "title": "LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models",
            "text": "Notably, H v shares the same embedding dimensions as the text tokens used by the LLM tokenizer. Different from the conventional architecture of multimodal large language models [4], LLaVA-Read incorporates a visual-text encoder f t to better capture textual and layout information, along with a high-resolution encoder for finer visual details. The objective of the visual-text encoder is to extract text from an image, yielding visual-text tokens H t = f t (X v ). Subsequently, we concatenate H v , H t , and H q , feeding them into the large language model to generate the desired response Y. \n\nIn designing LLaVA-Read, we hold the belief that a visual encoder should specialize in processing visual objects, while a lightweight visual-text encoder should focus on extracting text within images. This approach, we believe, enhances the efficiency of the visual components, as text recognition presents distinct patterns compared to visual object detection. Although high-resolution visual encoders can capture finer details, they also generate a larger number of visual tokens. To mitigate additional computational costs associated with employing two visual encoders in LLaVA-Read, we merge the output of these encoders while maintaining the same visual tokens as in LLaVA. More details on architectural design are elaborated in Section 3.1. In essence, LLaVA-Read offers a multimodal LLM framework that leverages multiple visual encoders to improve visual token learning and conversion efficiency. To foster enhanced collaboration between multiple visual encoders, we propose layout-aware tuning during the two-stage training of LLaVA-Read, as discussed in Sections 3.2 and 3.3.",
            "score": 0.5108863374938359,
            "section_title": "User Instructions:",
            "char_start_offset": 9014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1682
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54345703125
        },
        {
            "corpus_id": "268691871",
            "title": "ViT-Lens: Initiating Omni-Modal Exploration through 3D Insights",
            "text": "CLIP (Contrastive Language-Image Pretraining) is a powerful multimodal model trained on a large dataset of image-text pairs.It learns to map images and their corresponding text into a shared embedding space through contrastive learning.The resulting joint image-text embeddings enable CLIP to perform various multimodal tasks, showcasing remarkable generalization and zero-shot learning capabilities.CLIP's knowledge, acquired from extensive pretraining on image-text data, encapsulates rich information about the visual world and language.This inherent knowledge makes CLIP highly transferable, allowing it to excel in downstream tasks and domains without task-specific fine-tuning.\n\nBuilding upon CLIP, ULIP [68] introduces an efficient multimodal pretraining framework using triplets that encompass three modalities: (1) the 3D modality, extracted from 3D point cloud data; (2) the image modality, generated by rendering images from 3D object files with multiple viewpoints; and\n\n(3) the language modality, derived by converting dataset metadata into coherent sentences, including descriptive terms and category names.Subsequently, [69,40] scale up the pretraining 3D data and leverage a large language model [35,34,50] to automatically generate detailed captions from a comprehensive set of holistic views, reducing reliance on human annotations.All these methods learn 3D shape representations aligned with the pretrained CLIP embedding spaces of language and image.VIT-LENS follows the same training paradigm as these works but incorporates the pretrained vision part for 3D shape encoding, aiming to utilize the CLIP-ViT from the model perspective.",
            "score": 0.5108189540836072,
            "section_title": "Preliminary: CLIP for 3D Shape Representation Learning",
            "char_start_offset": 7597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 236
                },
                {
                    "start": 236,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 540
                },
                {
                    "start": 540,
                    "end": 683
                },
                {
                    "start": 685,
                    "end": 981
                },
                {
                    "start": 983,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1655
                }
            ],
            "ref_mentions": [
                {
                    "start": 710,
                    "end": 714,
                    "matchedPaperCorpusId": "254564501"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "260886966",
            "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models",
            "text": "For the fully fine-tuned image prompt models, the original text-to-image ability is almost lost. However, with the proposed IP-Adapter, we can generate images with multimodal prompts including image prompt and text prompt. We found that this capability performs particularly well on community models. In the inference stage with multimodal prompts, we adjust \u03bb to make a balance between image prompt and text prompt. Figure 8 displays various results with multimodal prompts using Realistic Vision V4.0 model. As we can see, we can use additional text prompt to generate more diverse images. For instance, we can edit attributes and change the scene of the subject conditioned on the image prompt using simple text descriptions. Compared with other existing methods, our method can generate superior results in both image quality and alignment with multimodal prompts.",
            "score": 0.5106683769546068,
            "section_title": "Multimodal Prompts",
            "char_start_offset": 25771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 868
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.465576171875
        },
        {
            "corpus_id": "278170900",
            "title": "Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment",
            "text": "Diffusion-based generative models [2,22,[27][28][29]31] have demonstrated exceptional capabilities in producing highquality images. However, when tasked with generating complex scenes that contain multiple distinct objects, these models often struggle with counting, attribute binding, and spatial relationships between different objects. This limitation leads to outputs that do not fully align with the user's intended descriptions. \n\nRecent studies [1,4,11,14,15,20,34,39,41,45,50] have introduced mechanisms such as boundary layouts and Figure 2. Architecture Comparison Between Single Agent Framework and Marmot. We decompose self-correction into counting, attributes, and spatial dimensions, achieving superior results using only 8B MLLMs and LLMs. Additionally, our PDSS ensures background consistency while mitigating artifacts. localized text prompts through users or Large Language Models (LLMs), in order to better align generative models with user intent, thereby enhancing control over image composition. Although these methods are effective in structuring scenes, they still struggle to maintain distinct attributes for individual objects in complex scenes. Another line of research [35,36,38,40] has proposed a framework that utilizes a Multimodal Large Language Model (MLLM) as an AI agent to identify issues within images and invoke editing tools to iteratively refine the images based on user intent, thereby enhancing alignment with user expectations. However, these self-correction frameworks all rely on a single agent and are highly dependent on the capabilities of the MLLM being used. For instance, open-source MLLMs such as LLaVA [16][17][18] often struggle to produce reliable outputs when generating complex instructions that can be parsed by regular expressions. Moreover, even closedsource MLLMs such as GPT-4 [21] exhibit unequal attention to various objects within images. \n\nTo address these challenges, we introduce Marmot, a universal self-correcting framework that leverages multiagent reasoning to achieve coordinated, multi-object selfcorrecting.",
            "score": 0.5103210513438455,
            "section_title": "Introduction",
            "char_start_offset": 756,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 37,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 40,
                    "end": 44,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 48,
                    "end": 52,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "261101003"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "255942528"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "266174339"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "267412534"
                },
                {
                    "start": 472,
                    "end": 475,
                    "matchedPaperCorpusId": "259991581"
                },
                {
                    "start": 475,
                    "end": 478,
                    "matchedPaperCorpusId": "254043880"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 481,
                    "end": 484,
                    "matchedPaperCorpusId": "267547419"
                },
                {
                    "start": 1201,
                    "end": 1204,
                    "matchedPaperCorpusId": "271050324"
                },
                {
                    "start": 1204,
                    "end": 1207,
                    "matchedPaperCorpusId": "265457140"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "matchedPaperCorpusId": "267068823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.403564453125
        },
        {
            "corpus_id": "266210376",
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "text": "Contrary to previous works [9,18,30,48], all embeddings in the multimodal sequence can receive supervision from the auto-regressive loss. During the generation stage, visual embeddings and text tokens can be generated auto-regressively without distinction, and subsequently decoded into raw images and text by the image detokenizer and text detokenizer, respectively. \n\nOwing to the unified modeling, the pre-training of the VL model can be conducted on large-scale image-text pairs and interleaved image-text data. Upon completion of pretraining, the model is capable of perceiving arbitrary multimodal input and generating responses varying in modalities (e.g., text, images or their interleaved contents), allowing it to generalize to a wide range of vision and language understanding and generation tasks in a zero-shot or few-shot manner. Moreover, the pre-trained model exhibits appealing emergent properties for multimodal in-context learning, as it can effectively tackle new unseen tasks when provided with multimodal prompts. The VL generative pre-trained transformer model, referred to as VL-GPT, holds the potential to serve as a powerful foundation model for the multimodal community, similar to the role of GPT family [4,29] in NLP. Our contributions are summarized as follows: \n\n\u2022 We propose an image tokenizer-detokenizer framework to convert images into continuous embeddings and reconstruct them, while exploring effective training methods for this framework. Through efficient training that requires an affordable computational cost, the image tokenizer and detokenizer can effectively retain both semantic information and pixel details of the original image. \u2022 We introduce VL-GPT, a generative pre-trained trans-former model for vision and language (VL) understanding and generation tasks. The model can be pre-trained on large-scale multimodal corpora in a unified autoregressive manner, i.e., predicting the next token in a multimodal sequence containing continuous visual embeddings and discrete text tokens without any discrimination.",
            "score": 0.5102854253629042,
            "section_title": "Introduction",
            "char_start_offset": 3629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 2059
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7509765625
        },
        {
            "corpus_id": "266359151",
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "text": "RAG has transcended its initial text-based questionanswering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains: \n\nImage. RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2 [177] leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zeroshot image-to-text conversions. The \"Visualize Before You Write\" method [178] employs image generation to steer the LM's text generation, showing promise in open-ended text generation tasks. \n\nAudio and Video. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data [179]. UEOP marks a significant advancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text conversion [180]. Additionally, KNN-based attention fusion leverages audio embeddings and semantically related text embeddings to refine ASR, thereby accelerating domain adaptation. \n\nVid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence [181]. \n\nCode. RBPS [182] excels in small-scale learning tasks by retrieving code examples that align with developers' objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion generation and program repair. For structured knowledge, the CoK method [106] first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks.",
            "score": 0.51028141078864,
            "section_title": "F. Multi-modal RAG",
            "char_start_offset": 60334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 219
                },
                {
                    "start": 222,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1807
                }
            ],
            "ref_mentions": [
                {
                    "start": 1291,
                    "end": 1296,
                    "matchedPaperCorpusId": "257232853"
                },
                {
                    "start": 1311,
                    "end": 1316,
                    "matchedPaperCorpusId": "259860357"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75732421875
        },
        {
            "corpus_id": "266573642",
            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
            "text": "EmbodiedGPT is an end-to-end multimodal foundation model for embodied AI with a \"chain-of-thought\" capability, enabling embodied agents to interact with the physical world more naturally. It also develops two datasets, Ego-COT and EgoVQA, and proposes a cost-effective training Fig. 29: The training paradigm of of ImageBind-LLM [139]. Figure is from [139]. approach for extracting task-relevant features from planning queries. The approach demonstrates state-of-the-art or comparable performance on multiple embodied tasks, including embodied control, planning, video captioning, and video QA, outperforming existing models on benchmark tasks. \n\nChatBridge is a novel multimodal language model that leverages large language models to bridge the gap between various modalities. It proposes a two-stage training approach to align different modalities with language and introduces a new multimodal instruction tuning dataset called MULTIS. \n\nVideoChat is a chat-centric video understanding system that integrates video foundation models and large language models. It proposes a video-centric instruction dataset emphasizing spatiotemporal reasoning and causal relationships, providing a valuable asset for training chatcentric video understanding systems. It also presents qualitative experiments showcasing the system's potential across various video applications and sets a standard for future research in the field of video understanding. As shown in Figure 30, the framework of VideoChat consists of two main components: VideoChat-Text and VideoChat-Embed. VideoChat-Text textualizes videos in stream by converting visual data into textual format using various vision models and prompts, allowing a pretrained large language model to address user-specified tasks based on the video text descriptions. On the other hand, VideoChat-Embed encodes videos as embeddings and combines video and language foundation models with a Video-Language Token Interface (VLTF) to optimize cross-modality, enabling the model to effectively communicate with users through a large language model. \n\nVideo-ChatGPT is a multimodal model that merges a pretrained visual encoder with a Large Language Model (LLM) to understand and generate detailed conversations about videos.",
            "score": 0.5101551817905602,
            "section_title": "Visual Assistant",
            "char_start_offset": 73028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2254
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76123046875
        },
        {
            "corpus_id": "268987752",
            "title": "Dynamic Prompt Optimizing for Text-to-Image Generation",
            "text": "Content generation AI-generated content (AIGC) [3, 23, 27, 29-31, 37, 43] has made revolutionary progress in recent years, particularly in natural language processing.Large language models such as BERT [6], GPT-1 to GPT-4 [2,18,24,25], and ChatGPT3 have demonstrated exceptional text understanding and generation ability.Their advancements have greatly influenced the generation of textto-image content.With the development of generative models [7,[33][34][35] and multi-modal pre-training techniques [26], text-to-image generative models such as DALL\u2022E 2 [29], Imagen [31], Stable Diffusion [30] and Versatile Diffusion [43] have showcased impressive performance in generating high-quality images.These breakthroughs have captured the attention of both academia and industry due to their potential impact on content production and applications in the open creative scene, etc.In this paper, the proposed dynamic prompt editing framework utilizes a language generation model to assist text-to-image generation.\n\nText-to-image prompt collection and analysis In recent years, several studies have been conducted to explore the",
            "score": 0.5100631361607506,
            "section_title": "Related work",
            "char_start_offset": 6193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 698
                },
                {
                    "start": 698,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1010
                },
                {
                    "start": 1012,
                    "end": 1124
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 205,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 445,
                    "end": 448,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.479248046875
        },
        {
            "corpus_id": "271245054",
            "title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
            "text": "By designing prompts to project multimodal inputs into the same embedding spaces, we show that MLLMs can represent multimodal inputs correctly even without fine-tuning.\u2022 We introduce a new framework, E5-V, to adapt MLLMs for achieving universal multimodal embeddings.With single modality training on text pairs, E5-v even achieve better multimodal embeddings than image-text pairs.\u2022 Extensive experiments on text-image retrieval and composed image retrieval tasks demonstrate the effectiveness of E5-V in representing multimodal information.E5-V successfully transfers single modality representation capabilities to multimodal embeddings by following task-specific prompts that were not included in the training data.To achieve universal multimodal embeddings, there are several works, such as and UNIIR [WCC + 23], fine-tune CLIP on interlevaed dataset with fusion modal to fuse the visual and language information.There are also some works like VISTA [ZLX + 24] or UniVL-DR [LXL + 22] feed text encoder with CLIP outputs to input visual information.However, it can harm the original text-image retrieval performance of CLIP, and is hard to make text encoder understand the visual information with only contrastive learning, which show poor zero-shot performance on composed image retrieval tasks.\n\nTo achieve universal multimodal embeddings, several works, such as UNIIR [WCC + 23], finetune CLIP with a fusion model to integrate visual and language information.Other works, like VISTA [ZLX + 24] or UniVL-DR [LXL + 22], feed the text embedding models with CLIP outputs to incorporate visual information.However, this approach can harm the original text-image retrieval performance of CLIP and makes it difficult for the text embedding models to understand visual information using only contrastive learning.As a result, these methods show poor zero-shot performance on composed image retrieval tasks.Moreover, these methods require large interleaved training data to achieve universal multimodal embeddings.",
            "score": 0.5099324632650573,
            "section_title": "Introduction",
            "char_start_offset": 3783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 381
                },
                {
                    "start": 381,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1298
                },
                {
                    "start": 1300,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1606
                },
                {
                    "start": 1606,
                    "end": 1810
                },
                {
                    "start": 1810,
                    "end": 1903
                },
                {
                    "start": 1903,
                    "end": 2010
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "252177604",
            "title": "A Review of Multi-Modal Learning from the Text-Guided Visual Processing Viewpoint",
            "text": "manipulation framework. Their framework has three components, image captioning, style image generation, and style transfer net. They can perform image manipulation without the style image, and follows a module-based generative model.\n\n3D scenes: Some studies in T2I explore deep generative models, especially GAN, for creating 3D scenes from the given text. However, due to the limited research in this field, the generated results are far from the real-world scenes and mostly rely on retrieval-based tasks [296][297][298].\n\nMotivated by the limitations of retrieval-based 3D scene generation, Text2Shape [120] proposed an end-to-end instance-level association learning framework for cross-modal associations between text and 3D shapes. First, it learns a joint embedding, inspired by [299], of text and 3D shapes for the text-to-shape retrieval task, then introduces a text-to-colored voxel generation task with conditional Wasserstein GAN, following [300]. For the new technique, two new datasets are shown to be effective for evaluation. This model is different from GAN-INT-CLS as it does not require a pre-trained model or massive annotated data for training.\n\nIn the previous method [120], generating high-resolution 3D shapes requires extensive GPU memory or a long training time. So, Fukamizu et al. [121] considered the lowresolution problem and followed a two-stage approach by using StackGAN knowledge.\n\nA different application of a text-conditioned deep generative model for the 3D scene is shown by Chen et al. [122]. They applied the knowledge of Graph scene parser [123] to obtain the layout by a graph-conditioned layout prediction network (GC-LPN) with language-conditioned texture GAN (LCT-GAN) to generate 3D models of houses. The overall task is split into building a layout and refining with texture synthesis. As a challenge to the proposed application, no dataset exists in the literature, so they introduced a new dataset called the text-to-3D house model.",
            "score": 0.5097897435308647,
            "section_title": "Supervised T2I",
            "char_start_offset": 92677,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 513,
                    "matchedPaperCorpusId": "5440348"
                },
                {
                    "start": 513,
                    "end": 518,
                    "matchedPaperCorpusId": "3842372"
                },
                {
                    "start": 1525,
                    "end": 1530,
                    "matchedPaperCorpusId": "211677591"
                },
                {
                    "start": 1581,
                    "end": 1586,
                    "matchedPaperCorpusId": "13937253"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48779296875
        },
        {
            "corpus_id": "270094821",
            "title": "Multi-Modal Generative Embedding Model",
            "text": "Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding. To explore the minimalism of multi-modal paradigms, we attempt to achieve only one model per modality in this work. We propose a Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and embedding objectives are encapsulated in one Large Language Model. We also propose a PoolAggregator to boost efficiency and enable the ability of fine-grained embedding and generation. A surprising finding is that these two objectives do not significantly conflict with each other. For example, MM-GEM instantiated from ViT-Large and TinyLlama shows competitive performance on benchmarks for multimodal embedding models such as cross-modal retrieval and zero-shot classification, while has good ability of image captioning. Additionally, MM-GEM can seamlessly execute region-level image caption generation and retrieval tasks. Besides, the advanced text model in MM-GEM brings over 5% improvement in Recall@1 for long text and image retrieval.",
            "score": 0.5096770430151931,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "273962846",
            "title": "Autoregressive Models in Vision: A Survey",
            "text": "To further enhance the efficacy of visual in-context learning, CoTVL (Ge et al., 2023) successfully applies visual chain-of-thought prompt tuning for vision-language modeling, and performs better in tasks that require more reasoning abilities. These models highlight the potential for autoregressive frameworks to serve as foundational architectures in multi-modal applications, promoting a more holistic approach to understanding and generating content across modalities. \n\nThe previously mentioned multimodal tokenization involves encoding images into tokens, allowing Large Language Models (LLMs) to process both visual and language signals in a unified space. Visionto-Language (V2T) Tokenizer (Zhu et al., 2024a) maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) (Lee et al., 2022b) encodes both image and text inputs as tokens, generating coherent multimodal outputs. These approaches improve image-text generation, though they typically predict pixels in a fixed order without considering random generation strategies. \n\nIn summary, the evolution of visual autoregressive models in the realm of multi-modality underscores their growing importance and versatility. From early integrations to sophisticated native architectures, these models continue to push the boundaries of what's possible in generating and understanding multimodal content. Future research directions may focus on improving model efficiency, scalability, and the ability to handle an even broader array of modalities.",
            "score": 0.5094333739402852,
            "section_title": "The Framework for Unifying Multimodal Understanding and Generation",
            "char_start_offset": 88771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1624
                }
            ],
            "ref_mentions": [
                {
                    "start": 698,
                    "end": 717,
                    "matchedPaperCorpusId": "268363788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7490234375
        },
        {
            "corpus_id": "274305669",
            "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis",
            "text": "Integrating vision and language has been a long-standing goal in artificial intelligence. Early work focused on image captioning [39], generating textual descriptions of image contents. Recent years have seen rapid progress in visual question answering [1,2], enabled by large-scale datasets [7] and deep neural encoder-decoder models [43]. \n\nMore advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework [3]. \n\nOne line of work explores joint embedding models to learn aligned vector representations for image regions and language fragments [15,19]. However, these approaches do not explicitly model interactions between modalities. \n\nRecently, pretrained language models, such as BERT [11] and GPT [4], have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text. More recent methods like VisualBERT [23] and LXMERT [38] utilize pretrained BERT weights for joint image-text comprehension. \n\nUnified architectures like ViLBERT [30], VL-BERT [36], UNITER [8], BLIP [21] integrate masked language modeling objectives alongside paired image-text prediction tasks within a single Transformer model. These models set new stateof-the-art results on downstream tasks like visual question answering (VQA), visual reasoning, and image retrieval. \n\nSpecifically, we utilize BLIP as our vision-language model backbone. BLIP uses a flexible multimodal encoder-decoder model that can handle both understanding and generation tasks. It also improves training data quality through generating new image captions and filtering noise. These advantages lead to strong performance. \n\nInstruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks. A key advantage these models have over traditional fine-tuning is they can adapt to new tasks without needing gradient updates or lots of specific training data. The instruction format allows rapid adaptation.",
            "score": 0.5093169895333786,
            "section_title": "Vision-Lauguage Model",
            "char_start_offset": 7308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1547
                },
                {
                    "start": 1550,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1872
                },
                {
                    "start": 1875,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2252
                },
                {
                    "start": 2253,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "1169492"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "3753452"
                },
                {
                    "start": 256,
                    "end": 258,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 335,
                    "end": 339,
                    "matchedPaperCorpusId": "1055111"
                },
                {
                    "start": 599,
                    "end": 602,
                    "matchedPaperCorpusId": "10137425"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "8517067"
                },
                {
                    "start": 894,
                    "end": 897,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1238,
                    "end": 1242,
                    "matchedPaperCorpusId": "208637516"
                },
                {
                    "start": 1265,
                    "end": 1268,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1275,
                    "end": 1279,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "273229053",
            "title": "Aria: An Open Multimodal Native Mixture-of-Experts Model",
            "text": "Alt texts directly extracted for web images are generally short, less descriptive, and noisy. It has been shown in previous work that synthetic data at scale can improve multimodal pre-training [Li et al., 2022]. We thus synthesize image captions using a small model which has learned to generate longer and more descriptive image captions by re-writing the alt texts. We create synthetic captions for 300M images in the LAION-400M dataset Schuhmann et al. [2021], resulting in a total of 70B multimodal tokens. \n\nDocument transcriptions and QA. To improve the model's capability of understanding text-heavy images, we transcribe document images into texts using public OCR methods. We also render images using plain text, chart json or table/equation latex code. In order to enhance the model's ability to not only transcribe text but also understand its meaning, we use a language model to create synthetic question-answering pairs. In total, our multimodal document data contains 102B tokens. \n\nVideo captions and QA. We collect 4.4M videos of varying lengths from a diverse range of sources. We train a model to generate frame-level dense descriptions for the videos. Then, we use a language model to generate question-answering pairs and video summarizations based on the dense video descriptions. In total, our video data contains 35B tokens. We select samples within 8K length for multimodal pre-training. Results of competing models are collected from verified official leaderboards or reruned with official settings.",
            "score": 0.5092644462120476,
            "section_title": "Synthetic image captions.",
            "char_start_offset": 9969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1525
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 211,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.377197265625
        },
        {
            "corpus_id": "231719672",
            "title": "VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs",
            "text": "Among the fundamental goals of AI is the development of conversational multimodal systems that can reliably perceive the real-world and communicate with humans in natural language. Progress in this area has been dramatically advanced in recent years by the introduction of large-scale benchmarks assessing the ability to interpret audiovisual information and translate this understanding to natural language. Prime examples include datasets for image or video captioning [10,38,51,24,56,28], question answering (QA) [5,13,54,58,19,36,46,26], as well as audio-visual dialog [11,1]. In order to perform well on such bench-marks, the model must accomplish several goals: (1) extract salient information from each individual modality, (2) effectively combine the different cues to address the given query, and (3) generate and present the results in humancomprehensible text.\n\nIn this paper, we present VX2TEXT, a simple videobased approach that embeds these three steps in a unified, end-to-end trainable framework. Objectives (1) and (2) are accomplished by utilizing modality-specific classifiers to convert the semantics from each input signal into a common semantic language space, which enables the application of powerful language models to directly interpret multimodal content. Specifically, our approach takes the textual labels of the top classes predicted by each classifier pretrained on existing datasets [9,14] and transforms them into word embeddings, using a pretrained language model [12,40]. The benefit of this solution is that it opens up the possibility to carry out multimodal fusion by means of powerful language encoders such as T5 [40] without the need to design specialized cross-modal network modules [33,29,57,35] or to resort to pretext tasks to learn to combine the different input signals [44,55,29]. Not only is such a design much simpler but it also leads to better performance compared to prior approaches.\n\nIn order to fulfill objective (3), we employ a generative text decoder [40], which transforms the multimodal features computed by the encoder into text, thus realizing the goal of generating",
            "score": 0.5085092169321387,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 475,
                    "end": 478,
                    "matchedPaperCorpusId": "6941275"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "206594535"
                },
                {
                    "start": 481,
                    "end": 484,
                    "matchedPaperCorpusId": "1026139"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "209217"
                },
                {
                    "start": 525,
                    "end": 528,
                    "matchedPaperCorpusId": "5714907"
                },
                {
                    "start": 528,
                    "end": 531,
                    "matchedPaperCorpusId": "3030826"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "3101860"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "1017389"
                },
                {
                    "start": 577,
                    "end": 579,
                    "matchedPaperCorpusId": "59316836"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "59316836"
                },
                {
                    "start": 1415,
                    "end": 1418,
                    "matchedPaperCorpusId": "206596127"
                },
                {
                    "start": 1418,
                    "end": 1421,
                    "matchedPaperCorpusId": "21519176"
                },
                {
                    "start": 1725,
                    "end": 1729,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1732,
                    "end": 1735,
                    "matchedPaperCorpusId": "219617394"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65625
        },
        {
            "corpus_id": "276782523",
            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
            "text": "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,2,15,19,59,62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder [45] are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM [15] extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities. Our work leverages the advancements made by these foundational models and explores how to adapt the foundational model to specific domains.",
            "score": 0.5083459429967252,
            "section_title": "Unified Multimodal Generative Models",
            "char_start_offset": 6395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1413
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 236,
                    "matchedPaperCorpusId": "262898422"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "261975252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86474609375
        },
        {
            "corpus_id": "272770579",
            "title": "Generating Visual Stories with Grounded and Coreferent Characters",
            "text": "Our generation model is trained on VIST++. It is able to identify characters across multiple images and generate stories with multimodal character chains. We leverage the visual understanding and text generation capabilities of Large Vision-Language models (LVLMs). Specifically, we employ OTTER (Li et al., 2023) as our backbone model, which has been instruction-tuned for multiple image understanding. OTTER (Li et al., 2023) is the instruction-tuned version of OpenFlamingo (Awadalla et al., 2023) 2). Characters are further labeled with a unique ID number, overlaid in the center of the mask, and characters with the same ID in different images are assumed to be the same person. These visual annotations or marks (Yang et al., 2023a) serve as visual prompts to OTTER. \n\nDuring training, the model learns to predict a story with character grounding and coreference information. Aside from visual prompting, grounding is facilitated by training the model to verbalize textual character chains and their grounding. The model learns to predict textual chains, i.e., it marks character mentions with a special symbol, and is explicit about which visual segmentation they refer to (e.g., [Ella](#4) refers to visual segment 4, [she](#4) refers to Ella and segment 4, whereas [We](#1, #2) refers to segments 1 and 2). \n\nWithin OTTER, we freeze the parameters of the language encoder (LLAMA-7B) and the vision encoder (CLIP VIT-L/14), and only update the parameters of the Perceiver (Jaegle et al., 2021) resampler module, the cross-attention layers added to the language encoder, and the input/output embeddings of the language encoder. This results in approximately 1.44 billion trainable parameters. \n\nInference During inference, the input to the model is a regular image sequence (without any character related annotations). We use the automated pipeline described in Section 2.1.1 to obtain visual character chains which are in turn converted into visual prompts, following the same method used during training.",
            "score": 0.508103245925354,
            "section_title": "Visual Story Generation Model",
            "char_start_offset": 16481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 1480,
                    "end": 1501,
                    "matchedPaperCorpusId": "232110866"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373046875
        },
        {
            "corpus_id": "271534005",
            "title": "LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models",
            "text": "What is 21% for? Boxes Words Figure 1: Model overview of LLaVA-Read, a multimodal LLM with dual encoders to handle both visual objects and texts. Given a text-rich image, the visual-text encoder extracts texts and their location information, feeding them to the OCR tokenizer. ViT-based low-resolution encoder (e.g., 336\u00d7336) focuses on the global visual information and convolution-based encoder (e.g., 768\u00d7768) focuses on visual details. The high-resolution encoder merges its information into low-resolution encoders, as not all details are useful in answering a question. powerful screenshot LMs to unlock complex tasks such as chart understanding and UI navigation. Multimodal LLMs for text-rich images can extract visual texts, which is similar to the visual text understanding problem. The major difference is that multimodal LLMs not only need to comprehend visual texts but also visual objects and their relationship. Inspired by previous work [47], LLaVA-Read performs an visual text understanding analysis of multimodal LLMs on synthetic data, revealing their impressive capability on shorter scene text understanding but limited proficiency in comprehending large amounts of textual content within a text-rich image. This observation motivates us to add an additional visual-text encoder to enhance reading ability of multimodal LLMs. \n\n3 LLaVA-Read: Enabling LLaVA to Read LLaVA-Read is designed to enhance the comprehension of textual information within images, particularly in text-rich images. An overview of the model is shown in Figure 1. LLaVA-Read comprises multiple visual encoders, a visual-text encoder, and a large language model (LLM) serving as the decoder. Given an input image X v , the visual encoders generate visual features \n\nwhere f v consists of two visual encoders. Subsequently, we employ a multi-layer perceptron (MLP) projection g to transform Z v into visual tokens H v = g(Z v ) for the large language model. Notably, H v shares the same embedding dimensions as the text tokens used by the LLM tokenizer.",
            "score": 0.5077600368046409,
            "section_title": "User Instructions:",
            "char_start_offset": 7065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2044
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483642578125
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "MiniGPT-4 [49] fixes the parameters of the language model and the visual model, and then uses only the projection layer to align the language decoder with LLaMA and the visual encoder with BLIP-2.LRV-Instruction effectively reduces hallucinations produced by LMMs, and maintaining a balanced ratio of positive and negative instances leads to a more robust model [198].MiniGPT-5 [50] combines the Stable Diffusion mechanism with LLM through special visual tokens.In addition to its original multi-modal understanding and text generation capabilities, it can also provide reasonable and coherent multi-modal output.Structure-CLIP [51]proposes a Knowledge Enhanced Encoder (KEE) that takes SGK as input to enhance multi-modal structured representation by integrating scene graph knowledge (SGK).In order to maximize the complementarity of highly similar tasks, CLIPbased MmAP [52] utilizes a gradient-driven task grouping method to divide tasks into several disjoint groups and assign a group-shared parameter to each group.MM-Interleaved [53] achieves direct access to fine-grained image features in the previous context during the generation process by introducing a multi-scale multi-image feature synchronizer module.This module enables models to be pre-trained endto-end on paired and interleaved image-text corpora to more efficiently follow complex cross-modal instructions.\n\nIn the real world, data are not always complete, and situations where certain modalities are missing may be encountered.This results in degraded model performance.Integrating the missing-aware prompt method into Transformerlike multimodal models is straightforward, which can alleviate the impact of missing modalities and enhance the robustness of the models [199,200,201,202,203].\n\nThe technical development of large multimodal models for vision and text has gone through many important stages, and it has essentially formed a unified paradigm that uses large language models as the core and leverages highquality datasets to enhance the fusion of visual and textual modalities.With the development of multimodal technology, modal fusion occupies a core position in the technological advancement of large multimodal models for vision and text.",
            "score": 0.5068650709248201,
            "section_title": "Recent Image-text Multimodal Models",
            "char_start_offset": 21037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 196,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 613
                },
                {
                    "start": 613,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1378
                },
                {
                    "start": 1380,
                    "end": 1500
                },
                {
                    "start": 1500,
                    "end": 1543
                },
                {
                    "start": 1543,
                    "end": 1762
                },
                {
                    "start": 1764,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 1740,
                    "end": 1745,
                    "matchedPaperCorpusId": "257365349"
                },
                {
                    "start": 1745,
                    "end": 1749,
                    "matchedPaperCorpusId": "248118952"
                },
                {
                    "start": 1749,
                    "end": 1753,
                    "matchedPaperCorpusId": "232170317"
                },
                {
                    "start": 1753,
                    "end": 1757,
                    "matchedPaperCorpusId": "248476361"
                },
                {
                    "start": 1757,
                    "end": 1761,
                    "matchedPaperCorpusId": "236459819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5634765625
        },
        {
            "corpus_id": "268819484",
            "title": "Neural Radiance Field-based Visual Rendering: A Comprehensive Review",
            "text": "Text-based 3D Generation Magic3D [164] An efficient text-to-3D content creation framework for fast and high-quality 3D mesh model generation through a two-stage optimization strategy using a low-resolution diffusion prior and a high-resolution potential diffusion model.Latent-NeRF [198] A Neural Radiation Field (NeRF) model for direct manipulation in potential space for text-guided 3D shape and texture generation.\n\nText2NeRF [199] A text-driven 3D scene generation framework that combines NeRF and a pre-trained text-to-image diffusion model enables the generation of diverse 3D scenes with complex geometries and high-fidelity textures from textual cues.\n\nText-to-3D Editing SKED [165] A 3D editing technique based on sketches and textual cues is capable of generating localized and meaningful edits by utilizing a pre-trained NeRF and a small number of guided sketches.Instruct-NeRF2NeRF [166] A text-command-based NeRF scene editing technique for consistent 3D editing of large-scale real-world scenes by iteratively updating the input image and optimizing the underlying scene.\n\nLERF [200] An approach that combines natural language processing with NeRF enables natural language querying and interaction with complex 3D scenes by embedding linguistic embedding of CLIP models in NeRF.\n\nLENeRF [201] A textual cueing-based 3D content editing framework for fine-grained and localized manipulation of 3D features by estimating 3D attention fields, supporting real-time editing, and maintaining high-quality visual effects.",
            "score": 0.5067898093279685,
            "section_title": "Category Model Highlight",
            "char_start_offset": 124657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 417
                },
                {
                    "start": 419,
                    "end": 659
                },
                {
                    "start": 661,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1085
                },
                {
                    "start": 1087,
                    "end": 1292
                },
                {
                    "start": 1294,
                    "end": 1527
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 38,
                    "matchedPaperCorpusId": "253708074"
                },
                {
                    "start": 282,
                    "end": 287,
                    "matchedPaperCorpusId": "253510536"
                },
                {
                    "start": 429,
                    "end": 434,
                    "matchedPaperCorpusId": "258822803"
                },
                {
                    "start": 685,
                    "end": 690,
                    "matchedPaperCorpusId": "257632144"
                },
                {
                    "start": 894,
                    "end": 899,
                    "matchedPaperCorpusId": "257663414"
                },
                {
                    "start": 1092,
                    "end": 1097,
                    "matchedPaperCorpusId": "257557329"
                },
                {
                    "start": 1301,
                    "end": 1306,
                    "matchedPaperCorpusId": "259224838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26611328125
        },
        {
            "corpus_id": "266998744",
            "title": "Developing ChatGPT for biology and medicine: a complete review of biomedical question answering",
            "text": "Transformer with a language model, showcasing improvements in data efficiency and transfer learning across various robot embodiments. \n\nOn the language understanding front, GPT-3.5, an enhanced version of GPT-3, addresses limitations in language understanding and generation. With its advanced architecture and the use of prompt engineering and ensemble methods, GPT-3.5 achieves state-of-the-art results on various language tasks and shows promise in medical benchmarks, enabling more accurate and contextually relevant responses. Building upon the successes of its predecessors, GPT-4 (Nori et al, 2023) further improves language generation with an advanced architecture that captures longrange dependencies and excels in understanding multimodal content. This model demonstrates enhanced performance and generates more coherent and contextually relevant text. MiniGPT-v2 (Chen et al, 2023) serves as a unified interface for vision-language tasks, achieving strong performance on visual question answering and grounding. MiniGPT-4 further enhances multimodal capabilities by combining a frozen visual encoder with a large language model. NExT-GPT (Wu et al, 2023) is an end-to-end multimodal language model capable of processing any combination of text, images, videos, and audio. It achieves universal multimodal understanding and supports any-to-any modality input and output. \n\nOverall, these models have made substantial strides in enhancing visual language modeling, effectively tackling various challenges in tasks such as image understanding, language comprehension, and generation within multimodal contexts.",
            "score": 0.5066734339310746,
            "section_title": "combines a Vision",
            "char_start_offset": 29797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 136,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1618
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66162109375
        },
        {
            "corpus_id": "265034144",
            "title": "AnyText: Multilingual Visual Text Generation And Editing",
            "text": "Diffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image. AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. AnyText can write characters in multiple languages, to the best of our knowledge, this is the first work to address multilingual visual text generation. It is worth mentioning that AnyText can be plugged into existing diffusion models from the community for rendering or editing text accurately. After conducting extensive evaluation experiments, our method has outperformed all other approaches by a significant margin. Additionally, we contribute the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations in multiple languages. Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality. Our project will be open-sourced on https://github.com/tyxsspa/AnyText to improve and promote the development of text generation technology.",
            "score": 0.5066003119635583,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.383544921875
        },
        {
            "corpus_id": "256868474",
            "title": "Augmented Language Models: a Survey",
            "text": "Leveraging other modalities. Prompts under the form of text may not contain enough context to correctly perform a given task. For example, a question does not call for the same answer if it is asked with a serious or ironic tone. Including various modalities into the context would probably be useful for LMs such as chatbots. As recently demonstrated by Hao et al. (2022) and Alayrac et al. (2022), LMs can also be used as a general-purpose interface with models pre-trained on different modalities. For example, Hao et al. (2022) take a number of pre-trained encoders that can process diverse modalities such as vision and language, and  (Schick et al., 2022), a LM trained to produce a plan of action and edit to the input text at each step. This process can be repeated until the generated text requires no further updates. <LM> denotes the start of the LM's output to the prompt, while </LM> denotes the end. connect them to a LM that serves as a universal task layer. The interface and modular encoders are jointly pre-trained via a semi-causal language modeling objective. This approach combines the benefits of causal and non-causal language modeling, enabling both in-context learning and open-ended generation, as well as easy fine-tuning of the encoders. Similarly, Alayrac et al. (2022) introduce Flamingo, a family of Visual Language Models (VLMs) that can handle any interleaved sequences of visual and textual data. Flamingo models are trained on large-scale multimodal web corpora containing interleaved text and images, which enables them to display in-context few-shot learning capabilities of multimodal tasks. With only a handful of annotated examples, Flamingo can easily adapt to both generation tasks such as visual question-answering and captioning, as well as classification tasks such as multiple-choice visual question-answering. Zeng et al. (2022) introduce Socratic Models, a modular framework in which various models pre-trained on different modalities can be composed zero-shot.",
            "score": 0.5064856955218264,
            "section_title": "Iterative LM calling.",
            "char_start_offset": 24889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6728515625
        },
        {
            "corpus_id": "276575523",
            "title": "Fine-Grained Video Captioning through Scene Graph Consolidation",
            "text": "To generate video-level descriptions that accurately reflect visual content, we developed a model that takes scene graphs as input and produce natural language descriptions. This model is designed to effectively capture key components and relationships within the scene graph in generated text. \n\nArchitecture We employ a modified encoder-decoder transformer architecture. To prepare the input sequence for the graph encoder, each node, edge, and attribute in the graph, represented as a word or phrase, is tokenized into NLP tokens. These tokens are mapped to their embeddings via an embedding lookup. For nodes consisting of multiple NLP tokens, their embeddings are averaged to form a single vector representation. Additionally, a [CLS] token is appended as a global node to prevent isolation among disconnected components and ensure coherence. The adjacency matrix serves as an attention mask, incorporating graph topology into the attention mechanism. The graph encoder's output is then used as key and value inputs for the cross-attention layers of the text decoder, which generates the final outputs. \n\nDataset For training, we collected approximately 2.5M text corpora that cover diverse visual scene contexts from various sources, including image caption datasets such as MS-COCO (Chen et al., 2015), Flickr30k (Young et al., 2014), TextCaps (Sidorov et al., 2020), Visual Genome (Krishna et al., 2017b), and Visual Genome paragraph captioning (Krause et al., 2017). To further enhance the dataset, we incorporated model-generated captions for Kinetics-400 (Kay et al., 2017) dataset, with four uniformly sampled frames per video. Note that neither the datasets nor the image VLMs used for generating frame captions are related to the target video captioning benchmarks. \n\nTraining The model is trained using a next-token prediction objective, aiming to reconstruct the source text conditioned on the scene graph: \n\nwhere t i represents the i th token in the source text, and N denotes the total number of tokens.",
            "score": 0.5062831367344471,
            "section_title": "Video Captioning",
            "char_start_offset": 10826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1922
                },
                {
                    "start": 1925,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 1351,
                    "end": 1373,
                    "matchedPaperCorpusId": "214693197"
                },
                {
                    "start": 1453,
                    "end": 1474,
                    "matchedPaperCorpusId": "14464447"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3271484375
        },
        {
            "corpus_id": "271050324",
            "title": "GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing",
            "text": "Let the cat accompany him on his right while changing to the art style. \n\nFigure 1: Visualized examples from GenArtist. It can accomplish various tasks, achieving unified generation and editing. For text-to-image generation, it obtains greater accuracy compared to existing models like SDXL and DALL-E 3. For image editing, it also excels in complex editing tasks. Therefore, it is nearly impossible for a well-trained model to meet all human requirements, and the use of only a single model is often sub-optimal. 2) Models still struggle with complex problems, such as lengthy and intricate sentences in text-to-image tasks or complicated instructions with multiple steps in editing tasks. Scaling up or finetuning models can alleviate this issue. However, since texts are highly variable, flexible, and can be easy to combine, there are always complex problems that a trained model cannot effectively handle. 3) Although meticulously designed, models still inevitably encounter some failure cases. Generated images sometimes fail to accurately correspond to the content of user prompts. Existing models lack the ability to autonomously assess the correctness of generated images, not to mention self-correcting them, making generated images unreliable. What we truly desire, therefore, should be a unified image generation and editing system, which can satisfy nearly all human requirements while producing reliable image results. \n\nIn this paper, we propose a unified image generation and editing system called GenArtist to address the above challenges. Our fundamental idea is to utilize a multimodal large language model (MLLM) as an AI agent, which acts as an \"artist\" and \"draws\" images according to user instructions. Specifically, in response to user instructions, the agent will analyze the user requirements, decompose complex problems, and conduct planning comprehensively to formulate the specific solutions. Then, it executes image generation or editing operations by invoking external tools to meet the user demands. After images are obtained, it finally performs verification and correction on the generated results to further ensure the accuracy of the generated images. The core mechanisms of the agent are: Decomposition of intricate text prompts. The MLLM agent first decomposes the complex problems into several simple sub-problems. For complicated text prompts in generation tasks, it extracts single-object concepts and necessary background elements.",
            "score": 0.5058038105926733,
            "section_title": "GenArtist (ours) GenArtist (ours)",
            "char_start_offset": 2037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 74,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1432
                },
                {
                    "start": 1435,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2473
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.583984375
        },
        {
            "corpus_id": "258546790",
            "title": "VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation",
            "text": "As argued in Section 1, a video-to-text model should first be pre-trained on the more abundant image-text data to learn fundamental visuallanguage representations. Fortunately, there have been a multitude of recent works on largescale image-text pre-training for generative visionlanguage tasks (Cho et al., 2021;Yang et al., 2022, inter alia). In this work, we adopt OFAlarge (Wang et al., 2022c) as the backbone of our model, which achieves state-of-the-art performance on generative image-text tasks such as image captioning and visual question answering. We thus focus on adapting an image backbone to videoto-text generation, and study a backbone-agnostic two-stage pre-training framework that is compatible with more effective generative image backbone models that may come out in the future. \n\nWe briefly introduce the OFA model in this section and refer the readers to the original paper (Wang et al., 2022c) for more details. OFA is a multimodal generative model that unifies the handling of image and text data in a single seq2seq transformer. An input image x i \u2208 R H\u00d7W \u00d7C is split into P fixed-size patches, each of which is projected into an H-dimensional embedding vector.2 Similarly, a text input x t = w 1 , . . . , w n is first tokenized into BPE tokens and then embedded to H-dimensional vectors. The image and text tokens can thus be concatenated and fed together to the encoder. On the decoder side, a single unified output vocabulary is used for all modalities, where images are discretized into tokens using sparse coding (Esser et al., 2021). \n\nOFA is trained on massive image-text data and multiple tasks, making it a flexible framework for a variety of vision-language tasks. For example, Image Captioning takes the input image as well as a text instruction \"What does the image describe?\"",
            "score": 0.5057101261959139,
            "section_title": "The Image-Text Backbone",
            "char_start_offset": 4739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 313,
                    "matchedPaperCorpusId": "231802355"
                },
                {
                    "start": 377,
                    "end": 397,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 896,
                    "end": 916,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 1544,
                    "end": 1564,
                    "matchedPaperCorpusId": "229297973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "256662241",
            "title": "Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models",
            "text": "In this paper, we presented a new approach for zero-shot storybook synthesis. Our pipeline utilized Large Language Models to generate text-based conditioning, which can replace human-crafted natural language prompts and guide image synthesis models. We then developed an iterative identity injection step, using a textual embedding and a face detector to guide the generation process of Latent Diffusion Models in terms of semantic changes and background preservation. Our experimental results demonstrated that our proposed framework outperforms both text-and imageguided semantic image editing baselines in terms of coherency preservation and background preservation.",
            "score": 0.5055883548054305,
            "section_title": "Conclusion",
            "char_start_offset": 23025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66796875
        },
        {
            "corpus_id": "258557566",
            "title": "AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion",
            "text": "Text-to-Image diffusion models [13,17,18,20,24] have demonstrated unprecedented success across a wide spectrum of generative applications [1,4,5,14,19], with farreaching influence on not only our research community but the general industry and public. Recently, text-to-video models [2,[6][7][8]10,12,15,23] began to deliver promising results, further expanding our output modality along the temporal axis. However, as these approaches are still in their infantile stage, several limitations render these frameworks yet incomplete. For example, as these models solely rely on the text prompt for guiding the entire generative process, they tend to struggle in modeling detailed temporal dynamics. Moreover, as their outputs lack accompanying audio, they are more or less closer to animated GIFs than proper videos, and even in those cases where we have the audio a priori, it is not straightforward to synchronize the video output with this additional condition. \n\nIn order to overcome these limitations, we propose to explicitly incorporate the audio modality, one of the most accessible data sources along with text and image, into our conventional text-to-image pipeline to accomplish a more controllable temporal extension. Specifically, given an off-the-shelf text-to-image diffusion model such as stablediffusion 1 , we use both the text and the audio to guide video synthesis, where the former focuses on visualizing the scene semantics while the latter is more responsible for the finegrained control of the temporal dynamics. At the high level, we frame video generation as a sequence of image translations starting from a base image (which can be either synthetic or real), and thus employ a well-performing image editing method such as prompt-to-prompt [5] to gradually alter the images in compliance with our input conditions. \n\nOur key contributions can be briefly summarized as: \n\n\u2022 To our best knowledge, we are the first to employ the combination of text and audio to guide a diffusion model for video synthesis. \n\n\u2022 We propose a simple yet effective framework for empowering text-to-image models for audiosynchronized video generation with no need for additional training or paired data.",
            "score": 0.5052720773862427,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2028
                },
                {
                    "start": 2031,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 41,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 41,
                    "end": 44,
                    "matchedPaperCorpusId": "248986576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385498046875
        },
        {
            "corpus_id": "254043934",
            "title": "CLIP2GAN: Toward Bridging Text With the Latent Space of GANs",
            "text": "Fig. 2 gives an overview of our framework, which consists of a pre-trained vision-language model (CLIP), a mapping network, and a pre-trained generation model (Style-GAN). Unlike previous work that uses a large number of image-text pairs for model training, our approach achieves text-free training by establishing a mapping relationship between the CLIP multi-modal embedding space and the StyleGAN latent space. \n\nOn the one hand, to achieve text-guided image generation without text training, we generate pseudo-text features by leveraging the image-text feature alignment of a pre-trained model. We require a universal multi-modal embedding space where the paired text and image features can be well aligned. The recent vision-language model CLIP achieves this by pre-training a large number of image-text pairs through Contrastive Learning, which is exactly what we need. On the other hand, given that StyleGAN has excellent latent space, we can perform a series of manipulations on the generated images by changing its latent code. We take advantage of the pre-trained StyleGAN2 as the model for image generation. With the help of StyleGAN's latent space, we can get high-quality text-guided image generation and image editing of images. \n\nTo generate images from text, we build a bridge between CLIP and StyleGAN through a mapping network. With this \n\nwhere C img (\u2022) denotes the image encoder of the CLIP model [36]. The image features f img are mapped to latent codes z of StyleGAN in w+ space by the mapping network as the input of the pre-trained StyleGAN, and the image x is generated by StyleGAN. x is expressed as follows, \n\nwhere M (\u2022) denotes the mapping network and G(\u2022) denotes the pre-trained StyleGAN model. By learning the consistency of the source image x and the generated image x , our generative model is implemented. The mapping network is capable of mapping the multimodal embedding space of CLIP into the w+ latent space of StyleGAN, which makes it possible to invert CLIP features back into the source images using StyleGAN.",
            "score": 0.5052332035058431,
            "section_title": "CLIP2GAN",
            "char_start_offset": 11113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1356
                },
                {
                    "start": 1359,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2053
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59912109375
        },
        {
            "corpus_id": "261030228",
            "title": "Text-Only Training for Visual Storytelling",
            "text": "Visual storytelling [1], a task aimed at generating narratives based on image sequences, has received significant interest due to its potential applications in diverse domains such as advertising, entertainment, and education. In comparison to other vision-to-language generation tasks, such as visual captioning [2], visual storytelling presents unique challenges stemming from its subjective and imaginative nature. As illustrated in Fig. 1, to create a coherent story that aligns with the visual input, each sentence must not only describe the corresponding image, but also maintain logical connections to both preceding and subsequent sentences. This dual requirement of ensuring cross-modality consistency while preserving narrative coherence constitutes the primary challenge of visual storytelling. \n\nExisting works often require large amounts of labeled data and attempt to learn both cross-modality alignment and story coherence simultaneously through end-to-end training. By training on large manually annotated data, these models are capable of generating coherent and visual-related stories. Subsequent advancements, such as the incorporation of external knowledge [3] and scene graphs [4], have further enriched generated stories with additional details. More recently, the employment of large pre-trained Transformer-based language models has led to considerable improvements in visual storytelling [5]. Nevertheless, the substantial cost associated with annotating and training extensive datasets remains a significant bottleneck, limiting the scalability of visual storytelling approaches. \n\nOn the other hand, the burgeoning capabilities of pre-trained models offer potential for leveraging these models to transfer knowledge to downstream tasks such as visual storytelling, facilitating more data-efficient learning. To this end, some prior works have combined generative language models [6][7][8] with cross-modality pretrained models [9] to explore text-only training for image captioning [10,11]. However, while these cross-modality models trained on paired image-text data successfully align text with individual images, they are limited in their capacity to comprehend the temporal structure of image sequences-an essential component of visual storytelling. \n\nMotivated by the observations discussed above, we propose a novel framework that leverages pretrained generative language models and cross-modality models for data-efficient visual storytelling. We formulate visual storytelling as a visual-conditioned story generation task.",
            "score": 0.5044563092869371,
            "section_title": "INTRODUCTION",
            "char_start_offset": 518,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1605
                },
                {
                    "start": 1608,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2280
                },
                {
                    "start": 2283,
                    "end": 2477
                },
                {
                    "start": 2478,
                    "end": 2557
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 316,
                    "matchedPaperCorpusId": "7988167"
                },
                {
                    "start": 1177,
                    "end": 1180,
                    "matchedPaperCorpusId": "199466112"
                },
                {
                    "start": 1198,
                    "end": 1201,
                    "matchedPaperCorpusId": "210178945"
                },
                {
                    "start": 1413,
                    "end": 1416,
                    "matchedPaperCorpusId": "235703153"
                },
                {
                    "start": 1912,
                    "end": 1915,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1954,
                    "end": 1957,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 2009,
                    "end": 2013,
                    "matchedPaperCorpusId": "244714558"
                },
                {
                    "start": 2013,
                    "end": 2016,
                    "matchedPaperCorpusId": "253244258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.642578125
        },
        {
            "corpus_id": "278501810",
            "title": "Selftok: Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning",
            "text": "We initialize the VLM from the pretrained Llama3-8B [2] model and expand its vocabulary with an additional 32,768 Selftok visual words. As a result, the model's vocabulary integrates both textual and visual tokens into a unified embedding space. As illustrated in Figure 2a, the VLM is trained using the standard language modeling objective, which aims to maximize the log-likelihood of multimodal token sequences in an AR fashion: \n\nwhere the sequence Y may consist of interleaved language and visual tokens, and thus   \u2208 Y denotes either a language token \u27e8  \u27e9 or a visual token \u27e8  \u27e9. Since both text and image content are represented as discrete token IDs, the prediction head is shared and supervised at each position using a cross-entropy loss. The training consists of the following two stages: \n\nStage1: Cross-modality Alignment. In this stage, we aim to learn the alignment between visual tokens and language tokens, thereby facilitating the transition of the pre-trained Llama3 model from LLM to VLM. To achieve this, we introduce four data formats designed to address the challenges of cross-modality alignment. Each format helps the model process and integrate vision and language inputs for coherent multimodal understanding and generation. The Text-to-Image format aligns caption with visual data, enabling image generation from textual descriptions. Conversely, the Image-to-Text format facilitates understanding tasks by associating visual data with textual descriptions. To address potential misalignments that can occur during text-to-image tasks, the Image-Only format is introduced, allowing the model to learn visual structure independently. Finally, the Text-Only data ensures the preservation of the model's linguistic capabilities, maintaining its ability to process and generate text. These formats and their functions are summarized in Figure 11, with special tokens such as [BOS]  Stage2: Cross-task Alignment. In this stage, we perform cross-task alignment to enable the model to learn human instructions across various tasks. This is accomplished through supervised fine-tuning (SFT) on datasets from three distinct tasks: 1) text-to-image generation, 2) image editing, and 3) image understanding.",
            "score": 0.5044358717959385,
            "section_title": "Training",
            "char_start_offset": 28020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2224
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326416015625
        },
        {
            "corpus_id": "265608927",
            "title": "Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training",
            "text": "With the significant progress made in text-to-image generation, there has been an increasing exploration of textto-3D generation where pretrained visual-language models [38,42] (e.g., CLIP [29]) or text-conditioned diffusion models [28,30,43,47] (e.g., Stable Diffusion [35]) are leveraged to generate realistic 3D objects or scenes from textual descriptions. DreamField [12] utilizes aligned image and text embeddings generated by CLIP to optimize the neural field. DreamFusion [28] proposes a Score Distillation Sampling (SDS) loss to use a pre-trained 2D diffusion model as a prior for optimization of neural field, which is then improved in Magic3D [17] with a two-stage optimization framework. These methods generate 3D objects or scenes from scratch, which cannot be used to edit existing real-world scenes due to the lack of full alignment between images and text descriptions.",
            "score": 0.5043338544378511,
            "section_title": "Text-to-3D Generation",
            "char_start_offset": 6704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 884
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 173,
                    "matchedPaperCorpusId": "238408362"
                },
                {
                    "start": 173,
                    "end": 176,
                    "matchedPaperCorpusId": "245006265"
                },
                {
                    "start": 189,
                    "end": 193,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "254125253"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "255340806"
                },
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "244799255"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "253708074"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4365234375
        },
        {
            "corpus_id": "275820726",
            "title": "Multi-aspect Knowledge Distillation with Large Language Model",
            "text": "Multimodal Large Language Models. Recently, Multimodal Large Language Models (MLLMs) [1,2,14,40,41] have shown significant performance improvements in multi-modal problems such as visual question answering and image captioning by leveraging large-scale datasets to learn a joint embedding space where images and their corresponding textual descriptions are closely aligned. GPT-4o [1] has the ability to get the context and has a humanlike text generation ability, showing strong performance not only in the natural language processing area but also in multi-modal tasks. InternVL [4] can address both text and image data and shows better performances in various multimodal tasks (such as visual understanding, language generation, and visual QA) while using fewer computing resources compared to other MLLMs. Motivated by this, we apply the rich knowledge of MLLMs to image classification. \n\nVisual tasks with linguistic information. Many studies [3,18,21,23,27,37,38] try to extract linguistic information from a large language model and use it to settle the visual problems. One method [18] leverages the linguistic knowledge for each visual category from LLM to generate the descriptions and use the descriptions in zero-shot image classification. Another method [37] creates a concise set of representative visual attributes from LLM by leveraging their learning-to-search method for interpretable visual recognition. While these methods focus on generating attributes for model training, our approach distills knowledge about various aspects, extending the model's output dimensions.",
            "score": 0.5035367617621954,
            "section_title": "Related Work",
            "char_start_offset": 4702,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1589
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 90,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "252111028"
                },
                {
                    "start": 960,
                    "end": 963,
                    "matchedPaperCorpusId": "258866192"
                },
                {
                    "start": 963,
                    "end": 966,
                    "matchedPaperCorpusId": "260682240"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "253735286"
                },
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "260682240"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "277596035",
            "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models",
            "text": "The MME-U benchmark framework presented here serves as a foundational step towards evaluating U-MLLMs on a diverse array of tasks encompassing multimodal understanding, generation, and their integration. This benchmark reveals the current landscape of U-MLLMs, highlighting their capabilities and areas for improvement. While these models demonstrate proficiency in handling various multimodal tasks, they struggle with balancing understanding and generation, handling complex instructions, and performing well on unify tasks. Moreover, current U-MLLMs . The generated results from various models in the text-to-image generation task, based on the following text prompt: A man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree. exhibit significant inconsistencies in aligning textual instructions with their visual outputs, highlighting the need for further research to improve multimodal reasoning and generation integration. However, this study simplifies the evaluation of unify tasks by framing image generation as multiple-choice questions, which may allow model \"hacking\". For instance, SEED-generated images may not meet style standards but achieve high similarity scores, inflating accuracy metrics. Future work will incorporate MLLM or CLIP scores for stricter evaluation. \n\nUnified Multimodal Large Language Models. Building on the success of MLLMs [13,30,42,44], recent studies U-MLLMs, which can understand and generate multiple modalities in an end-to-end manner. Some approaches have adopted a unified training objective, projecting both text and images into a discrete token space and employing a next-token prediction loss function for training [29,33,34]. \n\nThis training method and framework are notably straightforward. However, using discrete image tokens (e.g., extracted from VQVAE image features) may not be optimal for image understanding tasks.",
            "score": 0.5035224035066889,
            "section_title": "Conclusion and Limitation",
            "char_start_offset": 30499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2096
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73291015625
        },
        {
            "corpus_id": "270045165",
            "title": "V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM",
            "text": "The field of Natural Language Processing (NLP) has witnessed a significant transformation with the advent of Large Language Models (LLMs [21,20]).GPT-3 [2], one of the pioneering LLMs, marked a milestone by significantly scaling up the model size and training data size, showcasing exceptional performance in numerous NLP tasks and setting a trend for subsequent advancements in the field.Several models such as GPTs [18], PaLM [8], BLOOM [32], and LLaMA [29], have since emerged, each pushing the boundaries of LLMs.These models have demonstrated remarkable abilities in learning from in-context examples, reasoning, following instructions, and operating over long-context sequences.Recent endeavours in the field have concentrated on refining LLMs to better align with human instructions and feedback, with models like InstructGPT [23], ChatGPT [2], and GPT4 [22] standing out as exemplars in this regard.\n\nIn the context of building web agents, these LLMs have been leveraged extensively.However, they are primarily text-based and lack the capability to handle images or other modalities.This limitation has led to the development of Multimodal Large Language Models (MLLMs).MLLMs extend the capabilities of LLMs to understand and integrate information from multiple modalities, such as vision and audio [36].In the context of GUI automation, our primary focus is on MLLMs, where the input modalities include text and image, and the output is a corresponding text response.The architecture and functioning of MLLMs can vary, but they generally follow a similar pattern: An encoder for each data modality generates the embeddings for data of that modality, an embedding layer aligns embeddings of different modalities into the same multimodal embedding space, and then a LLM generates text responses.Models like Flamingo [1], Kosmos-1 [11], BLIP-2 [15], and PaLM-E [8] exemplify this approach.",
            "score": 0.5034677136151671,
            "section_title": "Related Work",
            "char_start_offset": 6544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 907
                },
                {
                    "start": 909,
                    "end": 991
                },
                {
                    "start": 991,
                    "end": 1091
                },
                {
                    "start": 1091,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1312
                },
                {
                    "start": 1312,
                    "end": 1476
                },
                {
                    "start": 1476,
                    "end": 1802
                },
                {
                    "start": 1802,
                    "end": 1895
                }
            ],
            "ref_mentions": [
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "257921533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "271974534",
            "title": "Parameter-Efficient Quantized Mixture-of-Experts Meets Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis",
            "text": "In this zero-shot setting, the task is to answer the question about the microscopic image solely based on the small-scale model's internal parametric knowledge. sLAVA comprises a vision encoder to capture the global context of microscopic images, and a text encoder that interprets end-user questions and the auxiliary user-provided image information. The image-grounded text encoder facilitates cross-modal learning by integrating visual information directly into text understanding, thereby generating a comprehensive multimodal representation grounded in the image's visual content. The image-grounded text decoder then synthesizes coherent and contextually relevant textual outputs based on the generated multimodal representations. Finally, the framework is jointly optimized using the binary cross-entropy loss for positive image-text matching and language modeling loss for contextually relevant text generation to answer end-user questions. \n\nimage, effectively highlighting more relevant visual regions along with their contextual relationships to understand and ground the questions within visual concepts. We incorporate a <cls> token to attend to and aggregate information from all image regions. The higher-level visual semantic representation of the global (<cls>) token represent the summary of the input image. (b) The text encoder plays a crucial role in analyzing and interpreting user input to understand the nature of the question. It leverages supporting text descriptions associated with the image to extract key details and provide accurate and relevant answers. We insert <image> token at the image location in the interleaved multimodal input. We append a <Encode> token to the tokenized text to facilitate multimodal integration, with its output embedding representing the fused image-text representation. To better capture the nuances of language and context, the text encoder leverages a pre-trained language model, Llama-2-7b (Touvron et al., 2023), to compute a high-level representation that captures the semantic meaning and relationships within the end-user question. We fine-tune Llama-2-7b using Dynamic Adaptation of Mixture of Quantized Parameter-Efficient Experts (DyA-MOQPEs) technique (details in the technical appendix) using the instruction-following dataset generated by GPT-4.",
            "score": 0.5032677416125924,
            "section_title": "Processed text",
            "char_start_offset": 9236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2320
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.421630859375
        },
        {
            "corpus_id": "277043716",
            "title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models",
            "text": "Generation Models: Large Vision-Language Models (LVLMs) integrate vision-language modality information to generate final language outputs. This evolution is marked by the integration of pre-trained vision encoders and large language models (LLMs), enabling LVLMs to process and generate language based on visual inputs. Recent advancements include architectures that employ learnable queries to distill visual information and align it with LLM-generated text, as well as models like LLaVA [30,31], InternVL [6], Ovis [33], and Qwen [43,45], which use projection layers to bridge visual features and textual embeddings. Additionally, the commercial closed-source LVLMs Claude-3.5-Sonnet (Anthropic) [2] and GPT-4o (OpenAI) [36] garner significant attention in contemporary society due to their advanced capabilities and widespread applications. Concretely, the application of LVLMs in VLP tasks extends to scenarios such as medical diagnosis [19,49], business operations [20,37], and education [9]. For Image-to-Image (I2I) Generation Models, previous architectures such as GANs [16], VAEs [24], and their variants [17,22,25] demonstrate performance to a certain extent. However, diffusion-based models, particularly DDPM [18] and its variants [28,35,48], have gained prominence due to their superior performance. Among these, CLIP-guided diffusion models, such as DALL-E 2 (UnCLIP) [41] and IP-Adapter [50], integrate the CLIP vision encoder [39] to enhance visual semantic perception, enabling the generation of highly realistic, diverse, and semantically rich images. These models have become dominant in both research and commercial applications. Concurrently, the development of Multimodal Large Language Models (MLLMs) like GPT-4 (OpenAI) [36] and Dreamina (ByteDance) [4]. While I2I tasks can also be expanded to fields such as artistic creation [47,51], fundamental scientific exploration [3,26], and historical archaeology [5,21].",
            "score": 0.50326768173246,
            "section_title": "Related Works",
            "char_start_offset": 5358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 945,
                    "matchedPaperCorpusId": "267657686"
                },
                {
                    "start": 945,
                    "end": 948,
                    "matchedPaperCorpusId": "270371398"
                },
                {
                    "start": 974,
                    "end": 977,
                    "matchedPaperCorpusId": "271051104"
                },
                {
                    "start": 993,
                    "end": 996,
                    "matchedPaperCorpusId": "270703507"
                },
                {
                    "start": 1078,
                    "end": 1082,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 1121,
                    "end": 1124,
                    "matchedPaperCorpusId": "222291664"
                },
                {
                    "start": 1243,
                    "end": 1247,
                    "matchedPaperCorpusId": "266191471"
                },
                {
                    "start": 1247,
                    "end": 1250,
                    "matchedPaperCorpusId": "251765371"
                },
                {
                    "start": 1250,
                    "end": 1253,
                    "matchedPaperCorpusId": "261558570"
                },
                {
                    "start": 1442,
                    "end": 1446,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5029296875
        },
        {
            "corpus_id": "252918469",
            "title": "Imagic: Text-Based Real Image Editing with Diffusion Models",
            "text": "or a text describing the original image [6,17,39,47,51].\n\nIn this paper, we propose a semantic image editing method that mitigates all the above problems. Given only an input image to be edited and a single text prompt describing the target edit, our method can perform sophisticated nonrigid edits on real high-resolution images. The resulting image outputs align well with the target text, while preserving the overall background, structure, and composition of the original image. For example, we can make two parrots kiss or make a person give the thumbs up, as demonstrated in Figure 1. Our method, which we call Imagic, provides the first demonstration of text-based semantic editing that applies such sophisticated manipulations to a single real highresolution image, including editing multiple objects. In addition, Imagic can also perform a wide variety of edits, including style changes, color changes, and object additions. To achieve this feat, we take advantage of the recent success of text-to-image diffusion models [47,50,53]. Diffusion models are powerful state-of-the-art generative models, capable of high quality image synthesis [16,22]. When conditioned on natural language text prompts, they are able to generate images that align well with the requested text. We adapt them in our work to edit real images instead of synthesizing new ones. We do so in a simple 3-step process, as depicted in Figure 3: We first optimize a text embedding so that it results in images similar to the input image. Then, we fine-tune the pre-trained generative diffusion model (conditioned on the optimized embedding) to better reconstruct the input image. Finally, we linearly interpolate between the target text embedding and the optimized one, resulting in a representation that combines both the input image and the target text. This representation is then passed to the generative diffusion process with the fine-tuned model, which outputs our final edited image.\n\nWe conduct several experiments and apply our method on numerous images from various domains. Our method outputs high quality images that both resemble the input image to a high degree, and align well with the target text. These results showcase the generality, versatility, and quality of",
            "score": 0.503235766022246,
            "section_title": "Introduction",
            "char_start_offset": 4009,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 43,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1148,
                    "end": 1152,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1152,
                    "end": 1155,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384765625
        },
        {
            "corpus_id": "248780435",
            "title": "DD-TIG at Constraint@ACL2022: Multimodal Understanding and Reasoning for Role Labeling of Entities in Hateful Memes",
            "text": "Four mainstream multimodal models of VL transformer architectures are applied in this work, namely: VisualBERT, UNITER, OSCAR, and ERNIE-Vil. \n\nVisualBERT (Li et al., 2019), known as the first image-text pre-training model, uses the visual features extracted by Faster R-CNN, concatenates the visual features and textual embeddings, and then feeds the concatenated features to a single transformer initialled by BERT. \n\nUNITER (Chen et al., 2020) learns contextualized joint representation of both visual and textual OSCAR (Li et al., 2020), instead of simply using image-text pair, adds object tags detected from the image and represent the image-text pair as a <Word, Tag, Image> triple to help the fusion encoder better align different modalities. \n\nERNIE-Vil (Yu et al., 2020), as a typical dualstream model, enhances the model with the application of scene utilizing scene graphs of visual scenes, which can learn the joint representations characterizing the alignments of the detailed semantics across vision and language. \n\nFor domain adaptation, we carry out continual pretraining on our dataset to reduce the distribution gap between the pretraining dataset and our memes dataset. Masked Language Modeling (MLM) pretraining task is taken on pretraining VisualBERTlarge, UNITER-large, and OSCAR-large model.",
            "score": 0.5028646553068067,
            "section_title": "Vision and Language Models",
            "char_start_offset": 5981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 144,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1315
                }
            ],
            "ref_mentions": [
                {
                    "start": 427,
                    "end": 446,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 523,
                    "end": 540,
                    "matchedPaperCorpusId": "215754208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6845703125
        },
        {
            "corpus_id": "269449406",
            "title": "TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation",
            "text": "Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a\"Screenwriter\", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the\"Rehearsal\". Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the\"Final Performance\". With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.",
            "score": 0.5025531999377324,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38232421875
        },
        {
            "corpus_id": "252781022",
            "title": "Visualize Before You Write: Imagination-Guided Open-Ended Text Generation",
            "text": "., 2022). Moreover, this line of work shows great potential in utilizing textual information to guide image synthesis. It comes naturally that one may attempt to complete the loop by using visual supervision to guide text generation.\n\nIn this work, we propose using machine-generated images to guide the language model (LM) in open-ended text generation. More specifically, we visualize machine imagination for the input context by rendering images with StableDiffusion (Rombach et al., 2022), a state-of-the-art text-to-image generator. The machine imagination acts as additional visual supervision to guide LMs in generating informative and coherent text in two ways. Firstly, the machine-generated images are introduced as the input to the LM in the form of the visual prefix. Secondly, we designed a contrastive training objective that enforces the generated text to be semantically similar to the visual supervision.\n\nWe conduct experiments on three open-ended text generation tasks, namely text completion, story generation, and concept-to-text generation. Extensive experiments in the few-shot settings show better or competitive performance to state-of-the-art baselines on both automatic metrics and human evaluation. Experiments with full-data settings show that introducing machine-generated visual supervision with our iNLG yields consistent improvements on various LM models including GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020), andT5 (Raffel et al., 2020).\n\nOur main contributions are as follows:\n\n\u2022 We introduce a novel paradigm that leverages machine-generated images to guide openended text generation. This endows the machines with the ability of creative visualization that human writers often demonstrate. \u2022 We distill the vision information from the pretrained multimodal models and further construct visual prefixes to guide language models performing text generation with teacher forcing and contrastive objectives. \u2022 Extensive experiments show the effectiveness of iNLG as a model-agnostic framework in open-ended text generation tasks, including text completion, story generation, and concept-to-text in both few-shot and full-data settings.",
            "score": 0.5025010615939165,
            "section_title": "Text Input + Visual Imagination",
            "char_start_offset": 2642,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1433,
                    "end": 1458,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52734375
        },
        {
            "corpus_id": "275544327",
            "title": "Multimodal Fake News Video Explanation: Dataset, Analysis and Evaluation",
            "text": "We chose Multimodal Large Language Models (MLLMs) and small pre-training models (SPMs) for comparison: MLLMs: 1) Qwen2-VL (Wang et al., 2024b): A series of multimodal large language models developed by Qwen team of Alibaba Cloud, with advanced image and video understanding capabilities. 2) LLaVA (Liu et al., 2023b): is a large-scale multimodal model of end-to-end train designed to understand and generate content based on visual input images and text instructions. 3) GPT-4o (Hurst et al., 2024): The latest version of GPT-4 Omni, an LLM developed by OpenAI for quick invocation. \n\nSPMs: 1) NCT (Tu et al., 2023): A neighborhood contrast converter is proposed to improve the model's ability to perceive various changes in complex scenes and to recognize complex syntactic structures. 2) HAAV (Kuo and Kira, 2023): each view in the multimode is encoded independently by a shared encoder, and the contrast loss is incorporated in the encoded view in a novel way. 3) AMFM (Zhang et al., 2024): dynamic enhancement of pre-trained visual features by learning the underlying visual relationship between frame level and video level embedment.",
            "score": 0.502421595264468,
            "section_title": "Comparative Systems",
            "char_start_offset": 27385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1138
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 316,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 598,
                    "end": 615,
                    "matchedPaperCorpusId": "257364956"
                },
                {
                    "start": 795,
                    "end": 815,
                    "matchedPaperCorpusId": "258888004"
                },
                {
                    "start": 972,
                    "end": 992,
                    "matchedPaperCorpusId": "268154033"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52978515625
        },
        {
            "corpus_id": "155992514",
            "title": "Text2Scene: Generating Abstract Scenes from Textual Descriptions",
            "text": "Most research on visually descriptive language has focused on the task of image captioning or mapping images to text [5,18,13,10,27,28,19,2]. However, there has been less work in the opposite direction of mapping text to images, this is, using text as an input to synthesize images [9,22,30,23]. Most of these recent approaches have leveraged conditional Generative Adversarial Networks (cGANs) to produce controllable image synthesis conditioned on text. While these works have managed to generate results of increasing quality, there are major challenges when attempting to synthesize images for complex scenes with multiple interacting objects. In our work, we do not attempt to produce pixel-level output but rather abstract representations containing the semantic elements that compose the scene but devoid of specific texture-level image information. Our method can be considered as an intermediate step for generation of more complex scenes. \n\nOur work is also related to prior research on using abstract scenes to mirror and analyze complex situations in the real world [31,32,6,26]. The most related is [32] where a graphical model was introduced to generate an abstract scene from the input textual description of the scene. Unlike this previous work, our method does not use a semantic parser to obtain a set of tuples but is trained directly from input sentences in an end-to-end fashion. Moreover, we show our method compares favorably to this previous work. Our work is also related to [29] where a sequence-to-sequence model is proposed to transform an abstract scene layout to text. Our work poses the problem in the opposite direction and demonstrates good results on both cartoon-like scenes and semantic layouts corresponding to real images. \n\nMost closely related to our approach are the recent works of [8] and [11], as these works also exploit some form of abstract scene generation. [8] targets image synthesis using conditional GANs (pixel-level output) but unlike prior works, it generates semantic object layouts as an intermediate step. In our work, we specifically focus on layout generation as an isolated task, and our method works for both cartoon-like scenes, and scenes corresponding to real images.",
            "score": 0.5022677690917401,
            "section_title": "Related Work",
            "char_start_offset": 5102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1760
                },
                {
                    "start": 1763,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 120,
                    "matchedPaperCorpusId": "13272863"
                },
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "13344783"
                },
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "8517067"
                },
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "1169492"
                },
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "1055111"
                },
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "3596155"
                },
                {
                    "start": 138,
                    "end": 140,
                    "matchedPaperCorpusId": "9662636"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "1515901"
                },
                {
                    "start": 288,
                    "end": 291,
                    "matchedPaperCorpusId": "1277217"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "3458362"
                },
                {
                    "start": 1078,
                    "end": 1082,
                    "matchedPaperCorpusId": "10554419"
                },
                {
                    "start": 1082,
                    "end": 1085,
                    "matchedPaperCorpusId": "5642345"
                },
                {
                    "start": 1085,
                    "end": 1087,
                    "matchedPaperCorpusId": "13912770"
                },
                {
                    "start": 1087,
                    "end": 1090,
                    "matchedPaperCorpusId": "6974607"
                },
                {
                    "start": 1112,
                    "end": 1116,
                    "matchedPaperCorpusId": "5642345"
                },
                {
                    "start": 1500,
                    "end": 1504,
                    "matchedPaperCorpusId": "9298377"
                },
                {
                    "start": 1824,
                    "end": 1827,
                    "matchedPaperCorpusId": "21012775"
                },
                {
                    "start": 1906,
                    "end": 1909,
                    "matchedPaperCorpusId": "21012775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5458984375
        },
        {
            "corpus_id": "277780150",
            "title": "3D CoCa: Contrastive Learners are 3D Captioners",
            "text": "Such systems illustrate the promise of MLLMs for 3D grounding, dense captioning, and dialogue-based interaction. \n\nHowever, these LLM-driven frameworks typically rely on an external language model and complex alignment procedures, treating captioning as just one of many tasks rather than a dedicated end-to-end objective. Consequently, fine-grained spatial details can be difficult to handle without additional tricks. In contrast, 3D CoCa takes a different route: it directly integrates multimodal pre-training into a unified captioning architecture. By jointly training a 3D scene encoder and a text decoder with a contrastive vision-language objective, 3D CoCa harnesses rich semantic priors from foundation models while remaining end-to-end trainable for the captioning task. This design eliminates the need for separate detection modules or post-hoc LLM integration; to our knowledge, 3D CoCa is the first to unify contrastive vision-language pre-training with 3D dense captioning in a single model, marking a novel paradigm in 3D captioning within the evolving MLLM-centered landscape.",
            "score": 0.5019693369719428,
            "section_title": "Related Works",
            "char_start_offset": 10982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 115,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1092
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59375
        },
        {
            "corpus_id": "270199547",
            "title": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
            "text": "Large Language Models.In recent years, the development of Large Language Models (LLMs) has significantly advanced the field of natural language processing.The debut of GPT-3 [6] marked a notable surge in performance, especially in few-shot and zero-shot learning scenarios, underscoring the substantial promise of LLMs.This potential was further demonstrated by subsequent enhancements in models such as ChatGPT [58], GPT-4 [59], Gemini [75,68], and Claude [2].Concurrently, open-source models have been rapidly evolving, including the LLaMA [76,77] series, Vicuna [14], Baichuan [85], Qwen [3], Mistral [32], and Yi [88].Notably, the open-source models Llama3 [56] and Mistral-MOE [57] have approached and, in some cases, surpassed the performance of closed-source models.Despite these advancements, LLMs inherently lack the capability to process or interpret multimodal data, limiting their application in scenarios requiring an understanding of more than just textual information.\n\nMultimodal Large Language Models.Multimodal Large Language Models (MLLMs) enhance the capabilities of LLMs by not only understanding and generating text but also interpreting and relating visual elements to textual descriptions [87].Most open-source MLLMs consist of several components, namely a vision encoder [66,73,24,90], a connector, and an LLM.The type of the connector can be roughly divided into three categories.The cross-attention-based methods isolate and integrate visual and text modalities within the LLM, as seen in models like Flamingo [1] and CogVLM [79].The query-based methods query visual embeddings via a transformer-like architecture and send the obtained visual embeddings along with the text to the LLM, exemplified by Blip-2 [38], Instruct-Blip [18], and Qwen-VL [4].The projection-based methods directly project the visual embeddings, align them to the text modality, and uniformly feed the mixed embeddings into the LLM for understanding and generation.",
            "score": 0.5014394947332824,
            "section_title": "Related Work",
            "char_start_offset": 5447,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 22,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 319
                },
                {
                    "start": 319,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 773
                },
                {
                    "start": 773,
                    "end": 983
                },
                {
                    "start": 985,
                    "end": 1018
                },
                {
                    "start": 1018,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1557
                },
                {
                    "start": 1557,
                    "end": 1777
                },
                {
                    "start": 1777,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 1296,
                    "end": 1300,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1306,
                    "end": 1309,
                    "matchedPaperCorpusId": "257767223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7041015625
        },
        {
            "corpus_id": "276094249",
            "title": "ModServe: Scalable and Resource-Efficient Large Multimodal Model Serving",
            "text": "The rapid advancement in generative AI has led to the development of large multimodal models (LMMs) capable of processing inputs across various modalities such as text, image, video, and audio. These models have demonstrated remarkable capabilities in tasks like image captioning [5,17,35], visual question answering [46,47], and multimodal dialogue systems [8,25,52]. This has led to a rapid adoption of LMMs in production services, including online applications where latency service-level objectives (SLOs) are critical. Unlike traditional large language models (LLMs) that process purely textual inputs using a single component, a decoderbased transformer architecture [55], LMMs handle fundamentally different types of inputs, each requiring distinct processing approaches. This heterogeneity introduces unique serving complexities that demand novel analysis and serving strategies. For Image-Text-to-Text models [21], the inference pipeline consists of multiple specialized stages: image preprocessing to transform raw images into tensor representations, image encoding to convert these tensors into image tokens, and a language model backend that combines text prompts with image tokens to generate text outputs. Currently, these stages are typically served as a monolithic system [4,23,56], where all components are integrated within a single serving instance and scaled together as a unified entity. \n\nUnfortunately, existing monolithic inference serving systems fail to account for multimodality, making them unable to scale effectively while meeting time-to-first-token (TTFT) SLOs, which now include image processing and encoding times. Figure 1 shows how a monolithic deployment struggles to scale as the number of images per request increases (a common scenario in multi-image or video workloads) resulting in sharp TTFT degradation. As a result, image-heavy requests can result in head-of-line (HoL) blocking, reducing system responsiveness and causing overprovisioning. \n\nOur Work. In this paper, we present the first comprehensive systems analysis of two leading LMM architectures: crossattention (CroAttn) and decoder-only (DecOnly), on both open-source LMMs and novel production LMM inference traces in Azure datacenters.",
            "score": 0.5013390704956161,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1985
                },
                {
                    "start": 1988,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 283,
                    "matchedPaperCorpusId": "235351128"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "257637217"
                },
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "249375629"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "274982704"
                },
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1288,
                    "end": 1291,
                    "matchedPaperCorpusId": "250243681"
                },
                {
                    "start": 1291,
                    "end": 1294,
                    "matchedPaperCorpusId": "261697361"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42529296875
        },
        {
            "corpus_id": "260682645",
            "title": "FASTER: A Font-Agnostic Scene Text Editing and Rendering Framework",
            "text": "Scene Text Editing (STE) is a challenging research prob-lem, that primarily aims towards modifying existing texts in an image while preserving the background and the font style of the original text. Despite its utility in numerous real-world applications, existing style-transfer-based approaches have shown sub-par editing performance due to (1) complex image backgrounds, (2) diverse font attributes, and (3) varying word lengths within the text. To address such limitations, in this paper, we propose a novel font-agnostic scene text editing and rendering framework, named FASTER, for simultaneously generating text in arbitrary styles and locations while preserving a natural and realistic appearance and structure. A combined fusion of target mask generation and style transfer units, with a cascaded self-attention mech-anism has been proposed to focus on multi-level text region edits to handle varying word lengths. Extensive evaluation on a real-world database withfurther subjective human eval-uation study indicates the superiority of FASTER in both scene text editing and rendering tasks, in terms of model per-formance and efficiency. The code and pre-trained models have been released in our Gi thub repo.",
            "score": 0.5012525874648562,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293212890625
        },
        {
            "corpus_id": "277741397",
            "title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering",
            "text": "Visual inputs are processed using a transformer-based archi- tecture that segments image into a grid of uniform patches. Let x \u2208 R H\u00d7W \u00d7C denote the input image, which is divided into N non-overlapping patches of size P \u00d7 P , resulting in a sequence x p \u2208 R N \u00d7(P 2 \u2022C) . Each patch is flattened and linearly projected into the shared embedding space. Additionally, positional embeddings are added to maintain the spatial relationships among patches. The resulting sequence of embeddings is then passed through a stack of transformer layers to generate the image embeddings. After generating both textual and visual embeddings, multimodal integration is performed by inserting the image embeddings into the text embedding sequence at predefined positions. Formally, E text \u2208 R L\u00d7d denote the text embeddings, and E image \u2208 R N \u00d7d denote the image embeddings. Given the designated positions for image tokens {i 1 , i 2 , . . . , i N }, the fused representation E fused is defined as: \n\nThis direct injection strategy eliminates the need for intermediate fusion layers or modality-specific adapters, enabling efficient and scalable multimodal integration. The fused embedding sequence is then passed to the language model component of VLMT, which employs encoder-decoder architecture with cross-modal attention mechanisms to facilitate contextual reasoning and enable downstream generation. \n\nThis architecture supports end-to-end training and inference over heterogeneous inputs while preserving the fine-grained semantics and structural integrity of each modality. It forms the foundation for both retrieval and generation tasks within the broader multimodal multi-hop question answering framework. \n\n2) VLMT Pretraining Framework: To support robust multimodal reasoning and effective vision-language integration, a three-stage pretraining framework is introduced for VLMT. The first two stages are designed to progressively align visual and linguistic representations, addressing the inherent inconsistency that arises when combining independently trained vision and language components. The final stage focuses on enhancing the model's capacity for visual question answering (VQA), enabling more accurate and contextually grounded reasoning over multimodal inputs.",
            "score": 0.5011378279064466,
            "section_title": "A. Vision-Language Multimodal Transformer (VLMT)",
            "char_start_offset": 18911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54345703125
        },
        {
            "corpus_id": "257405349",
            "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT",
            "text": "BLIP [142] combines an encoder and decoder together during pre-training, allowing for multimodal space alignment for both understanding and generation objectives. BLIP is composed of three parts, a unimodal encoder for extracting image and text features, an image-grounded text encoder which accepts image and text features as input, and an image-grounded text decoder, which accepts image features and outputs text. Except for the aligned encoder and decoder structure, the authors also designed several corresponding pre-training tasks to help the model better learn the multimodal dependency. \n\nFrozen deocders. Another way to efficiently perform to-text generation tasks is to freeze the large language model and train an image encoder only, which can also be seen as a way to perform multimodal prompting. Due to the success of prompting and in-context learning in NLP, there has been increased attention towards methods of this nature. This has led people to question whether such methods could be effective in multimodal settings as well. Frozen [143] first introduced incontext learning into vision language tasks. It freezes the language model and only trains the image encoder. The produced image representations will be embeded in the input embeddings of the language model. This method achieves state-of-the-art performance in various zero-shot and few-shot vision language tasks. Later, Alayrac et al. proposed Flamingo [144], which further explored multimodal in-context learning. Flamingo involves a frozen vision encoder and a frozen language encoder to get vision language representations, and utilizes gated cross-attention-dense layer to fuse the image representation into text representation. Recently, [145] proposed a method to realize VL dialogue with frozen language models, enabling the model to generate interleaved multimodal data. This method also freezes input encoders and train text-to-image and image-to-text linear maps to further encode and decode produced embeddings. However, it still remains a question why this kind of prompting based method work in multimodal generation. Some works have also been proposed to answer this question.",
            "score": 0.5010454843936637,
            "section_title": "Vision Language Generation.",
            "char_start_offset": 42059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 10,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 1053,
                    "end": 1058,
                    "matchedPaperCorpusId": "235658331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "260683334",
            "title": "Show Me the World in My Language: Establishing the First Baseline for Scene-Text to Scene-Text Translation",
            "text": "Machine Translation: It is a well-studied area [5,6,9,10,26,30] that aims to convert a text from its source language to a target language. Current state-of-theart models for machine translation are deep-learning based [6,9,30]. In the speech domain, Speech-to-Speech Translation (S2ST) aims to translate speech from one language to another while preserving the speaker's voice and accent [11,12,17]. Inspired by these works, we focus on text translation in the visual modality, which brings newer research challenges, such as preserving font properties and integrity of the image background, which need to be addressed to produce visually appealing translations. text [16,20], prioritizing accurate translation without addressing visually consistent text generation. A popular commercial product -Google lens3 also falls into this category. These approaches often resort to simply overlaying translated text on source images. While some studies explore end-to-end methods for text translation in images that generate text directly in pixel space [15,22], they typically deal with limited visual diversity in document-style images with plain backgrounds and fixed fonts -without tackling the complexities of scene text that we aim to address. The closest solution to our problem is Google Translate for images4 , a commercial product for visual translation of diverse scene text. However, its underlying technology remains proprietary and closed-source. We emphasize the need for the research community to study this problem openly, establish proper open-source solutions, create a benchmark, and define evaluation criteria -goals we pursue in this paper. Moreover, we observe that Google Translate still lacks translation quality and often fails to produce visually consistent results for complex cases, underlining the potential for better approaches. In Section 6.2, we provide a qualitative comparison between our work and Google Translate. \n\nEditing Text in Images: The problem of editing text in images has witnessed significant research interest in recent years [14,18,27,28,31,32]. This task aims to modify scene text to target content while retaining visual properties of the original text. SRNet [31] is one such method that learns an end-to-end GAN-based style-retention network.",
            "score": 0.5008738095187759,
            "section_title": "Related Work",
            "char_start_offset": 5028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1943
                },
                {
                    "start": 1946,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2289
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 50,
                    "matchedPaperCorpusId": "13978200"
                },
                {
                    "start": 50,
                    "end": 52,
                    "matchedPaperCorpusId": "52113461"
                },
                {
                    "start": 52,
                    "end": 54,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 60,
                    "end": 63,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "52113461"
                },
                {
                    "start": 221,
                    "end": 223,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 223,
                    "end": 226,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 388,
                    "end": 392,
                    "matchedPaperCorpusId": "118641713"
                },
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "232413877"
                },
                {
                    "start": 668,
                    "end": 672,
                    "matchedPaperCorpusId": "258960136"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "270924385"
                },
                {
                    "start": 2068,
                    "end": 2072,
                    "matchedPaperCorpusId": "235446843"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44677734375
        },
        {
            "corpus_id": "270380402",
            "title": "TRINS: Towards Multimodal Language Models that Can Read",
            "text": "Large multimodal language models have shown remarkable proficiency in understanding and editing images. However, a majority of these visually-tuned models struggle to comprehend the textual content embedded in images, primar-ily due to the limitation of training data. In this work, we introduce TRINS: a Text-Rich image11In this work, we use the phrase \u201ctext-rich images\u201d to describe images with rich textual information, such as posters and book covers. INStruction dataset, with the objective of enhancing the reading ability of the multimodal large language model. TRINS is built upon LAION22Work done during Q3 2023. using hybrid data annotation strategies that include machine-assisted and human-assisted annotation process. It contains 39,153 text-rich images, captions, and 102,437 questions. Specifically, we show that the number of words per annotation in TRINS is significantly longer than that of related datasets, providing new challenges. Furthermore, we introduce a simple and effective architecture, called a Language-Vision Reading Assistant (LaRA), which is good at understanding textual content within images. LaRA outperforms existing state-of-the-art multimodal large language models on the TRINS dataset as well as other classical benchmarks. Lastly, we conducted a comprehensive evaluation with TRINS on various text-rich image understanding and generation tasks, demonstrating its effectiveness.",
            "score": 0.5006769399397294,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4921875
        },
        {
            "corpus_id": "253581213",
            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
            "text": "We first operate entirely in the text domain, where we leverage a large language model to take in image captions and produce editing instructions and the resulting text captions after the edit. For example, as shown in Figure 2a, provided the input caption \"photograph of a girl riding a horse\", our language model can generate both a plausible edit instruction \"have her ride a dragon\" and an appropriately modified output caption \"photograph of a girl riding a dragon\". Operating in the text domain enables us to generate a large and diverse collection of edits, while maintaining correspondence between the image changes and text instructions. \n\nOur model is trained by finetuning GPT-3 on a relatively small human-written dataset of editing triplets: (1) input captions, (2) edit instructions, (3) output captions. To produce the fine-tuning dataset, we sampled 700 input captions from the LAION-Aesthetics V2 6.5+ [57] dataset and manually wrote instructions and output captions. See Table 1a for examples of our written instructions and output captions. \n\nUsing this data, we fine-tuned the GPT-3 Davinci model for a single epoch using the default training parameters. Benefiting from GPT-3's immense knowledge and ability to generalize, our finetuned model is able to generate creative yet sensible instructions and captions. See Table 1b for example GPT-3 generated data. Our dataset is created by generating a large number of edits and output captions using this trained model, where the input captions are real image captions from LAION-Aesthetics (excluding samples with duplicate captions or duplicate image URLs). We chose the LAION dataset due to its large size, diversity of content (including references to proper nouns and popular culture), and variety of mediums (photographs, paintings, digital artwork). A potential drawback of LAION is that it is quite noisy and contains a number of nonsensical or undescriptive captions-however, we found that dataset noise is mitigated through a combination of dataset filtering (Section 3.1.2) and classifier-free guidance (Section 3.2.1).",
            "score": 0.5004502817506067,
            "section_title": "Generating Instructions and Paired Captions",
            "char_start_offset": 7684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259521484375
        },
        {
            "corpus_id": "273346017",
            "title": "4-LEGS: 4D Language Embedded Gaussian Splatting",
            "text": "Video editing is a cumbersome task that requires extensive knowledge of specialized commercial software. The recent advancements in text-based generative modeling has shown great promise for streamlining this task. Given a single input video, current methods allow for modifying the appearance of the foreground or background regions [BTOAF * 22, WGW * 23, CHM23,GBTBD23]. However, prior works operate directly on image pixels, which limit the representative power of these methods, particularly for handling occlusions and geometric modifications and for synthesizing novel view directions and relighting effects. \n\nIn parallel with these advancements, the emergence of volumet-ric neural fields has opened up new possibilities for modeling dynamic scenes, captured by monocular videos [GSKH21] as well as multi-camera setups [BFO * 20, LSZ * 22]. Prior work has demonstrated their potential for performing tasks such as novel-view synthesis [AHR * 23], compositing [DFBD19, WYF * 23], and dense tracking of scene elements [LKLR23, WCC * 23]. However, these neural techniques lack the high level semantics embodied within the scene, neglecting the key role of language in providing a semantic interface for performing volumetric text-driven edits. \n\nIn this work, we seek to connect volumetric representations capturing dynamic scenes with text describing spatio-temporal regions within the scene. Our work, coined 4-LEGS (4D Language Embedded Gaussian Splatting), extends language embedded vol-umetric representations modeling static scenes to dynamic ones, allowing to localize semantics not only in 3D space but also on the temporal domain. Technically, we ground spatio-temporal embeddings into a dynamic Gaussian Splatting representation [LKLR23]. To this end, we leverage a recent video-text model [WHL * 23] for creating ground-truth rendered features. Once these features are grounded in the pretrained volumetric representation, we can extract volumetric probabilities, and render spatio-temporal maps from these probabilities -finding both the temporal segment and the pixel-level probabilities for each given text query.",
            "score": 0.5004238202049558,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 363,
                    "matchedPaperCorpusId": "237091258"
                },
                {
                    "start": 787,
                    "end": 795,
                    "matchedPaperCorpusId": "234482661"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2410888671875
        },
        {
            "corpus_id": "266044186",
            "title": "WonderJourney: Going from Anywhere to Everywhere",
            "text": "Our goal is to generate a potentially endless sequence of diverse yet coherently connected 3D scenes, which requires both geometric understanding of 3D scenes and visual common sense and semantic understanding of scene structures. To tackle this challenging task, we propose WonderJourney. The main idea is to generate a text description of the visual elements the next scene would contain, and then employ a text-guided visual generation module to make that 3D scene. \n\nWonderJourney is a modular framework that decomposes this task into scene description generation, visual scene generation, and visual validation, as in Fig. 2. Given an input image or text, we first pair it with the other modality by generating the image with a text-to-image model or by generating the description with a Vision Language Model (VLM). Then we generate the next-scene description by a Large Language Model (LLM). A visual scene generation module takes in the next-scene description and the current scene image to generate the next 3D scene represented by a colored point cloud. This generation process is then checked by a VLM to make sure there is no unwanted effects, or it gets regenerated. We note that our framework is modular such that each module can be implemented with the latest (pretrained) models and thus can easily leverage the quick advances of large language and vision models.",
            "score": 0.5003695390830596,
            "section_title": "Approach",
            "char_start_offset": 7563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1379
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.471435546875
        },
        {
            "corpus_id": "248512473",
            "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "text": "image and video understanding [10,11]. However, these models rely heavily on image annotations as labeled vectors and do not bake in knowledge of free-form human natural language, hindering their application to downstream tasks that involving both vision and language modalities. (2) Recently, a line of research [12,13,14] has shown the feasibility of image-text foundation model candidates by pretraining two parallel encoders with a contrastive loss on web-scale noisy image-text pairs. In addition to the visual embeddings for vision-only tasks, the resulting dual-encoder models can additionally encode textual embeddings to the same latent space, enabling new crossmodal alignment capabilities such as zero-shot image classification and image-text retrieval. Nonetheless, these models are not directly applicable for joint vision-language understanding tasks such as visual question answering (VQA), due to missing joint components to learn fused image and text representations. \n\n(3) Another line of research [15,16,17] has explored generative pretraining with encoder-decoder models to learn generic vision and multimodal representations. During pretraining, the model takes images on the encoder side and applies Language Modeling (LM) loss (or PrefixLM [3,16]) on the decoder outputs. For downstream tasks, the decoder outputs can then be used as joint representations for multimodal understanding tasks. While superior vision-language results [16] have been attained with pretrained encoder-decoder models, they do not produce text-only representations aligned with image embeddings, thereby being less feasible and efficient for crossmodal alignment tasks. \n\nIn this work, we unify single-encoder, dual-encoder and encoder-decoder paradigms, and train one image-text foundation model that subsumes the capabilities of all three approaches. We propose a simple model family named Contrastive Captioners (CoCa) with a modified encoder-decoder architecture trained with both contrastive loss and captioning (generative) loss.",
            "score": 0.5003422870543675,
            "section_title": "Image Encoder",
            "char_start_offset": 753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "235376986"
                },
                {
                    "start": 313,
                    "end": 317,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 317,
                    "end": 320,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "1169492"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46728515625
        },
        {
            "corpus_id": "260293142",
            "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
            "text": "The PaLI-X model architecture consists of a ViT-22B Dehghani et al. (2023) to process images, which can accept sequences of  images, leading to  \u00d7  tokens per image, where  is the number of patches per image. The image tokens passing over a projection layer is then consumed by an encoder-decoder backbone of 32B parameters and 50 layers, similar to UL2 Tay et al. (2023), which jointly processes text and images as embeddings to generate output tokens in an auto-regressive manner. The text input usually consists of the type of task and any additional context (e.g., \"Generate caption in \u27e8lang\u27e9\" for captioning tasks or \"Answer in \u27e8lang\u27e9: question\" for VQA tasks). \n\nThe PaLI-3B model trained on Language-Table (Table 1) uses a smaller ViT-G/14 (Zhai et al., 2022) (2B parameters) to process images, and UL2-3B (Tay et al., 2023) for the encoder-decoder network. \n\nThe PaLM-E model is based on a decoder-only LLM that projects robot data such as images and text into the language token space and outputs text such as high-level plans. In the case of the used PaLM-E-12B, the visual model used to project images to the language embedding space is a ViT-4B Chen et al. (2023b). The concatenation of continuous variables to textual input allows PaLM-E to be fully multimodal, accepting a wide variety of inputs such as multiple sensor modalities, object-centric representations, scene representations and object entity referrals.",
            "score": 0.5003364186629267,
            "section_title": "D. VLMs for RT-2",
            "char_start_offset": 37785,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 666
                },
                {
                    "start": 669,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1428
                }
            ],
            "ref_mentions": [
                {
                    "start": 747,
                    "end": 766,
                    "matchedPaperCorpusId": "235367962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3154296875
        },
        {
            "corpus_id": "262044099",
            "title": "Progressive Text-to-Image Diffusion with Soft Latent Direction",
            "text": "Figure 1: Existing text-to-image synthesis approaches struggle with textual prompts involving multiple entities and specified relational directions. We propose to decompose the protracted prompt into a set of short commands, including synthesis, editing and erasing operations, using a Large Language Model (LLM) and progressively generate the image. \n\nTo overcome these hurdles, we present the Stimulus, Response, and Fusion (SRF) framework, assimilating a stimulus-response generation mechanism along with a latent fusion module into the diffusion process. Our methodology involves employing a fine-tuned GPT model to deconstruct complex texts into structured prompts, including synthesis, editing, and erasing operations governed by a unified SRF framework. Our progressive process begins with a real image or synthesized background, accompanied by the text prompt, and applies the SRF method in a step-by-step approach. Unlike previous strategies that aggressively manipulate the cross-attention map (Wu et al. 2023;Ma et al. 2023), our operation guides the attention map via a soft direction, avoiding brusque modifications that may lead to discordant synthesis. Additionally, when addressing relationships like \"wearing\" and \"playing with\", we begin by parsing the relative positions of the objects with the help of GPT, after which we incorporate the relational description and relative positions into the diffusion model to enable object interactions. \n\nIn summary, we unveil a novel, progressive text-to-image diffusion framework that leverages the capabilities of a Language Model (LLM) to simplify language description, offering a unified solution for handling synthesis and editing patterns concurrently. This represents an advancement in textto-image generation and provides a new platform for future research.",
            "score": 0.500323031719116,
            "section_title": "\u00d7",
            "char_start_offset": 3559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1823
                }
            ],
            "ref_mentions": [
                {
                    "start": 1020,
                    "end": 1035,
                    "matchedPaperCorpusId": "257219948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474365234375
        },
        {
            "corpus_id": "265150437",
            "title": "What Large Language Models Bring to Text-rich VQA?",
            "text": "With the emergence of LLMs, Multimodal Large Language Models (MLLMs) have also made progress on a wide range of tasks including language, vision and vision-language tasks. MLLMs first connect image features into the same word embedding space, and then leverage the pre-trained LLMs to obtain natural language outputs. We group MLLMs into three categories: Flamingo family, BLIP family and others. \n\nFlamingo family. Flamingo (Alayrac et al., 2022) applies a Perceiver Resampler on vision features, and outputs texts through Chinchilla (Hoffmann et al., 2022)  Others. In the context of LLMs, more and more MLLMs are proposed by various researchers. MiniGPT-4 (Zhu et al., 2023) uses Vicuna (Chiang et al., 2023) as LLM, and only trains a projection layer. LLaVA (Liu et al., 2023a) also uses Vicuna, and applies language-only GPT-4 to generate   (Singh et al., 2019), OCRVQA (Mishra et al., 2019) and DocVQA (Mathew et al., 2021). StVQA derives from a challenge on scene text visual question answering, in which comprehending the textual details within a scene becomes essential to provide an accurate response. TextVQA is a concurrent dataset that also requires models to read and reason about texts in images to answer questions about them. OCRVQA consists of 207,572 images featuring book covers and encompasses over 1 million question-answer pairs related to these images. DocVQA stands for Document Visual Question Answering. In this task, a comprehensive understanding of a document image is crucial to furnish an accurate response. In comparison, StVQA and TextVQA rely more on scene recognition, while OCRVQA and DocVQA are only related to texts and their layout.",
            "score": 0.500284856190363,
            "section_title": "MULTIMODAL LARGE LANGUAGE MODELS",
            "char_start_offset": 6439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1671
                }
            ],
            "ref_mentions": [
                {
                    "start": 425,
                    "end": 447,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 846,
                    "end": 866,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 875,
                    "end": 896,
                    "matchedPaperCorpusId": "209413409"
                },
                {
                    "start": 908,
                    "end": 929,
                    "matchedPaperCorpusId": "220280200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "266326262",
            "title": "From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models",
            "text": "The future sees generative models that seamlessly integrate multiple modalities-text, image, sound, video, and more-offering a holistic understanding of the world and generating content that overcomes the limitations of current models. Recent advancements have already led to transformers capable of generating not just text, but also image, audio, and video [91]. These multimodal models are expected to evolve into sophisticated systems capable of processing and understanding inputs from various modalities simultaneously. \n\nIn the future, we anticipate the emergence of single applications and more advanced multimodal models. These systems would not only understand inputs from different sensory channels-such as visual, auditory, and textual-but also generate outputs in various forms, moving well beyond mere text generation. The integration of these modalities in a single model offers a more comprehensive approach to understanding complex real-world scenarios and creating more nuanced and contextually relevant outputs.",
            "score": 0.49998359902153644,
            "section_title": "Multimodal Models",
            "char_start_offset": 57486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1030
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "249642175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.413330078125
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "Fig. 3. Illustration of diffusion-based text-to-image generation models, where various conditions beyond text are introduced to steer the outcomes. The image generation is formulated as a pair of Markov chains: a forward process that gradually corrupts input data by adding Gaussian noise, and a reverse process that learns a parameterized distribution to iteratively denoise back to the input data. \n\nediting, subject-driven generation, and visual-conditional generation. UniReal [85] treats image-level tasks as discontinuous video generation, treating varying numbers of input and output images as frames, enabling seamless support for tasks such as image generation, editing, customization, and composition. GenArtist [86] provides a unified image generation and editing system, coordinated by a multimodal large language model (MLLM) agent. UniVG [87] treats multi-modal inputs as unified conditions with a single set of weights to enable various downstream applications. As research in this domain advances, it is expected that increasingly unified models will emerge, capable of addressing a broader spectrum of image generation and editing tasks. \n\nAutoregressive models. Autoregressive (AR) models define the joint distribution of a sequence by factorizing it into a product of conditional probabilities, whereby each element is predicted in turn based on all previously generated elements. This paradigm, originally devised for language modeling, has been successfully adapted to vision by mapping an image to a 1D sequence of discrete tokens (pixels, patches, or latent codes). Formally, given a sequence x = (x 1 , x 2 , ..., x N ), the model is trained to generate each element by conditioning all preceding elements: \n\nwhere \u03b8 is the model parameters. The training objective is to minimize the negative log-likelihood(NLL) loss: \n\nAs shown in Fig. 4, existing methods are divided into three types based on sequence representation strategies: pixelbased, token-based, and multiple-tokens-based models. 1) Pixel-based models. PixelRNN [88] was the pioneering method for next-pixel prediction. It transforms a 2D image into a 1D sequence of pixels and employs LSTM layers to sequentially generate each pixel based on previously generated values.",
            "score": 0.49996173173752556,
            "section_title": "Text-to-image Diffusion Model",
            "char_start_offset": 13316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "271050324"
                },
                {
                    "start": 2047,
                    "end": 2051,
                    "matchedPaperCorpusId": "8142135"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "264828939",
            "title": "CapsFusion: Rethinking Image-Text Data at Scale",
            "text": "Large Multimodal Models [3,28,35,48,56,65,70] serve as a powerful generalist for diverse multimodal tasks. Typical LMM generalist unifies image-to-text tasks only (e.g. image captioning and visual question answering). Recent studies such as Emu [50] further enhance the capabilities of multimodal generalist by enabling it to perform both image-to-text and text-to-image tasks in a zero-shot manner [22,29,30,57,69]. \n\nLearning Objective of LMM The LMM generalist ability originates from a GPT-style auto-regressive training objective [42], wherein the model learns to predict the next token in a sequence. As a result of this training paradigm, during inference, the model exhibits a remarkable capability to generate appropriate completions for a wide range of tasks. \n\nImage-text pairs are the most commonly used multimodal pretraining data for learning vision-language alignment. Specifically, given a dataset D consisting of imagetext pairs (I, T ), where I represents the image and T represents the text represented by a sequence of text tokens T = {t 1 , t 2 , . . . , t n }. The typical training objective is maximizing the conditional likelihood of text tokens T given I in an auto-regressive manner: Caption Generation Given raw image-text pairs, CAPS-FUSION first generates synthetic captions using image captioning models following [1,34]. In our preliminary experimental analysis (Figs. 1 to 3), we find that raw captions contain a wealth of real-world knowledge but are noisy, while synthetic captions have clean structures but lack in-depth real-world knowledge, which exhibits severe scalability issues. Building upon these observations, our objective is to develop a scalable framework to organically integrate information from both raw and synthetic captions, by training a model that learns to absorb the essence of both to create a comprehensive refined image-text dataset. \n\nCaption Fusion via ChatGPT ChatGPT exhibits impressive ability in following human instructions to accomplish tasks. In CAPSFUSION, we use ChatGPT to fuse raw and synthetic captions given a prompt.",
            "score": 0.4999349001891841,
            "section_title": "CAPSFUSION",
            "char_start_offset": 8660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 27,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1347,
                    "end": 1350,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69873046875
        },
        {
            "corpus_id": "265506621",
            "title": "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation",
            "text": "Figure 2. Model Architecture: CoDi-2 comprises a multimodal large language model that encompasses encoder and decoder for both audio and vision inputs, as well as a large language model. This architecture facilitates the decoding of image or audio inputs using diffusion models. In the training phase, our approach employs pixel loss obtained from the diffusion models alongside token loss, adhering to the standard causal generation loss. \n\npromise in transforming a subject into a new scene. However, they often necessitate task-specific or subject-specific tuning. This specialization often confines the models within the boundaries of their training data, impeding their ability to generalize beyond learned tasks or subjects. Kosmos-G [20] furthers the efforts for zero-shot image generation with in-context interleaved image and text. But its efforts is limited to image composition. Lastly, learning visual concepts and apply them in image generation is also a growing direction [9,11]. For example, multi-concept customization to text-to-image generation [11] requires the model to extract visual concept like a moon gate or a certain subject and apply them in image generation. The aspiration to develop a model with in-context multimodal reasoning abilities to transcend these limitations inspires our versatile framework that takes in task instructions and perform in-context zeroshot generation.",
            "score": 0.49988773951292953,
            "section_title": "Multimodal Large Language Model",
            "char_start_offset": 8543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1407
                }
            ],
            "ref_mentions": [
                {
                    "start": 989,
                    "end": 992,
                    "matchedPaperCorpusId": "254408780"
                },
                {
                    "start": 1063,
                    "end": 1067,
                    "matchedPaperCorpusId": "254408780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "273653892",
            "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies",
            "text": "In the following days, U-PaLM [69], FlAN-PaLM [35] and PaLM-2 were released. \n\n3) Multimodal Large Language Models: a) MLLM: Background: Multimodal Large Language Model (MLLM), is an extension of LLM which adopts multimodal information as input such as text, sound, video, etc. to enable multiple dimensional reasoning and text generation. \n\nBefore the emergence of MLLM, significant research efforts were dedicated to multi-modality. These efforts can generally be categorized into representative and generative paradigms. An exemplary work in the representative paradigm is CLIP [70], which serves as a foundational contribution. \n\nThis process yields a visual encoder [71][72] and a text encoder, effectively establishing a bridge for downstream multimodal tasks. In contrast, generative frameworks [73][74] approach multimodal tasks by transforming them into sequenceto-sequence tasks. MLLM distinguishes itself from previous multimodal research in two key aspects. b) MLLM: Architecture: Figure 1 illustrates the mainstream architecture of multimodal large language models, typically composed of three modules: a multimodal encoder, an LLM, and a modal connector. \n\nMultimodal Encoder. This module incorporates non-text inputs, such as images or audio, and encoding the raw infor- Modal Connector. This module serves as a crucial bridge between different modalities, allowing efficient communication with the LLM. It accomplishes this by projecting information into a space that the LLM can readily comprehend. Through training the connector, the encoded multimodal tokens can be transformed to LLM prompt tokens that illustrate the content presented by the image, video, etc. Consequently, the LLM will generate the expected content based on the request and prompt. \n\nB. Optimization , Datasets, and Evaluation of Large Language Models 1) Instruction Tuning: Instruction tuning in large language models has undergone significant development, evolving from initial efforts in multi-task fine-tuning without explicit instruction prompts to sophisticated techniques leveraging diverse tasks and templates.",
            "score": 0.4996899872629542,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 29224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 79,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "246634906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "274776700",
            "title": "Advances in Transformers for Robotic Applications: A Review",
            "text": "Original Transformers were developed for NLP application specifically to use encoder-decoder mechanism for for text. Multimodal Transformers are capable of processing multiple type of inputs, ViLBERT [86] extended the popular BERT [26] architecture introducing a co-attention mechanism, by exchanging key-value pairs in multi-headed attention, the structure incorporates linguistic queues to visual ones and vice versa, see Figure ?? \n\nModel Key Technique CLIP [112] Jointly learns embeddings for text and images, leveraging contrastive learning. FLAVA [130] Unified architecture for vision, language, and their joint representation. Data2Vec [7] Multimodal framework using the same architecture for vision, audio, and text. VisualGPT [74] Combines image features with text generation for vision-to-language tasks. DALL-E [115] Text-to-image generation using multimodal Transformer architecture. ALIGN [55] Pretrained on large-scale image-text pairs for robust visual and language tasks. MERLOT [162] Learns multimodal representation for video and text understanding. VideoBERT [132] Processes video frames and associated text for videolanguage tasks. \n\n\u2022 VisualBERT [74]: Integrates text and image data, with potential applications in robotic vision systems where contextual understanding of visual inputs is required. For example, in human-robot interaction scenarios, understanding both spoken commands and visual context enhances a robot's ability to interact with humans naturally. \n\n\u2022 UNITER [78]: Pretrained on multimodal datasets, it is useful for tasks requiring joint text and image understanding, such as scene description and semantic mapping, which can improve the situational awareness of service robots.",
            "score": 0.49966700060384855,
            "section_title": "Multimodal Transformers",
            "char_start_offset": 13314,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 235,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59912109375
        },
        {
            "corpus_id": "267897370",
            "title": "LLMBind: A Unified Modality-Task Integration Framework",
            "text": "In this work, to achieve both updateability and expandability, we introduce LLMBind, a unified framework to integrate models for various modality tasks. As depicted in Fig. 1(d), LLMBind is capable of processing two modalities: image and text. It processes these inputs through a visual encoder [46] and LLM [8], ultimately producing task-specific text prompts or token embeddings. For tasks such as video generation [10,52], image generation [43,58], audio generation [35,67], and image editing [5,14], we utilize text prompts as input to existing pre-trained generation models, significantly reducing the training complexity of our framework and enhancing its updateability. For modality tasks like image segmentation [29,34], classification [42,69], and detection [15] that cannot be solely addressed with text prompts, we extract relevant semantic visual information from images using learnable task-specific token embeddings. Subsequently, these embeddings are utilized as an auxiliary input to the corresponding modality-task models. \n\nTo guide the LLM to generate task-specific text prompts and token embeddings, we expand the vocabulary of LLM with learnable task-specific tokens, which can be divided into two categories: Task-Prompt Tokens and Semantic-Embedding Tokens. By training the LLM with specially designed instruction data pairs and taskspecific tokens, we can instruct the model to discriminate the task type from user input and generate the corresponding text prompt or token embedding for a given multimodal task. Moreover, to better unify different modality tasks within the shared LLM, we introduce a highly effective technique called Mixture-of-Experts (MoE), which has been used successfully in recent studies [1,13,13,30,51,55]. By incorporating this technique, our LLMBind can handle various modality tasks by utilizing different experts to collaboratively process multi-modal tasks, as shown in Fig. 2. \n\nIn practical applications, users often require multi-turn interactive generation and editing to achieve precise and satisfactory results.",
            "score": 0.49965611689792766,
            "section_title": "Image Generation Image Segmentation",
            "char_start_offset": 1986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 443,
                    "end": 447,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 469,
                    "end": 473,
                    "matchedPaperCorpusId": "256390486"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "250698823"
                },
                {
                    "start": 496,
                    "end": 499,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 724,
                    "end": 727,
                    "matchedPaperCorpusId": "258999674"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "254246917"
                },
                {
                    "start": 748,
                    "end": 751,
                    "matchedPaperCorpusId": "253735286"
                },
                {
                    "start": 1736,
                    "end": 1739,
                    "matchedPaperCorpusId": "5223939"
                },
                {
                    "start": 1739,
                    "end": 1742,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1742,
                    "end": 1745,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1748,
                    "end": 1751,
                    "matchedPaperCorpusId": "235417196"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "275342583",
            "title": "SceneBooth: Diffusion-based Framework for Subject-preserved Text-to-Image Generation",
            "text": "Due to the demand for personalizing image generation, subject-driven text-to-image generation method, which creates novel renditions of an input subject based on text prompts, has received growing research interest. Existing methods often learn subject representation and incorporate it into the prompt embedding to guide image generation, but they struggle with preserving subject fidelity. To solve this issue, this paper approaches a novel framework named SceneBooth for subject-preserved text-to-image generation, which consumes inputs of a subject image, object phrases and text prompts. Instead of learning the subject representation and generating a subject, our SceneBooth fixes the given subject image and generates its background image guided by the text prompts. To this end, our SceneBooth introduces two key components, i.e., a multimodal layout generation module and a background painting module. The former determines the position and scale of the subject by generating appropriate scene layouts that align with text captions, object phrases, and subject visual information. The latter integrates two adapters (ControlNet and Gated Self-Attention) into the latent diffusion model to generate a background that harmonizes with the subject guided by scene layouts and text descriptions. In this manner, our SceneBooth ensures accurate preservation of the subject's appearance in the output. Quantitative and qualitative experimental results demonstrate that SceneBooth significantly outperforms baseline methods in terms of subject preservation, image harmonization and overall quality.",
            "score": 0.4995175322455826,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447265625
        },
        {
            "corpus_id": "272550385",
            "title": "MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated Learning",
            "text": "Pretraining Dataset As discussed in Sec. 1, the wealth of open-source multimodal data, such as images and their captions, remains underutilized resources for pretraining FL models. Often, these datasets are noisy, unlabeled or contain elements that are too complex, making them unsuitable for straightforward pretraining of the compact FL models. However, given the current advanced capabilities in multimodal processing of MLLMs, we now have new, convenient methods to leverage such data for pretraining purposes. Utilizing GPT-4, akin to the approach used in LLaVA, we can transform complex image data collected from the internet into three main categories: \n\n\u2022 Conversation: This category includes dialogues between an assistant and an individual seeking specific information about a photo. The assistant's responses simulate observing the image directly, answering a variety of questions about the visual content, such as identifying objects, counting them, describing actions, pinpointing locations, and noting their spatial relations. \n\n\u2022 Detailed Description: To gain a thorough understanding of an image, we formulated a series of questions designed to elicit detailed descriptions. Responses to these questions were generated using GPT-4, enriching our dataset with nuanced insights into the images. Leveraging the aforementioned dataset formulations, we are equipped to facilitate the pretraining of FL models with the support of Multimodal Large Language Models. \n\nPretraining Mechanism Our pretraining mechanism draws inspiration from the structure of LLaVA, a highly effective and recent multimodal large language model. It consists of three key components: a visual encoder g, which is a frozen pretrained CLIP model; a projection layer designed to align the features of the visual model with the text domain embeddings, where the projection layer is a trainable matrix W ; and a part comprising a large language model (LLM), typically employing promising models such as Vicuna or LLaMA-2. The workflow of LLaVA proceeds as follows: For the data formats mentioned earlier, whether it be a conversation, detailed description, or complex reasoning, the input includes a text modality instruction X q (e.g., \"Could you provide a detailed description of this image?\") and an image X v .",
            "score": 0.49922866669247035,
            "section_title": "Global Multimodal Pretraining",
            "char_start_offset": 10808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2277
                },
                {
                    "start": 2278,
                    "end": 2296
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5390625
        },
        {
            "corpus_id": "266362590",
            "title": "Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model",
            "text": "Minority languages, such as Arabic, Thai, and Kazakh, not only have a significant number (reaching 5000 to 7000), but their low-resource nature also impedes the progress of computer vision, particularly in the domain of image generation. In recent years, with the advancement of diffusion models (Ho, Jain, and Abbeel 2020), significant progress has been made in generating realistic and prompt-aligned images (Rombach et al. 2022;Ramesh et al. 2022;Saharia et al. 2022). However, achieving accurate scene text generation remains challenging due to the fine-grained structure within the scene text. \n\nRecent efforts utilize diffusion models to overcome the limitations of traditional methods and enhance text rendering quality. For instance, Imagen (Saharia et al. 2022) and DeepFloyd (DeepFloydLab 2023) use the T5 series to gen-erate text better. While these methods are capable of generating structurally accurate scene text, they demand a large amount of training data which is not suitable for minority languages and still lack control over the generated scene text. Some researchers (Wu et al. 2019;Yang, Huang, and Lin 2020;Lee et al. 2021;Krishnan et al. 2023) exploit GAN (Goodfellow et al. 2014) based scene text editing methods to generate scene text, which is more controllable. However, these methods are confined to generating scene text at the string level and do not possess the capability to generate complete scene compositions. \n\nTo tackle these challenges, we propose a training-free framework, Diff-Text, and a simple yet highly effective approach for multilingual scene text image generation. Our proposed framework inherits the off-the-shelf diffusion model while specializing in text generation by localized attention constraint method along with positive and negative image-level prompts. Specifically, given a text to be rendered, we first render it to a sketch image and then detect the edge map which is used as the control input of our model. Our model generates a realistic scene image according to the control input and the prompt input which contains a description of a scene. However, the control inputs are easily treated as grotesque patterns instead of texts on signs or billboards.",
            "score": 0.4990624193599603,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 410,
                    "end": 431,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 431,
                    "end": 450,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 450,
                    "end": 469,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 749,
                    "end": 770,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1089,
                    "end": 1105,
                    "matchedPaperCorpusId": "262823915"
                },
                {
                    "start": 1105,
                    "end": 1131,
                    "matchedPaperCorpusId": "259251942"
                },
                {
                    "start": 1131,
                    "end": 1147,
                    "matchedPaperCorpusId": "236318544"
                },
                {
                    "start": 1147,
                    "end": 1168,
                    "matchedPaperCorpusId": "236318544"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27783203125
        },
        {
            "corpus_id": "273345282",
            "title": "FlexGen: Flexible Multi-View Generation from Text and Image Inputs",
            "text": "Figure 3: Overview of the framework. FlexGen is a flexible framework to generate controllable and consistent multi-view images, conditioned on a single-view image, or a text prompt, or both. The system incorporates a 3D-aware annotation method using GPT-4V and an adaptive dual-control module that integrates both a reference input image and text prompts for precise joint control. The condition switcher enhances flexibility, enabling the model to generate multi-view images based on image input, text input, or a combination of both modalities. lacking 3D-aware information. Alternatively, we construct a dataset consists of paired multi-view images and 3D-aware global-local text annotation, as shown in the Figure 4. Our dataset is based on Objaverse (Deitke et al., 2023), which provides basic shape and textual for each object, lacking high-quality textual description for each 3D asset. Therefore, we utilized an advanced large-scale multimodal model, GPT-4V, to generate high-quality textual descriptions for each 3D asset. Given these orthogonal views, GPT-4V not only summarizes a comprehensive global description of the object but also captures the intricate 3D relationships between its components. Specifically, the dataset construction process consists of three steps: rendering, captioning, and merging. (1) In the rendering stage, each 3D asset is rendered into four orthogonal multi-view images with a resolution of 512\u00d7512, which are tiled into a single image I out in a 2 \u00d7 2 layout. (2) The captioning step is performed using GPT-4V with tiled image, which generates 3D-aware global and local captions. \n\n(3) In the final step, the global and local descriptions are merged to form the \"global-local text description\" of the 3D asset. During training, we randomly select a portion of the local descriptions to simulate user behavior. \n\nMoreover, we incorporate material descriptions, such as metallic and roughness attributes, into the text annotations to enable material-controllable generation. We propose adding material descriptions that correspond to the materials used during rendering by blender Hess (2013).",
            "score": 0.49852707916325123,
            "section_title": "Global discription",
            "char_start_offset": 12084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 755,
                    "end": 776,
                    "matchedPaperCorpusId": "254685588"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384765625
        },
        {
            "corpus_id": "255749330",
            "title": "ChatGPT is not all you need. A State of the Art Review of large Generative AI models",
            "text": "Sometimes, it is also useful to obtain a text that describes an image, that is precisely the inverse mapping to the one that has been analyzed in the previous subsections. In this section, we analyze two models that perform this task, along with others: Flamingo and VisualGPT. \n\nFlamingo : A Visual Language Model created by Deepmind using few shot learning on a wide range of open-ended vision and language tasks, simply by being prompted with a few input/output examples [1]. Concretely, the input of Flamingo contains visually conditioned autoregressive text generation models able to ingest a sequence of text tokens interleaved with images and/or videos and produce text as output. A query is made to the model along with a photo or a video and the model answers with a text answer. Some examples can be observed in Figure 10. Flamingo models take advantage of two complementary models: a vision model that analyzes visual scenes and a large language model which performs a basic form of reasoning. The language model is trained on a large amount of text data. VisualGPT : VisualGPT is an image captioning model made by OpenAI [7]. Concretely, VisualGPT leverages knowledge from the pretrained language model GPT-2 [5]. In order to bridge the semantic gap between different modalities, a novel encoder-decoder attention mechanism [33] is designed with an unsaturated rectified gating function. Critically, the biggest advantage of this model is that it does not need for as much data as other image-to-text models. In particular, improving data efficiency in image captioning networks would enable quick data curation, description of rare objects, and applications in specialized domains. Most interestingly, the API of this model can be found on GitHub. We include three examples of text prompts generated by the model with respect to three images fed to the model in Figure 11.",
            "score": 0.49824284743723635,
            "section_title": "Image-to-Text models",
            "char_start_offset": 11700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1885
                }
            ],
            "ref_mentions": [
                {
                    "start": 1133,
                    "end": 1136,
                    "matchedPaperCorpusId": "235351128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53662109375
        },
        {
            "corpus_id": "277043335",
            "title": "Small Vision-Language Models: A Survey on Compact Architectures and Techniques",
            "text": "SimVLM [92] introduces a simplified vision-language pretraining approach by employing a single prefix language modeling (PrefixLM) [22] objective, which unifies bidirectional contextual representation and autoregressive text generation. Its architecture is based on a transformer encoder-decoder framework, where raw image patches are processed through a convolutional stage inspired by ResNet [33], and textual inputs are tokenized with positional encodings. The model is trained end-to-end on large-scale weakly supervised datasets, such as ALIGN [84], without relying on object detection modules or auxiliary losses. However, its reliance on noisy web-crawled data may result in subopti-mal performance in tasks requiring fine-grained understanding. The absence of explicit region-level reasoning limits its effectiveness in capturing precise spatial relationships and object interactions. High computational requirements, including the use of TPU v3 chips, pose scalability challenges. Additionally, dependence on weak supervision constrains the model's ability to learn detailed vision-language associations. In open-ended VQA [40] tasks, SimVLM struggles to generate meaningful responses without additional finetuning due to the noisiness of pretraining data. While demonstrating strong zero-shot capabilities, it still falls short in complex reasoning tasks compared to fully supervised models. Furthermore, its optimization for multimodal learning results in performance trade-offs in single-modality tasks like language understanding and image classification. \n\nFigure 5: BLIP Architecture: Proposed a multimodal mixture of encoder-decoder, a unified vision-language model which can operate in one of the three functionalities: (1) Unimodal encoder is trained with an image-text contrastive (ITC) loss to align the vision and language representations. (2) Image-grounded text encoder uses additional crossattention layers to model vision-language interactions, and is trained with a image-text matching (ITM) loss to distinguish between positive and negative image-text pairs. (3) Image-grounded text decoder replaces the bi-directional self-attention layers with causal self-attention layers, and shares the same cross-attention layers and feed forward networks as the encoder.",
            "score": 0.4980662532680167,
            "section_title": "Historical Overview",
            "char_start_offset": 8892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480712890625
        },
        {
            "corpus_id": "258999776",
            "title": "AvatarStudio: Text-Driven Editing of 3D Dynamic Human Head Avatars",
            "text": "Current methods for text-driven image synthesis using diffusion models can be classified into 2D [Brooks et al. 2023;Ruiz et al. 2022] and 3D [Aneja et al. 2022;Haque et al. 2023;Jain et al. 2022;Lin et al. 2022;Poole et al. 2022;Wang et al. 2021bWang et al. , 2022] ] approaches. While the former produces a wide range of visual edits in terms of content and style, the latter produces results that are 3D-consistent and thus can be viewed from an arbitrary camera angle. DreamBooth [Ruiz et al. 2022] is a 2D-based approach that handles the problem of finetuning large text-to-image diffusion models to a specific examined object. Here, multiple images (typically 3-5) of the same object is provided as input, while DreamBooth learns to associate a unique identifier to this object. This embeds the examined object in the output domain of the text-to-image diffusion model, thus allowing a wide variety of text-driven edits. Instruct-Pix2Pix [Brooks et al. 2023] takes a different approach for the same problem of text-driven image synthesis. Their idea is to generate paired synthetic data by utilizing the large language model of GPT-3 [Brown et al. 2020] together with the text-to-image model of Stable Diffusion. This strategy generalizes well to real user-instructions and real input images during test. Unlike our method, none of these 2D-based methods can generate results that are multi-view consistent [Brooks et al. 2023;Ruiz et al. 2022]. \n\n3D text-driven image synthesis methods can be classified as ones that utilize a CLIP embedding [Aneja et al. 2022;Jain et al. 2022;Wang et al. 2021bWang et al. , 2022] ] and others that use other means for optimizing their solution [Haque et al. 2023;Lin et al. 2022;Poole et al. 2022].",
            "score": 0.4978284431647757,
            "section_title": "Text-driven Diffusion Models for Image Synthesis",
            "char_start_offset": 12287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1451
                },
                {
                    "start": 1454,
                    "end": 1740
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 117,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 142,
                    "end": 161,
                    "matchedPaperCorpusId": "254247006"
                },
                {
                    "start": 944,
                    "end": 963,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 1140,
                    "end": 1159,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1413,
                    "end": 1433,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451904296875
        },
        {
            "corpus_id": "271915816",
            "title": "Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models",
            "text": "The open-source LLaMA framework (Touvron et al. 2023a,b;Chiang et al. 2023) has significantly enhanced generative vision tasks. The pioneering work GILL (Koh, Fried, and Salakhutdinov 2023) bridges the gap between Multimodal Large Language Models (MLLMs) (Liu et al. 2023;Zhu et al. 2023a) and diffusion models (Rombach et al. 2022), expanding multimodal capabilities such as image retrieval, novel image generation, and multimodal dialogue. Some studies (Ge et al. 2023a,b;Sun et al. 2023b;Feng et al. 2023) advance text-to-image generation using LLMs, while works including (Fu et al. 2023;Huang et al. 2023) further explore instruction-driven image editing. LLMs also play a crucial role in 3D generative tasks. For example, GALA3D (Zhou et al. 2024b) focuses on generating complex 3D scenes with multiple objects, and GaussianEditor (Fang et al. 2023) facilitates 3D scene editing. \n\nIn storytelling, LLMs have driven the creation of innovative applications. TaleCrafter (Gong et al. 2023) and Autostory (Wang et al. 2023c) enable interactive 2D story visualization, managing multiple characters and allowing for layout and structural modifications. Animate-A-Story (He et al. 2023) andVlogger (Zhuang et al. 2024) extend storytelling to videos. The former assimilates coherent stories from preexisting video clips by personalizing their appearances in alignment with the story's demand, while the latter approach behaves akin to a director, parceling lengthy vlog productions into controllable stages. This strategy not only simplifies the process but also ensures each segment contributes effectively to the entire narrative.",
            "score": 0.4977478097194643,
            "section_title": "LLM Assists AIGC",
            "char_start_offset": 7461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1631
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 272,
                    "matchedPaperCorpusId": "270095090"
                },
                {
                    "start": 311,
                    "end": 332,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 735,
                    "end": 753,
                    "matchedPaperCorpusId": "268532481"
                },
                {
                    "start": 1170,
                    "end": 1190,
                    "matchedPaperCorpusId": "259847699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64990234375
        },
        {
            "corpus_id": "257663414",
            "title": "Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions",
            "text": "We enable mask-free instructional edits by taking advantage of recent instructionbased 2D image-conditioned diffusion model [2], resulting in a purely-language-based interface that enables a wider range of intuitive and content-aware 3D editing. \n\nGenerating 3D Content Recent progress in pre-trained large-scale models has enabled rapid progress in the domain of generating 3D content from scratch, either by optimizing radiance fields through vision-language models like CLIP [14,16] or via text-conditioned diffusion models [35,37,36] as presented in DreamFusion [33] and its follow-ups [48,19,23]. While these approaches can generate 3D models from arbitrary text prompts, they lack (1) fine-grained control over the synthesized outputs, (2) the ability to generalize to scenes (i.e., anything beyond a single object isolated in space), and (3) any grounding in reality, producing entirely synthesized creations. Concurrent works such as RealFusion [21] and SparseFusion [55] explore grounding by providing one or few input images, where the unseen parts are hallucinated. In all of these approaches, a central challenge is congealing the inconsistent outputs of a 2D diffusion model into a consistent 3D scene. In this work, instead of creating new content, we focus on editing real captured NeRFs of fully observed scenes using 2D diffusion priors. One advantage of editing an existing NeRF scene (as opposed to generating 3D content from scratch) is that the captured images are by definition 3D consistent, suggesting that generated imagery should naturally be more consistent. This also helps avoid certain design decisions that result in the cartoon-ish appearance commonly seen in unconditional 3D content generation methods [33,48,19]. \n\nInstruction as an Editing Interface With the rise of large-language models (LLMs) like GPT [3] and Chat-GPT [29], natural language is emerging as the next \"programming language\" for specifying complex tasks. LLMs allow for the abstraction of a series of low-level specifications into an intuitive and user-friendly interface through the use of language, specifically instructions [31].",
            "score": 0.4973832959969188,
            "section_title": "Artistic Stylization of NeRFs",
            "char_start_offset": 5745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1747
                },
                {
                    "start": 1750,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 127,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1841,
                    "end": 1844,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.342041015625
        },
        {
            "corpus_id": "273185450",
            "title": "Human-in-the-loop Reasoning For Traffic Sign Detection: Collaborative Approach Yolo With Video-llava",
            "text": "Large language models (LLMs) employ transformer-based neural network architecture and are trained on vast amounts of text data to identify patterns and relationships in natural language. These models have demonstrated the ability to both interpret and produce text like human language. However, LLMs are primarily focused on processing text inputs and outputs, lacking the ability to directly perceive and reason about visual information like images and videos. This limitation has motivated research into extending LLMs to handle multi-modal inputs beyond just text. Vision-language models (VLMs), also known as large vision-language models (LVLMs), aim to bridge this gap by enabling LLMs to understand and generate responses based on both textual and visual inputs. These models typically involve integrating computer vision components like convolutional neural networks (CNNs) or vision transformers (ViTs) [11] to extract visual features, which are then aligned and fused with the text representations within the LLM architecture. Approaches like pre-training on large image-text or video-text datasets and instruction tuning [17] have shown promising results in grounding LLMs in visual contexts for tasks like visual question answering, image/video captioning, and multimodal reasoning. Recent visual-language models like Video-LLaVA [4] take this a step further by introducing a unified approach to visual-language modeling, integrating both image and video data into a single, coherent representation for large language models. Video-LLaVA is based on an auto-regressive language model, which is fine-tuned on multimodal instruction-following data generated by Llava1.5 [10] and VideoChat [12]. \n\nAs shown in Figure 2 the structure of Video-LLaVA is designed to effectively integrate visual and textual data into a unified representation for enhanced multi-modal reasoning. Central to its structure are the LanguageBind encoders [18], which extract features from both images and videos and map these visual signals into a shared textual feature space, facilitating a unified visual representation. These encoders are initialized from OpenCLIP [14], ensuring that the visual features are aligned with the language features, thereby creating a unified visual representation.",
            "score": 0.49717785184036556,
            "section_title": "Large Vision-Language Model",
            "char_start_offset": 6479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2281
                }
            ],
            "ref_mentions": [
                {
                    "start": 1131,
                    "end": 1135,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "266902630",
            "title": "MISS: A Generative Pretraining and Finetuning Approach for Med-VQA",
            "text": "In this paper, we present an image-text pretraining framework tailored for generative tasks and propose a joint text-multimodal encoder to simultaneously extract features from both images and text through multi-task pretraining. To better compare the advantages of our approach with others, we have constructed a baseline model for our study, which is based on ALBEF [18]. Specifically, in terms of model architecture, the baseline model comprises a ViT-Base as the backbone for the image encoder, consisting of 12 transformer layers; a BERT-based text encoder and multimodal encoder, wherein the first six layers of it serve as the text encoder, which is identical to the original BERT encoder, while the latter six layers incorporate cross-attention between the self-attention and feed-forward layers to function as the multimodal encoder. A BERT-based text decoder is connected to the multimodal encoder and used for causal language modeling. \n\nIn terms of the pre-training of the baseline model, it still follows ALBEF and sets up three pre-training tasks: Image-Text Contrastive Learning, Image-text Matching Learning, and Mask Language Modeling Learning. For fine-tuning the VQA task, we still use Mask Language Modeling Learning as the fine-tuning task. Since ALBEF still treats VQA as a RANK task in testing, we modified the output end of the model to enable it to directly generate text.",
            "score": 0.49717639172477246,
            "section_title": "Baseline",
            "char_start_offset": 16038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1396
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "236034189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        }
    ],
    "quotes": {
        "cost": 0.18289499999999995,
        "quotes": [
            {
                "idx": 0,
                "key": "[257405349 | Cao et al. | 2023 | Citations: 548]",
                "snippets": "Recently, [145] proposed a method to realize VL dialogue with frozen language models, enabling the model to generate interleaved multimodal data. This method also freezes input encoders and train text-to-image and image-to-text linear maps to further encode and decode produced embeddings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Vision Language Generation.",
                        "pdf_hash": "",
                        "start": 1713,
                        "end": 2002,
                        "sentence_offsets": [
                            {
                                "start": 1713,
                                "end": 1858
                            },
                            {
                                "start": 1859,
                                "end": 2002
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recently, [145] proposed a method to realize VL dialogue with frozen language models, enabling the model to generate interleaved multimodal data. This method also freezes input encoders and train text-to-image and image-to-text linear maps to further encode and decode produced embeddings."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[265220926 | Meng et al. | 2023 | Citations: 10]",
                "snippets": "To make up for this shortcoming, Dong et al. (2023) realize image generation by combining the model proposed by Liu et al. (2023a) with the Stable diffusion (Rombach et al., 2021). Building on this, Wu et al. (2023) further integrates it with the model from (Girdhar et al., 2023), utilizing its ability to align multiple modalities to generate and accept multiple modalities. Very recently, GPT-4 (OpenAI, 2023) has demonstrated its astounding ability to understand and generate in the modalities of images and voice.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[258564264 | Girdhar et al. | 2023 | Citations: 941]": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications \u2018out-of-the-box\u2019 including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks."
                },
                "metadata": [
                    {
                        "section_title": "MULTIMODEL LARGE LANGUAGE MODEL",
                        "pdf_hash": "",
                        "start": 1509,
                        "end": 2026,
                        "sentence_offsets": [
                            {
                                "start": 1509,
                                "end": 1689
                            },
                            {
                                "start": 1690,
                                "end": 1884
                            },
                            {
                                "start": 1885,
                                "end": 2026
                            }
                        ],
                        "ref_mentions": [
                            "245335280",
                            "258564264"
                        ],
                        "quote": "To make up for this shortcoming, Dong et al. (2023) realize image generation by combining the model proposed by Liu et al. (2023a) with the Stable diffusion (Rombach et al., 2021). Building on this, Wu et al. (2023) further integrates it with the model from (Girdhar et al., 2023), utilizing its ability to align multiple modalities to generate and accept multiple modalities. Very recently, GPT-4 (OpenAI, 2023) has demonstrated its astounding ability to understand and generate in the modalities of images and voice."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[266359151 | Gao et al. | 2023 | Citations: 1819]",
                "snippets": "RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "F. Multi-modal RAG",
                        "pdf_hash": "",
                        "start": 229,
                        "end": 332,
                        "sentence_offsets": [
                            {
                                "start": 229,
                                "end": 332
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[266573642 | Huang et al. | 2023 | Citations: 23]",
                "snippets": "As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Visual Assistant",
                        "pdf_hash": "",
                        "start": 1510,
                        "end": 2111,
                        "sentence_offsets": [
                            {
                                "start": 1503,
                                "end": 1737
                            },
                            {
                                "start": 1738,
                                "end": 1903
                            },
                            {
                                "start": 1904,
                                "end": 2111
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[267200040 | Li et al. | 2024 | Citations: 8]",
                "snippets": "Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Generating with Multi-modal Language Models",
                        "pdf_hash": "",
                        "start": 338,
                        "end": 1154,
                        "sentence_offsets": [
                            {
                                "start": 338,
                                "end": 559
                            },
                            {
                                "start": 560,
                                "end": 683
                            },
                            {
                                "start": 684,
                                "end": 864
                            },
                            {
                                "start": 865,
                                "end": 963
                            },
                            {
                                "start": 964,
                                "end": 1154
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[270688372 | Liu et al. | 2024 | Citations: 11]",
                "snippets": "Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023)Sun et al., 2023b;(Dong et al., 2023)Tian et al., 2024;(Zhan et al., 2024)Chen et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258959284 | Koh et al. | 2023 | Citations: 259]": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
                    "[261975252 | Dong et al. | 2023 | Citations: 201]": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.",
                    "[267750101 | Zhan et al. | 2024 | Citations: 136]": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 676,
                        "end": 933,
                        "sentence_offsets": [
                            {
                                "start": 676,
                                "end": 933
                            }
                        ],
                        "ref_mentions": [
                            "258959284",
                            "261975252",
                            "267750101"
                        ],
                        "quote": "Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023)Sun et al., 2023b;(Dong et al., 2023)Tian et al., 2024;(Zhan et al., 2024)Chen et al., 2023)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[271333958 | Zhao et al. | 2024 | Citations: 26]",
                "snippets": "In the general multimodal domain, some pioneering efforts [48,16,71,56] empower MLLMs with the ability to generate images beyond texts, vastly extending the versatility of multimodal models...Previous works [48,71,56] and our pilot experiments (Figure 2) have shown that multimodal generation often leads to a notable decline in performance due to the substantial inconsistency between language and vision modalities in the generation space. Prior studies [48,16,71]56] commonly rely on modality-specific supervised fine-tuning to bolster generative capacities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1329,
                        "end": 1518,
                        "sentence_offsets": [
                            {
                                "start": 1329,
                                "end": 1519
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the general multimodal domain, some pioneering efforts [48,16,71,56] empower MLLMs with the ability to generate images beyond texts, vastly extending the versatility of multimodal models"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1779,
                        "end": 2149,
                        "sentence_offsets": [
                            {
                                "start": 1779,
                                "end": 2028
                            },
                            {
                                "start": 2029,
                                "end": 2148
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Previous works [48,71,56] and our pilot experiments (Figure 2) have shown that multimodal generation often leads to a notable decline in performance due to the substantial inconsistency between language and vision modalities in the generation space. Prior studies [48,16,71]56] commonly rely on modality-specific supervised fine-tuning to bolster generative capacities."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[273962846 | Xiong et al. | 2024 | Citations: 14]",
                "snippets": "Vision-to-Language (V2T) Tokenizer (Zhu et al., 2024a) maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) (Lee et al., 2022b) encodes both image and text inputs as tokens, generating coherent multimodal outputs. These approaches improve image-text generation, though they typically predict pixels in a fixed order without considering random generation strategies.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[268363788 | Zhu et al. | 2024 | Citations: 21]": "In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multimodal datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the LLM's vocabulary. To achieve this, we present the Vision-to-Language Tokenizer; abbreviated as V2T Tokenizer, which transforms an image into a \u201cforeign language\u201d with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model. With this innovative image encoding, the LLM gains the ability not only for visual comprehension but also for image denoising and restoration in an auto-regressive fashion-crucially, without any fine-tuning. We undertake rig-orous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and visual question answering, as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at https://github.com/zh460045050/V2l-Tokenizer."
                },
                "metadata": [
                    {
                        "section_title": "The Framework for Unifying Multimodal Understanding and Generation",
                        "pdf_hash": "",
                        "start": 553,
                        "end": 1046,
                        "sentence_offsets": [
                            {
                                "start": 475,
                                "end": 663
                            },
                            {
                                "start": 664,
                                "end": 839
                            },
                            {
                                "start": 840,
                                "end": 1004
                            },
                            {
                                "start": 1005,
                                "end": 1156
                            }
                        ],
                        "ref_mentions": [
                            "268363788"
                        ],
                        "quote": "Vision-to-Language (V2T) Tokenizer (Zhu et al., 2024a) maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) (Lee et al., 2022b) encodes both image and text inputs as tokens, generating coherent multimodal outputs. These approaches improve image-text generation, though they typically predict pixels in a fixed order without considering random generation strategies."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[273969615 | Zhang et al. | 2024 | Citations: 3]",
                "snippets": "The latest advancements in RAG in the image domain have led to significant improvements [120]. RA-CM3 [121] enhances both text-to-image and image-to-text generation by combining the CLIP retriever and the CM3 Transformer generator, achieving a performance boost while reducing computational costs by over 30%. Mortaheb et al. introduced a reranking mechanism based on a relevance score model [122], improving context selection during retrieval and reducing hallucinations, thereby enhancing the quality of generated responses. Yu et al.'s VisRAG [123] framework bypasses the text parsing stage to directly process multi-modal documents containing both text and images, achieving substantial improvements in multi-modal tasks. Bonomo and Bianco's Visual RAG [124] expands the visual knowledge of large MLLMs without the need for fine-tuning by dynamically retrieving relevant examples, offering high computational efficiency. Riedler and Langer's work on multimodal inputs for industrial applications demonstrates that integrating both images and text in RAG systems significantly improves performance [125].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D. Multimodal RAG",
                        "pdf_hash": "",
                        "start": 740,
                        "end": 1847,
                        "sentence_offsets": [
                            {
                                "start": 740,
                                "end": 834
                            },
                            {
                                "start": 835,
                                "end": 1049
                            },
                            {
                                "start": 1050,
                                "end": 1266
                            },
                            {
                                "start": 1267,
                                "end": 1465
                            },
                            {
                                "start": 1466,
                                "end": 1664
                            },
                            {
                                "start": 1665,
                                "end": 1847
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The latest advancements in RAG in the image domain have led to significant improvements [120]. RA-CM3 [121] enhances both text-to-image and image-to-text generation by combining the CLIP retriever and the CM3 Transformer generator, achieving a performance boost while reducing computational costs by over 30%. Mortaheb et al. introduced a reranking mechanism based on a relevance score model [122], improving context selection during retrieval and reducing hallucinations, thereby enhancing the quality of generated responses. Yu et al.'s VisRAG [123] framework bypasses the text parsing stage to directly process multi-modal documents containing both text and images, achieving substantial improvements in multi-modal tasks. Bonomo and Bianco's Visual RAG [124] expands the visual knowledge of large MLLMs without the need for fine-tuning by dynamically retrieving relevant examples, offering high computational efficiency. Riedler and Langer's work on multimodal inputs for industrial applications demonstrates that integrating both images and text in RAG systems significantly improves performance [125]."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[274305669 | Kang et al. | 2024 | Citations: 0]",
                "snippets": "Recently, pretrained language models, such as BERT [11] and GPT [4], have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text. More recent methods like VisualBERT [23] and LXMERT [38] utilize pretrained BERT weights for joint image-text comprehension.\n\nUnified architectures like ViLBERT [30], VL-BERT [36], UNITER [8], BLIP [21] integrate masked language modeling objectives alongside paired image-text prediction tasks within a single Transformer model. These models set new stateof-the-art results on downstream tasks like visual question answering (VQA), visual reasoning, and image retrieval.\n\nInstruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recently, pretrained language models, such as BERT [11] and GPT [4], have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text. More recent methods like VisualBERT [23] and LXMERT [38] utilize pretrained BERT weights for joint image-text comprehension.\n\nUnified architectures like ViLBERT [30], VL-BERT [36], UNITER [8], BLIP [21] integrate masked language modeling objectives alongside paired image-text prediction tasks within a single Transformer model. These models set new stateof-the-art results on downstream tasks like visual question answering (VQA), visual reasoning, and image retrieval.\n\nInstruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[274597781 | Serra et al. | 2024 | Citations: 0]",
                "snippets": "Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings (Ge et al., 2023) have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture (Tang et al., 2023)[18](Zhan et al., 2024)[20].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258822817 | Tang et al. | 2023 | Citations: 188]": "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io",
                    "[263605855 | Ge et al. | 2023 | Citations: 137]": "The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent abilities in an open-world context. However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. We contend that the key to overcoming the present impasse lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. To this end, we introduce SEED, an elaborate image tokenizer that empowers LLMs with the ability to SEE and Draw at the same time. We identify two crucial design principles: (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. With SEED tokens, LLM is able to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by large-scale pretraining and instruction tuning on the interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has exhibited compositional emergent abilities such as multi-turn in-context multimodal generation, acting like your AI assistant.",
                    "[267750101 | Zhan et al. | 2024 | Citations: 136]": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 426,
                        "end": 699,
                        "sentence_offsets": [
                            {
                                "start": 426,
                                "end": 699
                            }
                        ],
                        "ref_mentions": [
                            "263605855",
                            "258822817",
                            "267750101"
                        ],
                        "quote": "Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings (Ge et al., 2023) have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture (Tang et al., 2023)[18](Zhan et al., 2024)[20]."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[276782523 | Zhao et al. | 2025 | Citations: 1]",
                "snippets": "SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231591445 | Radford et al. | 2021 | Citations: 29867]": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
                    "[261975252 | Dong et al. | 2023 | Citations: 201]": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                },
                "metadata": [
                    {
                        "section_title": "Unified Multimodal Generative Models",
                        "pdf_hash": "",
                        "start": 271,
                        "end": 1273,
                        "sentence_offsets": [
                            {
                                "start": 271,
                                "end": 507
                            },
                            {
                                "start": 508,
                                "end": 631
                            },
                            {
                                "start": 632,
                                "end": 831
                            },
                            {
                                "start": 832,
                                "end": 1109
                            },
                            {
                                "start": 1110,
                                "end": 1273
                            }
                        ],
                        "ref_mentions": [
                            "231591445",
                            "261975252"
                        ],
                        "quote": "SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[276929205 | Niu et al. | 2025 | Citations: 26]",
                "snippets": "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45][48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Unified Multimodal Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 735,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 176
                            },
                            {
                                "start": 177,
                                "end": 455
                            },
                            {
                                "start": 456,
                                "end": 624
                            },
                            {
                                "start": 625,
                                "end": 735
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45][48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[276929408 | Fang et al. | 2025 | Citations: 1]",
                "snippets": "Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[253581213 | Brooks et al. | 2022 | Citations: 1833]": "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.",
                    "[266374640 | Sun et al. | 2023 | Citations: 290]": "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."
                },
                "metadata": [
                    {
                        "section_title": "MLLM for Image Generation and Editing",
                        "pdf_hash": "",
                        "start": 561,
                        "end": 1202,
                        "sentence_offsets": [
                            {
                                "start": 561,
                                "end": 757
                            },
                            {
                                "start": 758,
                                "end": 941
                            },
                            {
                                "start": 942,
                                "end": 1202
                            }
                        ],
                        "ref_mentions": [
                            "266374640",
                            "253581213",
                            "253581838"
                        ],
                        "quote": "Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.019938,
        "cot": "For this query about multimodal large language models (MLLMs) that can generate both text and images with scene text in a single framework, I'll organize the information into clear dimensions:\n\n1. \"Introduction to MLLMs\" - This should be a synthesis paragraph to provide background on what MLLMs are and their general capabilities. This is essential context for understanding the rest of the answer. While there are no specific quotes for this section, it's necessary to include.\n\n2. \"Pioneering End-to-End Multimodal Generation Models\" - This should be a synthesis paragraph discussing the key models that pioneered the ability to generate both text and images in a unified framework. Quotes [2], [3], [10], [11], and [12] all discuss foundational models in this space.\n\n3. \"Models with Image-Text Generation Capabilities\" - This should be a list format to clearly enumerate specific models mentioned in the literature that can generate both text and images. This helps directly answer the user's question about \"key papers and models.\" Quotes [1], [4], [5], [6], [9], and [13] all mention specific models with these capabilities.\n\n4. \"Technical Approaches for Multimodal Generation\" - This should be a synthesis paragraph explaining the different technical methods used to achieve text-image generation in a single framework. Quotes [0], [3], [7], and [11] discuss specific technical approaches.\n\n5. \"Recent Advancements in Multimodal Retrieval and Generation\" - This should be a synthesis format discussing how retrieval-augmented generation approaches have been incorporated into multimodal systems. Quote [8] specifically addresses this dimension.\n\nThese dimensions cover the core aspects of the query while ensuring all quotes are included and organized in a logical flow from introduction to specific models to technical approaches and recent advancements.",
        "plan": {
            "Introduction to MLLMs (synthesis)": [],
            "Pioneering End-to-End Multimodal Generation Models (synthesis)": [
                2,
                3,
                10,
                11,
                12
            ],
            "Models with Image-Text Generation Capabilities (list)": [
                1,
                4,
                5,
                6,
                9,
                13
            ],
            "Technical Approaches for Multimodal Generation (synthesis)": [
                0,
                3,
                7,
                11
            ],
            "Recent Advancements in Multimodal Retrieval and Generation (synthesis)": [
                8
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to MLLMs",
                "tldr": "Multimodal Large Language Models (MLLMs) extend traditional LLMs by processing and generating multiple types of data including text, images, and audio. Recent advancements have enabled some MLLMs to generate both text and images with embedded text in a unified framework. (LLM Memory)",
                "text": "\nMultimodal Large Language Models (MLLMs) represent a significant evolution in AI capabilities, extending beyond the text-only focus of traditional Large Language Models (LLMs). While conventional LLMs process and generate exclusively textual data, MLLMs are designed to understand and work with multiple modalities simultaneously\u2014typically text and images, but increasingly also audio and video. This multimodal capability allows these models to process rich, real-world information that spans different forms of communication. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe development of MLLMs has progressed through several stages. Early models primarily focused on understanding images and responding with text (vision-to-language), exemplified by models like CLIP, which aligned visual and textual representations. The next wave introduced models capable of bidirectional understanding\u2014processing both text prompts about images and generating textual responses about visual content. These include models like BLIP, Flamingo, and GPT-4V. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nMost recently, research has advanced toward truly generative multimodal capabilities, where models can not only understand multiple modalities but also generate them. Of particular interest to the query are MLLMs that can generate both text and images containing embedded text (such as signage, documents, or captions) within a single end-to-end framework. These advanced models represent a significant technical challenge as they must maintain coherence between the generated image and any text that appears within it. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe ability to generate images with accurate, readable, and contextually appropriate embedded text has applications ranging from creative content generation to document design, signage creation, and educational materials. This capability requires sophisticated architectures that can maintain the semantic relationship between the visual elements and the textual content that appears within them. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Pioneering End-to-End Multimodal Generation Models",
                "tldr": "Several pioneering models have emerged that can generate both text and images in a unified framework. These include SEED, CoDi, AnyGPT, and DreamLLM, which use various approaches like discrete tokenization, diffusion processes, and unified architectures to enable multimodal generation capabilities. (9 sources)",
                "text": "\nThe development of end-to-end multimodal generation models represents a significant advancement in AI, with several pioneering approaches emerging in recent years. RA-CM3 stands as one of the first notable multimodal models capable of both retrieving and generating text and images within a unified framework. <Paper corpusId=\"266359151\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>\n\nSEED (Semantic-Enhanced Encoder-Decoder) introduced a crucial innovation by converting 2D image features into a sequence of causal semantic embeddings, which are then discretized into quantized visual codes with causal dependency. This approach enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, effectively unifying comprehension and generation tasks within a single framework. <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> The SEED architecture particularly emphasizes that image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, enabling the LLM to process visual and textual data through its original next-word prediction training paradigm. <Paper corpusId=\"263605855\" paperTitle=\"(Ge et al., 2023)\" isShortName></Paper>\n\nSEED-X further builds on this foundation by combining visual features extracted from the CLIP ViT encoder with text tokens and feeding them into a large language model, enabling both next-word prediction and image regression tasks in a unified architecture. <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>\n\nComposable Diffusion (CoDi) represents another significant breakthrough, offering a novel generative model capable of generating any combination of output modalities (language, image, video, or audio) from any combination of input modalities. CoDi's approach involves building a shared multimodal space by bridging alignment in the diffusion process, enabling synchronized generation of intertwined modalities. <Paper corpusId=\"258822817\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>\n\nAnyGPT demonstrates a different approach to multimodal generation, utilizing discrete representations for unified processing of various modalities including speech, text, images, and music. This model can be trained without altering the standard LLM architecture or training paradigms, relying instead on data-level preprocessing to integrate new modalities seamlessly. <Paper corpusId=\"267750101\" paperTitle=\"(Zhan et al., 2024)\" isShortName></Paper>\n\nDreamLLM takes yet another approach by focusing on the generative modeling of both language and image posteriors through direct sampling in the raw multimodal space. This method avoids the limitations of external feature extractors like CLIP and aims to achieve a more thorough multimodal understanding. DreamLLM can generate raw, interleaved documents containing both text and image contents with unstructured layouts. <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>\n\nOther notable contributions to unified multimodal models include Chameleon, which employs discrete tokenization for both visual and textual inputs to create a unified token space; Trans-fusion, which focuses on directly fusing visual encoding with language tokens; and Show-O, which combines autoregressive modeling with a discrete diffusion process. <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> Additionally, recent approaches like D-DiT have demonstrated the ability to achieve both Text-to-Image and Image-to-Text tasks using an end-to-end diffusion model. <Paper corpusId=\"276929205\" paperTitle=\"(Niu et al., 2025)\" isShortName></Paper>\n\nThese pioneering models collectively represent significant progress toward the goal of creating truly multimodal AI systems capable of both understanding and generating content across different modalities, including text and images with embedded text, within a single end-to-end framework.",
                "citations": [
                    {
                        "id": "(Gao et al., 2023)",
                        "snippets": [
                            "RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images."
                        ],
                        "paper": {
                            "corpus_id": 266359151,
                            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2275341478",
                                    "name": "Xinyu Gao"
                                },
                                {
                                    "authorId": "2275191447",
                                    "name": "Kangxiang Jia"
                                },
                                {
                                    "authorId": "2275530552",
                                    "name": "Jinliu Pan"
                                },
                                {
                                    "authorId": "2275171009",
                                    "name": "Yuxi Bi"
                                },
                                {
                                    "authorId": "2276187454",
                                    "name": "Yi Dai"
                                },
                                {
                                    "authorId": "2275540959",
                                    "name": "Jiawei Sun"
                                },
                                {
                                    "authorId": "2258800561",
                                    "name": "Qianyu Guo"
                                },
                                {
                                    "authorId": "2291409458",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1819
                        },
                        "score": 0.75732421875
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework."
                        ],
                        "paper": {
                            "corpus_id": 266573642,
                            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
                            "authors": [
                                {
                                    "authorId": "2115941903",
                                    "name": "Jiaxing Huang"
                                },
                                {
                                    "authorId": "2276743977",
                                    "name": "Jingyi Zhang"
                                },
                                {
                                    "authorId": "2276609830",
                                    "name": "Kai Jiang"
                                },
                                {
                                    "authorId": "49660254",
                                    "name": "Han Qiu"
                                },
                                {
                                    "authorId": "2237947102",
                                    "name": "Shijian Lu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 23
                        },
                        "score": 0.8525390625
                    },
                    {
                        "id": "(Ge et al., 2023)",
                        "snippets": [
                            "The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent abilities in an open-world context. However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. We contend that the key to overcoming the present impasse lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. To this end, we introduce SEED, an elaborate image tokenizer that empowers LLMs with the ability to SEE and Draw at the same time. We identify two crucial design principles: (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. With SEED tokens, LLM is able to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by large-scale pretraining and instruction tuning on the interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has exhibited compositional emergent abilities such as multi-turn in-context multimodal generation, acting like your AI assistant."
                        ],
                        "paper": {
                            "corpus_id": 263605855,
                            "title": "Making LLaMA SEE and Draw with SEED Tokenizer",
                            "authors": [
                                {
                                    "authorId": "51123495",
                                    "name": "Yuying Ge"
                                },
                                {
                                    "authorId": "2254048096",
                                    "name": "Sijie Zhao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Ziyun Zeng"
                                },
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "2256784925",
                                    "name": "Chen Li"
                                },
                                {
                                    "authorId": "2253795356",
                                    "name": "Xintao Wang"
                                },
                                {
                                    "authorId": "2257019659",
                                    "name": "Ying Shan"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 137
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."
                        ],
                        "paper": {
                            "corpus_id": 276782523,
                            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
                            "authors": [
                                {
                                    "authorId": "2248946985",
                                    "name": "Rui Zhao"
                                },
                                {
                                    "authorId": "2258958963",
                                    "name": "Weijia Mao"
                                },
                                {
                                    "authorId": "2344762475",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.86474609375
                    },
                    {
                        "id": "(Radford et al., 2021)",
                        "snippets": [
                            "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
                        ],
                        "paper": {
                            "corpus_id": 231591445,
                            "title": "Learning Transferable Visual Models From Natural Language Supervision",
                            "authors": [
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "2110935237",
                                    "name": "Jong Wook Kim"
                                },
                                {
                                    "authorId": "2004021329",
                                    "name": "Chris Hallacy"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "40087786",
                                    "name": "Gabriel Goh"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 29867
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io"
                        ],
                        "paper": {
                            "corpus_id": 258822817,
                            "title": "Any-to-Any Generation via Composable Diffusion",
                            "authors": [
                                {
                                    "authorId": "151270642",
                                    "name": "Zineng Tang"
                                },
                                {
                                    "authorId": "2155459391",
                                    "name": "Ziyi Yang"
                                },
                                {
                                    "authorId": "8652308",
                                    "name": "Chenguang Zhu"
                                },
                                {
                                    "authorId": "48262024",
                                    "name": "Michael Zeng"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 188
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhan et al., 2024)",
                        "snippets": [
                            "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"
                        ],
                        "paper": {
                            "corpus_id": 267750101,
                            "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
                            "authors": [
                                {
                                    "authorId": "2217531907",
                                    "name": "Jun Zhan"
                                },
                                {
                                    "authorId": "2087363104",
                                    "name": "Junqi Dai"
                                },
                                {
                                    "authorId": "2153258452",
                                    "name": "Jiasheng Ye"
                                },
                                {
                                    "authorId": "2118117212",
                                    "name": "Yunhua Zhou"
                                },
                                {
                                    "authorId": "2109797247",
                                    "name": "Dong Zhang"
                                },
                                {
                                    "authorId": "2284732560",
                                    "name": "Zhigeng Liu"
                                },
                                {
                                    "authorId": "2278615698",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2032236274",
                                    "name": "Ruibin Yuan"
                                },
                                {
                                    "authorId": "2143853895",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "2107897400",
                                    "name": "Linyang Li"
                                },
                                {
                                    "authorId": "146948229",
                                    "name": "Hang Yan"
                                },
                                {
                                    "authorId": "2276508494",
                                    "name": "Jie Fu"
                                },
                                {
                                    "authorId": "2067331064",
                                    "name": "Tao Gui"
                                },
                                {
                                    "authorId": "153345698",
                                    "name": "Tianxiang Sun"
                                },
                                {
                                    "authorId": "2284724292",
                                    "name": "Yugang Jiang"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                        ],
                        "paper": {
                            "corpus_id": 261975252,
                            "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                            "authors": [
                                {
                                    "authorId": "2056965063",
                                    "name": "Runpei Dong"
                                },
                                {
                                    "authorId": "2118643247",
                                    "name": "Chunrui Han"
                                },
                                {
                                    "authorId": "2211415443",
                                    "name": "Yuang Peng"
                                },
                                {
                                    "authorId": "3424017",
                                    "name": "Zekun Qi"
                                },
                                {
                                    "authorId": "2242581956",
                                    "name": "Zheng Ge"
                                },
                                {
                                    "authorId": "2161319190",
                                    "name": "Jinrong Yang"
                                },
                                {
                                    "authorId": "48096671",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "26913717",
                                    "name": "Jian\u2010Yuan Sun"
                                },
                                {
                                    "authorId": "2157473950",
                                    "name": "Hongyu Zhou"
                                },
                                {
                                    "authorId": "134085586",
                                    "name": "Hao-Ran Wei"
                                },
                                {
                                    "authorId": "2069029697",
                                    "name": "Xiangwen Kong"
                                },
                                {
                                    "authorId": "2185865433",
                                    "name": "Xiangyu Zhang"
                                },
                                {
                                    "authorId": "2244131405",
                                    "name": "Kaisheng Ma"
                                },
                                {
                                    "authorId": "2242612318",
                                    "name": "Li Yi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0
                    },
                    {
                        "id": "(Niu et al., 2025)",
                        "snippets": [
                            "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45][48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model."
                        ],
                        "paper": {
                            "corpus_id": 276929205,
                            "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2350309448",
                                    "name": "Yuwei Niu"
                                },
                                {
                                    "authorId": "2253434320",
                                    "name": "Munan Ning"
                                },
                                {
                                    "authorId": "2349744842",
                                    "name": "Mengren Zheng"
                                },
                                {
                                    "authorId": "2333420566",
                                    "name": "Bin Lin"
                                },
                                {
                                    "authorId": "2328406001",
                                    "name": "Peng Jin"
                                },
                                {
                                    "authorId": "2315613899",
                                    "name": "Jiaqi Liao"
                                },
                                {
                                    "authorId": "2253467098",
                                    "name": "Kun-Peng Ning"
                                },
                                {
                                    "authorId": "2256419313",
                                    "name": "Bin Zhu"
                                },
                                {
                                    "authorId": "2280992738",
                                    "name": "Li Yuan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.83984375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Models with Image-Text Generation Capabilities",
                "tldr": "Several models now enable simultaneous generation of text and images with embedded text, including GILL, Emu, DreamLLM, KOSMOS-G, AnyGPT, and BLIP-Diffusion. These models employ diverse approaches such as multimodal alignment, discrete tokenization, and diffusion techniques to generate coherent interleaved content. (9 sources)",
                "text": "\nHere is a list of notable models with image-text generation capabilities:\n\n- **GILL (Grounding-Instructed Language-Image Linking)**: Developed as one of the first models to fuse frozen text-only LLMs with pre-trained image encoders and decoders. GILL can condition on interleaved image and text inputs to generate coherent image outputs, significantly outperforming baseline generation models on complex language tasks. <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper>\n\n- **Emu**: A generative multimodal model with 37 billion parameters that demonstrates strong multimodal in-context learning abilities. Emu can handle tasks requiring on-the-fly reasoning, such as visual prompting and object-grounded generation, setting records on multiple multimodal understanding tasks in few-shot settings. <Paper corpusId=\"266374640\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>\n\n- **DreamLLM**: A learning framework that achieves versatile multimodal capabilities by focusing on generative modeling of both language and image posteriors through direct sampling in raw multimodal space. DreamLLM generates raw, interleaved documents containing both text and image contents with unstructured layouts, without relying on external feature extractors like CLIP. <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>\n\n- **AnyGPT**: A model that utilizes discrete representations for unified processing of various modalities including speech, text, images, and music. AnyGPT can be trained without altering standard LLM architecture or training paradigms, instead relying on data-level preprocessing to seamlessly integrate new modalities. <Paper corpusId=\"267750101\" paperTitle=\"(Zhan et al., 2024)\" isShortName></Paper>\n\n- **KOSMOS-G**: This model leverages a multimodal large language model to encode interleaved text-visual inputs and uses the U-Net architecture from Stable Diffusion v1.5 as the image decoder. <Paper corpusId=\"267200040\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>\n\n- **BLIP-Diffusion**: Specializes in synthesizing images by composing subjects with random backgrounds, giving it zero-shot, subject-driven text-to-image generation capabilities. However, its specific input template and training process limit scalability for multiple entities. <Paper corpusId=\"267200040\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>\n\n- **InstructPix2Pix**: While primarily focused on image editing rather than generation from scratch, this model allows users to edit images through descriptive language instructions. It performs edits quickly in a forward pass without requiring per-example fine-tuning or inversion. <Paper corpusId=\"253581213\" paperTitle=\"(Brooks et al., 2022)\" isShortName></Paper>\n\n- **Stable Diffusion**: A foundational latent diffusion model that significantly reduced computational requirements compared to pixel-based diffusion models while maintaining high visual fidelity. Many multimodal generative models build upon or integrate with Stable Diffusion for image generation capabilities. <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>\n\nThese models collectively represent significant advancements in unified multimodal generation, with each employing different technical approaches to achieve the challenging task of generating coherent text and images with embedded text within a single framework. The more recent models show improvements in generating interleaved content with stronger semantic alignment between textual and visual elements. <Paper corpusId=\"270688372\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271333958\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Koh et al., 2023)",
                        "snippets": [
                            "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence."
                        ],
                        "paper": {
                            "corpus_id": 258959284,
                            "title": "Generating Images with Multimodal Language Models",
                            "authors": [
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "47070750",
                                    "name": "Daniel Fried"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 259
                        },
                        "score": 0.8349609375
                    },
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."
                        ],
                        "paper": {
                            "corpus_id": 266374640,
                            "title": "Generative Multimodal Models are In-Context Learners",
                            "authors": [
                                {
                                    "authorId": "2264040642",
                                    "name": "Quan Sun"
                                },
                                {
                                    "authorId": "2263702215",
                                    "name": "Yufeng Cui"
                                },
                                {
                                    "authorId": "2264589602",
                                    "name": "Xiaosong Zhang"
                                },
                                {
                                    "authorId": "2264274388",
                                    "name": "Fan Zhang"
                                },
                                {
                                    "authorId": "23716915",
                                    "name": "Qiying Yu"
                                },
                                {
                                    "authorId": "2275808712",
                                    "name": "Zhengxiong Luo"
                                },
                                {
                                    "authorId": "2217456303",
                                    "name": "Yueze Wang"
                                },
                                {
                                    "authorId": "2275170369",
                                    "name": "Yongming Rao"
                                },
                                {
                                    "authorId": "2222717281",
                                    "name": "Jingjing Liu"
                                },
                                {
                                    "authorId": "2257137554",
                                    "name": "Tiejun Huang"
                                },
                                {
                                    "authorId": "2263959521",
                                    "name": "Xinlong Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 290
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                        ],
                        "paper": {
                            "corpus_id": 261975252,
                            "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                            "authors": [
                                {
                                    "authorId": "2056965063",
                                    "name": "Runpei Dong"
                                },
                                {
                                    "authorId": "2118643247",
                                    "name": "Chunrui Han"
                                },
                                {
                                    "authorId": "2211415443",
                                    "name": "Yuang Peng"
                                },
                                {
                                    "authorId": "3424017",
                                    "name": "Zekun Qi"
                                },
                                {
                                    "authorId": "2242581956",
                                    "name": "Zheng Ge"
                                },
                                {
                                    "authorId": "2161319190",
                                    "name": "Jinrong Yang"
                                },
                                {
                                    "authorId": "48096671",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "26913717",
                                    "name": "Jian\u2010Yuan Sun"
                                },
                                {
                                    "authorId": "2157473950",
                                    "name": "Hongyu Zhou"
                                },
                                {
                                    "authorId": "134085586",
                                    "name": "Hao-Ran Wei"
                                },
                                {
                                    "authorId": "2069029697",
                                    "name": "Xiangwen Kong"
                                },
                                {
                                    "authorId": "2185865433",
                                    "name": "Xiangyu Zhang"
                                },
                                {
                                    "authorId": "2244131405",
                                    "name": "Kaisheng Ma"
                                },
                                {
                                    "authorId": "2242612318",
                                    "name": "Li Yi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhan et al., 2024)",
                        "snippets": [
                            "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"
                        ],
                        "paper": {
                            "corpus_id": 267750101,
                            "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
                            "authors": [
                                {
                                    "authorId": "2217531907",
                                    "name": "Jun Zhan"
                                },
                                {
                                    "authorId": "2087363104",
                                    "name": "Junqi Dai"
                                },
                                {
                                    "authorId": "2153258452",
                                    "name": "Jiasheng Ye"
                                },
                                {
                                    "authorId": "2118117212",
                                    "name": "Yunhua Zhou"
                                },
                                {
                                    "authorId": "2109797247",
                                    "name": "Dong Zhang"
                                },
                                {
                                    "authorId": "2284732560",
                                    "name": "Zhigeng Liu"
                                },
                                {
                                    "authorId": "2278615698",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2032236274",
                                    "name": "Ruibin Yuan"
                                },
                                {
                                    "authorId": "2143853895",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "2107897400",
                                    "name": "Linyang Li"
                                },
                                {
                                    "authorId": "146948229",
                                    "name": "Hang Yan"
                                },
                                {
                                    "authorId": "2276508494",
                                    "name": "Jie Fu"
                                },
                                {
                                    "authorId": "2067331064",
                                    "name": "Tao Gui"
                                },
                                {
                                    "authorId": "153345698",
                                    "name": "Tianxiang Sun"
                                },
                                {
                                    "authorId": "2284724292",
                                    "name": "Yugang Jiang"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder."
                        ],
                        "paper": {
                            "corpus_id": 267200040,
                            "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
                            "authors": [
                                {
                                    "authorId": "2289660121",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2118734325",
                                    "name": "Xue Xu"
                                },
                                {
                                    "authorId": "2288740225",
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "2288393805",
                                    "name": "Xinyan Xiao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.8623046875
                    },
                    {
                        "id": "(Brooks et al., 2022)",
                        "snippets": [
                            "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."
                        ],
                        "paper": {
                            "corpus_id": 253581213,
                            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
                            "authors": [
                                {
                                    "authorId": "2679394",
                                    "name": "Tim Brooks"
                                },
                                {
                                    "authorId": "2248172435",
                                    "name": "Aleksander Holynski"
                                },
                                {
                                    "authorId": "1763086",
                                    "name": "Alexei A. Efros"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 1833
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rombach et al., 2021)",
                        "snippets": [
                            "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                        ],
                        "paper": {
                            "corpus_id": 245335280,
                            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "1796707",
                                    "name": "B. Ommer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 15768
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023)Sun et al., 2023b;(Dong et al., 2023)Tian et al., 2024;(Zhan et al., 2024)Chen et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 270688372,
                            "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2123130842",
                                    "name": "Minqian Liu"
                                },
                                {
                                    "authorId": "2136442661",
                                    "name": "Zhiyang Xu"
                                },
                                {
                                    "authorId": "2254334449",
                                    "name": "Zihao Lin"
                                },
                                {
                                    "authorId": "2243992517",
                                    "name": "Trevor Ashby"
                                },
                                {
                                    "authorId": "2218905547",
                                    "name": "Joy Rimchala"
                                },
                                {
                                    "authorId": "2307975528",
                                    "name": "Jiaxin Zhang"
                                },
                                {
                                    "authorId": "2238885968",
                                    "name": "Lifu Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.78125
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "In the general multimodal domain, some pioneering efforts [48,16,71,56] empower MLLMs with the ability to generate images beyond texts, vastly extending the versatility of multimodal models",
                            "Previous works [48,71,56] and our pilot experiments (Figure 2) have shown that multimodal generation often leads to a notable decline in performance due to the substantial inconsistency between language and vision modalities in the generation space. Prior studies [48,16,71]56] commonly rely on modality-specific supervised fine-tuning to bolster generative capacities."
                        ],
                        "paper": {
                            "corpus_id": 271333958,
                            "title": "Harmonizing Visual Text Comprehension and Generation",
                            "authors": [
                                {
                                    "authorId": "2268032623",
                                    "name": "Zhen Zhao"
                                },
                                {
                                    "authorId": "2299523904",
                                    "name": "Jingqun Tang"
                                },
                                {
                                    "authorId": "2267874275",
                                    "name": "Binghong Wu"
                                },
                                {
                                    "authorId": "2268054519",
                                    "name": "Chunhui Lin"
                                },
                                {
                                    "authorId": "2269740461",
                                    "name": "Shubo Wei"
                                },
                                {
                                    "authorId": "2268188994",
                                    "name": "Hao Liu"
                                },
                                {
                                    "authorId": "2249851862",
                                    "name": "Xin Tan"
                                },
                                {
                                    "authorId": "2241419999",
                                    "name": "Zhizhong Zhang"
                                },
                                {
                                    "authorId": "2267862616",
                                    "name": "Can Huang"
                                },
                                {
                                    "authorId": "2293421634",
                                    "name": "Yuan Xie"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 26
                        },
                        "score": 0.76904296875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Technical Approaches for Multimodal Generation",
                "tldr": "Multimodal generation models employ diverse technical approaches including discrete tokenization, unified token spaces, and visual-language fusion mechanisms. These techniques enable the processing and generation of interleaved text and images by mapping different modalities into compatible representations that can be handled by transformer architectures. (7 sources)",
                "text": "\nThe development of multimodal generation systems has required innovative technical approaches to bridge the gap between different data types. A fundamental challenge in these systems is creating a unified representation that allows both textual and visual information to be processed within the same model architecture.\n\nDiscrete tokenization has emerged as a particularly effective approach for multimodal generation. The SEED model exemplifies this strategy by converting 2D raster-ordered image features into a sequence of causal semantic embeddings, which are then discretized into quantized visual codes with causal dependency. These visual codes can then be decoded into generation embeddings aligned with the latent space of pre-trained models, enabling realistic image generation. This approach allows Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data within a single framework. <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>\n\nOther models have adopted similar tokenization strategies. The Vision-to-Language (V2T) Tokenizer maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) encodes both image and text inputs as tokens to generate coherent multimodal outputs. <Paper corpusId=\"273962846\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268363788\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>\n\nUnified architecture approaches have also proven successful. SEED-X combines visual features extracted from the CLIP ViT encoder with text tokens and feeds them into a large language model, enabling both next-word prediction and image regression tasks. This approach leverages the powerful visual representations from CLIP, which was trained on 400 million image-text pairs. <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>\n\nSeveral other technical strategies have been developed to achieve multimodal generation capabilities. Chameleon employs discrete tokenization for both visual and textual inputs, creating a unified token space processed by a transformer-based architecture. Trans-fusion introduces an advanced integration mechanism that directly fuses visual encoding with language tokens, allowing the model to translate visual information into textual formats while maintaining semantic integrity across modalities. Show-O combines autoregressive modeling with a discrete diffusion process for generating high-quality outputs aligned across modalities. <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>\n\nDreamLLM takes a different approach by focusing on direct sampling in the raw multimodal space, avoiding reliance on external feature extractors like CLIP. This approach enables DreamLLM to generate raw, interleaved documents with both text and image contents and unstructured layouts, effectively modeling joint multimodal distributions. <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>\n\nSome approaches have focused on enabling multimodal generation with minimal architectural changes. Recent research has demonstrated that VL dialogue with frozen language models can generate interleaved multimodal data by freezing input encoders and training text-to-image and image-to-text linear maps to encode and decode produced embeddings. <Paper corpusId=\"257405349\" paperTitle=\"(Cao et al., 2023)\" isShortName></Paper>\n\nThese technical approaches collectively represent different strategies for solving the central challenge of multimodal generation: creating unified representations that allow models to seamlessly process and generate both textual and visual information while maintaining coherence and semantic alignment between the modalities.",
                "citations": [
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework."
                        ],
                        "paper": {
                            "corpus_id": 266573642,
                            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
                            "authors": [
                                {
                                    "authorId": "2115941903",
                                    "name": "Jiaxing Huang"
                                },
                                {
                                    "authorId": "2276743977",
                                    "name": "Jingyi Zhang"
                                },
                                {
                                    "authorId": "2276609830",
                                    "name": "Kai Jiang"
                                },
                                {
                                    "authorId": "49660254",
                                    "name": "Han Qiu"
                                },
                                {
                                    "authorId": "2237947102",
                                    "name": "Shijian Lu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 23
                        },
                        "score": 0.8525390625
                    },
                    {
                        "id": "(Xiong et al., 2024)",
                        "snippets": [
                            "Vision-to-Language (V2T) Tokenizer (Zhu et al., 2024a) maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) (Lee et al., 2022b) encodes both image and text inputs as tokens, generating coherent multimodal outputs. These approaches improve image-text generation, though they typically predict pixels in a fixed order without considering random generation strategies."
                        ],
                        "paper": {
                            "corpus_id": 273962846,
                            "title": "Autoregressive Models in Vision: A Survey",
                            "authors": [
                                {
                                    "authorId": "2324168511",
                                    "name": "Jing Xiong"
                                },
                                {
                                    "authorId": "2330229027",
                                    "name": "Gongye Liu"
                                },
                                {
                                    "authorId": "2323103372",
                                    "name": "Lun Huang"
                                },
                                {
                                    "authorId": "2277988832",
                                    "name": "Chengyue Wu"
                                },
                                {
                                    "authorId": "2137407647",
                                    "name": "Taiqiang Wu"
                                },
                                {
                                    "authorId": "2248348669",
                                    "name": "Yao Mu"
                                },
                                {
                                    "authorId": "2278219862",
                                    "name": "Yuan Yao"
                                },
                                {
                                    "authorId": "2290542137",
                                    "name": "Hui Shen"
                                },
                                {
                                    "authorId": "2240538849",
                                    "name": "Zhongwei Wan"
                                },
                                {
                                    "authorId": "2266440308",
                                    "name": "Jinfa Huang"
                                },
                                {
                                    "authorId": "2290488386",
                                    "name": "Chaofan Tao"
                                },
                                {
                                    "authorId": "2272708319",
                                    "name": "Shen Yan"
                                },
                                {
                                    "authorId": "2290852491",
                                    "name": "Huaxiu Yao"
                                },
                                {
                                    "authorId": "2324779503",
                                    "name": "Lingpeng Kong"
                                },
                                {
                                    "authorId": "2330266685",
                                    "name": "Hongxia Yang"
                                },
                                {
                                    "authorId": "2328615184",
                                    "name": "Mi Zhang"
                                },
                                {
                                    "authorId": "2323043345",
                                    "name": "Guillermo Sapiro"
                                },
                                {
                                    "authorId": "2322460427",
                                    "name": "Jiebo Luo"
                                },
                                {
                                    "authorId": "2311889480",
                                    "name": "Ping Luo"
                                },
                                {
                                    "authorId": "2295135094",
                                    "name": "Ngai Wong"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 14
                        },
                        "score": 0.7490234375
                    },
                    {
                        "id": "(Zhu et al., 2024)",
                        "snippets": [
                            "In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multimodal datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the LLM's vocabulary. To achieve this, we present the Vision-to-Language Tokenizer; abbreviated as V2T Tokenizer, which transforms an image into a \"foreign language\" with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model. With this innovative image encoding, the LLM gains the ability not only for visual comprehension but also for image denoising and restoration in an auto-regressive fashion-crucially, without any fine-tuning. We undertake rig-orous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and visual question answering, as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at https://github.com/zh460045050/V2l-Tokenizer."
                        ],
                        "paper": {
                            "corpus_id": 268363788,
                            "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension",
                            "authors": [
                                {
                                    "authorId": "2288067906",
                                    "name": "Lei Zhu"
                                },
                                {
                                    "authorId": "2290913195",
                                    "name": "Fangyun Wei"
                                },
                                {
                                    "authorId": "2287888645",
                                    "name": "Yanye Lu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 21
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."
                        ],
                        "paper": {
                            "corpus_id": 276782523,
                            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
                            "authors": [
                                {
                                    "authorId": "2248946985",
                                    "name": "Rui Zhao"
                                },
                                {
                                    "authorId": "2258958963",
                                    "name": "Weijia Mao"
                                },
                                {
                                    "authorId": "2344762475",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.86474609375
                    },
                    {
                        "id": "(Radford et al., 2021)",
                        "snippets": [
                            "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
                        ],
                        "paper": {
                            "corpus_id": 231591445,
                            "title": "Learning Transferable Visual Models From Natural Language Supervision",
                            "authors": [
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "2110935237",
                                    "name": "Jong Wook Kim"
                                },
                                {
                                    "authorId": "2004021329",
                                    "name": "Chris Hallacy"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "40087786",
                                    "name": "Gabriel Goh"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 29867
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                        ],
                        "paper": {
                            "corpus_id": 261975252,
                            "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                            "authors": [
                                {
                                    "authorId": "2056965063",
                                    "name": "Runpei Dong"
                                },
                                {
                                    "authorId": "2118643247",
                                    "name": "Chunrui Han"
                                },
                                {
                                    "authorId": "2211415443",
                                    "name": "Yuang Peng"
                                },
                                {
                                    "authorId": "3424017",
                                    "name": "Zekun Qi"
                                },
                                {
                                    "authorId": "2242581956",
                                    "name": "Zheng Ge"
                                },
                                {
                                    "authorId": "2161319190",
                                    "name": "Jinrong Yang"
                                },
                                {
                                    "authorId": "48096671",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "26913717",
                                    "name": "Jian\u2010Yuan Sun"
                                },
                                {
                                    "authorId": "2157473950",
                                    "name": "Hongyu Zhou"
                                },
                                {
                                    "authorId": "134085586",
                                    "name": "Hao-Ran Wei"
                                },
                                {
                                    "authorId": "2069029697",
                                    "name": "Xiangwen Kong"
                                },
                                {
                                    "authorId": "2185865433",
                                    "name": "Xiangyu Zhang"
                                },
                                {
                                    "authorId": "2244131405",
                                    "name": "Kaisheng Ma"
                                },
                                {
                                    "authorId": "2242612318",
                                    "name": "Li Yi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cao et al., 2023)",
                        "snippets": [
                            "Recently, [145] proposed a method to realize VL dialogue with frozen language models, enabling the model to generate interleaved multimodal data. This method also freezes input encoders and train text-to-image and image-to-text linear maps to further encode and decode produced embeddings."
                        ],
                        "paper": {
                            "corpus_id": 257405349,
                            "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT",
                            "authors": [
                                {
                                    "authorId": "2211165440",
                                    "name": "Yihan Cao"
                                },
                                {
                                    "authorId": "2211077408",
                                    "name": "Siyu Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Yixin Liu"
                                },
                                {
                                    "authorId": "2143627170",
                                    "name": "Zhiling Yan"
                                },
                                {
                                    "authorId": "31336649",
                                    "name": "Yutong Dai"
                                },
                                {
                                    "authorId": "144019071",
                                    "name": "Philip S. Yu"
                                },
                                {
                                    "authorId": "46732871",
                                    "name": "Lichao Sun"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 548
                        },
                        "score": 0.76318359375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Advancements in Multimodal Retrieval and Generation",
                "tldr": "Recent research has focused on enhancing Retrieval-Augmented Generation (RAG) systems for multimodal content, with frameworks like RA-CM3, VisRAG, and Visual RAG improving both retrieval accuracy and generation quality. These advancements enable more efficient processing of documents containing both text and images, reducing computational costs while improving performance. (1 source)",
                "text": "\nThe integration of Retrieval-Augmented Generation (RAG) techniques with multimodal models represents one of the most promising recent advancements in the field. RA-CM3 has emerged as a notable innovation that enhances both text-to-image and image-to-text generation capabilities by combining a CLIP retriever with a CM3 Transformer generator. This integration not only improves performance but also achieves significant computational efficiency, reducing costs by over 30% <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nAnother important advancement comes from the introduction of specialized reranking mechanisms based on relevance score models. These approaches improve context selection during the retrieval phase, which directly reduces hallucinations in generated content and enhances overall response quality <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nVisRAG represents a significant leap forward by offering a framework that bypasses traditional text parsing stages to directly process multimodal documents containing both text and images. This direct processing approach has demonstrated substantial improvements in multimodal tasks where content needs to be understood and generated across different modalities <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nFor expanding the visual capabilities of multimodal models without expensive fine-tuning, Visual RAG offers a computationally efficient solution. This system dynamically retrieves relevant visual examples to enhance the model's knowledge, providing a practical pathway for improving visual-textual generation capabilities <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nIndustrial applications have also benefited from these advancements, with research showing that integrating both images and text in RAG systems leads to significantly improved performance in domain-specific contexts. This integration is particularly valuable for applications requiring precise understanding and generation of content with embedded textual elements in images <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nThese recent developments collectively demonstrate how retrieval-augmented approaches can enhance the capabilities of multimodal systems, enabling more accurate and efficient generation of interleaved text and image content. By leveraging external knowledge through sophisticated retrieval mechanisms, these systems overcome many limitations of traditional end-to-end models while maintaining the flexibility to generate coherent multimodal outputs.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "The latest advancements in RAG in the image domain have led to significant improvements [120]. RA-CM3 [121] enhances both text-to-image and image-to-text generation by combining the CLIP retriever and the CM3 Transformer generator, achieving a performance boost while reducing computational costs by over 30%. Mortaheb et al. introduced a reranking mechanism based on a relevance score model [122], improving context selection during retrieval and reducing hallucinations, thereby enhancing the quality of generated responses. Yu et al.'s VisRAG [123] framework bypasses the text parsing stage to directly process multi-modal documents containing both text and images, achieving substantial improvements in multi-modal tasks. Bonomo and Bianco's Visual RAG [124] expands the visual knowledge of large MLLMs without the need for fine-tuning by dynamically retrieving relevant examples, offering high computational efficiency. Riedler and Langer's work on multimodal inputs for industrial applications demonstrates that integrating both images and text in RAG systems significantly improves performance [125]."
                        ],
                        "paper": {
                            "corpus_id": 273969615,
                            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
                            "authors": [
                                {
                                    "authorId": "2270181751",
                                    "name": "Zilun Zhang"
                                },
                                {
                                    "authorId": "2174678931",
                                    "name": "Haozhan Shen"
                                },
                                {
                                    "authorId": "8200875",
                                    "name": "Tiancheng Zhao"
                                },
                                {
                                    "authorId": "2330774884",
                                    "name": "Yuhao Wang"
                                },
                                {
                                    "authorId": "2330612748",
                                    "name": "Bin Chen"
                                },
                                {
                                    "authorId": "2149196373",
                                    "name": "Yuxiang Cai"
                                },
                                {
                                    "authorId": "2093090552",
                                    "name": "Yongheng Shang"
                                },
                                {
                                    "authorId": "2111612160",
                                    "name": "Jianwei Yin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.78125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.116307
    }
}