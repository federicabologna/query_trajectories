{
    "query": "want LLM to evaluate the quality generated data GPT-4 wondering use GPT-4 the evaluator LLM or different LLM so that it does own generation the best practice thanks",
    "user_id": "lib_user",
    "task_id": "76a8e4c3-6744-400b-975a-c51ab4e32c4d",
    "timestamp": "2025-06-23T21:10:55.881106",
    "n_retrieval": 256,
    "n_retrieved": 271,
    "n_candidates": 5,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.120396,
    "decomposed_query": {
        "rewritten_query": "Evaluating the quality of data generated by GPT-4 and whether to use GPT-4 itself as the evaluator LLM or a different LLM for best practices.",
        "keyword_query": "LLM evaluate quality generated data GPT-4 evaluator best practice",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.01044,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Dealing with Data for RE: Mitigating Challenges while using NLP and Generative AI",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.16977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2107155",
                    "name": "S. Ghaisas"
                },
                {
                    "authorId": "2120316520",
                    "name": "Anmol Singhal"
                }
            ],
            "abstract": "Across the dynamic business landscape today, enterprises face an ever-increasing range of challenges. These include the constantly evolving regulatory environment, the growing demand for personalization within software applications, and the heightened emphasis on governance. In response to these multifaceted demands, large enterprises have been adopting automation that spans from the optimization of core business processes to the enhancement of customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a pivotal element of modern software systems. In this context, data plays an indispensable role. AI-centric software systems based on supervised learning and operating at an industrial scale require large volumes of training data to perform effectively. Moreover, the incorporation of generative AI has led to a growing demand for adequate evaluation benchmarks. Our experience in this field has revealed that the requirement for large datasets for training and evaluation introduces a host of intricate challenges. This book chapter explores the evolving landscape of Software Engineering (SE) in general, and Requirements Engineering (RE) in particular, in this era marked by AI integration. We discuss challenges that arise while integrating Natural Language Processing (NLP) and generative AI into enterprise-critical software systems. The chapter provides practical insights, solutions, and examples to equip readers with the knowledge and tools necessary for effectively building solutions with NLP at their cores. We also reflect on how these text data-centric tasks sit together with the traditional RE process. We also highlight new RE tasks that may be necessary for handling the increasingly important text data-centricity involved in developing software systems.",
            "corpus_id": 268032876,
            "sentences": [
                {
                    "corpus_id": "268032876",
                    "title": "Dealing with Data for RE: Mitigating Challenges while using NLP and Generative AI",
                    "text": "While a plethora of metrics exist to evaluate various NLP approaches, the integration of generative AI capabilities into SSNLPCores introduces fresh challenges when validating LLM output. The output necessitates evaluation against several parameters, the specifics of which depend upon the targeted task. When focusing on RE, the following parameters emerge as crucial based on our experience: \n\n-Factuality and Accuracy of Output: It's imperative that LLMs do not create or 'hallucinate' inaccurate text. -Information Retention: For certain RE tasks necessitating summarization or extraction of pertinent information, the generated output must not omit any critical details. -Societal Bias: The generated output must not infringe upon human rights and should safeguard the interests of all sensitive groups. \n\nCommonly used evaluation metrics such as accuracy, F-scores, BLEU, ROUGE, and perplexity are not specifically designed to cater to these parameters [13]. Additionally, most metrics assume the existence of ground truth for evaluation, which is frequently challenging to procure for specialized tasks, as previously discussed. Consequently, the expertise of domain experts is invaluable in validating the generated output. Their insights can be utilized to train models via Reinforcement Learning with Human Feedback (RLHF), a method being attributed as a major factor in the success of LLMs such as ChatGPT. \n\nBeyond utilizing domain experts to validate generated data, practitioners have also begun deploying LLMs themselves for evaluation purposes. Two predominant strategies for using LLMs as evaluators are as follows: \n\n-Self-Reflection [24]: Following the completion of a task by an LLM, the model is prompted to reconsider its generated output and identify any potential shortcomings. This method has demonstrated an enhancement in accuracy. \n\n-Engaging an Oracle-LLM for Evaluation [15]: This strategy involves the use of an auxiliary, typically more powerful, LLM for evaluation. For instance, the authors of Vicuna deployed GPT-4 as an 'oracle' LLM to compare and rank the outputs of various LLMs, including LLAMA, Alpaca, and Chat-GPT, thereby ascertaining the relative differences in generation quality.",
                    "score": 0.44953722352019515,
                    "section_title": "Validation of SSNLPCore Output",
                    "char_start_offset": 43596,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 304
                        },
                        {
                            "start": 305,
                            "end": 393
                        },
                        {
                            "start": 396,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 808
                        },
                        {
                            "start": 811,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1417
                        },
                        {
                            "start": 1420,
                            "end": 1560
                        },
                        {
                            "start": 1561,
                            "end": 1632
                        },
                        {
                            "start": 1635,
                            "end": 1801
                        },
                        {
                            "start": 1802,
                            "end": 1858
                        },
                        {
                            "start": 1861,
                            "end": 1998
                        },
                        {
                            "start": 1999,
                            "end": 2225
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 959,
                            "end": 963,
                            "matchedPaperCorpusId": "259360395"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76708984375
                }
            ],
            "relevance_judgement": 0.76708984375,
            "relevance_judgment_input_expanded": "# Title: Dealing with Data for RE: Mitigating Challenges while using NLP and Generative AI\n# Venue: arXiv.org\n# Authors: S. Ghaisas, Anmol Singhal\n## Abstract\nAcross the dynamic business landscape today, enterprises face an ever-increasing range of challenges. These include the constantly evolving regulatory environment, the growing demand for personalization within software applications, and the heightened emphasis on governance. In response to these multifaceted demands, large enterprises have been adopting automation that spans from the optimization of core business processes to the enhancement of customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a pivotal element of modern software systems. In this context, data plays an indispensable role. AI-centric software systems based on supervised learning and operating at an industrial scale require large volumes of training data to perform effectively. Moreover, the incorporation of generative AI has led to a growing demand for adequate evaluation benchmarks. Our experience in this field has revealed that the requirement for large datasets for training and evaluation introduces a host of intricate challenges. This book chapter explores the evolving landscape of Software Engineering (SE) in general, and Requirements Engineering (RE) in particular, in this era marked by AI integration. We discuss challenges that arise while integrating Natural Language Processing (NLP) and generative AI into enterprise-critical software systems. The chapter provides practical insights, solutions, and examples to equip readers with the knowledge and tools necessary for effectively building solutions with NLP at their cores. We also reflect on how these text data-centric tasks sit together with the traditional RE process. We also highlight new RE tasks that may be necessary for handling the increasingly important text data-centricity involved in developing software systems.\n## Validation of SSNLPCore Output\nWhile a plethora of metrics exist to evaluate various NLP approaches, the integration of generative AI capabilities into SSNLPCores introduces fresh challenges when validating LLM output. The output necessitates evaluation against several parameters, the specifics of which depend upon the targeted task. When focusing on RE, the following parameters emerge as crucial based on our experience: \n\n-Factuality and Accuracy of Output: It's imperative that LLMs do not create or 'hallucinate' inaccurate text. -Information Retention: For certain RE tasks necessitating summarization or extraction of pertinent information, the generated output must not omit any critical details. -Societal Bias: The generated output must not infringe upon human rights and should safeguard the interests of all sensitive groups. \n\nCommonly used evaluation metrics such as accuracy, F-scores, BLEU, ROUGE, and perplexity are not specifically designed to cater to these parameters [13]. Additionally, most metrics assume the existence of ground truth for evaluation, which is frequently challenging to procure for specialized tasks, as previously discussed. Consequently, the expertise of domain experts is invaluable in validating the generated output. Their insights can be utilized to train models via Reinforcement Learning with Human Feedback (RLHF), a method being attributed as a major factor in the success of LLMs such as ChatGPT. \n\nBeyond utilizing domain experts to validate generated data, practitioners have also begun deploying LLMs themselves for evaluation purposes. Two predominant strategies for using LLMs as evaluators are as follows: \n\n-Self-Reflection [24]: Following the completion of a task by an LLM, the model is prompted to reconsider its generated output and identify any potential shortcomings. This method has demonstrated an enhancement in accuracy. \n\n-Engaging an Oracle-LLM for Evaluation [15]: This strategy involves the use of an auxiliary, typically more powerful, LLM for evaluation. For instance, the authors of Vicuna deployed GPT-4 as an 'oracle' LLM to compare and rank the outputs of various LLMs, including LLAMA, Alpaca, and Chat-GPT, thereby ascertaining the relative differences in generation quality.",
            "reference_string": "[268032876 | Ghaisas et al. | 2024 | Citations: 0]"
        },
        {
            "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 50,
            "citation_count": 34,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2311.18702",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.18702, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1886879",
                    "name": "Pei Ke"
                },
                {
                    "authorId": "2122225897",
                    "name": "Bosi Wen"
                },
                {
                    "authorId": "2316634875",
                    "name": "Andrew Feng"
                },
                {
                    "authorId": "2308072332",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2181283109",
                    "name": "Xuanyu Lei"
                },
                {
                    "authorId": "2308160059",
                    "name": "Jiale Cheng"
                },
                {
                    "authorId": "2151486382",
                    "name": "Shengyuan Wang"
                },
                {
                    "authorId": "2051712753",
                    "name": "Aohan Zeng"
                },
                {
                    "authorId": "2243402027",
                    "name": "Yuxiao Dong"
                },
                {
                    "authorId": "2253869803",
                    "name": "Hongning Wang"
                },
                {
                    "authorId": "2260595820",
                    "name": "Jie Tang"
                },
                {
                    "authorId": "2254009342",
                    "name": "Minlie Huang"
                }
            ],
            "abstract": "Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.",
            "corpus_id": 270738200,
            "sentences": [
                {
                    "corpus_id": "270738200",
                    "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
                    "text": "Since most of the state-of-the-art LLMs can only be accessed via APIs, researchers start to automatically collect evaluation data by directly prompting GPT-4 and train their own evaluation models to provide stable and effective evaluations at a lower cost (Wang et al., 2024;Li et al., 2024;Kim et al., 2024).\n\nThe concurrent works similar to ours are the LLMs specially trained for evaluation tasks like PandaLM (Wang et al., 2024), JudgeLM (Zhu et al., 2023), and AUTO-J (Li et al., 2024).For comparison, our work is the first attempt to deal with the challenge of uninformative critique generation which commonly appears in recent LLMbased evaluation models especially without references.Instead of prompting GPT-4 directly, our proposed Eval-Instruct can fully utilize the connection among different evaluation tasks and settings to construct informative evaluation data, which are empirically shown to improve the quality of generated critiques.",
                    "score": 0.4459348822399049,
                    "section_title": "Related Work",
                    "char_start_offset": 5855,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 309
                        },
                        {
                            "start": 311,
                            "end": 491
                        },
                        {
                            "start": 491,
                            "end": 691
                        },
                        {
                            "start": 691,
                            "end": 950
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 256,
                            "end": 275,
                            "matchedPaperCorpusId": "259108266"
                        },
                        {
                            "start": 275,
                            "end": 291,
                            "matchedPaperCorpusId": "263829791"
                        },
                        {
                            "start": 413,
                            "end": 432,
                            "matchedPaperCorpusId": "259108266"
                        },
                        {
                            "start": 473,
                            "end": 490,
                            "matchedPaperCorpusId": "263829791"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63037109375
                },
                {
                    "corpus_id": "270738200",
                    "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
                    "text": "To investigate whether the critiques generated by our model can serve as feedback to improve the quality of LLM-generated texts, we employ Chat-GPT, GPT-4, and CRITIQUELLM to provide critiques for the generated texts of ChatGPT in the reference-free setting.Then, we instruct ChatGPT to modify its original generation based on the critiques.Finally, we use GPT-4 to perform referenced evaluations on the original texts and the modified texts generated by ChatGPT, respectively.\n\nThe results in Table 4 show that the critiques from CRITIQUELLM can serve as positive feedback whose contributed improvement on the overall score is close to that from the GPT-4's critiques.This further verifies the utility of CRITIQUELLM to provide informative critiques as scalable feedback that can guide LLMs towards better generation.We also notice that the critiques from Chat-GPT itself have a negative impact on the overall quality of its generated texts.This phenomenon is consistent with recent works that doubt the selfcorrection ability of LLMs without external inputs (Huang et al., 2023a;Stechly et al., 2023;Valmeekam et al., 2023).\n\nWe also report the evaluation scores before and after the critique-based modification across different tasks in Table 4.It is notable that the critiques from CRITIQUELLM can help enhance the quality of generated texts in a majority of tasks.However, in the tasks of logical reasoning, mathematics, and advanced Chinese understanding which are mostly hard tasks involving reasoning, the critiques from CRITIQUELLM seem to degrade the performance.We manually checked error cases and found that our model obtained misleading critiques on the reasoning process of generated texts.Since the evaluation of reasoning chains remains a challenging task (Golovneva et al., 2023) even for GPT-4, we leave further investigation in these tasks as future work.\n\nSince our experiment is a preliminary step towards utilizing critiques as feedback, we additionally have some findings which may inspire future research.",
                    "score": 0.4410060123142076,
                    "section_title": "Analysis of Critique as Feedback",
                    "char_start_offset": 22325,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 258
                        },
                        {
                            "start": 258,
                            "end": 341
                        },
                        {
                            "start": 341,
                            "end": 477
                        },
                        {
                            "start": 479,
                            "end": 669
                        },
                        {
                            "start": 669,
                            "end": 818
                        },
                        {
                            "start": 818,
                            "end": 942
                        },
                        {
                            "start": 942,
                            "end": 1126
                        },
                        {
                            "start": 1128,
                            "end": 1248
                        },
                        {
                            "start": 1248,
                            "end": 1369
                        },
                        {
                            "start": 1369,
                            "end": 1573
                        },
                        {
                            "start": 1573,
                            "end": 1704
                        },
                        {
                            "start": 1704,
                            "end": 1874
                        },
                        {
                            "start": 1876,
                            "end": 2029
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1772,
                            "end": 1796,
                            "matchedPaperCorpusId": "254685985"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.439697265625
                }
            ],
            "relevance_judgement": 0.63037109375,
            "relevance_judgment_input_expanded": "# Title: CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang\n## Abstract\nSince the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.\n## Related Work\nSince most of the state-of-the-art LLMs can only be accessed via APIs, researchers start to automatically collect evaluation data by directly prompting GPT-4 and train their own evaluation models to provide stable and effective evaluations at a lower cost (Wang et al., 2024;Li et al., 2024;Kim et al., 2024).\n\nThe concurrent works similar to ours are the LLMs specially trained for evaluation tasks like PandaLM (Wang et al., 2024), JudgeLM (Zhu et al., 2023), and AUTO-J (Li et al., 2024).For comparison, our work is the first attempt to deal with the challenge of uninformative critique generation which commonly appears in recent LLMbased evaluation models especially without references.Instead of prompting GPT-4 directly, our proposed Eval-Instruct can fully utilize the connection among different evaluation tasks and settings to construct informative evaluation data, which are empirically shown to improve the quality of generated critiques.\n\n## Analysis of Critique as Feedback\nTo investigate whether the critiques generated by our model can serve as feedback to improve the quality of LLM-generated texts, we employ Chat-GPT, GPT-4, and CRITIQUELLM to provide critiques for the generated texts of ChatGPT in the reference-free setting.Then, we instruct ChatGPT to modify its original generation based on the critiques.Finally, we use GPT-4 to perform referenced evaluations on the original texts and the modified texts generated by ChatGPT, respectively.\n\nThe results in Table 4 show that the critiques from CRITIQUELLM can serve as positive feedback whose contributed improvement on the overall score is close to that from the GPT-4's critiques.This further verifies the utility of CRITIQUELLM to provide informative critiques as scalable feedback that can guide LLMs towards better generation.We also notice that the critiques from Chat-GPT itself have a negative impact on the overall quality of its generated texts.This phenomenon is consistent with recent works that doubt the selfcorrection ability of LLMs without external inputs (Huang et al., 2023a;Stechly et al., 2023;Valmeekam et al., 2023).\n\nWe also report the evaluation scores before and after the critique-based modification across different tasks in Table 4.It is notable that the critiques from CRITIQUELLM can help enhance the quality of generated texts in a majority of tasks.However, in the tasks of logical reasoning, mathematics, and advanced Chinese understanding which are mostly hard tasks involving reasoning, the critiques from CRITIQUELLM seem to degrade the performance.We manually checked error cases and found that our model obtained misleading critiques on the reasoning process of generated texts.Since the evaluation of reasoning chains remains a challenging task (Golovneva et al., 2023) even for GPT-4, we leave further investigation in these tasks as future work.\n\nSince our experiment is a preliminary step towards utilizing critiques as feedback, we additionally have some findings which may inspire future research.",
            "reference_string": "[270738200 | Ke et al. | 2023 | Citations: 34]"
        },
        {
            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 127,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2401.07103",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.07103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145256331",
                    "name": "Zhen Li"
                },
                {
                    "authorId": "2279658967",
                    "name": "Xiaohan Xu"
                },
                {
                    "authorId": "2279548827",
                    "name": "Tao Shen"
                },
                {
                    "authorId": "2284826718",
                    "name": "Can Xu"
                },
                {
                    "authorId": "2308241851",
                    "name": "Jia-Chen Gu"
                },
                {
                    "authorId": "2308073132",
                    "name": "Yuxuan Lai"
                },
                {
                    "authorId": "2287928517",
                    "name": "Chongyang Tao"
                },
                {
                    "authorId": "2307142498",
                    "name": "Shuai Ma"
                }
            ],
            "abstract": "In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.",
            "corpus_id": 270391675,
            "sentences": [
                {
                    "corpus_id": "270391675",
                    "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                    "text": "Some works tune LLMs to provide quality ratings or labels for generated texts.Gekhman et al. (2023) employed FLAN-PaLM 540B (Chung et al., 2022) to annotate the quality of real model-generated summaries and utilized these annotated data as training data to tune a light-weight LLM (e.g., T5-11B) as a factual consistency summary evaluator, which predicts \"1\" if the summary demonstrates factual consistency and \"0\" otherwise.Yue et al. (2023) reused and repurposed the existing fact-checking, NLI, and summarization tasks datasets and obtained simulated data from open-domain QA datasets to tune lightweight LLMs for attribution evaluation, which generates attributable, extrapolatory or contradictory labels for the generated answer with given query and reference documents.Li et al. (2023a) created a dataset containing multiple scenarios and used GPT-4 (OpenAI, 2023) to generate evaluation judgments for each scenario as supervision signals to tune LLaMA as a generative evaluator, which can output overall quality rating for individual LLM-generated response in various scenarios.Wang et al. (2023a) repurposed existing datasets with proper anonymization and new personalized labels to tune LLaMA2 (Touvron et al., 2023) as a personalized story evaluation model which provides personalized evaluation for generated texts through outputting a grade in [1, 10] and detailed reviews.Kim et al. (2023a) prompted GPT-4 to construct training data, including reference answers and crafted diverse customized score rubrics, and used them to tune LLaMA to evaluate modelgenerated responses of given instruction, which is generalized to realistic user demands.Ke et al. (2023) instructed GPT-4 to collect referenced and reference-free training data with dialogue-based prompting, utilized to tune LLMs for evaluating the alignment of model-generated texts with human instructions through generating scores and explanations.(Liu et al., 2023a) constructed a referencefree instruction-tuning dataset tailored for multi-aspect evaluation across summarization, dialogue and data-to-text tasks.",
                    "score": 0.4530904223012332,
                    "section_title": "Tuning-based Evaluation",
                    "char_start_offset": 23998,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 78,
                            "end": 425
                        },
                        {
                            "start": 425,
                            "end": 775
                        },
                        {
                            "start": 775,
                            "end": 1085
                        },
                        {
                            "start": 1085,
                            "end": 1385
                        },
                        {
                            "start": 1385,
                            "end": 1655
                        },
                        {
                            "start": 1655,
                            "end": 1918
                        },
                        {
                            "start": 1918,
                            "end": 2084
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57275390625
                },
                {
                    "corpus_id": "270391675",
                    "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                    "text": "This may involve tailoring the aspects of evaluation relevant to each field.For example, while evaluating legal documents, aspects such as legal accuracy and adherence to the judicial system are crucial (Cui et al., 2023b), which differ significantly from the aspects relevant in medical or financial texts.Therefore, to enhance the efficacy of LLMs as evaluators in specialized domains, there's a pressing need to develop models that are not only domain-aware but also equipped with the capability to evaluate based on domain-specific criteria.\n\nUnified Evaluation.LLMs have been expanded w.r.t their broad capabilities beyond traditional single-task focuses, encompassing complex instructions like coding and open-ended real-world requirements (OpenAI, 2023; Significant Gravitas).Consequently, there is an increasing demand for more comprehensive and flexible evaluation methods.However, traditional evaluation methods and most current LLM-based evaluators are limited to constrained tasks and evaluation aspects (cf.Table 2).Some promising attempts have been made in this direction.For instance, MT-Bench (Zheng et al., 2023) uses GPT-4 as an evaluator across multiple domains for multi-turn questions.Yet, this is too confined to a few evaluation aspects and limits dialogue to two turns only.Another model, Auto-J (Li et al., 2023b), approaches from a data construction perspective, training a 13B LLM on user queries and GPT-4 generated responses across a wide range of real-world scenarios.It accommodates diverse evaluation protocols and has been validated in 58 different scenarios, even outperforming many proprietary LLMs.In light of increasingly complex user queries, we advocate that developing a more unified and contemporaneous evaluation protocol is a promising direction.Additionally, constructing high-quality, comprehensive datasets to train unified models holds great potential.Such advancements could significantly contribute to more effective and universal evaluations of LLMs.",
                    "score": 0.4592069904585303,
                    "section_title": "Challenges and Open Problems",
                    "char_start_offset": 43950,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 76
                        },
                        {
                            "start": 76,
                            "end": 307
                        },
                        {
                            "start": 307,
                            "end": 545
                        },
                        {
                            "start": 547,
                            "end": 566
                        },
                        {
                            "start": 566,
                            "end": 783
                        },
                        {
                            "start": 783,
                            "end": 882
                        },
                        {
                            "start": 882,
                            "end": 1020
                        },
                        {
                            "start": 1020,
                            "end": 1029
                        },
                        {
                            "start": 1029,
                            "end": 1086
                        },
                        {
                            "start": 1086,
                            "end": 1206
                        },
                        {
                            "start": 1206,
                            "end": 1298
                        },
                        {
                            "start": 1298,
                            "end": 1498
                        },
                        {
                            "start": 1498,
                            "end": 1634
                        },
                        {
                            "start": 1634,
                            "end": 1789
                        },
                        {
                            "start": 1789,
                            "end": 1899
                        },
                        {
                            "start": 1899,
                            "end": 2000
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.464599609375
                },
                {
                    "corpus_id": "270391675",
                    "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                    "text": "LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;Ouyang et al., 2022) to ensure more balanced and comprehensive assessments.\n\nDomain-Specific Evaluation.LLMs have been prevalent across various domains, such as law (Cui et al., 2023a), medicine (Singhal et al., 2023), finance (Yang et al., 2023a), etc.However, most LLMs employed as evaluators are designed for general domains and are not specifically tailored to any particular field.This lack of specialization poses significant challenges.On one hand, these LLMs often lack the requisite domain-specific knowledge, making it difficult for them to accurately assess the correctness of content within specialized fields.On the other hand, the evaluation prompts need to be meticulously designed for different domains.This may involve tailoring the aspects of evaluation relevant to each field.",
                    "score": 0.6183299975122041,
                    "section_title": "Challenges and Open Problems",
                    "char_start_offset": 42121,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 169,
                            "end": 372
                        },
                        {
                            "start": 372,
                            "end": 543
                        },
                        {
                            "start": 543,
                            "end": 692
                        },
                        {
                            "start": 692,
                            "end": 877
                        },
                        {
                            "start": 877,
                            "end": 1185
                        },
                        {
                            "start": 1187,
                            "end": 1214
                        },
                        {
                            "start": 1214,
                            "end": 1363
                        },
                        {
                            "start": 1363,
                            "end": 1496
                        },
                        {
                            "start": 1496,
                            "end": 1553
                        },
                        {
                            "start": 1553,
                            "end": 1732
                        },
                        {
                            "start": 1732,
                            "end": 1829
                        },
                        {
                            "start": 1829,
                            "end": 1905
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1051,
                            "end": 1072,
                            "matchedPaperCorpusId": "215548699"
                        },
                        {
                            "start": 1110,
                            "end": 1130,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 1305,
                            "end": 1327,
                            "matchedPaperCorpusId": "255124952"
                        },
                        {
                            "start": 1337,
                            "end": 1357,
                            "matchedPaperCorpusId": "254877751"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.39013671875
                },
                {
                    "corpus_id": "270391675",
                    "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                    "text": "It takes into account a diverse range of factors, including coherence, relevance, and fluency, to offer a holistic understanding of the quality of the hypothesis text.Besides, some studies explore error-oriented evaluation which focused on examining and explaining the specific errors in the hypothesis text, offering insights into why a particular score is derived.This category delves into the fine-grained aspects of generated content to identify and justify evaluation outcomes.\n\nFor instance, Yue et al. (2023) first defined different types of attribution errors, and then explored prompting LLMs or fine-tuning smaller LLMs on simulated and repurposed data from related tasks such as question answering (QA), fact-checking, natural language inference (NLI), and summarization.Xu et al. (2023) utilized GPT-4 to construct fine-grained analysis data to tune LLaMA to generate error analysis for generated text compared with reference text, after which this work utilized real model-generated response-reference pairs to refine and self-train evaluator.Furthermore, Jiang et al. (2023) sampled data from diverse text generation datasets, including summarization, translation and data2text, whose system outputs included real-world system output and GPT-4 synthesis, and prompted GPT-4 to curate error analysis to tune LLaMA for fine-grained evaluation.",
                    "score": 0.4489594309071214,
                    "section_title": "Tuning-based Evaluation",
                    "char_start_offset": 27964,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 167,
                            "end": 366
                        },
                        {
                            "start": 366,
                            "end": 482
                        },
                        {
                            "start": 484,
                            "end": 782
                        },
                        {
                            "start": 782,
                            "end": 1056
                        },
                        {
                            "start": 1056,
                            "end": 1355
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.326171875
                }
            ],
            "relevance_judgement": 0.57275390625,
            "relevance_judgment_input_expanded": "# Title: Leveraging Large Language Models for NLG Evaluation: Advances and Challenges\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, Shuai Ma\n## Abstract\nIn the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.\n## Tuning-based Evaluation\nSome works tune LLMs to provide quality ratings or labels for generated texts.Gekhman et al. (2023) employed FLAN-PaLM 540B (Chung et al., 2022) to annotate the quality of real model-generated summaries and utilized these annotated data as training data to tune a light-weight LLM (e.g., T5-11B) as a factual consistency summary evaluator, which predicts \"1\" if the summary demonstrates factual consistency and \"0\" otherwise.Yue et al. (2023) reused and repurposed the existing fact-checking, NLI, and summarization tasks datasets and obtained simulated data from open-domain QA datasets to tune lightweight LLMs for attribution evaluation, which generates attributable, extrapolatory or contradictory labels for the generated answer with given query and reference documents.Li et al. (2023a) created a dataset containing multiple scenarios and used GPT-4 (OpenAI, 2023) to generate evaluation judgments for each scenario as supervision signals to tune LLaMA as a generative evaluator, which can output overall quality rating for individual LLM-generated response in various scenarios.Wang et al. (2023a) repurposed existing datasets with proper anonymization and new personalized labels to tune LLaMA2 (Touvron et al., 2023) as a personalized story evaluation model which provides personalized evaluation for generated texts through outputting a grade in [1, 10] and detailed reviews.Kim et al. (2023a) prompted GPT-4 to construct training data, including reference answers and crafted diverse customized score rubrics, and used them to tune LLaMA to evaluate modelgenerated responses of given instruction, which is generalized to realistic user demands.Ke et al. (2023) instructed GPT-4 to collect referenced and reference-free training data with dialogue-based prompting, utilized to tune LLMs for evaluating the alignment of model-generated texts with human instructions through generating scores and explanations.(Liu et al., 2023a) constructed a referencefree instruction-tuning dataset tailored for multi-aspect evaluation across summarization, dialogue and data-to-text tasks.\n...\nIt takes into account a diverse range of factors, including coherence, relevance, and fluency, to offer a holistic understanding of the quality of the hypothesis text.Besides, some studies explore error-oriented evaluation which focused on examining and explaining the specific errors in the hypothesis text, offering insights into why a particular score is derived.This category delves into the fine-grained aspects of generated content to identify and justify evaluation outcomes.\n\nFor instance, Yue et al. (2023) first defined different types of attribution errors, and then explored prompting LLMs or fine-tuning smaller LLMs on simulated and repurposed data from related tasks such as question answering (QA), fact-checking, natural language inference (NLI), and summarization.Xu et al. (2023) utilized GPT-4 to construct fine-grained analysis data to tune LLaMA to generate error analysis for generated text compared with reference text, after which this work utilized real model-generated response-reference pairs to refine and self-train evaluator.Furthermore, Jiang et al. (2023) sampled data from diverse text generation datasets, including summarization, translation and data2text, whose system outputs included real-world system output and GPT-4 synthesis, and prompted GPT-4 to curate error analysis to tune LLaMA for fine-grained evaluation.\n\n## Challenges and Open Problems\nLLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;Ouyang et al., 2022) to ensure more balanced and comprehensive assessments.\n\nDomain-Specific Evaluation.LLMs have been prevalent across various domains, such as law (Cui et al., 2023a), medicine (Singhal et al., 2023), finance (Yang et al., 2023a), etc.However, most LLMs employed as evaluators are designed for general domains and are not specifically tailored to any particular field.This lack of specialization poses significant challenges.On one hand, these LLMs often lack the requisite domain-specific knowledge, making it difficult for them to accurately assess the correctness of content within specialized fields.On the other hand, the evaluation prompts need to be meticulously designed for different domains.This may involve tailoring the aspects of evaluation relevant to each field.\n...\nThis may involve tailoring the aspects of evaluation relevant to each field.For example, while evaluating legal documents, aspects such as legal accuracy and adherence to the judicial system are crucial (Cui et al., 2023b), which differ significantly from the aspects relevant in medical or financial texts.Therefore, to enhance the efficacy of LLMs as evaluators in specialized domains, there's a pressing need to develop models that are not only domain-aware but also equipped with the capability to evaluate based on domain-specific criteria.\n\nUnified Evaluation.LLMs have been expanded w.r.t their broad capabilities beyond traditional single-task focuses, encompassing complex instructions like coding and open-ended real-world requirements (OpenAI, 2023; Significant Gravitas).Consequently, there is an increasing demand for more comprehensive and flexible evaluation methods.However, traditional evaluation methods and most current LLM-based evaluators are limited to constrained tasks and evaluation aspects (cf.Table 2).Some promising attempts have been made in this direction.For instance, MT-Bench (Zheng et al., 2023) uses GPT-4 as an evaluator across multiple domains for multi-turn questions.Yet, this is too confined to a few evaluation aspects and limits dialogue to two turns only.Another model, Auto-J (Li et al., 2023b), approaches from a data construction perspective, training a 13B LLM on user queries and GPT-4 generated responses across a wide range of real-world scenarios.It accommodates diverse evaluation protocols and has been validated in 58 different scenarios, even outperforming many proprietary LLMs.In light of increasingly complex user queries, we advocate that developing a more unified and contemporaneous evaluation protocol is a promising direction.Additionally, constructing high-quality, comprehensive datasets to train unified models holds great potential.Such advancements could significantly contribute to more effective and universal evaluations of LLMs.",
            "reference_string": "[270391675 | Li et al. | 2024 | Citations: 15]"
        },
        {
            "title": "EHR Interaction Between Patients and AI: NoteAid EHR Interaction",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 42,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.17475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277238665",
                    "name": "Xiaocheng Zhang"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "2261455807",
                    "name": "Hong Yu"
                }
            ],
            "abstract": "With the rapid advancement of Large Language Models (LLMs) and their outstanding performance in semantic and contextual comprehension, the potential of LLMs in specialized domains warrants exploration. This paper introduces the NoteAid EHR Interaction Pipeline, an innovative approach developed using generative LLMs to assist in patient education, a task stemming from the need to aid patients in understanding Electronic Health Records (EHRs). Building upon the NoteAid work, we designed two novel tasks from the patient's perspective: providing explanations for EHR content that patients may not understand and answering questions posed by patients after reading their EHRs. We extracted datasets containing 10,000 instances from MIMIC Discharge Summaries and 876 instances from the MADE medical notes collection, respectively, executing the two tasks through the NoteAid EHR Interaction Pipeline with these data. Performance data of LLMs on these tasks were collected and constructed as the corresponding NoteAid EHR Interaction Dataset. Through a comprehensive evaluation of the entire dataset using LLM assessment and a rigorous manual evaluation of 64 instances, we showcase the potential of LLMs in patient education. Besides, the results provide valuable data support for future exploration and applications in this domain while also supplying high-quality synthetic datasets for in-house system training.",
            "corpus_id": 266690837,
            "sentences": [
                {
                    "corpus_id": "266690837",
                    "title": "EHR Interaction Between Patients and AI: NoteAid EHR Interaction",
                    "text": "We utilized the LLMs' ability in medical questions reasoning to evaluate the performance of collected conversation. We randomly selected 100 instances from the 876 generated based on MADE and combined them with 1000 test and 1000 validation data generated from MIMIC-III for LLM evaluation (totally 2100 cases). The LLMs used for evaluation were GPT-3.5-Turbo (referred to as Turbo in this section) and GPT-4. We aggregated the criteria into an evaluation prompt Table 4 to guide the LLMs in scoring each conversation. The results are shown in the Table 2. From the figure, it is evident that Turbo received higher scores than GPT-4, particularly in the Turbo NIP Explanation task. This discrepancy can be attributed to the strict evaluation criteria outlined in our evaluation prompt. Turbo, while slightly less capable than GPT-4, did not fully meet these criteria. Additionally, we observed that data generated by GPT-4 NIP, whether in the Q&A task or the Explanation task, exhibited more stable scores across both Turbo and GPT-4 evaluations. Notably, GPT-4 received a noticeably lower score in the Turbo NIP Explanation task when compared to Turbo. Human Evaluation Because our LLM evaluation has shown that the data generated by GPT-4 NIP is of higher quality and more stable, our human evaluation focused on GPT-NIP data to confirm the quality of the best synthetic data. We invited two medical students with clinical experience and three undergraduate students to conduct evaluations, with an equal split of 42 instances for both Q&A and explanation tasks (totally 84 cases). Among the 66 instances evaluated on GPT-4 NIP data, 52 instances had EHR notes from discharge instructions, while 14 were from MADE . The human evaluation results are presented in Table 2.",
                    "score": 0.529759848944197,
                    "section_title": "LLM Evaluation",
                    "char_start_offset": 12261,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 311
                        },
                        {
                            "start": 312,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 409
                        },
                        {
                            "start": 410,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1153
                        },
                        {
                            "start": 1154,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1717
                        },
                        {
                            "start": 1718,
                            "end": 1772
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55810546875
                }
            ],
            "relevance_judgement": 0.55810546875,
            "relevance_judgment_input_expanded": "# Title: EHR Interaction Between Patients and AI: NoteAid EHR Interaction\n# Venue: arXiv.org\n# Authors: Xiaocheng Zhang, Zonghai Yao, Hong Yu\n## Abstract\nWith the rapid advancement of Large Language Models (LLMs) and their outstanding performance in semantic and contextual comprehension, the potential of LLMs in specialized domains warrants exploration. This paper introduces the NoteAid EHR Interaction Pipeline, an innovative approach developed using generative LLMs to assist in patient education, a task stemming from the need to aid patients in understanding Electronic Health Records (EHRs). Building upon the NoteAid work, we designed two novel tasks from the patient's perspective: providing explanations for EHR content that patients may not understand and answering questions posed by patients after reading their EHRs. We extracted datasets containing 10,000 instances from MIMIC Discharge Summaries and 876 instances from the MADE medical notes collection, respectively, executing the two tasks through the NoteAid EHR Interaction Pipeline with these data. Performance data of LLMs on these tasks were collected and constructed as the corresponding NoteAid EHR Interaction Dataset. Through a comprehensive evaluation of the entire dataset using LLM assessment and a rigorous manual evaluation of 64 instances, we showcase the potential of LLMs in patient education. Besides, the results provide valuable data support for future exploration and applications in this domain while also supplying high-quality synthetic datasets for in-house system training.\n## LLM Evaluation\nWe utilized the LLMs' ability in medical questions reasoning to evaluate the performance of collected conversation. We randomly selected 100 instances from the 876 generated based on MADE and combined them with 1000 test and 1000 validation data generated from MIMIC-III for LLM evaluation (totally 2100 cases). The LLMs used for evaluation were GPT-3.5-Turbo (referred to as Turbo in this section) and GPT-4. We aggregated the criteria into an evaluation prompt Table 4 to guide the LLMs in scoring each conversation. The results are shown in the Table 2. From the figure, it is evident that Turbo received higher scores than GPT-4, particularly in the Turbo NIP Explanation task. This discrepancy can be attributed to the strict evaluation criteria outlined in our evaluation prompt. Turbo, while slightly less capable than GPT-4, did not fully meet these criteria. Additionally, we observed that data generated by GPT-4 NIP, whether in the Q&A task or the Explanation task, exhibited more stable scores across both Turbo and GPT-4 evaluations. Notably, GPT-4 received a noticeably lower score in the Turbo NIP Explanation task when compared to Turbo. Human Evaluation Because our LLM evaluation has shown that the data generated by GPT-4 NIP is of higher quality and more stable, our human evaluation focused on GPT-NIP data to confirm the quality of the best synthetic data. We invited two medical students with clinical experience and three undergraduate students to conduct evaluations, with an equal split of 42 instances for both Q&A and explanation tasks (totally 84 cases). Among the 66 instances evaluated on GPT-4 NIP data, 52 instances had EHR notes from discharge instructions, while 14 were from MADE . The human evaluation results are presented in Table 2.",
            "reference_string": "[266690837 | Zhang et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation",
            "venue": "International Conference on Human Factors in Computing Systems",
            "year": 2024,
            "reference_count": 95,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2409.07355",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.07355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313730766",
                    "name": "SeongYeub Chu"
                },
                {
                    "authorId": "2360877028",
                    "name": "Jong Woo Kim"
                },
                {
                    "authorId": "2052870942",
                    "name": "Mun Yong Yi"
                }
            ],
            "abstract": "This study introduces InteractEval, a framework that integrates the outcomes of Think-Aloud (TA) conducted by humans and LLMs to generate attributes for checklist-based text evaluation. By combining humans\u2019 flexibility and high-level reasoning with LLMs\u2019 consistency and extensive knowledge, InteractEval outperforms text evaluation baselines on a text summarization benchmark (SummEval) and an essay scoring benchmark (ELLIPSE). Furthermore, an in-depth analysis shows that it promotes divergent thinking in both humans and LLMs, leading to the generation of a wider range of relevant attributes and enhancement of text evaluation performance. A subsequent comparative analysis reveals that humans excel at identifying attributes related to internal quality (Coherence and Fluency), but LLMs perform better at those attributes related to external alignment (Consistency and Relevance). Consequently, leveraging both humans and LLMs together produces the best evaluation outcomes, highlighting the necessity of effectively combining humans and LLMs in an automated checklist-based text evaluation.",
            "corpus_id": 272593053,
            "sentences": [
                {
                    "corpus_id": "272593053",
                    "title": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation",
                    "text": "Performance comparison with baselines. We conducted a comparison against baseline evaluation methods to evaluate the performance of InteractEval. Building upon prior studies [24,27,42,49], we analyzed the correlation between InteractEval and groundtruth scores using Spearman's Rho, Kendall's Tau coefficients, and the difference between predicted scores and human-labeled scores using mean absolute errors (MAE). For a fair comparison, we used the performance data of the seven non-LLM-based traditional methods (see Table 9), G-Eval (GPT-4), and CheckEval (GPT-3.5 Turbo and GPT-4), as reported in [42], given that their experiments were conducted using the same experimental settings (sample-level correlation). Additionally, we reproduced the correlation results of G-Eval (GPT-3.5-Turbo) 9 and the MAE results of the entire baselines at the sample level using the source code provided by the original authors (where available) and following the implementation details described in their papers. Table 9 presents the main results, classifying the four dimensions into two key features by which generated texts are typically assessed: internal quality and external alignment [75,89]. Additionally, the table shows InteractEval's performance in the first and second trials using both GPT-3.5-Turbo and GPT-4, demonstrating stable sample-level correlations. In the both trials, InteractEval with GPT-4 outperforms or is at least comparable to the traditional non-LLM-based metrics, such as ROUGE-L, BLEU, METEOR, BERTScore, MOVERScore, BARTScore, and UniEval, in terms of correlation coefficients. As noted in [42], the results suggest that using a checklist enables an LLM to evaluate generated text in a manner more aligned with human judgment than traditional automatic metrics. \n\nWhen compared to G-Eval and CheckEval with the identical version of GPT, our model demonstrates superior performance. Moreover, even with GPT-3.5-Turbo,",
                    "score": 0.49367032880677564,
                    "section_title": "5.1.1",
                    "char_start_offset": 39604,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 38
                        },
                        {
                            "start": 39,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1186
                        },
                        {
                            "start": 1187,
                            "end": 1358
                        },
                        {
                            "start": 1359,
                            "end": 1598
                        },
                        {
                            "start": 1599,
                            "end": 1782
                        },
                        {
                            "start": 1785,
                            "end": 1902
                        },
                        {
                            "start": 1903,
                            "end": 1937
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 174,
                            "end": 178,
                            "matchedPaperCorpusId": "220768873"
                        },
                        {
                            "start": 1178,
                            "end": 1182,
                            "matchedPaperCorpusId": "269804457"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51318359375
                }
            ],
            "relevance_judgement": 0.51318359375,
            "relevance_judgment_input_expanded": "# Title: Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation\n# Venue: International Conference on Human Factors in Computing Systems\n# Authors: SeongYeub Chu, Jong Woo Kim, Mun Yong Yi\n## Abstract\nThis study introduces InteractEval, a framework that integrates the outcomes of Think-Aloud (TA) conducted by humans and LLMs to generate attributes for checklist-based text evaluation. By combining humans\u2019 flexibility and high-level reasoning with LLMs\u2019 consistency and extensive knowledge, InteractEval outperforms text evaluation baselines on a text summarization benchmark (SummEval) and an essay scoring benchmark (ELLIPSE). Furthermore, an in-depth analysis shows that it promotes divergent thinking in both humans and LLMs, leading to the generation of a wider range of relevant attributes and enhancement of text evaluation performance. A subsequent comparative analysis reveals that humans excel at identifying attributes related to internal quality (Coherence and Fluency), but LLMs perform better at those attributes related to external alignment (Consistency and Relevance). Consequently, leveraging both humans and LLMs together produces the best evaluation outcomes, highlighting the necessity of effectively combining humans and LLMs in an automated checklist-based text evaluation.\n## 5.1.1\nPerformance comparison with baselines. We conducted a comparison against baseline evaluation methods to evaluate the performance of InteractEval. Building upon prior studies [24,27,42,49], we analyzed the correlation between InteractEval and groundtruth scores using Spearman's Rho, Kendall's Tau coefficients, and the difference between predicted scores and human-labeled scores using mean absolute errors (MAE). For a fair comparison, we used the performance data of the seven non-LLM-based traditional methods (see Table 9), G-Eval (GPT-4), and CheckEval (GPT-3.5 Turbo and GPT-4), as reported in [42], given that their experiments were conducted using the same experimental settings (sample-level correlation). Additionally, we reproduced the correlation results of G-Eval (GPT-3.5-Turbo) 9 and the MAE results of the entire baselines at the sample level using the source code provided by the original authors (where available) and following the implementation details described in their papers. Table 9 presents the main results, classifying the four dimensions into two key features by which generated texts are typically assessed: internal quality and external alignment [75,89]. Additionally, the table shows InteractEval's performance in the first and second trials using both GPT-3.5-Turbo and GPT-4, demonstrating stable sample-level correlations. In the both trials, InteractEval with GPT-4 outperforms or is at least comparable to the traditional non-LLM-based metrics, such as ROUGE-L, BLEU, METEOR, BERTScore, MOVERScore, BARTScore, and UniEval, in terms of correlation coefficients. As noted in [42], the results suggest that using a checklist enables an LLM to evaluate generated text in a manner more aligned with human judgment than traditional automatic metrics. \n\nWhen compared to G-Eval and CheckEval with the identical version of GPT, our model demonstrates superior performance. Moreover, even with GPT-3.5-Turbo,",
            "reference_string": "[272593053 | Chu et al. | 2024 | Citations: 4]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "265212661",
            "title": "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization",
            "text": "Since GPT-4 coupled with LLMCompare is the best evaluation method we identified in \u00a73.3, we use it for the automatic benchmarking. To avoid the prohibitive cost, we treat GPT-4 (gpt-4-0314) as a baseline and evaluate the other systems by comparing them against GPT-4 only, following recent practices in automatic LLM benchmarking (Dubois et al., 2023;Zheng et al., 2023). 14 We evaluated 11 LLMs over the 100 data examples we used in human evaluation ( \u00a72.1). The prompt template for summary generation is shown in Appendix A.1.",
            "score": 0.6383096118460188,
            "section_title": "Evaluation Settings",
            "char_start_offset": 22274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 528
                }
            ],
            "ref_mentions": [
                {
                    "start": 330,
                    "end": 351,
                    "matchedPaperCorpusId": "258865545"
                },
                {
                    "start": 351,
                    "end": 370,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09600830078125
        },
        {
            "corpus_id": "270391675",
            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
            "text": "LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;Ouyang et al., 2022) to ensure more balanced and comprehensive assessments.\n\nDomain-Specific Evaluation.LLMs have been prevalent across various domains, such as law (Cui et al., 2023a), medicine (Singhal et al., 2023), finance (Yang et al., 2023a), etc.However, most LLMs employed as evaluators are designed for general domains and are not specifically tailored to any particular field.This lack of specialization poses significant challenges.On one hand, these LLMs often lack the requisite domain-specific knowledge, making it difficult for them to accurately assess the correctness of content within specialized fields.On the other hand, the evaluation prompts need to be meticulously designed for different domains.This may involve tailoring the aspects of evaluation relevant to each field.",
            "score": 0.6183299975122041,
            "section_title": "Challenges and Open Problems",
            "char_start_offset": 42121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1185
                },
                {
                    "start": 1187,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1363
                },
                {
                    "start": 1363,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1732
                },
                {
                    "start": 1732,
                    "end": 1829
                },
                {
                    "start": 1829,
                    "end": 1905
                }
            ],
            "ref_mentions": [
                {
                    "start": 1051,
                    "end": 1072,
                    "matchedPaperCorpusId": "215548699"
                },
                {
                    "start": 1110,
                    "end": 1130,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1305,
                    "end": 1327,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1337,
                    "end": 1357,
                    "matchedPaperCorpusId": "254877751"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39013671875
        },
        {
            "corpus_id": "266362469",
            "title": "AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs",
            "text": "As shown in Table 1, both human and LLM (GPT-4) evaluations agreed that GPT-4+RAG is the strongest model among all. However, the LLM evaluation rated the baseline GPT-4 as the second-best model, which differs from the human evaluation results. This discrepancy may be attributed to the findings of [16], which suggest that GPT-4 has a tendency to favor models trained on data derived from InstructGPT, GPT-4, or ChatGPT, even if the content is less factual or useful. \n\nIt is also worth noting that GPT-4 assigned much higher scores to the baseline models than the human evaluation. While the LLM evaluation did capture some differences between models (e.g., it also consistently showed that SFT alone makes LLaMA2-13b perform worse), it did not align well enough with human evaluation. To quantify the agreement between the human and LLM evaluations, we calculated the Pearson (r) and Spearman (\u03c1) correlation scores. Across 20 queries for the 9 models (180 responses), we found the Pearson coefficient between human and LLM-eval to be 0.662 and Spearman Coefficient to be 0.659. These scores indicate a moderate positive correlation between the two types of evaluation, suggesting that while there is some agreement between the human and LLM evaluations, there are also many differences. For detailed analysis on the correlation between different types of evaluation methods, please refer to Appendix E.2.",
            "score": 0.6003353101485506,
            "section_title": "Evaluation using GPT-4",
            "char_start_offset": 16933,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1407
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09222412109375
        },
        {
            "corpus_id": "263831267",
            "title": "A Closer Look into Automatic Evaluation Using Large Language Models",
            "text": "An LLM used as an evaluation metric should be affordable and accessible to whoever wants to use it. Based on this principle, we use ChatGPT (gpt3.5-turbo-0613) (OpenAI, 2022) for evaluation since it has lower cost and improved performance compared with other GPT-3.5 models. Chat-GPT is also used in LLM evaluation and G-Eval. While Liu et al. (2023) further use GPT-4 (OpenAI, 2023) in their experiments, we cannot use GPT-4 in our experiments since most people, including us, have limited or no access to GPT-4, making it utterly unsuitable as an evaluation metric. \n\nIn our preliminary experiments, we also try to use the best open LLM (at the time of writing this manuscript) on Open LLM leaderboard, the falcon-40b-instruct model (Almazrouei et al., 2023), but we find it cannot follow the instructions and rate the samples very well. Hence, we exclude open LLMs in our paper. \n\n3 Better Usage of LLM for Evaluation tion on SummEval when using GPT-4 for evaluation. By scrutinizing their results, we find that the correlations when using auto CoT and not using it often differ by less than 0.02. This raises two questions: (1) Is this difference statistically significant? (2) Does auto CoT yield higher correlations for different LLMs and datasets? To answer these questions, we use ChatGPT to rate the samples in SummEval and Topical-Chat using two sets of prompts, one with the evaluation steps generated using auto CoT and one without those evaluation steps. In this experiment, we follow G-Eval and restrict ChatGPT to output only a numeric score. Following Graham and Baldwin (2014), we use William's test for significance to see if the Pearson's r of using and not using auto CoT is statistically significantly different. We try to follow the prompts used in G-Eval when possible; still, we have to construct some prompts since Liu et al. (2023) only release part of the prompts and some of which are problematic.",
            "score": 0.5902857948489633,
            "section_title": "Large Language Models",
            "char_start_offset": 6112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 1568,
                    "end": 1593,
                    "matchedPaperCorpusId": "15132118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08526611328125
        },
        {
            "corpus_id": "269293024",
            "title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models",
            "text": "Claude-3 (RQ2) \n\nIn the overall benchmark result, we adopt GPT-4's direct output to the question as a reference for score 8, and use GPT-4 as the evaluator base for final scoring, as detailed in Setion 4.1. Although this approach is widely adopted, it may cause favor to responses similar to GPT-4. To counteract this bias, we re-evaluated the performance of the top-2 LLMs, GPT-4o, and Claude-3-opus, employing cross-validation techniques. We run 5-fold evaluation settings on randomly selected 200 cases,  using GPT and Claude as the reference generator and base evaluator respectively, and the third performed LLM, Qwen, as both reference generation and evaluation for a third-party evaluation. \n\nAs illustrated in Figure 4, when GPT-generated responses are used as references, GPT-4 consistently achieves higher scores no matter the evaluator models. When Claude-generated responses are adopted in evaluations, the results are comparable. In thirdparty assessments, GPT-4 also maintained a superior ranking over Claude-3. This order is consistent with the overall benchmark results, indicating that the potential bias towards GPT does not affect the benchmark ranking.",
            "score": 0.589981094380347,
            "section_title": "Cross Validation between GPT-4 and",
            "char_start_offset": 20397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 17,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043212890625
        },
        {
            "corpus_id": "258866019",
            "title": "Is GPT-4 a Good Data Analyst?",
            "text": "In this paper, we aim to answer the following research question: Is GPT-4 a good data analyst? To answer this question, we conduct preliminary studies on GPT-4 to demonstrate its potential capabilities as a data analyst. We quantitatively evaluate the pros and cons of LLM as a data analyst mainly from the following metrics: performance, time, and cost. Specifically, we treat GPT-4 (gpt-4-0314) 1 as a data analyst to conduct several end-to-end data analysis problems. The flow of our proposed framework is shown in Figure 1. According to the given question, the model has to identify the relevant tables and schemes in the databases that contain the necessary data, and then extract the data from the databases and organize it in a way that is suitable for figure generation. Then, it is required to analyze the data to identify trends, patterns, and insights that can help answer the initial question. Since there is no existing dataset for such data analysis problems, we choose one of the most related datasets NvBench (Luo et al., 2021) , and add the data analysis part on top. We design several automatic and human evaluation metrics to comprehensively evaluate the quality of the data extracted, charts plotted and data analysis generated. \n\nExperimental results show that GPT-4 can beat an entry-level data analyst and an intern data analyst in terms of performance and have comparable performance to a senior-level data analyst. In terms of the cost and time of our experiments, GPT-4 is much cheaper and faster than hiring a data analyst. However, since it is a preliminary study on whether GPT-4 is a good data analyst, we conduct some additional experiments and provide fruitful discussions on whether the conclusions from our experiments are reliable in real-life business from several perspectives, such as whether the questions are practical, whether the human data analysts we choose are representative, etc. These results sug- 1 The most advanced model in the GPT series at the time of this paper was written. \n\ngest further studies are needed before concluding whether GPT-4 is a good data analyst. To summarize, our contributions include:",
            "score": 0.5863969248761081,
            "section_title": "Introduction",
            "char_start_offset": 1942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2028
                },
                {
                    "start": 2031,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2159
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.083740234375
        },
        {
            "corpus_id": "271600495",
            "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
            "text": "The quality of the content generated by LLMs can be evaluated from both objective and subjective perspectives. The former aims to evaluate the capabilities of LLMs in an objective and quantitative way, usually by comparing LLMs' outputs with corresponding ground truth. Similarity metrics are calculated based on the outputs and references, such as BLEU Papineni et al. [2002], and ROUGE Lin [2004]. An alternative approach involves tasks as a classification task and forces LLMs to output closed options for accuracy calculation, such as MMLU Hendrycks et al. [2020] and CEVAL Huang et al. [2024]. The latter is applied to compare the performances of different LLMs by humans or other powerful LLM judges when facing more complicated scenarios. GPTScore Fu et al. [2023a] makes full use of GPT3 in achieving customized, multi-aspect and training-free evaluation. Chatbot Arena Zheng et al. [2024] showcases high agreement between GPT-4 and humans on making judgements. It indicates that leveraging LLM like GPT-4 for evaluating the quality of generated text could be a viable alternative to manual human assessment.",
            "score": 0.5668335841929297,
            "section_title": "Evaluation of LLMs",
            "char_start_offset": 5718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1116
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 376,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 878,
                    "end": 897,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.349853515625
        },
        {
            "corpus_id": "269605197",
            "title": "TREC iKAT 2023: A Test Collection for Evaluating Conversational and Interactive Knowledge Assistants",
            "text": "While comparing the results obtained from human and GPT-4 labels in Tables 4 and 5, we observe the same ranking of the methods in terms of both relevance and completeness, indicating that we could rely on the GPT-4 labels to evaluate newly generated responses, even if GPT-4 itself is among the runs.However, we clearly see that the absolute pointwise differences between the performance of the models are different, where the GPT-4 run seems to have been favored by the GPT-4 as evaluator, in line with the findings of Liu et al. [14], showing that LLMs tend to favor the text generated by themselves when they act as evaluators.Nevertheless, we find that for naturalness and completeness, they can still be used to rank the models.",
            "score": 0.5536283848937863,
            "section_title": "Potential biases.",
            "char_start_offset": 37741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 630
                },
                {
                    "start": 630,
                    "end": 733
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.216796875
        },
        {
            "corpus_id": "262045067",
            "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles",
            "text": "For this research question, we aim to provide the best practices when using GPT-4 as the evaluator for the MDDS task by conducting bias and correlation analyses. We focus on two major biases: position bias (i.e., whether the LLM evaluator favors certain positions over others) and verbosity bias (i.e. whether the LLM evaluator prefers shorter or longer texts). For all the experiments conducted in this analysis, we investigated summaries produced by GPT-4, GPT-3.5-Turbo, Vicuna-7B, and LongChat-7B-16K. The details of our prompts for the below experiments can be found in Appendix C.3. \n\nPosition Bias Position bias is most relevant to the pairwise comparison protocol. While previous work has shown that GPT-4 does exhibit position bias when used to assess text quality in conversational-focused tasks (Wang et al., 2023;Zheng et al., 2023), none of the prior studies have investigated whether such bias is also observed when evaluating faithfulness or coverage. To analyze position bias, we task GPT-4 with assessing a pair of summaries generated by two LLMs on which one is better, and then swap the positions of these two summaries and query GPT-4 again. We compute the percentage of times GPT-4 prefers the first or second summaries. \n\nWhen GPT-4 compared pairs of LLM-generated summaries to evaluate faithfulness and coverage, a strong position bias surfaced, favoring the second entry (Table 3). Position bias was particularly pronounced when assessing similar-quality summaries (see Figure 23a). Hence, we deduce that GPT-4 is unreliable when utilized as a pairwise evaluator in the MDDS task with respect to faithfulness and coverage. Interestingly, this outcome contradicts Zheng et al. (2023), implying that the position of bias for LLM-based evaluators could vary across different tasks. A breakdown of the position bias analysis can be found in Appendix D. \n\nVerbosity Bias To assess the verbosity bias of GPT-4 as an evaluator, we create extended summaries that maintain the semantic meaning.",
            "score": 0.5508311028232675,
            "section_title": "Analysis",
            "char_start_offset": 16082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1872
                },
                {
                    "start": 1875,
                    "end": 2009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10577392578125
        },
        {
            "corpus_id": "263671602",
            "title": "Evaluating Hallucinations in Chinese Large Language Models",
            "text": "Judge once Judge 5 times Consistency rate 93.33% 93.50% \n\nTable 4: The average consistency rate between human evaluations and GPT-4 evaluations across six models. \"Juage 5 times\" refers to instructing GPT-4 to generate judgments five times, and adopting the answer that appears most frequently as the final decision. \n\nDetermining whether the answer to a question contains hallucinations poses a significant challenge for human evaluators. Relying on human evaluation as a fair and scalable automated assessment method is not feasible, which in turn limits the usability of datasets. In recent, many work adopt AI feedback from some powerful instructionfollowing large language model like GPT-3.5 and GPT-4 for training and evaluation (Cheng et al., 2023;Zheng et al., 2023a;Li et al., 2023b;Fu et al., 2023). Besides, Wang et al. (2023b) found that using LLM-based evaluator for open-domain QA evaluation is better than other methods. The evaluation of TruthfulQA also employed models as scorers, which were achieved by fine-tuning two 6.7B GPT-3 models on data collected by the authors. We believe that we can use LLM-based evaluators to replace such fine-tuning methods. In our benchmark, we use GPT-4 (gpt-4-0613) as the evaluator. \n\nDuring evaluation, we put our criteria into the instruction for GPT-4. And we give GPT-4 correct answer examples for reference. The specific format of the evaluation prompt is in Appendix D.2. Due to the inability of GPT-4 to access top logits and to produce deterministic outputs, we employ GPT-4 to generate five judgments for voting and use the result with the highest number of votes as the final judgment and we set the temperature to 0 and top p to 0.5. \n\nWe conducted experiments to assess the consistency between GPT-4's evaluation results and human evaluation results, and evaluated the impact of GPT-4's randomness on the consistency rate. In particular,we sampled two questions from each domain of the three parts, totaling 100 questions.",
            "score": 0.536532963580099,
            "section_title": "EVALUATION METHOD",
            "char_start_offset": 17369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 58,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 735,
                    "end": 755,
                    "matchedPaperCorpusId": "258460987"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0240478515625
        },
        {
            "corpus_id": "273026179",
            "title": "ACE: A LLM-based Negotiation Coaching System",
            "text": "Other-feedback (N=129) \n\nNo-feedback Baseline (N=126) \n\nBetween-person Comparison As for the other components of our system, such as direct feedback and utterance revision, we do not have data and therefore could not perform automatic evaluations. Instead, we relied on small scale expert evaluations of outputs to choose the best models. For each model we tested (GPT-4, GPT-4o, and GPT-3.5) we generated 10-15 examples of output and presented them to two experts who then selected which output they preferred. Our evaluations showed that GPT-4 and GPT-4o performed the best and we ultimately chose GPT-4o due to its speed. \n\nFor our chat agent, we tested both GPT-4 and GPT-4o by having two experts perform 10 negotiations with the model and rate its performance. We found that GPT-4 was better based on this evaluation. Prior works have extensively tested the negotiation performance of various LLM chat agents and also found that GPT-4 achieves quality performance (Bianchi et al., 2024).",
            "score": 0.5322923974623409,
            "section_title": "ACE Condition (N=119)",
            "char_start_offset": 36466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 25,
                    "end": 53
                },
                {
                    "start": 56,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 992
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0225372314453125
        },
        {
            "corpus_id": "273877669",
            "title": "Bayesian Calibration of Win Rate Estimation with LLM Evaluators",
            "text": "For HANNA, OpenMEVA-MANS, and SummEval, we prompt a set of LLM evaluators to compare the outputs of generator models in the datasets. Specifically, we employ GPT-3.5-turbo-0125 (Ope-nAI, 2023) and Gemini-1.0-Pro (Team, 2024) as the evaluator models for our experiments. GPT-3.5 has been proved to have positive correlation with human annotations (Chiang and Lee, 2023a; Wang et al., 2023a), while Gemini-1.0-Pro's performance on LLM evaluation have not yet been widely studied in previous works. For each output pair, we prompted each LLM evaluator to rate the two outputs that are based on the same input and generated by two different generator models. For each LLM evaluator, we used three prompting strategies including Score-only, Rate-explain, and Analyze-rate following Chiang and Lee (2023b). For LLMBar, MT-Bench, LLMEval 2 , the LLM evaluation work has already been carried out by Zeng et al. (2024). For these three datasets, we selected the best LLM evaluators (GPT-4, PaLM 2, etc.) from the many ones used. More details regarding the specific LLM evaluator modes used for these datasets can be found in Appendix B.",
            "score": 0.5303839509333348,
            "section_title": "Evaluator settings",
            "char_start_offset": 16488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1127
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 389,
                    "matchedPaperCorpusId": "257378627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1541748046875
        },
        {
            "corpus_id": "266690837",
            "title": "EHR Interaction Between Patients and AI: NoteAid EHR Interaction",
            "text": "We utilized the LLMs' ability in medical questions reasoning to evaluate the performance of collected conversation. We randomly selected 100 instances from the 876 generated based on MADE and combined them with 1000 test and 1000 validation data generated from MIMIC-III for LLM evaluation (totally 2100 cases). The LLMs used for evaluation were GPT-3.5-Turbo (referred to as Turbo in this section) and GPT-4. We aggregated the criteria into an evaluation prompt Table 4 to guide the LLMs in scoring each conversation. The results are shown in the Table 2. From the figure, it is evident that Turbo received higher scores than GPT-4, particularly in the Turbo NIP Explanation task. This discrepancy can be attributed to the strict evaluation criteria outlined in our evaluation prompt. Turbo, while slightly less capable than GPT-4, did not fully meet these criteria. Additionally, we observed that data generated by GPT-4 NIP, whether in the Q&A task or the Explanation task, exhibited more stable scores across both Turbo and GPT-4 evaluations. Notably, GPT-4 received a noticeably lower score in the Turbo NIP Explanation task when compared to Turbo. Human Evaluation Because our LLM evaluation has shown that the data generated by GPT-4 NIP is of higher quality and more stable, our human evaluation focused on GPT-NIP data to confirm the quality of the best synthetic data. We invited two medical students with clinical experience and three undergraduate students to conduct evaluations, with an equal split of 42 instances for both Q&A and explanation tasks (totally 84 cases). Among the 66 instances evaluated on GPT-4 NIP data, 52 instances had EHR notes from discharge instructions, while 14 were from MADE . The human evaluation results are presented in Table 2.",
            "score": 0.529759848944197,
            "section_title": "LLM Evaluation",
            "char_start_offset": 12261,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1772
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "273404050",
            "title": "SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation",
            "text": "Specifically, the best reasoning scores for SBI-RAG, GPT-4, and GPT-3.5 Turbo were 0.588, 0.491, and 0.290, respectively. Paired sample t-tests showed that the differences between the SBI-RAG and the GPT models were significantly different at the 0.05 level (see Appendix E). These results suggest that schema-based reasoning can enhance the overall quality of reasoning, particularly in educational contexts, when compared to responses generated by LLMs alone. \n\nLLM-as-a-Judge Results: We implemented the LLM-as-a-Judge approach [36] to evaluate the quality of reasoning in the responses generated by both our Schema-Based RAG system and the baseline LLMs. This method allows for an objective, scalable evaluation by approximating human judgment through the use of LLMs. Our LLM-as-a-Judge process involves scoring responses based on clarity, logical progression, and completeness. Results showed that the Schema-Based RAG approach consistently outperformed GPT-4 and GPT-3.5 Turbo in terms of reasoning quality.For more details refer to Appendix G.",
            "score": 0.527033724200991,
            "section_title": "Evaluation",
            "char_start_offset": 11848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050994873046875
        },
        {
            "corpus_id": "272969091",
            "title": "Rehearsing Answers to Probable Questions with Perspective-Taking",
            "text": "To answer (RQ3), we assessed the quality of the generated answers manually, since there are no automatic metrics that can evaluate information richness and concreteness aspects. Three annotators, who graduated from financial or economics departments, were paid 36% higher than the minimum wage stipulated by law. The results are shown in Table 4. First, the scores in the answers generated by the LLM were generally higher than the manager's answers, indicating a satisfactory performance in conveying information. Second, with KG-AR, two out of three LLMs achieved higher INFO scores, indicating that using KGs can enhance information richness. Third, using KGs did not improve the models' ability to select better numerical evidence to provide concrete answers. These findings suggest that future work can also aim to enhance the concreteness of rehearsed answers. Drawing inspiration from Chiang and yi Lee (2023), we also used GPT-4 as an automatic evaluator to check whether future studies can rely on GPT-4 to evaluate the generated answers. 4 We established the evaluation framework based on previous studies (Zhong et al., 2022;Chan et al., 2023) and employed Pearson, Spearman, and Kendall correlation coefficients. Table 5 shows the statistics of correlation coefficients between GPT-4 and human evaluators. Overall, GPT-4's annotations are correlated with those of human evaluators. However, although GPT-4 has a moderate level (> 0.3) of correlation when evaluating from a concreteness aspect, there is still room for improvement in automatic evaluation. Our suggestion based on these findings is that future work can evaluate the results with GPT-4, but manual checking is still needed, given the current correlation coefficient level.",
            "score": 0.5262578399887653,
            "section_title": "Human Evaluation",
            "char_start_offset": 11487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1748
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1990966796875
        },
        {
            "corpus_id": "266999933",
            "title": "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step",
            "text": "End-to-end evaluation requires LLMs to generate the whole solution path S pred as well as the fi-Figure 2: Overview of the dataset construction process. By randomly sampling tools from the tool database, we prompt GPT-3.5 to generate initial queries and further refine them with GPT-4. After that, we develop a multi-agent framework to resolve queries with the provided tools, collecting both solution paths and tool responses. Finally, human experts are employed to verify the annotations and pick high-quality samples. \n\nnal answer A pred given one query data piece. We adopt the win rate proposed in ToolBench (Qin et al., 2023b) to gauge the overall performance, which evaluates LLM abilities by comparing their response quality against that of GPT-3.5. The result shows that our single-index evaluation is consistent with the overall performance.",
            "score": 0.5261652477314825,
            "section_title": "End-to-End Evaluation",
            "char_start_offset": 9745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 851
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.063720703125
        },
        {
            "corpus_id": "272986925",
            "title": "Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models",
            "text": "Metrics. To evaluate the ability of LLMs to solve these problems, we consider two metrics: pass rate and accuracy. Pass rate measures the ratio of executable code generated by an LLM, while accuracy measures the ratio of correct answers from the executable code. Accuracy is always no higher than the pass rate and is considered the more important metric as it evaluates final performance. \n\nProcess. Evaluating diverse answer formats with rule-based matching is challenging, and human evaluation is too labor-intensive. Thus, we automate evaluation using GPT-4o. First, we extract code snippets from LLM-generated answers using regular expressions. GPT-4o is then asked to check the correctness given the execution result. For problems with certain answers, such as true/false or calculation questions, GPT-4o assigns 1 point if the execution result matches the reference code's result, and 0 otherwise. For other problems, like drawing questions, GPT-4o matches key API usage: if the generated code contains m out of n key APIs, the accuracy point is m/n. Rationale. To validate GPT-based evaluation, we measure its stability (self-consistency) and alignment with human judgments (human-consistency). Higher stability means judgment scores are consistent across multiple evaluations, while higher human alignment indicates better quality. We use the agreement metric [65] to assess these consistencies. For n evaluations of the same answer, we take the highest number of evaluations m that received the same score and divide it by n to get the consistency. Self-consistency is the agreement among three GPT-4o evaluations, and human-consistency is the agreement between one GPT-4o evaluation and a manual evaluation. We evaluate all 512 problems with answers from Claude 3 Opus RAG 7, the best-performing closed-source model which will be introduced in Section 4.2, and present the results in Table 3, showing high self-consistency and human-consistency.",
            "score": 0.5259175122306625,
            "section_title": "Automated Evaluation",
            "char_start_offset": 10755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 1369,
                    "end": 1373,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1153564453125
        },
        {
            "corpus_id": "262013357",
            "title": "Investigating Answerability of LLMs for Long-Form Question Answering",
            "text": "With the emergence of LLMs like ChatGPT and open-source successful LLMs, it is extremely important to understand the capabilities and limitations of different LLMs. In order to test deeper reasoning abilities of LLMs by referring to longer contexts, we evaluate answers generated by LLMs on questions generated by ChatGPT on summaries of long documents. Results show that our proposed method of question generation poses a challenging setup for LLMs and shed light on performance gaps between massive LLMs and open-source LLMs. We hope our analysis motivates future research directions such as leveraging longer contexts in a constrained sequence length setting and developing better long-form text generation for smaller LLMs. \n\nIn this study, we propose an automatic evaluation setting to generate questions from summaries, and the generated answers from LLMs are evaluated using GPT-4 for different metrics. Experimental results show that our proposed evaluation setting proves to be a challenging setup for LLMs. However, our study might have some limitations. GPT-4 as evaluator While GPT-4 has shown a high correlation with human evaluation for long form text generation (Liu et al., 2023), the capabilities of using GPT-4 for evaluation is an active area of research in itself. Hence, our results might be limited by the undiscovered capabilities of GPT-4. ChatGPT for question generation Generating answers on questions prompted from ChatGPT might lead to optimistic results of ChatGPT. However, there exists limitations with other baselines to generate meaningful questions. We show extensive analysis of using other LLMs for question generation (Appendix A.2). \n\nUnknown training data Little is known about the training data distribution of massive LLMs like ChatGPT. Models trained with different methods and data distribution make the evaluation for fair comparison harder.",
            "score": 0.5235111490265358,
            "section_title": "Conclusion",
            "char_start_offset": 12997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1670
                },
                {
                    "start": 1673,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1885
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1171875
        },
        {
            "corpus_id": "258866019",
            "title": "Is GPT-4 a Good Data Analyst?",
            "text": "The potential for large language models (LLMs) like GPT-4 to replace human data analysts has sparked a controversial discussion. However, there is no definitive conclusion on this topic yet. This study aims to answer the research question of whether GPT-4 can perform as a good data analyst by conducting several preliminary experiments. \n\nWe design a framework to prompt GPT-4 to perform end-to-end data analysis with databases from various domains and compared its performance with several professional human data analysts using carefully-designed task-specific evaluation metrics. Our results and analysis show that GPT-4 can outperform an intern data analyst or a junior data analyst, and can achieve comparable performance to a senior data analyst, but further studies are needed before concluding that GPT-4 can replace data analysts.",
            "score": 0.5234093040362707,
            "section_title": "Conclusions",
            "char_start_offset": 26440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 840
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029876708984375
        },
        {
            "corpus_id": "269761675",
            "title": "A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine",
            "text": "Employing advanced AI models, such as ChatGPT and GPT-4, which align with human values, has become the predominant method for evaluating medical LLMs and MLLMs [168]. Wang et al. [168] conducted experiments on five natural language generation datasets, showing that ChatGPT, as an evaluation tool, outperformed traditional metrics in most cases and matched human evaluation. In the medical domain, Li et al. [18] asked GPT-4 to evaluate responses from itself and LLava-Med on criteria such as helpfulness, relevance, accuracy, and level of detail. Liu et al. [23] instructed GPT-4 to evaluate whether LLM responses were acceptable and whether their tone resembled that of human counselors. \n\nDespite its scalability and reduced reliance on human input, AI evaluation has notable limitations. Studies [169,61] have revealed that GPT-4, as an evaluation tool, tends to favor the first response when multiple answers are presented sequentially. Furthermore, GPT-4 tends to prefer longer responses and those it has generated itself [23]. To mitigate the limitations of the aforementioned methods, integrating multiple evaluation approaches may provide more reliable results. Additionally, training specialized LLMs or MLLMs through reinforcement learning or other methods to align with human judgment criteria could overcome AI evaluation's shortcomings.",
            "score": 0.5226062767974375,
            "section_title": "AI Evaluation",
            "char_start_offset": 52915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1350
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 165,
                    "matchedPaperCorpusId": "257378627"
                },
                {
                    "start": 179,
                    "end": 184,
                    "matchedPaperCorpusId": "257378627"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "258999820"
                },
                {
                    "start": 800,
                    "end": 805,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 805,
                    "end": 808,
                    "matchedPaperCorpusId": "257912848"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06939697265625
        },
        {
            "corpus_id": "268248855",
            "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "text": "We compared the performance of different baseline models on the paper test set (see Table 1). In traditional summary evaluation metrics, such as ROUGE, GPT-4.0 achieved the best results under zero-shot settings. Although ROUGE scores of ChatCite may be slightly lower than GPT-4.0 with zero-shot, its performance in quality metrics generated by LLMs and the preference of LLMs is far superior to results obtained directly from other LLM baselines. \n\nSurprisingly, GPT-4.0 performed poorly in fewshot settings.It is found that influenced by examples in the few-shot, resulting in irrelevant and erroneous summaries after case study. Notably, LitLLM with GPT-4.0 produced outcomes similar to GPT-4.0 in zero-shot but significantly lower than ChatCite. \n\nTherefore, we conclude that \"ChatCite performs best among LLM-based literature summarization methods, and the approach following the human workflow guidance is superior to the results obtained by the Chain of Thought (CoT) method.\" The results are automatically evaluated using ROUGE-1/2/L (F1) and the GPT-4.0 evaluator. G-Score represents the total score assessed by the GPT-4.0 evaluator, while G-Prf. indicates the model preferences among the five models.",
            "score": 0.5204974402761131,
            "section_title": "Main Results",
            "char_start_offset": 20049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 447
                },
                {
                    "start": 450,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1211
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07501220703125
        },
        {
            "corpus_id": "271903501",
            "title": "See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses",
            "text": "To validate the effectiveness, we first compare the gold labels and model-evaluate labels of GPT-4 responses on the full 1,835 instances, where GPT-4 evaluation achieves 85.35 precision, 95.89 recall and 90.31 F-1. As GPT-4 evaluation can be biased to GPT-4 output (Liu et al., 2023), we further construct a small set that consists of 115 instances, where each gold response is completely annotated by human annotators without the assistance of GPT-4. The GPT-4 evaluation achieves 87.14 precision, 96.83 recall and 91.73 F-1 on this set, which we consider is good enough. Such a higher-recall and lower-precision evaluation indicates that GPT-4 is highly reliable in identifying correct answers but with a few errors in classifying incorrect responses as \"correct\". It also suggests that the model performance evaluated by GPT-4 can be slightly overestimated or higher than its real performance. \n\nWe also experiment with the evaluation method following Wang et al. (2023), i.e., asking LLMs to explain why the model-generated response is correct or not, and then give a label for the response. However, we do not observe significant improvement.",
            "score": 0.5167663848601041,
            "section_title": "I Evaluation Validation",
            "char_start_offset": 48405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1533203125
        },
        {
            "corpus_id": "269757476",
            "title": "Synthetic Test Collections for Retrieval Evaluation",
            "text": "One potential issue with using synthetic queries and judgements in test collection construction is the possible bias these collections may exhibit towards systems that are based on a similar approach (similar language model) to the one that was used in the synthetic test collection construction process (e.g., synthetic test collections constructed using T5 might favour systems that are based on T5).In order to analyse the possible bias, we categorised the runs submitted to TREC DL 2023 based on the approach they use 3 (i.e., language models used in their ranking or retrieval pipeline), resulting in four different system categories: systems based on GPT (\u00d7), T5 (|), GPT + T5 (+) (i.e., a combination of GPT and T5), and others ( ) (i.e., traditional methods such as BM25, or any model that does not use either GPT or T5). Figure 3a shows that the synthetic test collection we have constructed that contains synthetic queries generated by LLMs (T5 and GPT-4) exhibits little to no bias towards LLM-based systems.\n\nTo further analyse possible bias that might arise from a system using a similar language model as the one used in test collection construction, Figure 3b shows how system performance computed on synthetic test collections generated using queries generated by GPT-4 compare with system performance on real test collections.It can be seen that synthetic test collections based on GPT-4 do not systematically overestimate the performance of systems based on GPT.Similarly, as can be seen in Figure 3c, synthetic test collections with queries generated using T5 exhibit almost no bias towards systems based on T5.Similar results were obtained on other evaluation metrics such as average precision and NDCG@100, results for 3 To this end, we carefully analysed the metadata file of submissions.GPT models are GPT-4 or GPT-3.5 and T5 models include MonoT5, FlanT5, and RankT5.which are omitted due to space limitations.",
            "score": 0.515942737337516,
            "section_title": "ANALYSIS OF BIAS IN SYSTEM EVALUATION",
            "char_start_offset": 12763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 1019
                },
                {
                    "start": 1021,
                    "end": 1343
                },
                {
                    "start": 1343,
                    "end": 1480
                },
                {
                    "start": 1480,
                    "end": 1630
                },
                {
                    "start": 1630,
                    "end": 1810
                },
                {
                    "start": 1810,
                    "end": 1891
                },
                {
                    "start": 1891,
                    "end": 1934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0360107421875
        },
        {
            "corpus_id": "258841126",
            "title": "On Learning to Summarize with Large Language Models as References",
            "text": "BRIO.Llama-2 cannot outperform GPT-3.5 under human evaluation, even though they are favored by the evaluation methods based on GPT-4 (Figure 4). Therefore, we further investigate this discrepancy between human and LLM-based evaluation. \n\nHuman-LLM Alignment We use the expert evaluation results to evaluate the performance of LLM-based evaluation as well as the crowdannotation, by computing their agreements with the majority vote of expert evaluation on evaluation group 2 and 3 in Table 6: Performance comparison of LLM-based evaluation and crowd-annotation in terms of their agreements with expert evaluation. G-EVAL-3.5 and G-Eval-4 are G-Eval scores based on GPT-3.5 and GPT-4 respectively. GPTScore is based on GPT3D3. GPT-3.5Rank and GPT-4Rank are two versions of the GPTRank. \n\nmance, and GPT-4 outperforms GPT-3.5. \n\n(2) GPT-4 with GPTRank can already outperform the performance of individual crowd-workers, while majority voting from crowd-workers still achieves the highest agreement.",
            "score": 0.5134851782369607,
            "section_title": "Meta-Analysis of LLM-based Evaluation",
            "char_start_offset": 18515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 996
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03216552734375
        },
        {
            "corpus_id": "270218703",
            "title": "Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization",
            "text": "Refinement can be powered by various LLMs.In this paper, we choose the newest versions of GPT-3.5 (gpt-3.5-turbo-0125)and GPT-4 (gpt-4-0125-preview) models from OpenAI4 to draft, critique, and refine the outcomes due to their strong instruction-following capabilities.We also explore the performance of a strong open-source LLM (Mixtral 8\u00d77B (Jiang et al., 2024)).\n\nWe use the LLMCompare as our evaluation protocol, which compares two candidate outputs and then selects the better one (Zheng et al., 2023;Wang et al., 2023).This is because LLMCompare coupled with GPT-4 is the best evaluation protocol, as mentioned in Liu et al. (2023).The evaluation prompts are shown in Appendix A.\n\nWe evaluate the generated summaries from the three quality dimensions as introduced in Liu et al. ( 2023): (1) Overall Quality measures the overall excellence of the summary following the summary requirements.(2) Missing Information assesses whether the summary omits any essential article details pertinent to the summary requirements.\n\n(3) Irrelevant Information examines whether the summary contains extraneous information that falls outside the scope of the summary requirements.",
            "score": 0.5134456896911086,
            "section_title": "Models and Metrics",
            "char_start_offset": 5615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 42,
                    "end": 118
                },
                {
                    "start": 118,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 364
                },
                {
                    "start": 366,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 637
                },
                {
                    "start": 637,
                    "end": 684
                },
                {
                    "start": 686,
                    "end": 895
                },
                {
                    "start": 895,
                    "end": 1022
                },
                {
                    "start": 1024,
                    "end": 1169
                }
            ],
            "ref_mentions": [
                {
                    "start": 485,
                    "end": 505,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09722900390625
        },
        {
            "corpus_id": "271769421",
            "title": "Guiding Large Language Models via External Attention Prompting for Scientific Extreme Summarization",
            "text": "Setup. In addition to using traditional automatic metrics, we explore the use of LLMs as evaluators to assess the quality of generated TLDRs. This approach aims to address the limitations of automatic metrics, which may not fully capture the nuances of human preferences and the overall quality of the summaries (Zheng et al., 2023). By leveraging LLMs, we seek to provide a more comprehensive and human-aligned evaluation of the generated TL-DRs. \n\nTo this end, we employ GPT-4 (version gpt-4-0613), which is currently recognized as one of the most advanced LLMs, to assess the quality of TL-DRs generated by our method compared to a baseline method. \n\nWe randomly sample 100 data points and collect TLDRs generated by GPT-3.5 using both the baseline SP method and our EAP method under a fewshot setting. The evaluation criteria focus on the TLDR's ability to concisely capture the key aspects of a scientific paper while maintaining faithfulness to the source. To mitigate position bias (Zheng et al., 2023), each pair is evaluated twice with the positions of the two TLDRs swapped. A method is considered the winner for a sample only if both evaluations favored the same TLDR. In cases of conflicting results or ties, the sample is marked as a tie. The above process is repeated with three different random seeds, resulting in three rounds of evaluation. \n\nResults. The results of GPT-4 evaluation are summarized in Table 3, showing that the proposed EAP method consistently outperforms the baseline SP method, achieving an average win rate of 37.33 % compared to the baseline's 26.00 %. The average tie rate is 36.67 %, with an average conflict rate of 23.67 %. These results indicate that the EAP method significantly enhances the performance of LLMs in generating TLDRs, as judged by GPT-4.",
            "score": 0.5133051633580972,
            "section_title": "LLM as Evaluator",
            "char_start_offset": 19188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 447
                },
                {
                    "start": 450,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1796
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.383544921875
        },
        {
            "corpus_id": "268358029",
            "title": "FewFedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning",
            "text": "GPT-4 compares responses from an instruction-tuned LLM with reference responses from another LLM API (e.g., text-davinci-003 or GPT-3.5-turbo) or human for each corresponding instruction in the test sets.To improve the quality of the evaluations and mitigate the positional effects of GPT-4's assessments, we implement a dual-sided scoring system, as described in previous studies (Zhang et al., 2023b;Zheng et al., 2024).This system evaluates each output comparison twice, alternating the order of the instructiontuned model output and the reference output.Appendix B provides the evaluation prompt used by GPT-4.\n\nModels and Training Details.Our experiment utilizes the popular LlaMa-2-7B model (Touvron et al., 2023)   In Table 2, we observe that FedIT algorithms lag behind federated few-shot algorithms, while centralized algorithms consistently outperform all federated counterparts.This observation underscores two key points: (1) Developing robust federated few-shot algorithms is essential, particularly when local clients possess limited training data.",
            "score": 0.5123892779161607,
            "section_title": "Experimental Setup",
            "char_start_offset": 17216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 614
                },
                {
                    "start": 616,
                    "end": 644
                },
                {
                    "start": 644,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 1062
                }
            ],
            "ref_mentions": [
                {
                    "start": 381,
                    "end": 402,
                    "matchedPaperCorpusId": "264426685"
                },
                {
                    "start": 402,
                    "end": 421,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0333251953125
        },
        {
            "corpus_id": "258841126",
            "title": "On Learning to Summarize with Large Language Models as References",
            "text": "We now conduct experiments using GPTRank for model training and evaluation. The reference LLMs we choose are GPT-3.5 and GPT-4 (OpenAI, 2023) since they have shown state-of-the-art performance on summarization evaluation (Liu et al., 2023a). 8 To enable a more accurate evaluation, we choose GPT-3.5 as the baseline model and use the LLMs to conduct a pairwise comparison between different systems and GPT-3.5. To reduce the positional bias in LLM evaluation results as noted by Wang et al. (2023b), we evaluate each summary pair in both directions and take the average of results. In addition, we allow the LLMs to predict a tie between two summaries. 9  In Figure 1, we report the pairwise comparison results of different models against GPT-3.5 under both GPT-3.5 and GPT-4's evaluation. We note: \n\n(1) As in \u00a73.1.2, using better references and contrastive learning helps the model to achieve better LLM-based evaluation results. \n\n(2) Interestingly, GPT-3.5 prefers both BRIO.GPT-3.5 and BRIO.GPT-4 over its own outputs in the pairwise comparison, suggesting that contrastive learning can efficiently optimize the summarization model for a specific evaluation metric. \n\n(3) LLM-based evaluation results vary across different LLMs. For example, while GPT-3.5 prefers BRIO.GPT-4 over itself, GPT-4 prefers GPT-3.5. (4) BRIO.GPT-3.5 can outperform BART.GPT-4 despite the fact that BRIO.GPT-3.5 is trained with a reference LLM that is supposedly weaker, indicating the advantage of contrastive learning. The reference-based evaluation results can be found in Table 2.",
            "score": 0.5119907496208178,
            "section_title": "Learning with GPTRank",
            "char_start_offset": 10692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1566
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 240,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07861328125
        },
        {
            "corpus_id": "269626718",
            "title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge",
            "text": "Language Models such as ChatGPT have started to reach nearhuman performance in many tasks [9], which has sparked interest in using them for evaluating the output of other LLMs.\n\nThe idea of using a large language model to judge the output of other LLMs -LLMs-as-judges -was first studied in the work of Zheng et al [44].The authors proposed three variations of the LLMs-as-judges paradigm: (1) pairwise comparison (i.e., selecting which of two LLMs output is the best), (2) single answer grading (i.e., scoring an LLM single answer), and (3) reference-based grading (i.e., single answer grading with respect to a reference solution).Across the three scenarios, results showed that models such as GPT-4 can reach over 80% agreement with humans, matching the level of inter-agreement between humans.\n\nOur work is thus particularly close to efforts investigating whether ChatGPT models can effectively act as a judge in more specific domains such as law [37].In this work, we use GPT-4 as a single answer grading judge, where we ask the LLM to directly assign a score to the feedback generated by another LLM (GPT-3.5).\n\nWhile recent work [32,45] has already used GPT-4 as part of their evaluation strategies, also in educational settings [8,21], the question of how effective LLMs (or in particular GPT-4) can be in assessing the quality of the feedback generated by other LLMs has not been studied explicitly.",
            "score": 0.5118496365221996,
            "section_title": "Using Language Models as Judges",
            "char_start_offset": 5464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 178,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 633
                },
                {
                    "start": 633,
                    "end": 797
                },
                {
                    "start": 799,
                    "end": 956
                },
                {
                    "start": 956,
                    "end": 1116
                },
                {
                    "start": 1118,
                    "end": 1408
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "256868854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33740234375
        },
        {
            "corpus_id": "269626718",
            "title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge",
            "text": "This may be due to either the performance of the LLM used as a judge, or the human-LLM agreement on what is to be considered a mistake.The ability of a judge language model to grade the output of another LLM strongly depends on its ability to solve the specific problem at hand [44].That is, for GPT-4 to decide whether GPT-3.5 identified all issues in a program (completeness) would require GPT-4 itself to identify them in the first place.Similarly, asking GPT-4 to identify whether GPT-3.5 is hallucinating elements of its feedback (selectivity criterion) is related to GPT-4 not hallucinating the same issue.Some evidence for the shortcomings of GPT-4 is provided in the work of Phung et al. [29], which shows that although the model is quite good at providing programming feedback, the generated feedback are not void of mistakes.This may be even more true in our study where we use Dart, a programming language in which GPT-4 is most likely less proficient as in Python.The observed performance outcomes may also be affected by a discrepancy in the way a language model perceives errors compared to a human.It is also possible there are cases where the judge is correct and the original label is incorrect.\n\nResearch and practical implications.We feel it is important to highlight that our results do not provide a \"per-feedback\" guarantee of the quality of the judgement, but rather, a statistical overview of the current ability of GPT-4 to appropriately judge such feedback.These results suggest that automated evaluation of feedback is not yet ready to rely exclusively on LLM evaluation -human judgement is still necessary.In particular, we believe that LLM judge evaluation should not be used as a way to assess the quality of a single generation (before being given to a student).However, an LLM judge could potentially be an assistant to a human evaluator, and an LLM evaluation could also be used as a comparative method.For instance, it could be used in evaluating the quality of prompting techniques and generation parameters, or exploring alternative language models (e.g; open source onces) across a large number of generations.",
            "score": 0.5083997076054978,
            "section_title": "Automatic Feedback Evaluation",
            "char_start_offset": 15836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 283
                },
                {
                    "start": 283,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1113
                },
                {
                    "start": 1113,
                    "end": 1212
                },
                {
                    "start": 1214,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1483
                },
                {
                    "start": 1483,
                    "end": 1634
                },
                {
                    "start": 1634,
                    "end": 1793
                },
                {
                    "start": 1793,
                    "end": 1936
                },
                {
                    "start": 1936,
                    "end": 2147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0885009765625
        },
        {
            "corpus_id": "265212888",
            "title": "Safer-Instruct: Aligning Language Models with Automated Preference Data",
            "text": "In this section, we discuss how we evaluate all the models on harmlessness, helpfulness, and some LLM benchmarks. \n\nHarmlessness. As stated above, existing research shows that GPT-4 reaches human-level performance in terms of evaluating LLMs' generations (Li et al., 2023;Zheng et al., 2023), which inspires us to employ GPT-4 to evaluate the harmlessness of the model. The evaluation setup and the prompt template we use is based on Ji et al. (2023), which is proven to have a human agreement of more than 95%. The full prompt template can be found in Appendix A.4. In addition to GPT-4, we also employ Claude 3 as a different judge using the same prompt template. We test all the models with a mix of held-out samples from the SI dataset, the Anthropic's helpfulness and harmlessness (HH) dataset (Bai et al., 2022a), and the Beaver Tail (Ji et al., 2023) evaluation dataset. Note that even though the prompts of the Beaver Tail training dataset are sourced from the Red Teaming dataset (Ganguli et al., 2022), its evaluation dataset is a mix of GPT-3.5 generated and human-written prompts9 . Our final test set comprises a total of 900 samples with an even distribution among the three datasets. \n\nHelpfulness. In addition to harmlessness, we also want to make sure that the models are still helpful after safety preference training. We use MT-bench (Zheng et al., 2023), a set of challenging multi-turn open-ended questions to evaluate models. To automate the evaluation process, we prompt GPT-4 to act as judges and assess the quality of the models' responses. GPT-4 will output a score out of 10. Higher scores mean better generation quality. We use FastChat's implementation of the benchmark10 . \n\nBenchmark Performance. In addition to harmlessness and helpfulness, we also test models on popular benchmarks to evaluate different model capabilities.",
            "score": 0.508119926375874,
            "section_title": "Evaluation",
            "char_start_offset": 21766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 272,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 272,
                    "end": 291,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1353,
                    "end": 1373,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1175537109375
        },
        {
            "corpus_id": "267212206",
            "title": "Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation",
            "text": "For instance, it frequently avoids formulating treatment plans or predicting outcomes, as reflected in Figure 2d's comparison of inconclusive predictions between the two models (a) GPT-3.5-turbo 16.25% v. GPT-4 29.46% for inconclusive cost prediction; and (b) GPT-3.5-turbo 18.79 v. GPT-4 38.31% for inconclusive hospitalization prediction. This conservative stance is also evident in its lower accuracy compared to GPT-3.5-turbo (GPT-3.5-turbo 59.30% v. GPT-4 31.49%, figure 2c) in predicting deceased outcomes. GPT-4 often resorts to generic advice like 'consult with healthcare providers', which might be insufficient for accurate medical guidance. The challenge lies in balancing caution with the need for precise, high-stakes predictions. Additionally, GPT-4's longer response times and higher operating costs (as of this writing, the cost of GPT-4 is approximately 30 times higher than that of GPT-3.5-turbo) limit its practical utility in real-world scenarios. In practice, our expected wait time to not trigger OpenAI's API error is ~2 seconds for GPT-3.5-turbo, and ~15 seconds for GPT-4. \n\nThis study focuses on illustrating bias in LLMs, such as GPT-3.5-turbo and GPT-4. Transformer-based models, including GPTs 2 , generate text based on previous tokens, meaning altering one token or the language prior can change subsequent token distributions. Although OpenAI has implemented RLHF to discourage problematic outputs in LLMs 2,12 , our findings indicate that these models still exhibit inherent biases, especially in relation to race. Moreover, our study highlights that discouraging 'harmful' outputs in LLMs can lead to an overly optimistic bias, especially in critical scenarios. Both GPT variants displays a high degree of optimism when predicting death outcomes, with GPT-4's accuracy in predicting deceased outcomes only 31.49% compared to 59.30% for GPT-3.5-turbo (Figure 2c).",
            "score": 0.5071117936892824,
            "section_title": "Main",
            "char_start_offset": 6344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1896
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01262664794921875
        },
        {
            "corpus_id": "270878360",
            "title": "Assessing the Code Clone Detection Capability of Large Language Models",
            "text": "These studies have endeavored to explore and evaluate the efficacy of LLMs in understanding and generating code [11,24,35], generation of code documentation [20] or repairing code [19].To the best of our knowledge, the study by Wang et al. [37] is the only to evaluate the performance of LLMs on code clone detection.However, this study is limited to ChatGPT, and does not explore diverse types of code clones or multiple sources of datasets (including real-world code clones and clones generated by the GPT model itself).\n\nTo fill this gap, in this paper, we aim to systematically evaluate the capabilities of two LLM models (i.e., GPT-3.5 and GPT-4) in identifying code clones.Furthermore, as Integrated Development Environments (IDEs) become integrated with LLM tools (e.g., GPT-4 and Microsoft Copilot) and software engineers are more numerous to leverage their capabilities for various software engineering tasks (e.g., code generation and refactoring), we would like to assess whether there is a difference between the performance of LLM models at identifying human-generated code clones in comparison to LLM-generated code clones.\n\nOur paper aims at answering the following Research Questions (RQs):\n\n\u2022 RQ1: How does the performance of GPT-3.5 compare to the performance of GPT-4 across various code clone types and code similarity levels?\u2022 RQ2: Do GPT models exhibit different performances when assessing human-generated versus LLMgenerated code clones?If so, how do these differences manifest between GPT-3.5 and GPT-4?\n\nThe rest of the paper is organized as follows.Section 2 summarises the background of our study.Section 3 describes the approaches we used to select data and design GPT prompt.In Section 4, we analyze the performance of GPT through the data we selected.Finally, Section 5 concludes the paper.",
            "score": 0.5063395435244297,
            "section_title": "Introduction",
            "char_start_offset": 2061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 185,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 522
                },
                {
                    "start": 524,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 1137
                },
                {
                    "start": 1139,
                    "end": 1206
                },
                {
                    "start": 1208,
                    "end": 1346
                },
                {
                    "start": 1346,
                    "end": 1461
                },
                {
                    "start": 1461,
                    "end": 1528
                },
                {
                    "start": 1530,
                    "end": 1576
                },
                {
                    "start": 1576,
                    "end": 1625
                },
                {
                    "start": 1625,
                    "end": 1705
                },
                {
                    "start": 1705,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 1821
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 122,
                    "matchedPaperCorpusId": "267260102"
                },
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "252089836"
                },
                {
                    "start": 240,
                    "end": 244,
                    "matchedPaperCorpusId": "267660829"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057281494140625
        },
        {
            "corpus_id": "272397953",
            "title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey",
            "text": "The LLM-based evaluator has been found to exhibit certain biases and limitations. [122] note that LLM-based evaluator inevitably has position bias (i.e., when using GPT-4 for pairwise comparison, the evaluation results can be easily hacked by altering the order in which candidate answers appear in context). Another bias of LLM-based evaluator is that the LLM-based evaluator exhibits a tendency to prefer more verbose outputs [161], shows a predisposition towards outputs generated by models similar to itself [38,161], and displays a limited capability in evaluating subjects like mathematics, reasoning, and other areas that still pose challenges for LLMs [151]. To systematically quantify the performance of LLM-based Evaluators, several works have introduced meta-evaluation benchmarks. FairEval [122], MT-Bench [161], and LLMEval [158] assess whether LLM-based evaluators demonstrate high agreement with humans by utilizing manually annotated preference datasets. [151] proposed a meta-evaluation benchmark called LLMBar, which includes an Adversarial set. Notably, all models, including GPT-4, struggled on the adversarial set without the use of additional strategies.",
            "score": 0.5058953180220558,
            "section_title": "Limitations",
            "char_start_offset": 54166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1176
                }
            ],
            "ref_mentions": [
                {
                    "start": 428,
                    "end": 433,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 818,
                    "end": 823,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093017578125
        },
        {
            "corpus_id": "265876432",
            "title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation",
            "text": "When evaluating LLMs' zero-shot capabilities for human vs. LLM disinformation detection, we find GPT-3.5-Turbo, LLaMA-GPT, and LLaMA-2 are more accurate on detecting LLM-generated disinformation, compared to human-authored disinformation (Fig. 4). On human-authored data, GPT-3.5-Turbo's accuracy ranges from 55-66%, while on LLM-generated data, it achieves 60-85%. Dolly-2 shows the lowest accuracy on both human (51-52%) and LLM (47-50%) disinformation. \n\nRQ2.1 Finding: LLMs struggle more to detect human-written disinformation, compared to LLM-generated variants. \n\nRQ2.2: Self-generated vs. Externally-generated GPT-3.5-Turbo-135B displays strong self-detection, outperforming other LLMs overall and across disinformation variants, i.e., minor, major, and crit-  ical (Fig. 5). However, LLaMA-GPT excels as the top external detector of GPT-3.5-Turbo-generated disinformation. LLaMA-2 shows moderate external detection abilities. Regardless of self or external detection capacity, by and large, LLMs struggle to accurately detect minor paraphrased and perturbed disinformation. \n\nRQ2.2 Finding: GPT-3.5-Turbo is good at self-detection, and LLaMA-GPT is the best external detector.",
            "score": 0.5051224802160275,
            "section_title": "RQ2.1: Human vs. LLM-generated",
            "char_start_offset": 22942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1081
                },
                {
                    "start": 1084,
                    "end": 1184
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06561279296875
        },
        {
            "corpus_id": "268358817",
            "title": "How Well Do Multi-modal LLMs Interpret CT Scans? An Auto-Evaluation Framework for Analyses",
            "text": "Furthermore, they constructed a dataset based on LLM evaluation results and used knowledge distillation to train a smaller model, which achieves evaluation capabilities comparable to GPT-4. Similarly, Liu et al. (2024) proposed MRScore, a framework akin to LLM-RadJudge.Zhu et al. (2024) proposed a method that combines the expertise of professional radiologists with LLMs such as GPT-3.5 and GPT-4.Using In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning to align LLM evaluations with radiologist standards, experimental results demonstrated greater alignment with expert evaluations, surpassing traditional NLG metrics such as BLEU, ROUGE, and METEOR.Despite these advancements, there is still no automated system for validating the clinical accuracy of CT reports, largely because of the scarcity of high-quality datasets and the complexity of CT imaging, which involves a broad range of body parts and requires extensive anatomical knowledge.This paper introduces GPTRadScore, a novel evaluation framework that assesses the capabilities of multimodal LLMs.It uses a decomposition method based on GPT-4, which mimics clinicians' evaluation processes, comparing AI-generated descriptions with the actual ground truth across factors such as body part, location, and type.",
            "score": 0.5048604961702423,
            "section_title": "Related Works",
            "char_start_offset": 5668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 971
                },
                {
                    "start": 971,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1297
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132568359375
        },
        {
            "corpus_id": "269213992",
            "title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
            "text": "Based on the selected criteria, we use GPT-4 to asynchronously generate one or more candidate assertions as code or a grader prompt to be evaluated by an LLM.For each criterion, we issue one call to GPT-4 to generate multiple candidate assertions within markers in a streaming fashion.Every time we detect the end of marker in any GPT-4 response, we parse the candidate assertion and submit it to EvalGen's executor, which will run it on LLM pipeline outputs.Generating multiple candidate assertions improves the probability that there is at least one implementation that aligns with developer expectations.Moreover, for code-based assertions, LLMs occasionally synthesize erroneous functions (e.g., hallucinating a function in a Python library), requiring several candidate assertions.\n\nGrading Sampler.This component samples LLM pipeline outputs for the user to give binary feedback on (thumbs up/down).\n\nWhen the user grades an LLM output, we update internal estimates of alignment for each candidate assertion, and we sample the next output for the user to grade.\n\nOnce the user does not want to grade LLM outputs anymore, or is finished grading all outputs, for each criterion, we select the candidate assertion with the highest alignment with the user's grades.The user can provide a threshold for the false failure rate (as defined in Section 5) such that EvalGen only selects assertions that do not exceed this threshold.",
            "score": 0.5041283321250203,
            "section_title": "IMPLEMENTATION 4.1 System Architecture",
            "char_start_offset": 17268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 459
                },
                {
                    "start": 459,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 786
                },
                {
                    "start": 788,
                    "end": 804
                },
                {
                    "start": 804,
                    "end": 905
                },
                {
                    "start": 907,
                    "end": 1067
                },
                {
                    "start": 1069,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1429
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3349609375
        },
        {
            "corpus_id": "277940468",
            "title": "Sustainability via LLM Right-sizing",
            "text": "To assess the quality of outputs generated by each LLM, we implemented a standardized evaluation procedure using three different LLMs as evaluators: GPT-4o, Gemini-2.0-Flash, and o3-mini. This triad was selected to ensure robustness across evaluator types: GPT-4o represents a state-of-the-art, high-performance model, Gemini-2.0-Flash offers a lightweight, general-purpose option, and o3-mini exemplifies a compact \"thinking model\" architecture. Using multiple evaluators allows us to triangulate scoring consistency and assess potential biases introduced by individual LLM preferences. \n\nEach evaluator received identical scoring prompts and independently rated the outputs of the eleven tested models across ten evaluation criteria (cf. Table 4). These criteria builds on prior work in LLM evaluation (Chiang et al. 2024;Ji et al. 2023;Long et al. 2024) and are adapted to reflect core dimensions of output quality especially relevant for occupational tasks: (1) core qualities, (2) factual integrity, and (3) ethical and social responsibility. \n\nEach criterion was scored on a 10-point Likert scale, where 1 indicated poor performance and 10 represented excellent performance. The evaluators processed outputs without knowledge of which model generated them and, importantly, also assessed their own outputs in a blind fashion. This introduces a potential for selffavoring bias. However, our comparative evaluation across the three evaluators (cf. Figure 1 in Appendix B on OSF) shows that GPT-4o was not rated disproportionately higher by itself compared to Gemini-2.0-Flash or o3-mini. This is important as for the following inference statistics and analyses, we use GPT-4o only: using the intra-class correlation coefficient (ICC), a two-way random-effects model with absolute agreement was applied (ICC[2,3]). The resulting ICC was < .50, indicating bad agreement according to the interpretation guidelines by Koo and Li (2016). Thus, we proceeded to report the main results using scores from GPT-4o as the primary evaluator with the most robust and reliable scores. This is in line with other recent work,",
            "score": 0.5034951865334785,
            "section_title": "Evaluation Criteria and Scoring",
            "char_start_offset": 15376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2114
                }
            ],
            "ref_mentions": [
                {
                    "start": 804,
                    "end": 824,
                    "matchedPaperCorpusId": "268264163"
                },
                {
                    "start": 824,
                    "end": 839,
                    "matchedPaperCorpusId": "259501579"
                },
                {
                    "start": 1918,
                    "end": 1935,
                    "matchedPaperCorpusId": "1837377"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.362548828125
        },
        {
            "corpus_id": "268856493",
            "title": "METAL: Towards Multilingual Meta-Evaluation",
            "text": "We prompt GPT-4 to generate good and bad-quality summaries.As noted in \u00a73.1, for lower tempera-ture values we observed that GPT-4 did not generate bad summaries.We use a temperature of 1 and observe some variation of quality across all our metrics except problematic content.This could be due to the content filter applied to these models.Therefore, it is difficult to study the capability of such models on this metric.We evaluate the generations from GPT-4 using GPT-3.5-Turbo,GPT-4, and PaLM2.Recent work has shown that LLMs prefer their own outputs.Although this might have affected our evaluations, exploring this is beyond the scope of our work.In our work, we mainly focused on investigating how well LLM ratings align with human ratings across various metrics and languages.All summaries generated and evaluated in our study are by the same model, we do not compare them against human-written summaries or summaries generated by other models.Lastly, LLMs are also shown to have scale region bias and we do not calibrate for this in our study, expecting it to be standardized across all their ratings.In the future, it would interesting to explore their impact on our evaluation.",
            "score": 0.5014054659875491,
            "section_title": "Limitations",
            "char_start_offset": 30132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 59,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 275
                },
                {
                    "start": 275,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03167724609375
        },
        {
            "corpus_id": "271097661",
            "title": "Source Code Summarization in the Era of Large Language Models",
            "text": "For other automated evaluation methods, in most cases, their correlation with human evaluation is negative or weakly positive.Based on the above observations, we draw the conclusion that compared with other automated evaluation methods, the GPT-4-based method is more suitable for evaluating the quality of summaries generated by LLMs.In the subsequent RQs, we uniformly employ the GPT-4-based method to assess the quality of LLM-generated summaries.To make the output scores of GPT-4 more deterministic, we set the temperature to 0 when using GPT-4 as the evaluator.\n\n\u270e Summary \u25b6 Among all automated evaluation methods, the GPT-4-based method overall has the strongest correlation with human evaluation.Therefore, it is recommended to adopt the GPT-4-based method to evaluate the quality of LLM-generated summaries.\u25c0 B. RQ2: How effective are different prompting techniques in adapting LLMs to the code summarization task?\n\n1) Experimental Setup.The experimental dataset comprises 600 samples from Java, Python, and C datasets collectively.\n\n2) Experimental Results.Table V presents the scores reported by the GPT-4 evaluation method for summaries generated by four LLMs using five prompting techniques.Observe that when the base model is CodeLlama-Instruct, few-shot prompting consistently performs best on all three datasets.When the base model is StarChat-\u03b2, chain-of-thought prompting performs best on all the Java and C datasets, while expert prompting excels on the Python dataset.When selecting GPT-3.5 as the base model, the simplest zero-shot prompting surprisingly achieves the highest scores on the Java and C datasets, and is only slightly worse than few-shot prompting on the Python dataset.When using GPT-4 as the base model, chain-of-thought prompting overall performs best.\n\nFor the specific LLM and programming language, there is no guarantee that intuitively more advanced prompting techniques will surpass simple zero-shot prompting.For example, on the Java dataset, when selecting any of StarChat-\u03b2, GPT-3.5, and GPT-4 as the base model, few-shot prompting yields lower scores than zero-shot prompting.",
            "score": 0.5005993709162495,
            "section_title": "Correlation between Automated Evaluation and Human",
            "char_start_offset": 30676,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 450
                },
                {
                    "start": 450,
                    "end": 567
                },
                {
                    "start": 569,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 816
                },
                {
                    "start": 816,
                    "end": 923
                },
                {
                    "start": 925,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1041
                },
                {
                    "start": 1043,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1705
                },
                {
                    "start": 1705,
                    "end": 1790
                },
                {
                    "start": 1792,
                    "end": 1953
                },
                {
                    "start": 1953,
                    "end": 2123
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11578369140625
        },
        {
            "corpus_id": "266210545",
            "title": "Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models",
            "text": "Fig. A5: GPT-4 score sometimes exhibits excessive confidence in low-quality responses, though widely adopted as metrics in general MLLMs [12,31]. In this case, a fully wrong response still receives a GPT-4 score of 66.7%. \n\nods [61,78], the quality scores are assessed using Spearman Rank Correlation Coefficient (SRCC) and Pearson Linear Correlation Coefficient (PLCC). \n\nGPT-4 score. To compute the GPT-4 score for one response, we input GPT-4 with the following contents: \"context\", \"question\", \"answer_1\", \"answer_2\", and \"instruction\". The \"instruction\" directs GPT-4 to assess two answers (\"answer_1\" and \"answer_2\") to the same \"question\" using the \"context\" knowledge, and then rate each answer on a scale of [0, 10]. The \"context\" contains all information in the human-labeled questionnaire, which is the correct knowledge for GPT-4 to evaluate answers. The \"question\" refers to the quality-related question posed to the MLLMs. The \"answer_1\" is actually the ground-truth response, which is labeled by human-evaluators. The \"answer_2\" is the response of the MLLM to be evaluated. Then, the score of \"answer_2\" (i.e., the MLLM to be evaluated) relevant to \"answer_1\" (i.e., the ground-truth) serves as the GPT-4 score of \"answer_2\". We calculate the final metric by averaging the scores across all samples. As discussed in the main paper, the GPT-4 score tends to be overly generous for low-quality responses, resulting in inflated scores. One example is shown in Fig. A5, where a fully wrong response receives a GPT-4 score of 66.7%. \n\nReasonable rate evaluated by humans. Human evaluators are requested to label each response as either reasonable or unreasonable, based on the provided images and corresponding responses. One \"reasonable\" response should adhere to three criteria. (1) The response must accurately identify the most important one quality issue that directly influences the comparison result.",
            "score": 0.4997554469179045,
            "section_title": "Context (human-labeled",
            "char_start_offset": 40247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1542
                },
                {
                    "start": 1545,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1917
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0809326171875
        },
        {
            "corpus_id": "270440428",
            "title": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery",
            "text": "GPT-4 Scoring Prompt. In Table 13, we present the prompts utilized to instruct GPT-4 in scoring the outputs of LLMs in CS generation tasks, encompassing both FITB and Open-ended questions. \n\nConsistency between GPT-4 Scoring and Manual Scoring. To assess the effectiveness of GPT-4 scoring in evaluating LLM responses, we conduct a consistency experiment between GPT-4 prediction scores and manual scores. For Fill-in-the-blank and Open-ended types, we randomly sample 100 instances from the GPT-4 scoring samples and employ three human annotators to score these predicted results. These three annotators all hold bachelor's degrees in computer science. Their scoring criteria were consistent with the evaluation standards provided to GPT-4 and can be found in Table 13. In Table 11, we report the consistency scores among human annotators (measured by Cronbach's alpha), as well as the consistency scores between the average human annotation scores and GPT-4 scoring (measured by Pearson correlation coefficient). The excellent consistency between human and GPT-4 scores validates the effectiveness of GPT-4 scoring. \n\nTable 11: Consistency between GPT-4 scoring and human scoring.",
            "score": 0.49849055465871633,
            "section_title": "D.2 DETAILS OF GPT-4 SCORING",
            "char_start_offset": 42653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1182
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0390625
        },
        {
            "corpus_id": "265212753",
            "title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation",
            "text": "Table 4 presents the performance of various LLMs in tasks of program synthesis, code translation, and code repair across evaluations of different difficulties. Detailed experimental results are provided in Tables 14 to 33 in the appendix, while case studies for each task are reported in Tables 45 to 48. \n\nGPT-4 and GPT-3.5 excel in three different code generation tasks due to their advanced training methods and high-quality data. GPT-3.5 handles easy problems effectively, while GPT-4 outperforms it on more challenging ones. Setting different levels of difficulty helps to show the strengths and weaknesses of various LLMs, and shows the importance of choosing the right difficulty level when evaluating LLMs. Other LLMs lag behind GPT-4 and GPT-3.5 on both easy and hard tasks. They struggle to provide correct solutions for hard problems, which limits their usefulness in real-world programming applications. For these LLMs, it is easier to fix buggy code than to generate solutions from scratch. CodeScope is a valuable addition to the field of code generation, as it can evaluate the LLMs' ability to solve real-world programming problems more accurately. CodeScope solves the problem of HumanEval's benchmark accuracy rate being too high (94.4%) (Zhou et al., 2023), which means it is too easy.",
            "score": 0.4974815306252167,
            "section_title": "Difficulty",
            "char_start_offset": 19467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038238525390625
        },
        {
            "corpus_id": "265233456",
            "title": "Stratified Evaluation of GPT\u2019s Question Answering in Surgery Reveals Artificial Intelligence (AI) Knowledge Gaps",
            "text": "We therefore encourage future research to benchmark GPT-4 Turbo to quantify its performance. \n\nFifth, LLMs are stochastic, and thus it is possible for GPT to respond differently to questions if asked on multiple occasions. In this study, we asked GPT each question once, as is common in other papers assessing LLM performance [10]. Future research gauging the variability in LLM responses to multiple instances of the same question is a potentially fruitful area. \n\nSixth, the comparison between our results and the published results in Toma et al.'s study [2] should be interpreted with two caveats. First, we cannot be sure of the exact sub-version of GPT used by the authors, other than the major release milestones of GPT-3.5 and GPT-4. OpenAI iterates on their models regularly, and thus it is possible that our questions were analysed with a slightly different version of GPT-3.5 or GPT-4 than those used by Toma et al. [2]. Second, the filtering of questions between ours and the comparison paper may be different. For example, the comparison paper may include questions with multiple best answers, which we exclude. If this is the case, it is unknown how much this would affect the comparability of our results, as it depends on whether there is a systematic difference in GPT's ability to answer questions with single versus multiple best answers. Future research could help answer this question by comparing LLM performance on 'single' and 'multi' questions in the MedMCQA dataset.",
            "score": 0.4962138540598907,
            "section_title": "Discussion",
            "char_start_offset": 18372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 95,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1491
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "255124952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0167236328125
        },
        {
            "corpus_id": "267760095",
            "title": "Event-level Knowledge Editing",
            "text": "To validate the effectiveness of using GPT-4 as a scorer in the Tendency-G evaluation, we conduct a manual review of GPT-4's scoring. Specifically, we randomly sample 120 questions and corresponding model-generated answers, with 60 from Mistral 7B and 60 from GPT-4. One of our authors scores this data. Similar to previous work (Bai et al., 2023;Chan et al., 2023), we calculate Spearman's \u03c1 and Kendall's \u03c4 coefficients between the model's overall scores and the manually assigned overall scores, which are 74.4% and 69.8%, respectively. These results indicate a strong positive correlation between scores given by GPT-4 and humans. This suggests that GPT-4's scoring generally aligns with human assessment but still leaves room for improvement. Additionally, GPT-4 tends to overestimate LLMs' performance, with an average score of 4.34 compared to the human-assigned average of 4.15. Nonetheless, as an automated, low-cost evaluation approach, it is sufficiently effective.",
            "score": 0.495971713360207,
            "section_title": "Human Evaluation of GPT-4 Scorer",
            "char_start_offset": 25476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 976
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 347,
                    "matchedPaperCorpusId": "259095491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038116455078125
        },
        {
            "corpus_id": "276902641",
            "title": "SKG-LLM: Developing a Mathematical Model for Stroke Knowledge Graph Construction Using Large Language Models",
            "text": "In this evaluation, experts evaluate the relationships and present the ranking as a consolidated weighted average. For this average, Cohen's kappa was used, which is calculated as follows: \n\nIn this equation po is the observed agreement among raters, and pe is the expected agreement by chance. Nonetheless, regarding the Accuracy (GPT-4), Wikidata narrowly wins at 89.66% while SKG-LLM is at the lowest point of 87.5%. This implies that while SKG-LLM performs superiorly in human-reviewed precision and consistency, there is an about area for improvement in accuracy as a whole. \n\nThe method of accuracy (GPT-4) is one of the critical evaluation methods in large language models, such as SKG-LLM. Here, the overall accuracy of the model gets evaluated using GPT-4. This essentially means judging the correctness of all relationships drawn from the data, and how aligned they are to scientific facts or existing data. Thus, in other words, Accuracy defines the percentage of all relationships and information created by the model which links with existing and validated data. It has always been across kinds that SKG-LLM outperforms in expert review scoring highest F1 score of 89.76 (Table 6). This implies that manual validation considerably improves the precision-recall balance. GPT-4 integration also scores on its F1 performance of 89.02, signifying its high automatic extraction capability. On the contrary, StrokeKG records a lower F1 score of 84.26 ostensibly because of its low precision-even though recall is too high. This suggests StrokeKG retrieves relations well, but with higher false positives compared to SKG-LLM. Within the set of comparisons made on the different NER models, SKG-LLM is consistent across all categories, and this is the highest F1 score achieved so far, by SKG-LLM (GPT-4), which is 90.97, thus indicating that integration with GPT-4 increases both precision and recall. Another very good score is given by SKG-LLM (Expert Review), with an F1 score of 90.84, further proving that expert validation adds value in the maintenance of balanced precision and recall.",
            "score": 0.4959273774667361,
            "section_title": "Expert Review",
            "char_start_offset": 25076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0489501953125
        },
        {
            "corpus_id": "270062858",
            "title": "Automatically Generating Numerous Context-Driven SFT Data for LLMs across Diverse Granularity",
            "text": "To conduct a more comprehensive evaluation, we utilized GPT-4 [2] (we use gpt-4-0125-preview) to judge as the referee on the baseline method and our approach.By using GPT-4 to compare the outputs of the model fine-tuned on our generated data with the baseline methods, we can better understand the advantages of our model and avoid biases stemming from manual preferences.We provide the query in the DailyM test set as inputs and get the outputs of the model fine-tuned using our method and baseline method on the DailyM dataset.To facilitate a nuanced evaluation, we categorized the queries into three levels of granularity: detail question (e.g., how deep does the abortion needle penetrate?),concept question (e.g., what is the difference between emergencies and crises?), and macro question (e.g., what impact does faith have on us?).We present the detailed guidance for categorizing in Table 11.\n\nFor the reliability of the results, we extract relevant references to the questions in the dataset corpus to assist GPT-4 in making decisions.Then, we ask GPT-4 to compare the outputs generated by the two models, with the template shown in below text box.To mitigate the potential impact of position bias of LLMs, we implement a robust evaluation strategy that for each pair of outputs, we swap their positions and queried GPT-4 twice.In cases where the two responses were not consistent, we continued to inquire until we obtained a unanimous answer.The comparison results are shown in Figure 4.",
            "score": 0.49504633944691556,
            "section_title": "G.2 GPT-4 Judge",
            "char_start_offset": 66420,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 838
                },
                {
                    "start": 838,
                    "end": 900
                },
                {
                    "start": 902,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1157
                },
                {
                    "start": 1157,
                    "end": 1337
                },
                {
                    "start": 1337,
                    "end": 1452
                },
                {
                    "start": 1452,
                    "end": 1497
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.094482421875
        },
        {
            "corpus_id": "273374964",
            "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
            "text": "To demonstrate that Auto-PRE effectively reduces the bias in single evaluator LLM (like GPT-4), we conducted the following experiments: Previous research (Chu et al., 2024;Zeng et al., 2023) has shown that one of the biases present in GPT-4 is that it may prefer answers generated by LLMs in the GPT series, regardless of the actual quality of the answers. To clearly demonstrate the negative impact of this bias on experimental outcomes, we used GPT-3.5-turbo and ChatGLM2-6B as two LLMs generating the answers to be tested and conducted experiments on the Xsum and NF_CATS datasets in pairwise evaluation format. rate method = p method \u2212 p human p human \u00d7 100% (1) \n\nTable 4 presents the experimental results. Rate is calculated using Equation 1, where p denotes the proportion of evaluation results that favor GPT-3.5-turbo. The rate value refers to the percentage change in the method's preference for GPT-3.5turbo compared to human preference. It is noteworthy that the rate does not necessarily correlate with accuracy, as it does not consider the order of answers. However, it can, to some extent, reflect a method's preference bias for specific answers. \n\nFrom the results, we can observe a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods. This suggests that GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability. Conversely, Auto-PRE enhances overall performance and reliability by collaborating with various types of LLMs.",
            "score": 0.4950343577672874,
            "section_title": "Bias Analysis",
            "char_start_offset": 21326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 666
                },
                {
                    "start": 669,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226806640625
        },
        {
            "corpus_id": "272593053",
            "title": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation",
            "text": "Performance comparison with baselines. We conducted a comparison against baseline evaluation methods to evaluate the performance of InteractEval. Building upon prior studies [24,27,42,49], we analyzed the correlation between InteractEval and groundtruth scores using Spearman's Rho, Kendall's Tau coefficients, and the difference between predicted scores and human-labeled scores using mean absolute errors (MAE). For a fair comparison, we used the performance data of the seven non-LLM-based traditional methods (see Table 9), G-Eval (GPT-4), and CheckEval (GPT-3.5 Turbo and GPT-4), as reported in [42], given that their experiments were conducted using the same experimental settings (sample-level correlation). Additionally, we reproduced the correlation results of G-Eval (GPT-3.5-Turbo) 9 and the MAE results of the entire baselines at the sample level using the source code provided by the original authors (where available) and following the implementation details described in their papers. Table 9 presents the main results, classifying the four dimensions into two key features by which generated texts are typically assessed: internal quality and external alignment [75,89]. Additionally, the table shows InteractEval's performance in the first and second trials using both GPT-3.5-Turbo and GPT-4, demonstrating stable sample-level correlations. In the both trials, InteractEval with GPT-4 outperforms or is at least comparable to the traditional non-LLM-based metrics, such as ROUGE-L, BLEU, METEOR, BERTScore, MOVERScore, BARTScore, and UniEval, in terms of correlation coefficients. As noted in [42], the results suggest that using a checklist enables an LLM to evaluate generated text in a manner more aligned with human judgment than traditional automatic metrics. \n\nWhen compared to G-Eval and CheckEval with the identical version of GPT, our model demonstrates superior performance. Moreover, even with GPT-3.5-Turbo,",
            "score": 0.49367032880677564,
            "section_title": "5.1.1",
            "char_start_offset": 39604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "220768873"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "269804457"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51318359375
        },
        {
            "corpus_id": "269626718",
            "title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge",
            "text": "For instance, it could be used in evaluating the quality of prompting techniques and generation parameters, or exploring alternative language models (e.g; open source onces) across a large number of generations.Still, even in these scenarios, ideally, we would not be using GPT-4 as a judge for research fully on its own, but as a complement to human evaluations [8].\n\nOpen-source judges.Using GPT-4 as a judge has shown promises, in particular in research [44].However, the proprietary nature of the state-of-the-art LLM has led to the rise of judges based on opensource LLMs.PandaLM [40] is an open-source LLM that has been fine-tuned for selecting the best response to an (instruction, input) pair and providing an explanation together with its decision.In similar works, JudgeLM [46] focuses on increasing the performance of the 'judge' LLM, while Prometheus [15] emphasizes achieving granular feedback according to a custom score rubric.\n\nAll these works follow a common scheme: they create a highquality imitation dataset, where the inputs are human-curated and high-performance closed models like GPT-4 provide the desired outputs; then, they fine-tune one or more open-source models on the dataset.Although their results are promising, closed models such as GPT-4 remain the state-of-the-art model, which is why we chose to use GPT-4 for our study.",
            "score": 0.4929769056228304,
            "section_title": "Automatic Feedback Evaluation",
            "char_start_offset": 17772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 367
                },
                {
                    "start": 369,
                    "end": 388
                },
                {
                    "start": 388,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 942
                },
                {
                    "start": 944,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1356
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09503173828125
        },
        {
            "corpus_id": "261822211",
            "title": "Less is More for Long Document Summary Evaluation by LLMs",
            "text": "As an ablation study, Table 4 shows the results of experiments using GPT-3.5, a smaller model than GPT-4. Unlike G-Eval, GPT-3.5 showed an overwhelmingly lower correlation than GPT4 in all data sets and settings, meaning that a GPT-4 scale model should be used as the backbone for long-document summary evaluation. We also tested open LLM alternatives such as Mistral-7B (Jiang et al., 2023), but we observed similar trends with GPT-3.5. Thus, we only utilize GPT-4 in this study.",
            "score": 0.49265975343277957,
            "section_title": "B Correlation performance between human ratings and model-based scoring",
            "char_start_offset": 14395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 480
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0640869140625
        },
        {
            "corpus_id": "276944319",
            "title": "Performance Evaluation and Application Potential of Small Large Language Models in Complex Sentiment Analysis Tasks",
            "text": "However, while a reduction in parameter size reduces computational costs, it may also affect performance, and the effectiveness of small LLMs like Llama3-7B in different tasks varies. Although previous research [26] has shown that some sLLMs can achieve competitive results, their reliability and applicability require further systematic evaluation to determine whether they can replace large models in practical applications. \n\nThe purpose of this study is to evaluate the performance of sLLM on the ABSA task, especially its potential for use in resource-constrained environments. We propose to use the locally deployed open-source sLLM instead of GPT-3.5 to address the data privacy concerns associated with closed-source models. Previously, we evaluated our method using GPT-3.5 [27] and achieved positive results. GPT-4.0 has excelled in a variety of tasks, even surpassing humans in some challenging ones [28], [29]. Recently, LLM-as-a-judge has become a popular topic, and the results of using LLM to evaluate LLM are used as an optional option. For these reasons, in this study, we use GPT-4o to generate reference answers, which is the more powerful than GPT-4.0, serve as the labels for evaluation. Since GPT-4o-generated answers are not necessarily the absolute ground truth, our evaluation focuses on measuring agreement between sLLM and GPT-4o rather than traditional accuracy metrics. We employ a comprehensive set of evaluation methods, including agreement-based metrics, to select the model that best meets the task requirements. Although the fact that some high-scoring models perform poorly in real-world applications [30], [31], we employ a comprehensive set of evaluation methods to select the model that best meets the task requirements. In this study, we select several open-source sLLMs with parameter sizes below 10B and conduct a comprehensive evaluation of their performance, resource consumption, and applicability in real-world applications. \n\nIn summary, we devised a methodology to validate the usability of sLLM on ABSA tasks, our key contributions are: \n\n\u2022 We conduct a detailed performance comparison on",
            "score": 0.4926446458224272,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1969
                },
                {
                    "start": 1972,
                    "end": 2084
                },
                {
                    "start": 2087,
                    "end": 2136
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038970947265625
        },
        {
            "corpus_id": "264426756",
            "title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
            "text": "These are our main findings: firstly, we show how traditional reference-based evaluation metrics are inadequate at predicting or replacing human judgement. It is unclear whether this is due to the limitations of the metrics or to the poor quality of references of large open source datasets, or both. While automatic metrics might have been an adequate proxy to evaluate previous models, they seem unable to reliably capture the performance of latest-generation LLMs which now generate ac-ceptable output that is significantly different from the gold reference. Secondly, we prove that even open-source models outperform the gold standard reference of large and well-established datasets according to human evaluators. This shows how data quality is now one of the main bottlenecks in evaluation research. Finally, we reveal how GPT-4 has reasonable alignment with human judgement when ranking different models on most tasks and metrics; we did however observe some variations, with lower alignment in some metrics than in others. Our code is available at https://github.com/ protagolabs/seq2seq_llm_evaluation.",
            "score": 0.4919210966113105,
            "section_title": "Introduction",
            "char_start_offset": 1823,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1111
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07452392578125
        },
        {
            "corpus_id": "267406182",
            "title": "LLM-based NLG Evaluation: Current Status and Challenges",
            "text": "As mentioned above, despite the exciting performance of prompting LLMs like Chat-GPT and GPT-4 for NLG evaluation, several shortcomings in practice are inevitable, such as high costs, possibly irreproducible results, and potential biases in LLMs. In response, recent research has shifted towards fine-tuning smaller, open-source LLMs specifically for evaluation purposes, aiming to achieve performance close to GPT-4 in NLG evaluation. Representative works of this type include PandaLM (Wang et (Hu et al. 2024), CompassJudger-1 (Cao et al. 2024) and Self-Taught (Wang et al. 2024b). Their main ideas are similar, involving the elaborate construction of high-quality evaluation data, followed by fine-tuning open-source foundation LLMs with specific methods. Nevertheless, there are certain discrepancies in the designs across different works, such as the usage of references and evaluation criteria. We have summarized the key different components of these methods in Table 2 and Table 3 for comparison, which will be elaborated in the following sections.",
            "score": 0.49182174098290615,
            "section_title": "Fine-tuning LLMs",
            "char_start_offset": 31620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1056
                }
            ],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 511,
                    "matchedPaperCorpusId": "270737769"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1414794921875
        },
        {
            "corpus_id": "273185876",
            "title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
            "text": "Our results show that GPT-4o and Cohere+ is a powerful protocol formulator (Table 3). We found our work compatible to previous work (O'Donoghue et al., 2023). \n\nIs applying domain knowledge an effective strategy for evaluation? We applied domain knowledge by predefining the finite set of actions performed in biology labs. To evaluate the efficacy of this method, we compare the responses generated with predefined actions included in the prompts to those generated without them (Table 4). The performance is enhanced for most models, with the exception of the Recall. Further research should be conducted to explore these findings. \n\nCan the original protocol itself serve as a baseline? Evaluation of LLMs in SPFT in previous work requires manual processes and pseudocode extraction step in SPFT. However, evaluation using the original protocol itself completely eliminates the manual processes of pseudofunction evaluation and the GPT-generated pseudocode extraction step, thereby enhancing flexibility and automation. To this end, we evaluate using the original protocol as a baseline. While scores obtained using this approach is not close to the maximum score (Table 3), we observe that the relative ranking of the models remains relevant to the results of using the pseudocode as a baseline. \n\nWill LLM as an evaluator prefer responses from itself? It is reported that LLM as an evaluator prefer responses from itself over human responses in text summarization tasks (Liu et al., 2023). Therefore, a potential concern is that the evaluator may prefer outputs from itself regardless of its quality. While results in Table 2 and 4 address this concern, Table 3 shows that Llama-3 as an evaluator does not prefer its outputs over that of GPT-4. Our results suggest that GPT's preference for its own responses in previous work (Liu et al., 2023) may be a phenomenon unique to GPT.",
            "score": 0.491771535798017,
            "section_title": "Evaluating LLMs on SPFT",
            "char_start_offset": 17042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 157,
                    "matchedPaperCorpusId": "264172681"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07159423828125
        },
        {
            "corpus_id": "268230489",
            "title": "DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling",
            "text": "Apart from RM-based methods, if only the preference order of response pairs needs to be given, a simple way is to call the public language model API through zero-shot or few-shot. It can also be combined with BoN or DPO to optimize LLM. For a more comprehensive evaluation, in preference consistency experiments, we have added the GPT-4 baseline. \n\n\u2022 GPT-4 (OpenAI, 2023) We use the most advanced gpt-4-1106-preview5 as the evaluator to evaluate a pair of preference data. Each time, we swap the positions of the responses and make two requests, and we will re-request until we get identical results from two requests as the final results. \n\nWe try to find well-prompted instruction and use both zero-shot and one-shot to evaluate and do not use few-shot for comparison, for we find it will make the context much longer and the result is not better than one-shot. There are also some other generative evaluation methods. Since they have reported in their papers that their performance is just similar with (Ke et al., 2023) GPT-4 or even worse than (Wang et al., 2023) GPT-4, we just use GPT-4 to represent these methods.",
            "score": 0.4915786162178849,
            "section_title": "B.3.3 Generative Baseline",
            "char_start_offset": 26299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1121
                }
            ],
            "ref_mentions": [
                {
                    "start": 1006,
                    "end": 1023,
                    "matchedPaperCorpusId": "265506605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050140380859375
        },
        {
            "corpus_id": "267740536",
            "title": "Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting",
            "text": "The results, illustrated in Figure 3, indicate a significant performance advantage of GPT-4 over GPT 3.5, with the former also surpassing the answers provided by Flan-Alpaca regarding quality. \n\nHow Each reference LLM Agrees with IW/CO Answers We check whether the LLM-generated answer is the same as the IW/CO answer [according to cosine similarity between extracted embeddings of two responses] \u2192 indicates the quality of the generated LLM answer for FEWL evaluation. \n\nResults about agreement with IW/CO answers are attached in Figure 4. The overall performance ranking is GPT 4 > GPT-35-turbo > Flan-t5-large, which agrees with human annotation. In the scenario where we evaluate LLMs using question-answering tasks in specific domains, i.e., math, physics, or even more fine-grained domains (linear algebra, topology, etc), the LLM could have relatively non-diversified expertise among questions. Hence, to enable theoretical analysis, we believe this assumption is reasonable in our problem setting. \n\nMoreover, this expertise score is a relevant score. To be more specific, \n\n(1) If there exist three LLMs (Flan-Alpaca, GPT 3.5, and GPT 4), that have significant diversified expertise, as evaluated by human annotators on 100 Truthful-QA questions (Figure 3) where the performance rank of the three models is GPT4 > GPT3 >> Flan-Alpaca. \n\nWe perform the expertise estimation of each LLM on the 100 questions, by showing the boxplot (Figure 4) of the number of agreements between an LLM and each Intentionally Wrong (IW) answer or COrrected (CO) answer. The performance ranking is the same as the human annotator. Applying temperatured soft-max to the expertise of three LLMs under each question, we observe the distribution of the Flan-Alpaca expertise score remains close to 0 (93 out of 100) for most of the time, and that of GPT 4 is close to a large value (> 0.9) for most of the time (95 out of 100).",
            "score": 0.49121262139031807,
            "section_title": "C.3 The Cost of Obtaining \u03bb",
            "char_start_offset": 31429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 195,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1912
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.026702880859375
        },
        {
            "corpus_id": "271432591",
            "title": "The Power of Combining Data and Knowledge: GPT-4o is an Effective Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of Lung Cancer",
            "text": "In addition to GPT-4o, we also evaluated GPT-3.5 (ChatGPT) as a baseline to investigate the capabilities of LLMs in using machine learning models' results to predict LNM. The experimental results for GPT-3.5 are presented in Table 4. It is evident that GPT-3.5 did not outperform the machine learning models, even when provided with the predicted probabilities from these models. This finding emphasizes the importance of advanced reasoning abilities and extensive medical knowledge in LLMs for clinical prediction tasks.",
            "score": 0.4901663712233344,
            "section_title": "LLMs' capabilities of using ML models",
            "char_start_offset": 14307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 521
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044525146484375
        },
        {
            "corpus_id": "273638468",
            "title": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs",
            "text": "LLMs, especially GPT-4 (Achiam et al., 2023) are widely used as evaluators or judges due to their strong alignment with human judgments (Zheng et al., 2023;Liu et al., 2023;Kumar et al., 2024;Bai et al., 2024). Building upon previous studies, we employ GPT-4 as a judge to detect bias in the content generated by LLMs on our FairMT-Bench. Specifically, we provide GPT-4 with the complete generation of LLMs in all turns of multi-turn dialogues, along with their corresponding original biased descriptions. We instruct GPT-4 to consider the context in assessing whether each turn of LLM generation contains or supports the biased description. The complete details of the GPT-4 evaluation instructions are provided in Appendix A.3. Additionally, we employ Llama-Guard-3-8B (Llama Team, 2024), an open-source content safety classification LLM, as an auxiliary detection tool. For the detection results, we report the bias rate, which is the proportion of multi-turn dialogue groups detected as biased relative to the total number of multi-turn dialogue groups. Finally, we conduct manual verification on GPT-4's evaluations. Detailed verification procedures and results are provided in Appendix A.4.",
            "score": 0.48987436320512023,
            "section_title": "EVALUATION",
            "char_start_offset": 13409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1196
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 156,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 156,
                    "end": 173,
                    "matchedPaperCorpusId": "229332119"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.166259765625
        },
        {
            "corpus_id": "268064056",
            "title": "EduQuick: A Dataset Toward Evaluating Summarization of Informal Educational Content for Social Media",
            "text": "Based on our observations, we offer the following suggestions to fellow researchers who rely on GPT-4 or other LLMs for evaluation tasks. Due to the necessity for thorough analysis and experiment design for each point, we only provide our insights and potential suggestions. \n\nFine-Tuning for Summarization: When feasible, consider fine-tuning your LLM on a dataset specifically tailored for summarization tasks. \n\nIterative Feedback Loop: Implement an iterative feedback mechanism that fosters collaboration between the LLM and human evaluators e.g., using a reward mechanism. See Stiennon et al. (2022) Objective Evaluation Metrics: Explore the pos-sibility of introducing objective evaluation metrics where the model provides scores based on mathematical formulas rather than relying solely on subjective criteria. \n\nComparative Evaluations: If you have access to multiple LLMs with similar capabilities, consider conducting comparative evaluations. Pair one model's generated output with another model's evaluation and vice versa. \n\nThe empirical study serves as a vital step in validating the quality and adherence of the GPT-4 generated TikTok content to the designated prompt design and example context.",
            "score": 0.48927194166410193,
            "section_title": "Recommendations for Enhancing GPT-4's Evaluation Competence",
            "char_start_offset": 24036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1210
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2313232421875
        },
        {
            "corpus_id": "271903501",
            "title": "See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses",
            "text": "In addition to GPT-4, we experiment with other closed-source LLMs and open-source LLMs on SC-G4, in both zero-shot and few-shot settings Baseline Systems. We compare Gemma-7B (Team et al., 2024), Phi3-7B-instruct (Abdin et al., 2024), Mixtral-8\u00d77B-instruct (Jiang et al., 2024), Gemini-1-pro (Team et al., 2023), Llama models (Touvron et al., 2023b;Dubey et al., 2024), Claude-3-Sonnet,6 and GPT-3.5-turbo and GPT-4o models (Achiam et al., 2023;Brown et al., 2020) with GPT-4 (OpenAI, 2023). \n\nEvaluation Data. For zero-shot experiments, we evaluate LLMs on the whole SC-G4 data. \n\nFor few-shot fine-tuning, we experiment under 2 settings as shown in We fine-tune the model on 8 A100 GPUs on our training sets for 5 epochs7 and choose the best checkpoint by their loss on validation sets. We use the AdamW optimizer (Loshchilov & Hutter, 2017) and set the learning rate as 2e-4. \n\nEvaluation Protocol. For GPT-4, we directly evaluate their performance when annotating the gold responses (subsection 3.3). However, it is not practical to conduct manual evaluation for all models, which can be costly. Therefore, we evaluate model performance via prompting GPT-4 (Liu et al., 2023) to compare the model response and gold response, i.e.: \n\nwhere l, q, h and r are output label (\"correct\" or \"incorrect\"), query, gold and model-generated responses, respectively. Appendix B presents the prompt in detail. We show that this evaluation method achieves a moderate-to-high F-1 score of around 90 in Appendix I.",
            "score": 0.48881403746690655,
            "section_title": "Experimental Setup",
            "char_start_offset": 13134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1234
                },
                {
                    "start": 1237,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1502
                }
            ],
            "ref_mentions": [
                {
                    "start": 445,
                    "end": 464,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017852783203125
        },
        {
            "corpus_id": "267740270",
            "title": "Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements",
            "text": "Considering the large number of test samples, we utilize GPT4 as the judge for evaluation, which has become widely accepted, as noted in several studies (Touvron et al., 2023b;Chiang et al., 2023;Dettmers et al., 2023;Liu et al., 2023b;Chiang and Lee, 2023). Research has demonstrated that the evaluations made by GPT-4 align well with human judgments (Zheng et al., 2023;Li et al., 2023c). 3 esponse Quality: \n\nThe evaluation of Response Quality follows Chen et al. (2023c); Li et al. (2023bLi et al. ( , 2024b)), which involves a detailed rating system for the responses generated by the model. This system compares responses generated by two different LLMs on various dimensions, including helpfulness, relevance, accuracy, and level of detail. We also address the issue of positional bias in the LLM judge system, as discussed in the studies by Ko et al. (2020); Wang et al. (2023b) by presenting models' responses in two separate sequences for evaluation by the LLM judge. We then analyze the responses for each instruction by comparing them through a \"Win-Tie-Loss\" system. Then the win score will be calculated for better comparison:",
            "score": 0.487948557622435,
            "section_title": "LLM Judge",
            "char_start_offset": 15578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1140
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 196,
                    "matchedPaperCorpusId": "258461287"
                },
                {
                    "start": 236,
                    "end": 257,
                    "matchedPaperCorpusId": "258461287"
                },
                {
                    "start": 849,
                    "end": 865,
                    "matchedPaperCorpusId": "216868500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053314208984375
        },
        {
            "corpus_id": "271915803",
            "title": "Diagnosing and Remedying Knowledge Deficiencies in LLMs via Label-free Curricular Meaningful Learning",
            "text": "Since LLMs have strong and broad capabilities, evaluating LLMs becomes a tough and widely concerned issue. Some works constructed benchmarks or evaluation data to evaluate LLMs from general and specialized perspectives, such as natural language understanding [22,36,67], reasoning [15,55,80], math [37,38], coding [47,77], etc. \n\nDifferent from utilizing benchmarks with objective questions, some works started utilizing LLMs to evaluate LLMs subjectively. Liu et al. [39] applied GPT-4 as an evaluator to assess the quality of generated text in various pesperctives (such as coherence) and achieved better alignment with human evaluators. Bai et al. [2] evaluated the performance of existing LLMs with self-evaluation and peer-evaluation, achieving more precise judgments of existing LLMs. Furthermore, Zheng et al. [79] devised a chat framework to evaluate LLMs based on the discussions among LLMs. \n\nDifferent from previous works, our work utilizes relative entropy to diagnose the knowledge deficiencies in LLMs with the help of a knowledge base. We also follow previous work to adopt objective benchmarks to evaluate the performance of enhanced LLMs.",
            "score": 0.4873427044864413,
            "section_title": "Evaluation of LLMs",
            "char_start_offset": 6933,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1155
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 263,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "245385766"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 651,
                    "end": 654,
                    "matchedPaperCorpusId": "259095491"
                },
                {
                    "start": 817,
                    "end": 821,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382080078125
        },
        {
            "corpus_id": "268532413",
            "title": "TnT-LLM: Text Mining at Scale with Large Language Models",
            "text": ". We first calculate the coverage of the LLM-generated taxonomies on the BingChat-Phase1-L-Multi dataset, where both LLM systems achieve very high coverage (>99.5%) on both user intent and conversational domain taxonomies.\n\nWe then conduct the accuracy and relevance evaluation tasks to assess the quality of the taxonomies generated by different methods on the small English-only evaluation dataset BingChat-Phase1-S-Eng.We report the inter-rater agreement (Cohen's Kappa [6] between two raters and Fleiss' Kappa [7] among multiple raters) in Table 1.The agreement is moderate ( > 0.4) on intent and domain accuracy as well as intent relevance, while the agreement on domain relevance is fair (Fleiss \u2032  = 0.379). 3Interestingly, for the tasks with moderate agreement, the GPT-4 evaluator agrees more with the human majority than the humans do among themselves.This suggests that GPT-4 can be a consistent and reliable evaluator.\n\nFigure 4a shows the main results on label accuracy and use case relevance from human evaluations on BingChat-Phase1-S-Eng.We observe our TnT-LLM using GPT-4 outperforms other 0.50 0.75 1.00 human evaluation GPT-4 (TnT-LLM) GPT-3.5-turbo(TnT-LLM) ada2 + GPT-4 ada2 + GPT-3.5-turboinstructor-xl + GPT-4 instructor-xl + GPT-3.methods in most cases.Compared to GPT4, we find that GPT-3.5-Turbotends capture conversation topics (domains) well, but often fails to generate labels that are aligned with the user intent instruction.Likewise, we notice that some embedding methods (ada2 + GPT-4, instructor-xl + GPT-4) perform well in terms of producing accurate domain labels, on par with TnT-LLM instantiated with GPT-3.5-Turbo, but fail to capture the user intent behind the conversations.",
            "score": 0.4873360199619323,
            "section_title": "Results",
            "char_start_offset": 25837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 224,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 930
                },
                {
                    "start": 932,
                    "end": 1054
                },
                {
                    "start": 1054,
                    "end": 1168
                },
                {
                    "start": 1168,
                    "end": 1211
                },
                {
                    "start": 1211,
                    "end": 1255
                },
                {
                    "start": 1255,
                    "end": 1277
                },
                {
                    "start": 1277,
                    "end": 1321
                },
                {
                    "start": 1321,
                    "end": 1456
                },
                {
                    "start": 1456,
                    "end": 1715
                }
            ],
            "ref_mentions": [
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "15926286"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "145183399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060760498046875
        },
        {
            "corpus_id": "262216950",
            "title": "Frustrated with Code Quality Issues? LLMs can Help!",
            "text": "The issue of plausible but incorrect fixes is well-known [36,43]. CORE may generate code that passes the static check (a plausible fix) but changes semantics of the input code in unintended ways. The developer can review the statically-validated code-revisions to filter out such cases. Unit tests can also help catch such cases, but they are not always available or may themselves be incomplete. LLMs have been shown to be effective in assessing and supervising quality of output from other LLMs [13,31], thereby helping reduce the efforts required for human review. Using LLMs, especially GPT-4, for evaluating code generations has been attempted recently. Olausson et al. [39] also have a dual LLM setup, where they use the feedback from GPT-4 in the form of critique to modify the prompt of the proposer LLM for code generation tasks. Zhuo [57] constructs an elaborate prompt for GPT-3.5-Turbo to perform two aspects of evaluation of code generations, namely, code usefulness and evaluation-based functional correctness. Inspired by these findings, to reduce the burden on the developer, we employ a second instance of LLM (GPT-4) as a ranker to score the candidates produced by the proposer LLM based on (1) the correctness of issue resolution, and (2) preserving the functional correctness of the original code. The code generation datasets studied in [57] consist mostly of small code snippets, unlike our setting where we use large real source code files. We work with code diffs in the Ranker LLM prompt, and in our investigations, GPT-4 is substantially better in terms of reasoning with code diffs compared to GPT-3.5-Turbo that Zhuo [57] employs.",
            "score": 0.48687353950498957,
            "section_title": "LLMs as verifiers",
            "char_start_offset": 47186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1658
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "5987616"
                },
                {
                    "start": 61,
                    "end": 64,
                    "matchedPaperCorpusId": "7597529"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457275390625
        },
        {
            "corpus_id": "266551253",
            "title": "LLM-SAP: Large Language Models Situational Awareness-Based Planning",
            "text": "Experiments assess the feasibility of using LLMs, like GPT-4 and Claude-2, to score FSM plans and compare their rankings with human evaluations, as shown in Appendix Table 9. The models are tested with ranking FSM plans in pairs, as well as in groups of 4 and 6. These rankings are then compared to expert human rank orders to measure accuracy. Tests find both GPT-4 and Claude-2 could rank FSM pairs with 75.7% agreement to human ranking, evidencing reliability for comparative evaluation. However, their accuracy significantly decreases when ranking groups of 4 or more FSMs. Table 10 and Fig. 6 in Appendix B show that GPT-4 and Claude-2 align most closely with human judgment when evaluating outputs generated by the models themselves. For example, GPT-4's scoring of its own FSMs closely matches expert rankings. This demonstrates that LLMs can provide accurate comparative assessments, particularly for outputs from their own model family. The experiments highlight the potential of LLMs to serve as evaluators that mimic human appraisals of planning formalisms. By focusing on relative rather than absolute assessments, variability is minimized.",
            "score": 0.48656965525009926,
            "section_title": "LLM Evaluators",
            "char_start_offset": 16982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1152
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2481689453125
        },
        {
            "corpus_id": "258352761",
            "title": "ICE-Score: Instructing Large Language Models to Evaluate Code",
            "text": "Data Contamination Evaluations on recent closed-source LLMs have been criticized for the possibility of data contamination (Aiyappa et al., 2023), where the model may have already seen the evaluation datasets during training, due to the opaque training details of these models. For instance, Kocmi and Federmann (2023) conducted an empirical study on a few closed-source LLMs, including GPT-3.5, and suggested that LLMs are the state-of-the-art evaluators of translation qual-ity, based on the evaluation of the WMT22 Metric Shared Task (Freitag et al., 2022). However, as most of the evaluated models were trained on data prior to 20225 , it is highly likely that these models have been trained with some human-rated translation quality data. Similarly, G-EVAL (Liu et al., 2023) shows that GPT-3.5 and GPT-4 are the stateof-the-art evaluators of natural language generation (NLG) with the evaluation of three NLG datasets. However, as these human-annotated datasets were released before 2021, it is probable that they were included in the training data of GPT-3.5 and GPT-4. In contrast, our work is minimally impacted by data contamination, as we report the data release year in Table 6. Our analysis suggests that only CoNaL and HumanEval (Python) datasets may have been contaminated, and it is unlikely that GPT-3.5 has seen any human annotation or generated code during training. \n\nHuman-aligned Evaluation Beyond Code Generation While our study has shown that LLMs can achieve state-of-the-art performance in evaluating the functional correctness and usefulness of generated source code, the question remains as to whether LLMs can be utilized to evaluate code intelligence tasks beyond code generation. Allamanis et al. (2018) have identified several downstream applications such as code translation, commit message generation, and code summarization. While some studies have investigated the human evaluation of these tasks, none of them have released the annotation data or fully described the human evaluation criteria.",
            "score": 0.48619195497896267,
            "section_title": "Discussion",
            "char_start_offset": 19626,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 537,
                    "end": 559,
                    "matchedPaperCorpusId": "256461176"
                },
                {
                    "start": 1711,
                    "end": 1734,
                    "matchedPaperCorpusId": "207591052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285400390625
        },
        {
            "corpus_id": "267301349",
            "title": "PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
            "text": "We study the pairwise win rate among various LLMs evaluated by PROXYQA and compare the results with human evaluation to validate the effectiveness of PROXYQA. \n\nSetup Five well-educated postgraduate students are engaged; all have not participated in annotating the meta and proxy-questions of PROXYQA; they are required to score and rank the randomly sampled reports generated by different LLMs. The scoring guideline is shown in Appendix A.4. We sampled ten meta-questions from PROXYQA and employed four LLMs to generate comprehensive reports. As a further comparison, we also follow the settings in MT-Bench (Zheng et al., 2023) that adopt LLM-as-judges, which directly rate the generated report based on the scoring guideline. \n\nSimilarly, we utilize GPT-Seperate (GPT-S), which  evaluates a single report at a time, and GPT-Batch (GPT-B), which evaluates and compares multiple reports simultaneously, to score and rank each report. Given that five human evaluators are involved in the comparison, we ensure fairness and robustness by requiring GPT-S, GPT-B, and our proposed PROXYQA to evaluate each report five times. We then calculate the average win rate based on the pairwise comparison. \n\nResult GPT-as-judges over-confident on the contents generated by GPT models, while ProxyQA's choice is highly correlated with humans. As shown in Figure 5, majority of the evaluation results highly correlated with the human's choices. Specifically, evaluators generally recognize the quality of the reports generated by GPT-4-Turbo and New Bing, i.e., their win rates are much higher than those of Vicuna and LLama2-Chat. It is worth noting, however, that GPT-S and GPT-B exhibit overconfidence in the quality of the reports generated by GPT-4-Turbo compared to New Bing. In contrast, both human evaluators and ProxyQA exhibit a preference for New Bing over GPT-4-Turbo. This outcome attests to the effectiveness of ProxyQA and demonstrates the correlation between ProxyQA and human evaluation.",
            "score": 0.48612539199477744,
            "section_title": "Win Rate",
            "char_start_offset": 19866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 610,
                    "end": 630,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339111328125
        },
        {
            "corpus_id": "264591440",
            "title": "Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction Following: A Case Study of Arabic",
            "text": "\u2022 The use of an LLM as a judge for evaluating LLMs has issues related to bias. As reported in (Zheng et al., 2023), in pairwise comparisons, the judge tends to favor its own answers compared to other models. For example, that study shows that GPT-4 favors itself with 10% higher win rate and Claude-V1 favors itself with 25% higher win rate. On the other hand, GPT-3.5 does not appear to favor itself. \n\n\u2022 Using GPT-4 as the judge and as an LLM under study might favor it in the scores. However, the score margin to the closet competitor is big enough to make any potential deviation in the scores insignificant, and we adhered to the original MT-Bench setup in the choice of judge in order to mirror the results and measure multilingual competency. \n\n\u2022 Other LLM judges than GPT-4 could be considered for evaluating the responses. However, the choice of alternative judges is currently rather limited when considering Arabic. The proficiency of models such as Claude or Bard in Arabic are not yet proven. Alternatively, multiple LLMs could be used for this task. A voting judgment mechanism could be considered over multiple open LLMs. \n\n\u2022 While GPT-4 exhibits competence in Arabic, its proficiency in the language falls short of its mastery of English. This discrepancy may have had an impact on certain aspects of our analyses, especially when comparing Arabic results to English results. \n\n\u2022 We used the same judgment prompt as in the English MT-Bench for the purpose of consistency. However, we note that the judgment prompt does not acknowledge important aspects such as safety and harmlessness of LLM responses. Also, the MT-score is a metric that combines multiple dimensions such as relevance, helpfulness, and creativity together to give an aggregate verdict. It might be useful to analyze model performance separately on these dimensions for a better understanding.",
            "score": 0.48593104924836583,
            "section_title": "Judging",
            "char_start_offset": 22417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1876
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0304412841796875
        },
        {
            "corpus_id": "267320882",
            "title": "Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios",
            "text": "The main results are illustrated in Table 2. As seen, we can have the following observations: GPT-4 achieves the best performance. Among all evaluated LLMS, GPT-4 demonstrates superior performance, achieving an overall score of 76.04% on Chinese-dataset and 74.58% on Englishdataset, and with particularly impressive results in the realms of tool creation and tool usage, setting the pilot of acting as skillful tool agent. Besides, Mistral-7B, GPT-3.5, and Qwen-72B also get competitive performance, and Mistral-7B gets the best of two worlds between exhibiting good tool utilization abilities while keeping efficient model scale. The larger the model scale, the better the tool utilization ability. Regarding open-source LLMs, we evaluate models at three scales, approximately 7B, 13B, and 70B parameters. The results indicate that tool utilization ability improves as the model scale increases. This finding aligns with prior research, which has shown that increasing of the model parameters bolsters the capabilities of LLMs (Chung et al., 2022;Wei et al., 2022). While most open-source LLMs demonstrate competitive planning performance, a notable gap still persists in tool creation and usage compared to closed-source LLMs, especially for smaller-scale models. This discrepancy may stem from the more intricate JSON output format requirements inherent in the evaluation dimensions of tool creation and usage. These dimensions are critical areas to focus on for enhancing LLMs' abilities in tool utilization. Language affects the tool utilization ability. Although most of LLMs support both English and Chinese, their capabilities in these two languages differ. For open-source LLMs, in most cases, the Chinese-orientated LLMs (e.g. Qwen, Baichuan2) show better tool utilization ability on Chinese-dataset, while English-orientated LLMs (e.g. LLaMA2, Vicuna) perform better on Englishdataset. We attribute this discrepancy to variations in multilingual understanding resulting from differ- ences in the proportions of training data. However, despite their English orientation, GPT-3.5 and GPT-4 actually exhibit slightly weaker performance on English-dataset.",
            "score": 0.4857975856658656,
            "section_title": "Main Results",
            "char_start_offset": 17945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2164
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0234222412109375
        },
        {
            "corpus_id": "275788916",
            "title": "Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review",
            "text": "On a critical note, Kabir et al. [75] analyzed ChatGPT's responses to 517 StackOverflow programming questions, revealing that 52% of the answers contained incorrect information and 77% were verbose. \n\nReliability & Validity of LLMs (10.46%, N=16): This domain focuses on evaluating and improving LLM outputs. The first stream of work includes analyses determining the validity of applying LLMs to specific contexts. For example, He et al. [58] compared GPT-4 and Mechanical Turk pipelines for sentence labeling tasks from scholarly articles, showing that combining crowd and GPT-4 labeling increases accuracy. Another example is Kabir et al. [75], evaluating the validity of using LLMs' to answer programming questions. The second stream involves tools designed to enhance the reliability or validity of LLM outputs. For instance, HILL identifies and highlights hallucinations in LLM responses, allowing users to handle responses with greater caution [93]. EvalLM enables interactive evaluation of LLM outputs based on user-defined criteria across multiple prompts [81]. AI Chain is a visual programming tool for crafting LLM prompts, which improved the quality of task outcomes as well as the transparency, controllability, and the sense of collaboration when interacting with the black-box LLMs [168]. \n\nWell-being & Health (9.15%, N=14): This domain refers to the management and prevention of health-related disorders and illnesses, or interactions with health data or with healthcare providers. 5  One thread of work involves assisting practitioners in providing better care. For example, Yang et al. [176] designed a GPT-3-based decision support tool that draws on the biomedical literature to generate AI suggestions. Yildirim et al. [178] worked with radiologists to explore the design space for incorporating LLMs into radiology. Another thread involves support for patients in self-tracking, self-diagnosing, and self-managing their illnesses. For instance, Sharma et al. [141] used a fine-tuned GPT-3 model to improve selfguided mental health interventions through cognitive restructuring, a technique to overcome negative thinking. MindfulDiary leveraged GPT-4 to support psychiatric patients' journaling [78].",
            "score": 0.4854813580724897,
            "section_title": "Risks to Society",
            "char_start_offset": 20495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 37,
                    "matchedPaperCorpusId": "260611083"
                },
                {
                    "start": 439,
                    "end": 443,
                    "matchedPaperCorpusId": "268032396"
                },
                {
                    "start": 642,
                    "end": 646,
                    "matchedPaperCorpusId": "260611083"
                },
                {
                    "start": 951,
                    "end": 955,
                    "matchedPaperCorpusId": "268358769"
                },
                {
                    "start": 1065,
                    "end": 1069,
                    "matchedPaperCorpusId": "262459331"
                },
                {
                    "start": 1297,
                    "end": 1302,
                    "matchedPaperCorpusId": "238353829"
                },
                {
                    "start": 1605,
                    "end": 1610,
                    "matchedPaperCorpusId": "258217332"
                },
                {
                    "start": 1740,
                    "end": 1745,
                    "matchedPaperCorpusId": "267783109"
                },
                {
                    "start": 1981,
                    "end": 1986,
                    "matchedPaperCorpusId": "264439507"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.097900390625
        },
        {
            "corpus_id": "259924919",
            "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision",
            "text": "Several studies directly utilize this state-of-the-art LLM of Ope-nAI, since it demonstrates excellent performance across a wide range of generation and reasoning tasks. For example, Xie et al. utilize GPT-4 to generate fuzzing inputs [67], while Vikram et al. employ it to generate property-based tests with the assistance of API documentation [34]. In addition, some studies conduct experiments using both GPT-4 and Chat-GPT or other LLMs to provide a more comprehensive evaluation of these models' performance. In their proposed LLMempowered automatic penetration testing technique, Deng et al. find that GPT-4 surpasses ChatGPT and LaMDA from Google [62]. Similarly, Zhang et al. find that GPT-4 shows its performance superiority over ChatGPT when generating the fuzz drivers with both the basic query strategies and enhanced query strategies [66]. Furthermore, GPT-4, as a multi-modal LLM, sets itself apart from the other mentioned LLMs by showcasing additional capabilities such as generating image narratives and answering questions based on images [149]. Yet we have not come across any studies that explore the utilization of GPT-4's image-related features (e.g., UI screenshots, programming screencasts) in software testing tasks.",
            "score": 0.4845436653715771,
            "section_title": "LLM Models",
            "char_start_offset": 65055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1241
                }
            ],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "259501083"
                },
                {
                    "start": 654,
                    "end": 658,
                    "matchedPaperCorpusId": "260887370"
                },
                {
                    "start": 1057,
                    "end": 1062,
                    "matchedPaperCorpusId": "265157736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0301513671875
        },
        {
            "corpus_id": "273661912",
            "title": "CARMO: Dynamic Criteria Generation for Context-Aware Reward Modelling",
            "text": "We utilized the criteria C generated by GPT-4 to fine-tune small language model, enabling it to autonomously generate evaluation factors similar to those produced by GPT-4. Additionally, we fine-tuned another small language model on the feedback generated by GPT-4, allowing it to produce feedback F and scores S that closely align with GPT-4's assessments. This fine-tuning process enhances the model's ability to evaluate responses and generate feedback with a level of quality similar to GPT-4, enabling autonomous and high-quality evaluation of LLM outputs. This approach also facilitates open-sourcing the models, as we are no longer reliant on GPT-4's closed nature. Moreover, despite a negligible drop in performance compared to GPT-4, our fine-tuned models-using only 13B parameters-outperform many state-of-the-art (SOTA) open-source models, which typically operate with much larger architectures (e.g., 175B parameters). This efficiency gain makes our solution highly competitive in terms of both performance and scalability.",
            "score": 0.48422145804060335,
            "section_title": "Fine-Tuning Models on Generated Factors and Feedback",
            "char_start_offset": 13197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1035
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.200927734375
        },
        {
            "corpus_id": "267587759",
            "title": "Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation",
            "text": "GPT-based technologies allow modern researchers to analyze their data with the help of prompts. A researcher can use GPT, LLM, and associated technologies for data analysis and critical research by leveraging their ability to perform complex textual analysis and pattern recognition in large datasets, aiding in tasks like detecting nuanced patterns in financial texts [23,24,75,76]. These models also excel at solving intricate problems, such as those in discrete mathematics, showing significant improvements in advanced versions like GPT-4. Furthermore, they provide innovative methodologies for analyzing statistical data, offering insights and predictions with higher efficiency compared to traditional methods. For example, the paper in [23] presents a novel system for generating data visualizations directly from natural language queries using LLMs like ChatGPT and GPT-3. The system, named Chat2VIS (as demonstrated in Figure 7), demonstrates efficient and accurate end-to-end solutions for visualizing data based on user queries. It addresses the challenge of interpreting natural language in data visualization and utilizes advanced LLMs to convert free-form natural language into appropriate visualization code. The study includes case studies and comparisons of GPT-3, Codex, and ChatGPT performances in generating visualizations from various types of queries, highlighting their potential in rendering visualizations from natural language, even when queries are ambiguous or poorly specified. The study in [24] explores the application of GPT-3 for statistical data analysis. It proposes a method for analyzing large datasets using GPT-3 to predict insights from calculated statistics. The research addresses the limitations of existing methods and compares traditional statistical analysis with machine learning approaches using GPT-3. It includes experiments on different datasets like e-commerce sales, heart attacks, and telecom churn rates, assessing GPT-3's performance in providing insights and its accuracy compared to traditional methods. The study also discusses the pros and cons of using GPT-3 in research, focusing on performance, accuracy, and reliability.",
            "score": 0.48303233660577094,
            "section_title": "Research Analysis",
            "char_start_offset": 35696,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 376,
                    "matchedPaperCorpusId": "256878028"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "256878028"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012481689453125
        },
        {
            "corpus_id": "267320303",
            "title": "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate",
            "text": "Results Table 4 compares the agreement rate between SCALEEVAL's meta-evaluation and each LLM evaluator across criteria and scenarios. We observe that gpt-4-turbo, when serving as an evaluator, has the highest agreement rates with our meta-evaluation, particularly in the scenarios of brainstorming, dialog, and ODG with the helpfulness criterion. It stands out with the highest overall average score of 0.780. However, our selected open-source model evaluator, auto-j, outperforms gpt-4-turbo in evaluating coding questions based on the helpfulness criterion. In addition, it exhibits the highest agreement rate with our metaevaluation in the judgement scenario, according to the helpfulness criterion, indicating it as the most capable evaluator in this setting. It also achieves comparable results with other closed-source models like claude-2 and gpt-3.5-turbo in most of the other scenarios. \n\nWhile gpt-4-turbo performs the best as an evaluator in a majority of scenarios, it is not necessarily the best choice when we take into consideration its relatively high API costs. In fact, both the more affordable version (gpt-3.5-turbo) and our selected free, open-source model (auto-j) show comparable performance in scenarios like judgement and writing. For coding-related evaluations, the slightly less expensive claude-2 could be a more cost-effective alternative to gpt-4-turbo. \n\n8 Exp-III: Meta-Evaluation with Criteria Prompt Format Variations Q3: How do the qualities of criteria prompts influence the robustness of LLMs as evaluators in different scenarios? Prior studies have revealed that variations in prompts can substantially affect the behavior of LLMs, particularly with the text they generate. With this in mind, we define various formatted criteria for evaluating LLM responses under each scenario. This approach aims to examine the extent to which different formats of criteria prompts influence both the performance and robustness of LLMs as evaluators. \n\nSetup We define five variations of the same criteria prompts: shortened, gibberish, shuffled, flipped, and masked (see Table 7 under Appendix A for detailed format).",
            "score": 0.4828997047694916,
            "section_title": "Exp-II: Meta-Evaluation vs. LLM Evaluators",
            "char_start_offset": 27455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1974
                },
                {
                    "start": 1977,
                    "end": 2142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2213134765625
        },
        {
            "corpus_id": "267783118",
            "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
            "text": "Automatic Evaluation We evaluate the generated surveys using a range of automatic metrics including ROUGE, BERTScore (Zhang* et al., 2020), MoverScore (Zhao et al., 2019), UniEval (Zhong et al., 2022) and BARTScore (Yuan et al., 2021). \n\nTab. 1 provides an overview of results for the following LLMs: LLaMa2 (13B, 70B), PaLM2 (textbison), GPT-3.5 (Turbo-0613) as well as GPT-4 (0613) across different prompt settings. We first notice that GPT-4 consistently outperforms other baselines, obtaining a significant improvement of around 2% to 20% when enhancing prompts. Specifically, GPT-4 OSP achieves the top spot under most situations. However, it is not to say that prompt enrichment always yields positive results. For instance, in the case of LLaMa2, oneshot and description prompts perform better than OSP. As for PaLM2, four types of prompts obtain similar results. When we add external knowledge (GPT-4 OS+IR), there is some improvement compared to GPT-4 OS. As our primary goal is to study the extent of knowledge LLMs possess in this task, we mainly focus on analyses in settings without external data. However, additional analysis about GPT-4 OS+IR setting can be found in Appendix B. \n\nHuman and GPTs Evaluation We employ two NLP experts, GPT-4 and G-Eval (Liu et al., 2023a) to evaluate surveys generated by the best GPT-4 OSP setting, focusing on 6 perspectives: Readability, Relevancy, Hallucination, Completeness, Factuality. Both GPT models and humans are required to score each aspect on a scale from 1 to 5, following the same guidelines. The detailed guidance can be found in Appendix A. It's important to note that we implement a pre-selection stage in the choice of human experts (Appendix A). Tab. 2 shows that both human experts and GPTs agree that the generated surveys perform well across most aspects, though the completeness exhibits marginally lowest scores.",
            "score": 0.48255297726433133,
            "section_title": "Evaluation Metrics",
            "char_start_offset": 6836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1885
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 170,
                    "matchedPaperCorpusId": "202540033"
                },
                {
                    "start": 1266,
                    "end": 1285,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1385498046875
        },
        {
            "corpus_id": "273228278",
            "title": "AutoFeedback: An LLM-based Framework for Efficient and Accurate API Request Generation",
            "text": "Accuracy. The accuracy is calculated by dividing the number of API request samples that meet user requirements by the total number of samples. Due to the lack of API servers for API Bank and MP-API, we consider the generated API request that is identical to the standard answers as meeting user requirements. For the ToolAlpaca-single and -mix, we followed the approach in [6] which passes API requests and responses to GPT-4 for evaluation. \n\n2) Process Correctness. For the scenario of multiple API calls (ToolAlpaca-mix), the call unrelated to the task could lead to unnecessary cost. We use it to measure the optimality of API request sequences. Same as [6], we employ GPT-4 to assess whether the API requests generated by the LLM are consistent with the standard answer. \n\n3) Overhead. Existing commercial LLMs all charge for the number of tokens generated, and [28] uses the token number to denote the generated overhead. We further measure the interaction overhead using 1 N N i=1 tokeni Accuracy , which indicate the average overhead required for 1% improvement in accuracy. We used the same evaluation prompt in the paper [6] for Accuracy and Process Correctness. In addition, we selected three experts to evaluate the results under the same conditions to demonstrate the effectiveness of GPT-4 evaluation.",
            "score": 0.4824270030504974,
            "section_title": "1)",
            "char_start_offset": 20602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1315
                }
            ],
            "ref_mentions": [
                {
                    "start": 867,
                    "end": 871,
                    "matchedPaperCorpusId": "254564450"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.130859375
        },
        {
            "corpus_id": "267095281",
            "title": "Raidar: geneRative AI Detection viA Rewriting",
            "text": "Yelp Arxiv Reviews Abstract % that Machine Generated Text are Preferred Human Written Text 53.3% 26.7% \n\nRobustness of Our Method to LLM Fine-tuning. We run the experiment on GPT-3.5-Turbo and GPT-4-Turbo. GPT-4-Turbo can be roughly treated as a realistic, advanced, continual fine-tuned LLM on new real-world data from GPT-3.5-Turbo. We show the results in Table 9. Our method is robust to LLM finetuned. Despite a drop in detection performance, it still outperforms the established state-of-the-art zero-shot detector.",
            "score": 0.4822902438951083,
            "section_title": "Methods",
            "char_start_offset": 35912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 105,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 520
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0162811279296875
        },
        {
            "corpus_id": "273532563",
            "title": "An Adaptive Framework for Generating Systematic Explanatory Answer in Online Q&A Platforms",
            "text": "To comprehensively evaluate the effectiveness of our answers, we designed four types of evaluation metrics: 4.2.1 LLM-Based Evaluation. Following the recent trend of using LLMs to judge the quality of output text (especially in reference-free evaluation settings), we employed GPT-4 to assess the quality of generated answers. The evaluation is based on five different criteria. We then ask the evaluation model to rate each answer on a 5-point Likert scale or to make pairwise comparisons between the ideas generated by different models. Detailed human-crafted criteria used to guide the evaluation are provided in Table 3.",
            "score": 0.480146237821812,
            "section_title": "Evaluation Metrics",
            "char_start_offset": 20837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 624
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.433349609375
        },
        {
            "corpus_id": "271212662",
            "title": "CIBench: Evaluating Your LLMs with a Code Interpreter Plugin",
            "text": "Fig. 4. Evaluation modes: In end-to-end mode, the LLM addresses the user's question (bottom) within the context of its response, while in oracle mode, it answers the user's question (bottom) within the context of ground truth. \n\nTopic Candidates Generation After selecting modules, we prompt (Appendix B) GPT-4 to summarize 50 topics for each module, to encapsulate the vast majority of the module's functionalities, thereby offering precise guidance for the subsequent generation of more targeted questions. \n\nTasks Generation and Refinement We sample a module and topic, then prompt GPT-4 to generate questions and code based on the prompt in Fig. 3. The prompt is designed to enable GPT-4 to generate a Jupyter notebook with sequential steps and various outputs, including numeric answers, structured output, and visualizations, mirroring real-world scenarios. Despite our request for concise descriptions, the generated content may lack conciseness and continuity. To address this, we undertake iterative refinement of these tasks. This process involves presenting both good and bad cases, along with additional modifications, to enhance the quality of questions and reduce ambiguity. Details of prompts used for refinement are in Appendix B. \n\nHuman Refinement Despite the excellent capabilities of LLMs, we found that the generated tasks often focus on typical datasets, such as Titanic and Iris, which limits task diversity. Additionally, ensuring perfect accuracy in question descriptions and result correctness is challenging. \n\nTo enhance diversity, we summarize template tasks based on the generated tasks and existing highquality tutorials available in various Python libraries (Fig. 11). With minor modifications, these template tasks can be adapted to a wide range of datasets. To further diversify the benchmark, we collect new datasets through two approaches: 1) We prompt GPT-4 to generate datasets tailored to different templates, leveraging GPT-4's ability to flexibly specify characteristics and data attributes. \n\n2) We incorporate the latest datasets from the past year, ensuring authenticity and diversity, and significantly reducing the likelihood that the model has previously encountered this data. \n\nTo improve quality, we employ several experts who use a code interpreter for manual double-checking.",
            "score": 0.47946027210351316,
            "section_title": "End-to-End Mode Oracle Mode",
            "char_start_offset": 9032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2224
                },
                {
                    "start": 2227,
                    "end": 2327
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2022705078125
        },
        {
            "corpus_id": "271328602",
            "title": "Human-Interpretable Adversarial Prompt Attack on Large Language Models with Situational Context",
            "text": "Our methodology discussed in later sections produces responses from different LLMs which are then evaluated on a harmfulness scale of 1-5 using GPT-4 Judge by Qi et al. (2023). Zeng et al. (2024) also evaluate the harmful responses in their research using GPT-4 Judge.",
            "score": 0.47808638561092043,
            "section_title": "A.2 Additional Related Work",
            "char_start_offset": 18680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 268
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00908660888671875
        },
        {
            "corpus_id": "260735554",
            "title": "An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures",
            "text": "Is using LLMs worth it in this context? We found that both LLMs in our experiment were capable of simpler forms of analysis, such as distinguishing whether a vulnerability was actually exploited. However, for more complex questions that require some amount of context or judgment, neither LLM achieved a high level of agreement with the CNCF analysts or our manual raters. We believe the current generation of off-the-shelf LLMs does not offer a high enough level of agreement with expert judgment to make it a useful assistant in this context. One potential path to improving performance is fine-tuning the LLM using baseline knowledge such as this catalog, and then applying it on future issues [28]. \n\nWill LLMs be a viable alternative to manual analysis in the future? In the past few years, OpenAI's GPT models have advanced from simple tasks (GPT-1, GPT-2) to the performance reported here (GPT-3.5). The recent GPT-4 model is more impressive still [32]. We expect the next generation of LLMs will be suitable aids or replacements for this class of manual analysis. \n\nFuture Work. The scope of this analysis could be broadened to encompass additional LLMs, such as Claude [9] and \n\nCohere [19]. Additional prompt engineering, and tailoring the prompts per LLM, might improve the accuracy of the results. Lastly, the analysis could be extended to include a wider range of articles and failures beyond those found in the CNCF catalog [6,7].",
            "score": 0.4779284577388925,
            "section_title": "DISCUSSION",
            "char_start_offset": 25887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1444
                }
            ],
            "ref_mentions": [
                {
                    "start": 697,
                    "end": 701,
                    "matchedPaperCorpusId": "265223070"
                },
                {
                    "start": 1438,
                    "end": 1441,
                    "matchedPaperCorpusId": "252111147"
                },
                {
                    "start": 1441,
                    "end": 1443,
                    "matchedPaperCorpusId": "250089299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036712646484375
        },
        {
            "corpus_id": "257804696",
            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "text": "Will G-EVAL prefer LLM-based outputs? One concern about using LLM as an evaluator is that it may prefer the outputs generated by the LLM itself, rather than the high-quality human-written texts. To investigate this issue, we conduct an experiment on the summarization task, where we compare the evaluation scores of the LLM-generated and the human-written summaries. We use the dataset collected in Zhang et al. (2023), where they first ask freelance writers to write high-quality summaries for news articles, and then ask annotators to compare human-written summaries and LLM-Figure 2: Averaged G-EVAL-4's scores for humanwritten summaries and GPT-3.5 summaries, divided by human judges' preference. generated summaries (using . \n\nThe dataset can be divided in three categories: 1) human-written summaries that are rated higher than GPT-3.5 summaries by human judges, 2) human-written summaries that are rated lower than GPT-3.5 summaries by human judges, and 3) human-written summaries and GPT-3.5 summaries are rated equally good by human judges. We use G-EVAL-4 to evaluate the summaries in each category, and compare the averaged scores. 2 he results are shown in Figure 2. We can see that, G-EVAL-4 assigns higher scores to humanwritten summaries when human judges also prefer human-written summaries, and assigns lower scores when human judges prefer GPT-3.5 summaries. However, G-EVAL-4 always gives higher scores to GPT-3.5 summaries than human-written summaries, even when human judges prefer humanwritten summaries. We propose two potential reasons for this phenomenon: \n\n1. NLG outputs from high-quality systems are in natural difficult to evaluate. The authors of the original paper found that inter-annotator agreement on judging human-written and LLM-generated summaries is very low, with Krippendorff's alpha at 0.07.",
            "score": 0.4776879202542541,
            "section_title": "Analysis",
            "char_start_offset": 15229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1833
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12158203125
        },
        {
            "corpus_id": "263829791",
            "title": "Generative Judge for Evaluating Alignment",
            "text": "We refer to this test set as Eval-C, with 58\u00d74 = 232 query-response pairs. \n\nTask III: Overall Rating for Single Response (Eval-R) In this task, we evaluate the usefulness of the final rating for single-response evaluation in two ways: (1) The first is to use the ratings as verbal \"rewards\" to help improve the base policy models through the Best-of-N selection (Lightman et al., 2023;Gao et al., 2023), i.e., selecting the best response among the first N candidates with the assigned rewards, and use GPT-4 to grade the selected response. Generally, a more reliable model will select a better response with a higher GPT-4 rating more often. \n\n(2) The second is to calculate the response-level correlations between model-generated ratings and GPT-4 ratings. To save cost, we only collect the GPT-4 ratings on the previous \"best-of-N \" responses. The test set for this task is built on the basis of Eval-C by sampling 2 out of 4 queries for each scenario. We ask two different base LLMs (LLaMA-2-chat-7B and Vicuna-7B-v1.5) to generate 32 responses for each query through uniform sampling (temperature set as 1.0). We refer to this test set as Eval-R, with 58\u00d72=116 queries and 116\u00d732=3,712 query-response pairs for each base LLM.",
            "score": 0.4771570426451049,
            "section_title": "TASK AND TEST SET",
            "char_start_offset": 17904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 77,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1230
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 403,
                    "matchedPaperCorpusId": "252992904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1302490234375
        },
        {
            "corpus_id": "267759866",
            "title": "Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues",
            "text": "Therefore, we analyzed whether the same evaluator gave higher scores to counsellors' utterances or GPT-4-generated utterances when the context was identical. In cases where the counsellor made multiple consecutive utterances before the client responded, the average rating for all utterances was used as the rating for the counsellor's utterance. \n\nThe results of the comparison are shown in Table 6. We found that more counsellors' utterances were rated as superior compared to GPT-4 utterances that were rated superior (34.8 % vs 30.5%). However, the difference was small, and 34.7% of the utterances were rated as equal. We also confirmed that the quality of GPT-4 responses was very close to that of the human counsellors' responses. \n\nCounsellor win 284 (34.8%) GPT-4 win 249 (30.5%) Tie 283 (34.7%) \n\nTable 6 Comparison of ratings for responses to the same context \n\nWe confirmed that LLM-generated responses were competitive with human responses despite having not used few-shot examples except for given annotated context in our prompt for GPT-4. Given that it is likely possible to further enhance the performance of GPT-4, we believe that there is potential for providing real-world counselling using an LLM-based system.",
            "score": 0.4771177557083992,
            "section_title": "Subjective evaluation by professional counsellors",
            "char_start_offset": 8588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03643798828125
        },
        {
            "corpus_id": "258947321",
            "title": "Benchmarking large language models for biomedical natural language processing applications and recommendations",
            "text": "As the quantitative results on text summarization and generation demonstrated, commonly used automatic evaluations such as Rouge, BERT, and BART scores consistently favored the fine-tuned BART's generated text, while manual evaluations show different results, indicating that GPT-3.5 and GPT-4 had competitive accuracy and much higher readability even under the zero-shot setting. Existing studies also reported that the automatic measures on LLM-generated text may not correlate to human preference 35,47 . The MS^2 benchmark used in the study also discussed the limitation of automatic measures, specifically for text summarization 50 . Additionally, the results highlight that completeness is a primary limitation when adapting GPT models to biomedical text generation tasks despite its competitive accuracy and readability scores. \n\nLast, our evaluation on both performance and cost demonstrates a clear trade-off when using LLMs in practice. GPT-4 had the overall best performance in the 12 benchmarks, with an 8% improvement over GPT-3.5 but also at a higher cost (60 to 100 times higher than GPT-3.5). Notably, GPT-4 showed significantly higher performance, particularly in question-answering tasks that involve reasoning, such as over 20% improvement in MedQA compared to GPT-3.5. This observation is consistent with findings from other studies 27,38 . Note that newer versions of GPT-4, such as GPT-4 Turbo, may further reduce the cost of using GPT-4. \n\nThese findings lead to recommendations for downstream users to apply LLMs in BioNLP applications, summarized in Fig. 4. It provides suggestions on which BioNLP applications are recommended (or not) for LLMs, categorized by conditions (e.g., the zero/few-shot setting (B) (C) Fig. 3 | Qualitative evaluation results on accuracy, completeness, and readability. A The overall results of the fine-tuned BART, GPT-3.5 zero-shot, GPT-4 zeroshot, and LLaMA 2 zero-shot models on a scale of 1 to 5, based on random 50 testing instances from the PubMed Text Summarization dataset.",
            "score": 0.4767434094609795,
            "section_title": "Discussions",
            "char_start_offset": 34796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "261242750"
                },
                {
                    "start": 503,
                    "end": 505,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 634,
                    "end": 636,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 1353,
                    "end": 1356,
                    "matchedPaperCorpusId": "265466787"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019989013671875
        },
        {
            "corpus_id": "266361781",
            "title": "Large language models facilitate the generation of electronic health record phenotyping algorithms",
            "text": "The average interrater reliability was 0.59 [0.50-0.68], indicating a moderate to substantial agreement among experts, considering the categorical nature of the data (rather than dichotomous) and the variations in scoring criteria among different experts. 32 y mapping the experts' assessments of \"Good\", \"Medium\", or \"Poor\" to numerical scores of 3, 2, and 1, respectively, GPT-4 (mean [95% confidence interval]: 2.57 [2.40-2.75]) and GPT-3.5 (2.43 [2.25-2.60]) exhibited significantly higher overall expert evaluation scores than Claude 2 (1.91 [1.68-2.13]) and Bard (1.20 [1.09-1.31]) (Figure 2A). GPT-4 marginally outperformed GPT-3.5, though the differences were not statistically significant. Moreover, the \u03b2-prompting strategy did not significantly differ from the \u03b1-prompting, according to experts' evaluation (Figure 2B). Furthermore, experts assigned higher scores to LLMs for their effectiveness in generating phenotyping algorithms for T2DM and hypothyroidism compared to dementia (Figure 2C). The radar plot shown in Figure 2D displays the average scores for each involved LLM across the three axes of evaluation. There are two key findings. First, GPT-4 and GPT-3.5 were rated consistently rated better than Claude 2 and Bard in following instructions, algorithmic logic, and SQL executability. Second, GPT-4 was considered to be on par with GPT-3.5 in its ability to follow instructions and SQL executability, yet it surpassed GPT-3.5 in its algorithmic logic. Based on these findings, we continued our investigation with GPT-4 and GPT-3.5.",
            "score": 0.47668178732754607,
            "section_title": "Expert assessments",
            "char_start_offset": 13035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1555
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 258,
                    "matchedPaperCorpusId": "11077516"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00518035888671875
        },
        {
            "corpus_id": "259274988",
            "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
            "text": "Can GPT-4 be used in automatic evaluation? \n\nSince the food effect summarization would be used to assist PSG assessment, it is imperative for FDA professionals to evaluate the quality of the summary, though the evaluation by human is quite laborious and expensive. In this work, we find that GPT-4 evaluation provides a cheap and reasonable alternative to human evaluation. When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies. \n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\".",
            "score": 0.47664722534563764,
            "section_title": "Which model performs better in the study?",
            "char_start_offset": 33838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 45,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2253
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044281005859375
        },
        {
            "corpus_id": "268231092",
            "title": "Comparing Large Language Models and Human Programmers for Generating Programming Code",
            "text": "In this study, we found that GPT-4 is the best-performing LLM in generating programming code. GPT-4's performance is substantially influenced by various prompt strategies. The best prompt strategy utilizes GPT-4's code interpreter function to test the validity of example test cases and benefits from the error messages from previous unsuccessful attempts. In LeetCode and GeeksforGeeks contests, GPT-4 exhibits stronger coding performance than most human programmers, while other LLMs perform similarly to average human programmers. GPT-4 demonstrates reliable coding performance across various programming languages and shows a strong ability to translate code between different programming languages. \n\nThese results suggest that GPT-4 may serve as a reliable assistant in generating programming code for practical applications. GPT-4 may empower individuals with little or no programming expertise to solve programming tasks of easy or medium difficulty. For those who have advanced programming expertise, GPT-4 may share the workload, allowing human programmers to focus on more challenging tasks. Thus, GPT-4 introduces a new paradigm in human-AI collaboration, paving the way for a more efficient and inclusive process in generating programming code. \n\nIt is worth noting that generating programming code is only a subarea of software engineering. The ability of LLMs to manage projects, design system architectures, create user interfaces, test and maintain code, and perform many other tasks related to software engineering has not been evaluated in this study. In addition, LLMs relies on well-documented descriptions of programming tasks and comprehensive sets of test cases for validating the generated programming code. This reliance may necessitate additional effort and expertise in writing prompts and establishing a software testing infrastructure.",
            "score": 0.475864939338559,
            "section_title": "Discussion",
            "char_start_offset": 21218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 703
                },
                {
                    "start": 706,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1865
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019561767578125
        },
        {
            "corpus_id": "273228278",
            "title": "AutoFeedback: An LLM-based Framework for Efficient and Accurate API Request Generation",
            "text": "For the API-BANK and MP-API datasets, due to the lack of API servers, we compared them with the standard answers in the dataset to determine their correctness. This is a rigorous and effective evaluation method. However, API requests that are inconsistent with standard answers in real-world scenarios could still meet the user requirement. Therefore, for Toolalpaca-single and -mix datasets, we used the LLM as an agent to better evaluate the quality of API request generation. \n\nAlthough the LLM struggles to generate API requests, it is advisable to evaluate the results. As these are two tasks with different contexts, the evaluation task only requires the LLM to play the role of the user without requiring excessive professional knowledge. This method has been widely applied [6][33][34] [35] and is considered to be more aligned with human preferences. We perform API parsing on the LLM output to avoid potential bias caused by direct evaluation by GPT-4. Simultaneously, we invited three human experts to rate the experimental results to validate the effectiveness of the LLM evaluation. Human experts rate each sample on 0, 1, where 1 represents the API requests meeting requirements. As presented in Table 8, we calculated the average scores and variances of three human experts on each sample and compared them with the scores of the LLM (GPT-4). It is evident that the scores assigned by human experts closely align with those generated by the LLM, with both exhibiting minimal variance (the variance is 0.2222 for the ratings [1, 0, 0] and [1, 1, 0]). We measure the correlation of evaluations between human experts and the LLM using Spearman coefficient \u03c1 \u2208 [\u22121, 1] \n\n[36]. The larger the absolute value of \u03c1 is, the more correlated it is. As observed in Figure 10, the result is indicative of a high positive correlation between GPT-4 and human expert ratings, with \u03c1 ranging from 0.77 to 0.96 on the ToolAlpaca-single and -mix datasets.",
            "score": 0.47496640310746907,
            "section_title": "Evaluation Effectiveness",
            "char_start_offset": 31359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 794,
                    "end": 798,
                    "matchedPaperCorpusId": "260887105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49658203125
        },
        {
            "corpus_id": "272917134",
            "title": "A Comparative Study of Large Language Models in Explaining Intrinsically Disordered Proteins",
            "text": "Its training paradigm involves predicting subsequent tokens (I.e., the basic units of text or code that an LLM AI uses to process and generate language) within a document. The vast corpus of training data hails from a variety of sources, encompassing publicly accessible data (e.g., internet-based data) and datasets obtained through licensing from third-party entities [8] . To enhance its performance, GPT-4 has been fine-tuned using a technique known as Reinforcement Learning from Human Feedback (RLHF) [8] . to users in a palatable, conversational manner [9] . One evaluation of GPT-3.5, a predecessor of GPT-4, found that the AI language model was able to score at or near the 60% pass mark for all three of the United States Medical Licensing Examinations (USMLEs), while medical students spend anywhere from 300-400 hours of focused study time to prepare for solely the first of these three examinations [10][11] . Further exploration of GPT-4's capabilities has shown its aptitude to tackle a broad range of intricate prompts across disciplines such as mathematics, coding, vision, law, and psychology while maintaining a human-like output [9][12] . These analyses of ChatGPT demonstrate the power of AI tools to further understand complex topics, including IDPs. As such, we posit that harnessing the power of AI tools like LLMs could significantly enhance the comprehension and recognition of IDPs within the scientific community. \n\nIn this study, we present an empirical analysis that assesses the performance of several LLMs, including GPT-4, GPT-3.5, Google Bard (PaLM 2), GPT-4, and GPT-4 with Bing Browser plugin, in the context of education on Intrinsically Disordered Proteins (IDPs). The analysis is based on a dataset gathered from a survey and focuses on five evaluation categories: Accuracy, Relevance, Depth of Understanding, Clarity, and Overall Quality. \n\nAs part of the evaluation, we engage in a dialogue with the AI models to explore various facets of IDPs, including common misconceptions, challenges in research, and the future trajectory of the field.",
            "score": 0.47487224640407183,
            "section_title": "Introduction",
            "char_start_offset": 2158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 2082
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012725830078125
        },
        {
            "corpus_id": "272827399",
            "title": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension",
            "text": "Accuracy Evaluation of Counterfactual Data A primary concern is whether the synthetic data accurately matches the correctly labeled option. In order to evaluate this, we choose five outstanding LLMs, including Mixtral-8\u00d77B-Instruct, GPT-3.5 (gpt-3.5-turo-0613), LLaMA2-70B-chat, LLaMA3-70B-Instruct and GPT-4 (gpt-4o). Non-GPT series models evaluate all counterfactual data. Due to budget constraints, a random subset of 200 samples from the generated dataset are evaluated by GPT-3.5 and GPT-4. We utilize 3shot CoTs to evaluate the accuracy. As illustrated in Table 3, the accuracy assessed by LLaMA3-70B-Instruct and GPT-4 can reach 82.19% and 93% respectively, indicating PODA can generate high-quality counterfactual data. Moreover, accuracies for other models vary between approximately 75% and 79%, reflecting the significant challenges and complexities presented by these data. Overall, the generated data can have applicability in both training and evaluation domains.",
            "score": 0.474715113978513,
            "section_title": "Evaluation of Data Quality",
            "char_start_offset": 19539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 977
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.189208984375
        },
        {
            "corpus_id": "270404317",
            "title": "The Relevance of Large Language Models for Project Management",
            "text": "The aims of this work in assessing the competencies of LLMs require a systematic approach in order to be able to acquire the needed data and answer the research question. While several LLMs are available, including some as open source, for many, their instantiation requires significant computational and storage resources. Therefore, it was decided to use already hosted LLMs where access can be obtained, i.e., OpenAI's ChatGPT-3.5 (GPT version 3-first released on 30 November 2022), OpenAI's ChatGPT-4 (GPT version 4-first released on 14 March 2023), and Google's Bard (first released on 21 March 2023). The research carried out is categorized as empirical, it is grounded on the belief that direct observation is an appropriate way to measure reality and generate truth about the world [57]. The focus is on experiments with LLMs, and capturing the interactions with them in order to analyze and understand their competencies. \n\nThe selection of both ChatGPT-3.5 and ChatGPT-4 is seen as necessary as these are two different generation models, differing in the number of parameters and capabilities among them [58] as well as with Bard. However, one should not consider that because ChatGPT-4 is a more advanced model, it is also always a better one, as the empirical tests show, e.g., in Tables 3 and 6, ChatGPT-3.5 scores better than ChatGPT-4. So, both are needed for the comparative analysis because the assumption that a later model always performs better than the previous one does not hold. \n\nA series of questions are asked to the selected LLMs and the responses are evaluated for correctness. The questions asked stem from the PMI's PMP Exam Prep, which contains questions aimed at candidates who want to acquire the project management certificate of the PMI [59]. The questions were input via the web interface of the models, and no parameters were adjusted for the model (e.g., temperature). A zero-shot approach is followed, without prompt engineering or other efforts in order to achieve the best performance from the LLM.",
            "score": 0.47411489045235256,
            "section_title": "V. RESEARCH METHODOLOGY",
            "char_start_offset": 19259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 1114,
                    "end": 1118,
                    "matchedPaperCorpusId": "258832575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007755279541015625
        },
        {
            "corpus_id": "260209167",
            "title": "Evaluating Large Language Models for Radiology Natural Language Processing",
            "text": "The rapid development of LLMs has been revolutionizing the field of natural language processing [52,53,54] and domains that benefit from NLP [55,56,57,58,36,59,60,28,61,62]. These powerful models have shown significant performance in many NLP tasks, like natural language generation (NLG) and even Artificial General Intelligence (AGI). However, utilizing these models effectively and efficiently requires a practical understanding of their capabilities and limitations, and overall performance, so evaluating these models is of paramount importance. \n\nTo compare the capabilities of different LLMs, researchers usually test with benchmark datasets in various fields (such as literature, chemistry, biology, etc.), and then evaluate their performance according to traditional indicators (such as correct answer rate, recall rate, and F1 value). The most recent study from OpenAI [63] includes the pioneering research study that assesses the performance of large language models (i.e. GPT-4) on academic and professional exams specifically crafted for educated individuals. The findings demonstrate exceptional performance of GPT-4 across a diverse array of subjects, encompassing the Uniform Bar Exam and GRE. Furthermore, an independent study conducted by Microsoft reveals that GPT-4 outperforms the USMLE, the comprehensive medical residents' professional examination, by a significant margin [64]. Holmes et al. [65] explore the utilization of LLMs in addressing radiation oncology physics inquiries, offering insights into the scientific and medical realms. This research serves as a valuable benchmark for evaluating the performance in radiation oncology physics scenarios of LLMs. \n\nUnlike the studies above that used traditional assessment research, Zhuang et al. [66] introduced a novel cognitive science-based [67] methodology for evaluating LLMs. Specifically, inspired by computerized adaptive testing (CAT) in psychometrics, they proposed an adaptive testing framework for evaluating LLMs that adjusts the characteristics of test items, such as difficulty level, based on the performance of individual models. They performed fine-grained diagnosis on the latest 6 instruction-tuned LLMs (i.e.",
            "score": 0.4735530007728728,
            "section_title": "Evaluating Large Language Models",
            "char_start_offset": 7054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "250635911"
                },
                {
                    "start": 163,
                    "end": 166,
                    "matchedPaperCorpusId": "255776274"
                },
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "252369176"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "259917027"
                },
                {
                    "start": 1416,
                    "end": 1420,
                    "matchedPaperCorpusId": "257921233"
                },
                {
                    "start": 1820,
                    "end": 1824,
                    "matchedPaperCorpusId": "237100591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0189056396484375
        },
        {
            "corpus_id": "258833685",
            "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
            "text": "We use the ChatGPT \"gpt-3.5-turbo-0301\" snapshot ( \u00a7 4.2) for all three methods. By using a fixed snapshot, we ensure all evaluations are conducted with the same LLM model. In addition, we evaluate with the GPT-4 \"gpt-4-0314\" snapshot ( \u00a7 4.3) using the best evaluation method determined by ChatGPT to check for any potential improvement. Given that ChatGPT and GPT-4 are amongst the top performing LLMs, we use their performance to estimate the potential of LLMs as reliable evaluators. Additional results using three different-sized Llama 2 models (Touvron et al., 2023) are reported in Appendix D, which all performs worse. Similar to Luo et al. (2023) and Wu et al. (2023), we set the temperature to 0 and reset the dialogue history for each evaluation instance. \n\nDataset We use the SummEval benchmark dataset (Fabbri et al., 2021). This dataset contains expert human annotations for coherence, consistency, fluency, and relevance on the generation results from 12 abstractive systems (see details in Appendix table 21) on the CNN/DM dataset (Hermann et al., 2015). Each evaluated system generates summaries for the same 100 news articles, and each summary is scored by 3 expert annotators from 1 to 5. The annotations achieve with a high kappa coefficient of 0.713 (Fabbri et al., 2021). We further calculate the annotations' standard deviations across each evaluated system in Appendix Table 20. Given a step size of 1, the standard deviations are considered very small, thus suggesting that this dataset has a high level of human agreement. Following Chiang and Lee (2023), Chhun et al. (2022), and Guan and Huang (2020), we use the average human scores as the reference scores.",
            "score": 0.4732510750056089,
            "section_title": "Setups",
            "char_start_offset": 14247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1686
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1376953125
        },
        {
            "corpus_id": "264487351",
            "title": "PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering",
            "text": "In addition, PreWoMe outperforms both of the additional baselines. Especially considering that Query Refinement was optimized for ASQA and Step-by-Step w. TD was designed for (QA) 2 , these findings suggest that our approach is versatile in handling arbitrary types of misleading questions. \n\nFor GPT-3.5, we found that the vanilla LLM shows better performance on some datasets than PreWoMe. Through manual analysis, we observed that GPT-3.5 lacks the ability to generate highquality feedbacks and actions, compared to GPT-4 (See Appendix D). Thus we conducted experiments where we replaced feedback and action (F,A) with those generated by GPT-4 and generated the answer on GPT-3.5. The results outperformed not only the vanilla GPT-3.5, but also the vanilla GPT-4, demonstrating that 1) misleading (F,A) can induce noise, 2) high-quality (F,A) is transferable between different models, and 3) can positively impact LLMs to the extent where GPT-3.5 outperforms GPT-4. \n\nFor further validation on different N-shot (N = 4, 8), we conducted additional experiments, which can be found in Appendix E.",
            "score": 0.4723408862492603,
            "section_title": "Main Experiment",
            "char_start_offset": 11607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1096
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.023468017578125
        },
        {
            "corpus_id": "270560071",
            "title": "Problematic Tokens: Tokenizer Bias in Large Language Models",
            "text": "The findings of this study provide valuable insights into the challenges and potential solutions for improving LLMs' performance and ethical alignment when faced with data constraints. Our evaluation of GPT-4 and GPT-4o with long and short tokens highlights several key points: \n\nA. Impact of Data Quality: \n\nRelying on publicly available data, particularly in restrictive environments like China, introduces significant quality issues. The prevalence of low-quality content, such as gambling and pornography links in the training data, can lead to ethical misalignments and biases in the generated content. Our results show that GPT-4 generally outperforms GPT-4o in generating sentences that accurately incorporate both long and short tokens. However, the difference in performance between the two models narrows when shorter tokens are used, suggesting that token length plays a crucial role in model accuracy. Additionally, preliminary observations indicate that these abnormal tokens also negatively influence the output quality of GPT-3.5, further highlighting the importance of addressing data quality issues across different versions of LLMs.",
            "score": 0.47221664101822486,
            "section_title": "VI. DISCUSSION",
            "char_start_offset": 21855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06744384765625
        },
        {
            "corpus_id": "265018956",
            "title": "DialogBench: Evaluating LLMs as Human-like Dialogue Systems",
            "text": "Since GPT-4 is trained on online data, GPT-4 may encode biases that perpetuate stereotypes, discrimination, or marginalization of specific languages or communities. This results in DialogBench potentially generating toxic and harmful instances Furthermore, we induce GPT-4 to generate a certain proportion of unfriendly dialogues for evaluating LLMs in unfriendly scenarios, which can reflect the true level of LLMs as human-like dialogue systems. Accordingly, this might lead to some unkind and harmful instances. In addition, we employ three experts to manually do these evaluation questions. We pay 0.2 to each expert for each instance.",
            "score": 0.47204701936396326,
            "section_title": "Ethics Statement",
            "char_start_offset": 26294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 639
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0205230712890625
        },
        {
            "corpus_id": "269484364",
            "title": "MetaRM: Shifted Distributions Alignment via Meta-Learning",
            "text": "To evaluate the effectiveness of our method, we assess it by comparing its win rate with other baselines.Specifically, we randomly select 100 prompts from the test datasets and generate the responses from our method and baselines, respectively.We then provide these pairs of prompts and responses to human evaluators, asking them to determine which response is of higher quality, more useful, and harmless.During the entire evaluation process, the human evaluators are unaware of the responses' sources.Additionally, some studies indicate that GPT-4's evaluation of the responses aligns closely with that of human evaluators (Chang et al., 2023;Zheng et al., 2023a).Meanwhile, GPT-4 is noted for being more cost-effective and efficient compared to human evaluators, while also offering greater consistency in evaluation results (Zheng et al., 2023c).So we also utilize GPT-4 to evaluate the performance of MetaRM against other baselines.To mitigate the impact of irrelevant bias on GPT-4 evaluations such as response length and position, we randomly assign the order of the responses in GPT-4 evaluation prompts.The GPT-4 prompts for evaluation can be found in Appendix A.2.This indicates that our method can maintain the RM's ability to modeling human preferences in the gradient descent, while making it adapt to the new distribution by using the meta-process.",
            "score": 0.4718332857916706,
            "section_title": "Metrics & Evaluation",
            "char_start_offset": 17254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 244
                },
                {
                    "start": 244,
                    "end": 406
                },
                {
                    "start": 406,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 937
                },
                {
                    "start": 937,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1362
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0489501953125
        },
        {
            "corpus_id": "265018956",
            "title": "DialogBench: Evaluating LLMs as Human-like Dialogue Systems",
            "text": "Since the dialogue style is related to the personalities of both speakers, we require For position bias of correct answers, GPT-4 does not guarantee that the correct answers in all generated evaluation questions are evenly distributed among the candidate options. Furthermore, we observe that several LLMs have their selection preferences, shown in Figure 7. Specifically, we calculate the accuracy of these LLMs when placing the correct answers in different positions on the whole evaluation set. Therefore, the accuracy of LLMs may be inaccurate when we apply the evaluation instances that GPT-4 generates without correction. To mitigate position bias, we assign the position of the correct answer among candidate options randomly (Zheng et al., 2023). It can be effective at a large scale with the correct expectations. \n\nData Filter. The generated evaluation set inevitably contains low-quality instances. Inspired by Zhou et al. (2022), we propose to adopt GPT-4 to filter out low-quality instances. We prompt GPT-4 to check whether the multiple-choice questions are correct. The prompt is displayed in  (2) Supervised instruction-tuning LLMs: which mostly release from the academia and companies. Except for GPT-4 and ChatGPT, the remaining are open-sourced LLMs. In addition, we test the human level in these dialogue tasks. Specifically, we randomly choose 50 evaluation instances for each task and then employ 3 experts to do these questions. Finally, a question is considered correct if at least 2 experts answer it correctly. These results can reveal not only the quality of DialogBench but also the human level of this benchmark. \n\nEvaluation Method. For the above LLMs, we use accuracy as the metric and adopt different evaluation methods. (1) Pre-trained LLMs: each option content is independently scored by concatenating it with the instruction along with the given dialogue and question as a prompt and computing the probability of \"option content\". Specifically, we calculate the perplexity of each option content and then choose the label corresponding to the option content with the lowest perplexity as the predicted answer. This evaluation method is consistent with the training method of pre-trained LLMs (i.e., next token prediction), stimulating the optimal performance of LLMs.",
            "score": 0.47150781798441066,
            "section_title": "A multi-turn dialogue generated by Speaker1 and Speaker2",
            "char_start_offset": 14488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2302
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16162109375
        },
        {
            "corpus_id": "272146505",
            "title": "SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval",
            "text": "As discussed in [11], there is a potential bias towards LLM-generated text when using LLM for evaluation. Hence, to explore this bias effect, we first categorise the submissions to the DL-23 into four categories according to whether they are based on GPT (\u00d7), T5(|), GPT + T5 (+) or others (\u2022). Figure 2 shows the system order agreement between the use of our SynDL and DL-23 with highlighted different types of systems. We observe that a high agreement can still be observed between human assessment and language model judgments in this case. GPT-based systems do not get higher ranks when evaluated with GPT-4 generated relevance judgments in our SynDL test collection. \n\nOverall, we experimentally verify that our SynDL test collection is of a high quality, which not only exhibits a high agreement with the human assessors across the comparison to multiple sets of test collections but also shows a robust evaluation outcome when evaluated on the potential bias about using the identical language models.",
            "score": 0.47134012703976946,
            "section_title": "RESOURCE EVALUATION",
            "char_start_offset": 16622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 1008
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 20,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1077880859375
        },
        {
            "corpus_id": "267391111",
            "title": "A systematic evaluation of the performance of GPT\u20104 and PaLM2 to diagnose comorbidities in MIMIC\u2010IV patients",
            "text": "Among the various LLM-based tools, GPT-4, an advanced language model developed by OpenAI [9], has shown considerable promise for medical applications.GPT-4 has been shown to be capable of solving medical challenge problems [10] and making highly accurate differential diagnoses: previous research diagnosed 48 patients with GPT-3 [11], 50 patients with GPT-4 [12] and 70 challenging cases with GPT-4 [13].\n\nHowever, these studies have only evaluated GPT-4's ability to diagnose on small numbers of real patients or on synthetic cases.While these initial results are promising, a more comprehensive evaluation with a larger sample size is needed to substantiate these findings.The limited scope of previous research leaves a gap in our understanding of GPT-4's performance and applicability in real-world health care settings, particularly in high-stress environments such as intensive care units.\n\nTo address this gap, the present study aims to conduct a large-scale evaluation of GPT-4's diagnostic performance using the Medical Information Mart for Intensive Care IV (MIMIC-IV) data set.MIMIC-IV is a publicly available deidentified data set containing comprehensive medical information from critical care units [14][15][16].This data set offers a unique opportunity to test GPT-4's ability to diagnose various conditions across a broad spectrum of patients in a critical care setting.Importantly, our study includes 1000 patients, far exceeding the scope of previous evaluations.\n\nIn conducting this evaluation, it is essential to note that we do not employ a fine-tuned version of GPT-4.Instead, we focus on evaluating its inherent capabilities for diagnosing medical conditions.This approach enables us to assess the base model's strengths and limitations, providing a foundation for future research and development efforts.\n\nThrough this comprehensive evaluation, we aim to contribute significantly to the ongoing discourse on AI's role in health care, especially the potential of LLMs to enhance diagnostic accuracy.Additionally, we hope to shed light on the prospects of such models in improving patient care and boosting health care efficiency, which are key concerns in modern health care systems worldwide.",
            "score": 0.47132319234460957,
            "section_title": "| INTRODUCTION",
            "char_start_offset": 2438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 405
                },
                {
                    "start": 407,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 896
                },
                {
                    "start": 898,
                    "end": 1089
                },
                {
                    "start": 1089,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1387
                },
                {
                    "start": 1387,
                    "end": 1482
                },
                {
                    "start": 1484,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1683
                },
                {
                    "start": 1683,
                    "end": 1829
                },
                {
                    "start": 1831,
                    "end": 2023
                },
                {
                    "start": 2023,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 404,
                    "matchedPaperCorpusId": "259162481"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "263894593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02276611328125
        },
        {
            "corpus_id": "266163300",
            "title": "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models",
            "text": "The hyper-parameters are tuned according to actual performance. We show the values of these hyper-parameters in Table 2. For the implementation of COTTON and other base models, we utilize the PyTorch 10    [26], Gemini [52], gpt-3.5-turbo, and gpt-4 [53]). Among these LLMs, Gemini, gpt-3.5-turbo, and gpt-4 are currently considered to be the most promising LLMs in code generation tasks and have shown promising performance. Table 3 presents the performance of \u2113LMs and LLMs across all evaluation metrics on the HumanEval-CoT and OpenEval-CoT datasets. In terms of lexical similarity, \u2113LMs generally performed worse than LLMs. For example, using the METEOR metric, LLMs like gpt-4 can achieve scores around 0.35 on the HumanEval-CoT dataset and around 0.37 on the OpenEval-CoT dataset. In contrast, the majority of \u2113LMs scored below 0.3 on both datasets. In terms of semantic perspective, most \u2113LMs are difficult to generate highquality CoTs, while LLMs demonstrated better performance. For example, based on the Consistency metric, gpt-4 can achieve scores above 0.96 on the HumanEval-CoT dataset and above 0.87 on the OpenEval-CoT dataset. In contrast, the majority of \u2113LMs scored below 0.6 in both data sets. In addition, in terms of the Consistency metric, gpt-3.5-turbo and gpt-4 achieved the best results on the OpenEval and HumanEval datasets, respectively, yet the cost of calling gpt-4 is 20 times higher than that of gpt-3.5-turbo. Therefore, we choose gpt-3.5-turbo as the Teacher Model in our study. \n\nBy further analysis based on varying models and parameter sizes, we can achieve interesting insights. Among CodeGen, StarCoder, CodeT5+, and CodeLlama models, StarCoder and CodeLlama showed potential in generating high-quality CoTs for guiding code generation. This could be attributed to the pre-training dataset of StarCoder and CodeLlama.",
            "score": 0.4713047082334543,
            "section_title": "Implementation Details and Running Platform",
            "char_start_offset": 30571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "252715691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02386474609375
        },
        {
            "corpus_id": "266051233",
            "title": "Applying Large Language Models and Chain-of-Thought for Automatic Scoring",
            "text": "Specifically, we could call GPT API multiple times and consider the most frequent predicted label mentioned by the responses as the final prediction. In this way, we increase the reliability of the advanced nucleus sampling. This approach is inspired by both strands of research: (1) machine learning fields ensembling the answers from multiple models, and (2) educational studies asking two or three educators to independently grade the same student submission. By incorporating multiple grading instances, we aim to decrease the variance of the predictions, thereby enhancing the reliability and consistency of the grading process. \n\nThe issue of yielding reliable results from LLMs is a complicated matter since it is related to the power of the model itself. For example, using greedy sampling or nucleus sampling with GPT-4 could return different results from using these with GPT-3.5, since the former has broader general knowledge and problem solving abilities (OpenAI, 2023). However, the impact of these mixed conditions on the performance of LLMs, particularly of GPT, on automatic scoring has not been explored yet, to our knowledge. \n\nIn this study, we conducted experiments using both greedy sampling and nucleus sampling, with both LLMs that are more powerful (GPT-4) or less powerful (GPT3.5). Our study distinctively compares the performance of GPT-4 and GPT-3.5 in handling complex, authentic student responses to science items. This comparative aspect is critical in highlighting the evolutionary strides in LLMs and their practical efficacy in educational contexts. Our unique contribution lies in our methodological approach: we analyzed extensive data sets, prioritize the authenticity of student responses, and focus on obtaining explainable and interpretable results. This approach not only benchmarks the performance differences between GPT-4 and GPT-3.5 but also sets a new standard in evaluating the practical utility of LLMs in educational assessments --comparing different prompt engineering methods and hyperparameters simultaneously. Our findings offer novel insights into the operational dynamics of these models, contributing significantly to the growing body of knowledge on the effective use of LLMs in educational settings. The details of the experiments and the results are presented in Methods and Findings.",
            "score": 0.47128991534848935,
            "section_title": "Yielding Reliable Results from GPT",
            "char_start_offset": 22806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2258
                },
                {
                    "start": 2259,
                    "end": 2344
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036285400390625
        },
        {
            "corpus_id": "270869629",
            "title": "FineSurE: Fine-grained Summarization Evaluation using LLMs",
            "text": "It is interesting to observe how the evaluation agreement varies based on the choice of LLMs, given the abundance of open-source and proprietary LLMs.\n\nSuccess Ratio.The primary limitation of opensource LLMs is their comparatively lower success ratio in following prompts, compared to proprietary LLMs; only Llama3-70B-Inst exhibits a high success ratio comparable to proprietary LLMs.Upon analyzing failure cases, the top three reasons are:\n\n(1) the output is either not in JSON format or an incorrect JSON format, (2) the output consists of meaningless text, e.g., python codes or no output at all, and (3) the JSON output includes only a few lines of sentences or keyfacts.Furthermore, the maximum token length in context for open-source LLMs is notably shorter compared to proprietary LLMs.GPT-4 series can process up to 128K tokens, whereas open-source LLMs generally handle up to 8K input tokens.This results in prompt truncation when handling lengthy input texts, potentially leading to failures in generating accurate outputs in text summarization.\n\nAgreement with Human Score.Tables 5-6 summarize the correlation of nine different LLMs with human judgment, computed only for the successful cases of adhering to the prompt.Although the recent Llama3-70B-Inst shows strong agreement with humans, in general, there is a noticeable gap between open-source and proprietary LLMs.Regarding open-source LLMs, the agreement with human scores increases with the model size; for example, Llama2-70B exhibits a higher correlation coefficient than Phi-2.Additionally, instruction tuning also plays a role, as observed in Mixtral-8x7b's performance, which improved significantly after instruction tuning.In contrast, all the proprietary LLMs exhibit high correlation coefficient.Particularly, more recent and powerful LLMs exhibit better performance, i.e., GPT-4-turbo > GPT-3.5turbo,GPT-4-omni > GPT-4-turbo.",
            "score": 0.4712759166892283,
            "section_title": "LLMs as Evaluators Comparison",
            "char_start_offset": 22182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 152,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 385
                },
                {
                    "start": 385,
                    "end": 441
                },
                {
                    "start": 443,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1056
                },
                {
                    "start": 1058,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1231
                },
                {
                    "start": 1231,
                    "end": 1382
                },
                {
                    "start": 1382,
                    "end": 1550
                },
                {
                    "start": 1550,
                    "end": 1699
                },
                {
                    "start": 1699,
                    "end": 1774
                },
                {
                    "start": 1774,
                    "end": 1879
                },
                {
                    "start": 1879,
                    "end": 1904
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02630615234375
        },
        {
            "corpus_id": "268353040",
            "title": "Elephants Never Forget: Testing Language Models for Memorization of Tabular Data",
            "text": "We assume that we have query access to an LLM that we have not trained ourselves.We do not have access to the training data, model architecture, or the model's probability distribution over tokens.We also assume that the model is a chat model, although this assumption is not critical.In short, we assume that we have API access to a proprietary LLM such as GPT-4 or Claude 2.\n\nWe would like to evaluate the LLM on a downstream task with tabular data.For example, we might want to study the efficacy of different fine-tuning techniques, optimize the prompts used for in-context learning, or determine the LLMs ability to make accurate statements about the mean, min, max or standard deviation of columns in the data.Because we do not know what data the LLM was trained on, we should be worried that any measured performance might not be due to our applied technique,\n\nas a baseline, completion with a random draw from the feature's marginal distribution (i.e., with a random sample from the values of the next column).If GPT-3.5 (green) or GPT-4 (blue) are able to complete rows with true values from the original dataset at a rate higher than the baseline (yellow), this is evidence that the LLMs have seen the data before and memorized parts of it.We show this for 8 different features on 2 datasets.The mean and 95% confidence intervals are shown.\n\nbut because the LLM has already seen the datasets during training that we are using for evaluation.\n\nIn fact, the LLM might even have memorized the datasets verbatim [4].\n\nProblem.How can we test whether an LLM has seen a tabular dataset during it's training?If there is evidence that an LLM has seen a dataset, can we assess the degree of contamination?\n\nDatasets.In this study, we use well-known machine learning datasets that are freely avaialable on the Internet, and also less well-known datasets that are from after the cutoff data of the LLMs training, or which have never been made available on the Internet.",
            "score": 0.47060340376108556,
            "section_title": "Problem Setup",
            "char_start_offset": 4451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 81,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 376
                },
                {
                    "start": 378,
                    "end": 451
                },
                {
                    "start": 451,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 866
                },
                {
                    "start": 868,
                    "end": 1018
                },
                {
                    "start": 1018,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1302
                },
                {
                    "start": 1302,
                    "end": 1350
                },
                {
                    "start": 1352,
                    "end": 1451
                },
                {
                    "start": 1453,
                    "end": 1522
                },
                {
                    "start": 1524,
                    "end": 1532
                },
                {
                    "start": 1532,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 1706
                },
                {
                    "start": 1708,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0196075439453125
        },
        {
            "corpus_id": "258331653",
            "title": "Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery",
            "text": "Challenges with concordance are to be expected as GPT-3.5 and GPT-4 do not have access to local EHR used to formulate numerous consultation reports. We note that retrospective analyses of EHR data are often limited by sample size, so agreement (12.1% -19.7%) may provide a false sense of reassurance, while disagreement (30.3% -22.7%) does not necessarily mean that the LLM response is not useful. This difficulty of objectively measuring usefulness of LLM provided responses to answer bedside clinical queries, limits our ability to comment on the LLM's usefulness. Another limitation of this study is the small question set with an overrepresentation of internal medicine and dermatology and of descriptive questions (Supplemental Table S1). \n\nAlthough general purpose LLMs might not serve specific clinician information needs out of the box, we see immense potential in these tools as evidenced by a ~63% increase in agreement and a 25% reduction in disagreements from GPT-3.5 to GPT-4, without any sophisticated prompt engineering. Additional customization to reduce hallucinated references (2 for GPT-3.5 and 9 for GPT-4) may already be possible. For example, Microsoft's Bing service provides a Chat mode that weaves together the generative AI capabilities of GPT-4 with traditional search and retrieval to ground its generations in web content. Several modes are provided that control the degree to which the system leverages the creative powers of GPT-4 to generate information beyond web content. We found that, using the 'More Precise' mode, for questions where GPT-4 answered with erroneous references, led to responses without the hallucinated references. See Supplemental Table S4 for the original question, GPT-4 response, and Bing Chat response using the 'More Precise' mode. \n\nOur evaluation set up differs from typical benchmarking in two key ways: 1) we used questions that arose as information needs during the course of care delivery, and 2) we compared LLM responses with a report obtained from a consultation service designed for serving bedside information needs often referring to data collected locally as stored and drawn from the EHR.",
            "score": 0.47057230141128686,
            "section_title": "Discussion",
            "char_start_offset": 13777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 2161
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.023956298828125
        },
        {
            "corpus_id": "263608642",
            "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
            "text": "Other work has studied using language models for science. Language models such as MultiVerS [62] can validate claims against scientific literature in the domains of COVID-19, public health, and climate change. However, Auer et al. [4] found that ChatGPT struggled to answer challenging questions from research papers across topics like computer science, engineering, chemistry, geology, immunology, economics, and urban studies. \n\nSimilar studies have been performed in computer science. In natural language processing (NLP), Gao et al. [23] investigated whether LLMs could generate a survey of knowledge for NLP concepts, Table 1. A summary of the papers selected for GPT-4 to generate assumptions, analysis plans, and code. We report each paper's venue, number of citations in the ACM Digital Library, and a brief description of the paper's analysis. We also report number of assumptions and modules generated by GPT-4. such as A* search. In an evaluation with NLP experts, the authors found GPT-4 generated reasonable explanations of these concepts, but sometimes generated factually incorrect knowledge. Researchers also have applied LLMs for research in human-computer interaction. Wu et al. [66] replicated seminal crowdsourcing papers using LLMs, while other work found that GPT-3 could generate synthetic data for both open- [28] and closed-ended [58] questions in interviews and surveys. Lastly, Xiao et al. [67] found that GPT-3 could perform deductive qualitative coding on datasets. Our work builds upon the literature by studying whether LLMs can analyze methodology and write code pipelines to repeat analyses, rather than returning factual knowledge or generating research data. Compared to prior work, our study specifically focuses on quantitative empirical research methods rather than qualitative ones. Finally, we extend our understanding of LLMs' scientific knowledge by studying its performance in software engineering research.",
            "score": 0.470455100319386,
            "section_title": "Language Models for Science",
            "char_start_offset": 7654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "258507546"
                },
                {
                    "start": 1333,
                    "end": 1337,
                    "matchedPaperCorpusId": "258218228"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "247585215"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "257758261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0535888671875
        },
        {
            "corpus_id": "273403983",
            "title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents",
            "text": "4  Human-Model Agreements of Idea Arena. To assess the reliability of our model-based evaluation within Idea Arena, we analyze the agreements between the preferences of the human judges and the LLM judges. We follow Zheng et al. (2024) to compute the agreement, which is defined as the probability that two judges agree on the winner of one specific arena match. Figure 5 shows the pairwise agreement between humans and several state-ofthe-art LLMs, including GPT-4o, Gemini-1.5-Pro-Exp-08277 , and Claude-3.5-Sonnet8 . We observe an average agreement of 70.8% between GPT-4o and humans. This finding indicates a strong alignment between humanbased and model-based evaluations, thereby highlighting the robustness of Idea Arena in evaluating the quality of generated research ideas. Moreover, GPT-4o demonstrates the highest level of agreement with humans among all tested LLMs. Therefore, we will utilize GPT-4o as the LLM judge for subsequent analytical experiments. Additionally, we present the agreement on individual criteria between GPT-4o and human evaluators in Omits the Future Trend Prediction module, prompting the LLM to consolidate ideas directly based on the provided input information. 3) -Entities: Skips inputting entity definitions during idea generation.To ensure fair comparison, each variant is scored against the full CoI Agent, with 2/1/0 points for win/tie/lose in 50 matches, for a maximum of 100 points. \n\nResults in Table 2 show that all variants negatively affect idea quality. Excluding the CoI construction stage has the most significant impact, emphasizing the importance of organizing literature based on progressive relationships to enhance the LLM's understanding of trends. Removing the Future Trend Prediction reduces novelty, as the LLM lacks insight into potential forward-thinking ideas. Although slight improvements in clarity and feasibility are observed, these are not substantial, likely due to evaluation variability. Finally, omitting entity information reduces clarity and effectiveness, as the LLM generates more abstract ideas without grounding in specific concepts. This highlights the value of entity information in enhancing the clarity and practical relevance of ideas.",
            "score": 0.47041131529465846,
            "section_title": "RESULTS",
            "char_start_offset": 17339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 235,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03839111328125
        },
        {
            "corpus_id": "263141267",
            "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond",
            "text": "With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc. Our analysis sheds light on many of these questions, aiming to improve the transparency of advanced LLMs.",
            "score": 0.47037496066944495,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058563232421875
        },
        {
            "corpus_id": "270514555",
            "title": "Automatic, Meta and Human Evaluation for Multimodal Summarization with Multimodal Output",
            "text": "We use GPT-4 with vision (\"gpt-4-vision-preview\") as the multimodal LLM.To evaluate the quality of our proposed metric, we leverage our proposed meta-evaluation benchmark and describe the results in Section 4.2.",
            "score": 0.4697741801524048,
            "section_title": "Experiment 4.1 RQ1: Automatic Evaluation",
            "char_start_offset": 16206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 72,
                    "end": 211
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0831298828125
        },
        {
            "corpus_id": "267750144",
            "title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation",
            "text": "To further analyze our self-rewarding score, we present its distribution in Figure 4. Additionally, Figure 5 illustrates the relationship between GPT-4's preference annotation win rate on the original dataset and the dataset generated by the LLM itself using the same prompt, without using contrastive prompt pairs. \n\nTo further analyze our self-rewarding score, we present the distribution of our self-rewarding score and the relationship between GPT-4's preference annotation win rate on the original dataset and the dataset generated by the LLM itself using the same prompt (without using contrastive prompt pairs) in Figures 4  and 5. We can see that on the original dataset, GPT-4's evaluation win rate is similar across all ranges of our self-rewarding score. This indicates that our self-rewarding score may not be an effective measure for evaluating response quality in this context. However, on the dataset generated by the LLM itself using the same prompt (without using contrastive prompt pairs), GPT-4's evaluation win rate increases with the self-rewarding score. Combined with the results in Figure 3, we can see that our self-rewarding score effectively evaluates the quality of responses generated by the LLM itself. However, it may not be applicable to text from other sources, further confirming our theoretical analysis in Section B. \n\nIt is worth noting that in the case of using the same prompt, we also use contrastive prompt pairs to evaluate the quality of responses. This suggests that the relevance of our self-rewarding score is primarily to whether the data is generated by the LLM itself, rather than being closely associated with the specific content of the original prompt.",
            "score": 0.4694399231548144,
            "section_title": "C Further Experiments about Self-Rewarding Score",
            "char_start_offset": 35551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1704
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44921875
        },
        {
            "corpus_id": "269032901",
            "title": "CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs' (Lack of) Multicultural Knowledge",
            "text": "After the MCQ is initially formulated, we attempt to use GPT-3.5-turbo as our Verifier (Brown et al., 2020).Users revise and verify the MCQ iteratively based on the Verifier response.In this section, we evaluate the viability of this setup by investigating whether the Verifier serves as a good estimator for the degree of challenge for other LLMs.We assume that users revise their MCQs based on the Verifier response and its corresponding confidence score.\n\nOur LLM Verifier is able to create challenging questions for all tested models.To delineate the effect of LLM assistance, we analyze the MCQs created by users without LLM assistance on other LLMs in Fig. 3 (Left).We found that LLM performance decreases with the number of revisions, especially for llama-70b by 38.73% and yi-34b by 23.82%.GPT-3.5-turbo(our Verifier model) decreased from 48.91% to 37.5%.GPT-4-turbo, which is the best performance model among all, decreased from 60.87% to 50.00% on the 10th revision.This shows that our LLM Verifier is capable of increasing the difficulty of questions for most models, as the number of revisions increases based on the response of LLM Verifier.\n\nIt implies that using only one model (GPT-3.5-turbo)as the Verifier can effectively construct challenging data for most of the models.However, if the goal of future work is to improve the stronger model (GPT-4-turbo), the relatively small decrease in it suggests that we need to randomly select different models as verifier, including GPT-4 model to help constructing a more challenging dataset.",
            "score": 0.46916756347168387,
            "section_title": "LLM assistance on Question Verifier",
            "char_start_offset": 12089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 457
                },
                {
                    "start": 459,
                    "end": 538
                },
                {
                    "start": 538,
                    "end": 672
                },
                {
                    "start": 672,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 811
                },
                {
                    "start": 811,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1154
                },
                {
                    "start": 1156,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1551
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06903076171875
        },
        {
            "corpus_id": "269757226",
            "title": "Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs",
            "text": "To validate the scores and rankings given by human judges, and to ensure our evaluation is objective, accurate, and referenceable, we utilize the LLM-as-a-judge method as a cost-effective mean to complement the aforementioned human scoring results.This approach employs a finetuned LLM as the evaluator for pairwise comparisons, as illustrated in Figure 2. In pairwise comparisons, the LLM judge is presented with a question and two corresponding answers and asked to discern which of the two is better.If the judge considers that there is no difference in quality between the two answers, he/she can declare a tie.In the pretest, we evaluated three LLM judge candidates: a fine-tuned GPT 3.5-Turbo model (the most advanced version of GPT available for fine-tuning), a fine-tuned Ernie-Bot 4, and an unfine-tuned GPT4-Turbo.Initially, we developed a training dataset for pairwise comparisons by collecting human preference judgments on a selection of Chinese answer pairs.This dataset, combined with pairwise comparison data derived from the chatbot_arena_conversations dataset2 , was used to fine-tune the two LLM judges.Subsequently, we tasked these three models with conducting pairwise comparisons on a segment of answer data and compared their judgments against the human-generated validation set.The fine-tuned GPT 3.5-Turbo exhibited significantly higher consistency with human judges than the other two, establishing it as the most appropriate judge for our evaluation practices.Statistical evidence shows that the consistency rate between this LLM and human judges exceeds 67%, and in some tasks, it even surpasses 70%.Given that agreement across human judges in pairwise comparisons ranges between 60% and 85%\n\n3 in previous research, these results demonstrate the feasibility and effectiveness of using a fine-tuned GPT3.5-Turbo judge for evaluation.",
            "score": 0.46912763298142923,
            "section_title": "Supplementary Assessment with LLM-as-a-Judge: Ranking Natural Language Proficiency of LLMs in Chinese Contexts",
            "char_start_offset": 16459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1122
                },
                {
                    "start": 1122,
                    "end": 1302
                },
                {
                    "start": 1302,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1628
                },
                {
                    "start": 1628,
                    "end": 1719
                },
                {
                    "start": 1721,
                    "end": 1861
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3251953125
        },
        {
            "corpus_id": "271065007",
            "title": "It Cannot Be Right If It Was Written by AI: On Lawyers' Preferences of Documents Perceived as Authored by an LLM vs a Human",
            "text": "Finally, Janatian et al (2023) suggest using GPT-4 to extract pathways from real-world legislation to support the development of legal expert systems. Their evaluation yielded that 60% of the generated pathways were equivalent or superior to manually created ones. \n\nSupport for Legal Research and eDiscovery Various tasks related to legal research, legal information retrieval, drafting or eDiscovery can be supported by LLMs. Savelka et al (2023b) demonstrated the effectiveness of GPT-4 augmented with a legal information retrieval module, which significantly improved the accuracy of explanations of legal concepts. These findings correspond with Blair-Stanek et al ( 2024), who used non-augmented models. The authors reported poor performance of most stateof-the-art LLMs (including GPT-4) in basic legal text-handling tasks, referring to them as 'sloppy paralegals'. Integrating LLMs with knowledge bases (Cui et al, 2024) and other tools allowing, e.g., factual lookups (Schick et al, 2023) and gathering references (Nakano et al, 2022) can boost performance. However, the existing research is focused on domains outside of law. Huang et al (2023) emphasized the importance of domain-specific knowledge over the general experiences distilled from ChatGPT, as demonstrated through supervised fine-tuning tasks. Henseler and van Beek (2023) used GPT-4 as a supportive tool for handling data in digital investigation, entrusting it with tasks like summarization, evaluation and visualization of chat messages. Their work showcased the potential of LLMs in eDiscovery processes. Ioannidis et al (2023) utilized LLMs for regulatory compliance, providing identification, summarization and the impact level assessment of legal rules impacting businesses. Lam et al (2023) employed LLMs to streamline contract drafting processes, facilitating tasks such as template selection or clause modification to suit specific contexts agreed upon by parties. Similarly, Trozze et al (2024) used LLMs to support legal drafting in securities cases, noting models' inability to reason but leading to reduced drafting time.",
            "score": 0.46844345074121274,
            "section_title": "Use of LLMs in Legal Domain",
            "char_start_offset": 19474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 264
                },
                {
                    "start": 267,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 9,
                    "end": 30,
                    "matchedPaperCorpusId": "265067125"
                },
                {
                    "start": 977,
                    "end": 997,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 1317,
                    "end": 1345,
                    "matchedPaperCorpusId": "260442638"
                },
                {
                    "start": 1582,
                    "end": 1604,
                    "matchedPaperCorpusId": "140075357"
                },
                {
                    "start": 1755,
                    "end": 1771,
                    "matchedPaperCorpusId": "260442567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046630859375
        },
        {
            "corpus_id": "261048971",
            "title": "GameEval: Evaluating LLMs on Conversational Games",
            "text": "To better evaluate the performance of LLMs under complex scenarios, especially for real-world application, many works employ preferences provided by humans or other powerful models to compare the performance of different LLMs. \n\nHuman-based evaluation takes the human's preference to judge the performance of LLMs (Liang et al. 2022;Zheng et al. 2023b,a;Novikova et al. 2017;Ziems et al. 2023). Liang et al. (2022) conducted human evaluations in summarization and disinformation scenarios on six distinct models. Chatbot Arena (Zheng et al. 2023a) is one of the most famous benchmarks which introduce anonymous, randomized battles between pairwise models. A user can chat with two anonymous models side-by-side and vote for the better one he thinks. It adopts the Elo rating system to give the final score of participating models. Considering the high cost of manual evaluation, recent works use a powerful LLM as an alternative to scoring the output of models under evaluation (Lin and Chen 2023). MT-Bench (Zheng et al. 2023a) offers an extensive collection of meticulously crafted questions aimed at evaluating the proficiency of models in managing multi-turn dialogues. They employ GPT-4 as a surro-gate for humans to select the better output generated by participating models. Alpaca-Eval (Dubois et al. 2023) focuses on models' instruction-following ability across various natural language processing tasks. It evaluates models by measuring the proportion that a powerful LLM prefers its outputs over the outputs from a reference model. Vicuna-Eval (Chiang et al. 2023) uses GPT-4 to create a series of questions and asks GPT-4 to rate the quality of the models' output based on helpfulness, relevance, accuracy, and details. However, preference bias exists in model-based evaluation. Automatic evaluators (e.g., GPT-4) prefer models that generate longer outputs and/or that were fine-tuned on the model underlying the evaluator (Dubois et al. 2023).",
            "score": 0.4683881367471854,
            "section_title": "Preference-based Evaluation",
            "char_start_offset": 7660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 375,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 527,
                    "end": 546,
                    "matchedPaperCorpusId": "368182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1270751953125
        },
        {
            "corpus_id": "270878599",
            "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
            "text": "To assess whether the evaluation results from GPT-4 are biased, we conduct additional human evaluations for scoring. We randomly selected 25 samples from each configuration (i.e., a column Table 4: Results of LLMs on existing bias evaluation datasets under the recognition and selection task settings. We use WB, SS, RB, and CP to represent WinoBias (Zhao et al., 2018), StereoSet (Nadeem et al., 2021), RedditBias (Barikeri et al., 2021), and CrowS-Pairs (Nangia et al., 2020), respectively. We use the micro F1 score in % as the evaluation metric, along with the RtA (Refuse to Answer) rate shown in the brackets. Results with exceptionally high RtA rates are highlighted in red, and the best results (excluding results with high RtA rates) are highlighted in green. in the table). We recruit 20 volunteers and asked each of them to assess the bias of 100 samples. In this setup, each sample is evaluated by 5 volunteers. Results are provided in Table 3, and we have the following observations: (1) Human-GPT-4 Alignment. Humans are generally aligned with GPT-4 in terms of evaluation performance in most cases. This suggests that GPT-4 could serve as a viable and reliable tool for evaluating bias in generated content. This is a significant insight, as it validates GPT-4's potential use as a scalable alternative to human evaluation, particularly when manual evaluation is costly or infeasible at large scales. \n\n(2) Lower Bias Scores in Human Evaluations. \n\nInterestingly, the bias scores from human evaluators are slightly lower than those generated by GPT-4 itself. This observation implies that GPT-4's superior performance as an evaluator does not stem from an inherent bias in favor of its own generated outputs. Instead, the slight difference between human and GPT-4 ratings could be attributed to subtle factors such as individual perspectives on bias or cultural influences, Nevertheless, the gap is small enough to indicate that GPT-4 is generally unbiased in its assessments of its own content.",
            "score": 0.46816720207776663,
            "section_title": "HUMAN EVALUATION",
            "char_start_offset": 20202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 2010
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 381,
                    "end": 402,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 415,
                    "end": 438,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 456,
                    "end": 477,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1448974609375
        },
        {
            "corpus_id": "269293456",
            "title": "LLMChain: Blockchain-Based Reputation System for Sharing and Evaluating Large Language Models",
            "text": "To assess the credibility and capabilities of LLMs, several studies have introduced diverse evaluation methods, including pairwise comparison, single-answer grading, or referenceguided grading, employing another LLM as an evaluator.[2], [15].These methodologies offer advantages in scalability and interoperability.Nevertheless, it comes with notable limitations: 1) Position Bias, where the evaluator tends to favor the initial model; 2) Verbosity Bias, where the evaluator prefers longer responses over shorter ones; and 3) Self-Enhancement/Promotion Bias, where the judging model prioritizes its own text or that generated from a similar model.Moreover, evaluating a LLM using another LLM appears paradoxical since the evaluator itself is subject to evaluation.On the other hand, alignment-based methods are used to make large-scale alignment research more accessible like OpenAssistant conversations [20], which is a corpus of conversations that resemble interactions with assistants, created and annotated by humans.Nonetheless, alignment-based methods face some scalability challenges and annotation expenses.In Core-GPT [21] and [22], authors focus on assessing the credibility of LLMs.Core-GPT [21] proposes an approach that combines open-access scientific literature with LLMs to improve their reliability and trustworthiness.However, its methodology's scope is limited to two LLMs, \"GPT3.5\"and \"GPT-4\", failing to illuminate the credibility gap between open-source and commercial models.In contrast, the approach proposed in [22] introduces an automated workflow designed to manage an increased number of requests/responses, facilitating the assessment of the credibility of multiple LLMs.In G-Eval [16], which is a framework that leverages large language models, used a Chain-of-Thoughts (CoT) and a form-filling paradigm to evaluate the quality of Natural Language Generation (NLG) outputs.G-Eval experimentation involves two generation tasks: text summarization and dialogue generation.However, here again, the methodology is limited to only two LLMs which are \"GPT3.5\"and \"GPT-4\".",
            "score": 0.4680351270692513,
            "section_title": "A. LLMs Evaluation",
            "char_start_offset": 5918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1115
                },
                {
                    "start": 1115,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1400
                },
                {
                    "start": 1400,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1699
                },
                {
                    "start": 1699,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 1999
                },
                {
                    "start": 1999,
                    "end": 2082
                },
                {
                    "start": 2082,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1127,
                    "end": 1131,
                    "matchedPaperCorpusId": "259501770"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "259501770"
                },
                {
                    "start": 1709,
                    "end": 1713,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2432861328125
        },
        {
            "corpus_id": "274280574",
            "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
            "text": "One approach in this direction relies on the judge LLMs themselves to generate the synthetic feedback. For example, Wu et al. (2024) construct pairwise feedback for judgment enhancement by prompting the policy LLMs to evaluate their own judgments. Wang et al. (2024f) prompt the LLM to generate a \"noisy\" version of the original instruction and use the corresponding response to this corrupted instruction as the inferior response. Wang et al. (2024a) prompt GPT-4-Turbo to generate multiple pieces of evidence based on the original evidence for each instance, categorizing them into completely irrelevant evidence, partially irrelevant evidence and highly related evidence to train a hallucination judgment LLMs. Park et al. (2024) build OFFSETBIAS, a pairwise preference dataset that leverages GPT-4 to generate bad, off-topic and erroneous responses and perform difficulty filtering. For safety judging, Xie et al. (2024a) adopt GPT-4 as the classifier to map each data point to a predefined safety category to train an automated evaluator. Different from previous works, Li et al. (2024e) adopt GPT-4 to synthesize both pairwise and pointwise data to train a generative judge LLM. For pointwise data, they adopt a \"divide-andconquer\" strategy, where two critiques are collected from GPT-4 for a single response, combined into  2023) introduce JudgeLM and propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges.",
            "score": 0.46781747199250334,
            "section_title": "Data Source",
            "char_start_offset": 21067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1535
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 267,
                    "matchedPaperCorpusId": "271709606"
                },
                {
                    "start": 714,
                    "end": 732,
                    "matchedPaperCorpusId": "271064337"
                },
                {
                    "start": 1075,
                    "end": 1092,
                    "matchedPaperCorpusId": "263829791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2105712890625
        },
        {
            "corpus_id": "270559721",
            "title": "We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs",
            "text": "Model Selection. For our experiments, we chose the models that were the highest ranked on the EvalPlus leaderboard (as of January 20, 2024) [40]. During the creation of our model list, we ignored the fine-tuned versions that were ranked below their corresponding foundational models and only selected one fine-tuned version of the same foundational model of the same parameter size [40]. EvalPlus maintains a ranking of the top performing LLMs for code correctness according to a rigorous code synthesis evaluation framework. Our goal was to include a mix of top-performing base models and a few of the best-performing fine-tuned variants. We also included the GPT series of models (GPT-3.5, GPT-4, and GPT-4 Turbo) in our experiments, which currently hold the top rankings on the leaderboard. GPT models are widely considered as SOTA in terms of code generation models at the time of writing and add value to our experiments as representative commercial models. The models were not modified or altered in any way prior to testing; they are strictly \"off-the-shelf.\" Table 1 provides a complete list of the models that we tested in our experiments. \n\nTable 1: Details of the models that were evaluated.",
            "score": 0.4676496804934327,
            "section_title": "Code Generation",
            "char_start_offset": 20743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1202
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "258437095"
                },
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "258437095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006488800048828125
        },
        {
            "corpus_id": "277151254",
            "title": "ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging Knowledge Graph",
            "text": "4.1.1 Models. In ECKGBench 2 , we assess the e-commerce capabilities of existing advanced LLMs, including five close-sourced LLMs, i.e., Gemini-1.5-pro, GPT-4 turbo/GPT-4 [29] and Claude-3, Qwen2max [26], and seven open-sourced LLMs, i.e., ChatGLM3-6B [10], Yi-1.5-6B/Yi-1.5-34B [40], Llama3-8B/Llama3-70B [33], Qwen2-7B/Qwen2-72B [26,38]. Following the default parameters, we apply the API for large-scale generation for close-sourced LLMs. For open-sourced LLMs, we download and test the model from Huggingface 3 . Notice that we use fine-tuned models (e.g., Chat, Instruct) in RQ1 for fairness. In contrast, in RQ3, we use base models to encourage diversified generation, exploring the knowledge boundary of models. \n\n4.1.2 Metrics. We evaluated Accuracy in RQ1. The accuracy depends on whether the generation of LLMs conforms to the ground truth. In RQ2, we assess the Quality of datasets using LLM or human assistance. Moreover, we define Inconsistency to evaluate the 2 https://github.com/ming429778/ECKGBench. 3 https://huggingface.co/ reliability: measure whether the model's answers to two questions are inconsistent, where the questions are generated by fixing KG triples and conducting negative sampling twice (i.e., two questions have different false choices). In RQ3, we use metrics (SC, Precsion, Recall) defined in Table 3.",
            "score": 0.46750996081948465,
            "section_title": "Experimental Setup",
            "char_start_offset": 17057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 6,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1338
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.032470703125
        },
        {
            "corpus_id": "267039006",
            "title": "Peer review of GPT-4 technical report and systems card",
            "text": "Limited access to training data and processes prevents confidence over bias. \n\nA model's outputs reflect the data they are trained upon; therefore, without clarity of training processes and data, the extent and ways in which GPT-4 contains encoded biases and interests of third-party data sources is unclear. \n\nMore confidence and uncertainty estimation are needed, particularly in high-risk settings. \n\nGPT-4 will perform differently on different tasks and spheres of knowledge based on the information it is trained on. However, without providing users with an estimation of confidence, each response will be taken with the same, despite different performances. This is particularly troublesome in high-risk settings, such as healthcare. \n\nConcerns of privacy and IP should be acknowledged. \n\nData used to train GPT-4 will be used in outputs. However, if GPT-4 has been trained on personal or sensitive information, then this is at risk of being revealed to another user. This is also true for any text that contains details of documents and codes covered by IP rights. \n\nLLMs will require multiple teams' input during design, evaluation, and oversight. \n\nBiases are complex and require multiple teams to evaluate a model for different pockets of response types. In addition, external input will be required to ensure that a \"competitive landscape\" does not lead to conflict with data privacy or the perpetuation of biases. \n\nWorld-changing technology should include values from around the world. \n\nThe outputs of this model that are deemed appropriate will reflect the training processes and feedback given. Therefore, there is the potential to lock in the values of the group designing this process. To ensure this technology benefits all groups, a diverse group from around the world should be involved in this process. and unconventional weapons, cybersecurity attacks, interactions with other systems, economic impacts, acceleration, and overreliance. Across all criteria, the report authors relied on human evaluators who assessed the quality and coherence of the generated text using specific examples that can trigger and identify any problematic content output. The report, therefore, contributes to future basic and applied research by helping to articulate the evaluation metrics and providing examples of reporting. Future applications of GPT-4 in local contexts should include the effort to conduct, document, and report their model testing evaluations, potentially enhanced by testing against existing content and databases.",
            "score": 0.46685748781923386,
            "section_title": "Limitations",
            "char_start_offset": 11457,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 79,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1155
                },
                {
                    "start": 1158,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2329
                },
                {
                    "start": 2330,
                    "end": 2540
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.123779296875
        },
        {
            "corpus_id": "268231092",
            "title": "Comparing Large Language Models and Human Programmers for Generating Programming Code",
            "text": "The performance of seven large language models (LLMs) in generating programming code using various prompt strategies, programming languages, and task difficulties is systematically evaluated. GPT\u20104 substantially outperforms other LLMs, including Gemini Ultra and Claude 2. The coding performance of GPT\u20104 varies considerably with different prompt strategies. In most LeetCode and GeeksforGeeks coding contests evaluated in this study, GPT\u20104, employing the optimal prompt strategy, outperforms 85 percent of human participants in a competitive environment, many of whom are students and professionals with moderate programming experience. GPT\u20104 demonstrates strong capabilities in translating code between different programming languages and in learning from past errors. The computational efficiency of the code generated by GPT\u20104 is comparable to that of human programmers. GPT\u20104 is also capable of handling broader programming tasks, including front\u2010end design and database operations. These results suggest that GPT\u20104 has the potential to serve as a reliable assistant in programming code generation and software development. A programming assistant is designed based on an optimal prompt strategy to facilitate the practical use of LLMs for programming.",
            "score": 0.4665229094401586,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0059814453125
        },
        {
            "corpus_id": "269010004",
            "title": "Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?",
            "text": "(Zheng et al., 2023) finds that GPT-4 has the ability to match human preferences when comparing two different LLM generated answers to a question.(Liu et al., 2023) presents a framework called G-Eval which uses chain-of-thought prompting and form-filling to evaluate text outputs for coherence.The LLM Prometheus (Kim et al., 2024) is a fine-tuned Llama2-13B LLM designed to act as an evaluator LLM that is aligned with human preferences.(Xie et al., 2023) investigate the use of GPT-4 to evaluate medical notes in terms of factuality.To the best of our knowledge, this work is the first to evaluate the performance of an LLM in reviewing clinical notes for specific criteria of consistency.ACI-Bench (Yim et al., 2023) provides results from comparison of full-note vs division-based note generation techniques, which corresponds to our GENMOD and SPECMOD designs, respectively.Their work finds that SPECMOD results in higher performance then GENMOD.However, ACI-Bench uses a smaller dataset of 207 encounters with an average conversation length of 1,300 tokens.Using our larger proprietary dataset with more average tokens per conversation, we seek to understand whether the comparison of GENMOD to SPECMOD is sensitive to the size of the dataset as well as the dimension upon which the two models are measured (i.e. in terms of full note consistency).",
            "score": 0.4664775405270676,
            "section_title": "Related Work",
            "char_start_offset": 6177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 878
                },
                {
                    "start": 878,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1353
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 164,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.473876953125
        },
        {
            "corpus_id": "259287417",
            "title": "Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors",
            "text": "For a given method and scenario, we have 25 total instances for evaluation, comprising a problem and program (5 \u00d7 5 instances). For each instance, ChatGPT and GPT-4 perform n ChatGPT = n GPT-4 = 1 query to their corresponding LLMs through web platforms to generate one output per instance; Tutor has n Tutor = 2 human experts that independently generate two outputs per instance. 5 We describe further scenario-specific details in the subsequent sections. \n\nMetrics and evaluation process. We will introduce scenario-specific performance metrics in the subsequent sections. We have n evals = 2 human evaluators who provide annotations to assess the quality of generated output for each instance w.r.t. corresponding performance metrics. In our evaluation, this set of n evals = 2 human evaluators is same as n Tutor = 2 human experts that are part of the Tutor method. More concretely, each of the n evals human evaluators independently annotates the quality of generated outputs for ChatGPT, GPT-4, and Tutor (only for the n Tutor \u22121 human experts by excluding the evaluator themselves). Then, for each method, results are first aggregated across 25 instances or across 5 instances when reporting problem-specific results. Finally, we aggregate results across n evals human evaluators and report averaged results as mean (stderr). We provide scenario-specific details in the subsequent sections. \n\nRemark. Since we want LLMs to play the role of experienced digital tutors and assistants, a natural question is whether they can solve five problems used in the evaluation, i.e., generate correct solution programs. Before evaluating ChatGPT and GPT-4 on different programming education scenarios, we checked their problem-solving capabilities by querying them with suitable prompts consisting of a problem description along with a solution template as input and instruction to generate a solution program as output. GPT-4 was able to solve all five problems; Figure 3",
            "score": 0.46575628526671664,
            "section_title": "MERGESTRS BP-25",
            "char_start_offset": 4727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12286376953125
        },
        {
            "corpus_id": "271218870",
            "title": "Efficacy of Various Large Language Models in Generating Smart Contracts",
            "text": "Overall Findings \n\nFrom the qualitative and quantitative data collected, we can determine several key insights about the performance of large language models (LLMs) in smart contract generation: \n\n1. Best Performers: Claude and GPT-4-o emerged as the best performers in our evaluations. These models demonstrated superior capabilities in generating accurate and functional smart contracts compared to other models tested. \n\n2. GPT-4 vs. GPT-3.5: GPT-4 significantly outperformed GPT-3.5 in terms of code accuracy and overall performance. In particular, notice Table 1 & Table 4 where GPT-4 performed at an 89% accuracy for \"Lock\" compared to 0% for GPT-3.5 The advancements in GPT-4 over its predecessor are evident in its ability to handle complex tasks more effectively and generate more reliable code. \n\n3. GPT-4-o Improvements: GPT-4-o represents an improvement over previous models, particularly in generating accurate smart contracts. The enhancements in this model contribute to better handling of the specific requirements of smart contract coding. \n\n4. Code Quality Issues: Despite the improvements, the quality of code generated by LLMs is still lacking. Many instances of redundant code, outdated code, and compilation errors were observed, especially with GPT-3.5. This highlights the need for further refinement and training to improve code generation quality (see Table 3 & Table 6). Code was also rather inefficient compared to human-written equivalent contracts. \n\n5. Security Overlooked: Except for Claude, all models tended to overlook security issues unless explicitly mentioned in the prompt. This indicates a critical area where LLMs need improvement to ensure the generation of secure smart contracts. For example, in Case 2, both Bard and GPT 4-o had a vulnerability in which it allowed an \"unlock time\" to be set earlier than the current time of generation. This could be exploited by programs that can alter times to invalidate transfers. This is a massive security risk and underscores one of the main reasons LLMs are not viable for industry use in smart contract generation currently.",
            "score": 0.46511204121857647,
            "section_title": "4.4",
            "char_start_offset": 18490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 19,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2112
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00760650634765625
        },
        {
            "corpus_id": "272689563",
            "title": "Generating API Parameter Security Rules with LLM for API Misuse Detection",
            "text": "GPT-4 is a powerful LLM that outperforms GPT-3.5. We explored whether using GPT-4 directly can achieve better results in generating APSRs. We provided the same prompt used for raw APSRs generation to GPT-4 and compared the results with GPTAid (using GPT-3.5) on D gt . GPT-4 achieves a recall of 0.67 and a precision of 0.21, compared to the 0.71 recall and 0.92 precision of GPTAid. These results are significantly lower than those of GPTAid, demonstrating GPTAid's effectiveness in generating APSRs. Our analysis shows that GPT-4 is more prone to the stereotypical inference problem mentioned in Section V-C than GPT-3.5, leading to more missed APSRs.",
            "score": 0.4650857691966316,
            "section_title": "A. Exploration on GPT-4",
            "char_start_offset": 60371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 653
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.009124755859375
        },
        {
            "corpus_id": "262138540",
            "title": "Prompt engineering of GPT-4 for chemical research: what can/cannot be done?",
            "text": "However, there are limits to the amount of information that can be included in a prompt (8k or 32k tokens in GPT-4), making it difficult to infer from a wide range of cutting-edge information. Therefore, there is a need for constructing local LLMs that learn specialized data from scratch or through low-cost methods like finetuning, which is being considered worldwide [75]. From a practical perspective, one of the strengths of an LLM operating on a local computer is security. To use GPT-4, data must be sent to a cloud server, but with a local LLM, computations are completed within the laboratory, reducing the barrier when handling confidential information. \n\nThe third issue is c) the inference capabilities of the LLM itself. LLMs have been known to make mistakes in rudimentary mathematical processing and provide answers based on incorrect knowledge. There is still room for improvement in long-term planning capabilities, which seem to be lacking for the realization of fully automated chemical research [2][3][4][5][6]. There may not be much that chemists can contribute to solving this problem. However, deep learning is evolving at a revolutionary pace. Chemists may need to be prepared for the emergence of artificial general intelligence or superintelligence [8]. \n\nIn addition to technical analysis, a profound exploration of the ethical implications of using LLMs in this scientific domain is conducted. LLMs like GPT-4 can inadvertently produce inaccurate or misleading information. This risk is particularly salient in chemistry, where the propagation of false information could instigate hazardous experiments. Accountability issues arise if LLMs dispense harmful or incorrect information, with the intricate training process and involvement of multiple stakeholders adding complexity. \n\nThese ethical considerations, although intricate and far-reaching, are paramount for the responsible and beneficial integration of LLMs like GPT-4 into chemical research. Future endeavors should aim to construct guidelines and best practices to ethically harness the power of LLMs in this discipline.",
            "score": 0.4646975667220335,
            "section_title": "Issues to be addressed",
            "char_start_offset": 42493,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1806
                },
                {
                    "start": 1809,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2109
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.027801513671875
        },
        {
            "corpus_id": "264811464",
            "title": "Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models",
            "text": "human) when Debugging sub-categories were collapsed. Figure 2 provides detailed insight into the differences in handling the student help request classification task between the GPT-3.5 (zero-shot and fine-tuned) and GPT-4 (zero-shot). \n\nFigure 3 demonstrates the key benefit of performing the classification with LLMs, such as GPT-3.5 or GPT-4, as compared to traditional ML algorithms or smaller LLMs. The LLMs performed reasonably even if no or very little (n < 100) labeled data were available. A smaller LLM (RoBERTa base) required several hundred labeled data points to match the zero-shot performance of GPT-4, while a traditional ML algorithm such as random forest required even more labeled data (consistent with findings in other domains [32,34]). A small amount of labeled data (n \u2243 100) was sufficient for the fine-tuned GPT-3.5 to perform comparably to humans on the easier task of labeling the four top-level categories. While it was also possible to match human performance on the more challenging task with the Debugging sub-categories, a larger amount of labeled data was required (n \u2243 400).",
            "score": 0.46462897489708566,
            "section_title": "Results",
            "char_start_offset": 11752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1108
                }
            ],
            "ref_mentions": [
                {
                    "start": 748,
                    "end": 752,
                    "matchedPaperCorpusId": "258557199"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.011962890625
        },
        {
            "corpus_id": "273375563",
            "title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
            "text": "With the emergence of LLMs' capabilities and their large-scale deployment as assistances or tools serving humans, how to evaluate the quality of LLMs' responses is receiving increasing attention. In realworld scenarios, the vast majority of instances do not have a specific ground truth that is definitively better than others. Thus, some previous work has attempted to evaluate the quality of generated responses using the subjective evaluation. This evaluation is often carried out by either using human raters or a powerful model, such as GPT-4, to score the generated text. This subjective evaluation, compared to the objective evaluation, better reflects the alignment of LLMs with human needs. However, to the best of our knowledge, existing research on prompt sensitivity has not involved an analysis of prompt sensitivity on the subjective evaluation.",
            "score": 0.46410266583617754,
            "section_title": "Prompt Sensitivity on the Subjective Evaluation",
            "char_start_offset": 14338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 859
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26025390625
        },
        {
            "corpus_id": "266741775",
            "title": "Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review",
            "text": "The potential of ChatGPT as an intelligent tool for problem-solving. \n\nVerbal insight problems were administered to ChatGPT: the first set was referred to as \"practice problems,\" while the second set was referred to as \"transfer problems\". \n\nChatGPT's global performance in the practice and transfer problems was identical to the most likely results in the human sample. Evaluating the performance of the GPT-4 on a range of cognitive psychology datasets: CommonsenseQA, SuperGLUE, MATH, and HANS. \n\nThe GPT-4 has revolutionary potential to help machines bridge the gap between human and machine reasoning. Shiffrin and Mitchell (2023) Decision-making mechanisms and other psychological qualities of LLMs. \n\nTwo sets of experiments were conducted, using situational prompts from the psychological literature and prompts not in the GPT-3 training corpus. \n\nGPT-3 outperforms humans on some tasks and performs poorly on others. GPT-3 showed surprising abilities in decision-making, information search, and thoughtfulness, but performed poorly in causal reasoning. Marjieh et al. (2023) How LLMs predict human sensory judgments across multiple sensory modalities. \n\nUsing LLMs such as GPT-3\uff0c GPT-3.5 and GPT-4 to predict human judgments in six sensory modalities (pitch, loudness, color, sound, taste, and timbre). \n\nLLMs can successfully predict human perceptual judgments in six modalities. Huang and Chang (2022) How to improve and direct the reasoning of these models. \n\nMethods for assessing the reasoning ability of LLMs: fully supervised fine-tuning, cueing and contextual learning, problem decomposition, mixed-methods. \n\nImproving the reasoning ability of LLMs requires the use of training data, model architectures, and optimization goals specific to reasoning.",
            "score": 0.4638752708553883,
            "section_title": "Orru et al. (2023)",
            "char_start_offset": 12596,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 71,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1768
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00557708740234375
        },
        {
            "corpus_id": "271064558",
            "title": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step Insight Generation",
            "text": "Existing LLM evaluations frameworks focus on handling structured outputs and predominantly rely on pre-formatted prompts to assess code completion (Wu et al., 2023;Zhang et al., 2023a;2024a;Yao et al., 2023). While recent advancements have seen autonomous agents specializing in intricate data science tasks, including analysis, visualization, and modeling (Qian et al., 2023;2024;Zhang et al., 2023a), evaluations for these methods often depend on extensive human effort (Cheng et al., 2023) or use more powerful LLMs to assess the output (Dubois et al., 2023;Belyi et al., 2024). \n\nAnother work uses the \"Capture the Flag\" principle, where insights are planted into a dataset as flags to evaluate whether models can uncover them (Laradji et al., 2023). G-Eval (Liu et al., 2023) is a recent technique used to evaluate the quality of free-form texts in terms of factuality and coherence. In this work, we use a variation of G-Eval to score how well the predicted insights are aligned with the ground-truth insight. \n\nText-to-Analytics Agents Chen et al. ( 2023) explore the application of GPT variants within a data visualization context, highlighting the strengths and limitations of these models. More recent LLM-based data analysis agents include Code Interpreter (OpenAI, 2024) and Pandas Agent (LangChain, 2024) that are capable of processing multiple data formats and answering questions about them. Ma et al. (2023) propose InsightPilot, an advanced automated tool that leverages LLMs to enhance data exploration by automatically identifying goals and generating targeted intentional queries. Vacareanu et al. (2024) showed that LLMs can also perform regression tasks, enhancing their predictive abilities. Additional studies assess the data analysis capabilities of GPT-4 and propose an end-to-end framework for automating data processes (Cheng et al., 2023;Wang et al., 2023;2024;Hong et al., 2024).",
            "score": 0.46343574773087737,
            "section_title": "LLM Evaluation Frameworks",
            "char_start_offset": 30097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1909
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 207,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 472,
                    "end": 492,
                    "matchedPaperCorpusId": "258866019"
                },
                {
                    "start": 540,
                    "end": 561,
                    "matchedPaperCorpusId": "258865545"
                },
                {
                    "start": 762,
                    "end": 780,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.192138671875
        },
        {
            "corpus_id": "270688337",
            "title": "On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey",
            "text": "Benchmark Evaluation.The performance of downstream models trained on the generated data can also reflect the generation quality to some extent (Yu et al., 2023b;Chung et al., 2023b).Specifically, the impact of synthetic data can be evaluated from multiple dimensions except for the specialized capabilities of the downstream models.For example, TruthfulQA enables the assessment of a model's ability to identify true claims (Sun et al., 2023); NIV2 is employed to evaluate a model's language comprehension and reasoning abilities across multiple tasks (Wang et al., 2023e).\n\nOpen Evaluation.For open-ended benchmarks, evaluation by humans or auxiliary models is necessitated due to the absence of standardized answers.To fully leverage the preference outputs of the auxiliary models, multiple evaluation strategies have been designed, such as response ranking (Xu et al., 2023a), four-level rating system (Wang et al., 2023e) and Elo scores (Bai et al., 2022).To further reduce evaluation costs, Sun et al. ( 2023); Xu et al. (2023a) utilize the automatic evaluation framework based on GPT-4 proposed by Vicuna for evaluation.However, general LLMs may lack enough knowledge for domain-specific tasks, which hinders them to provide effective evaluation (Bran et al., 2023).Therefore, collecting human assessment data to fine-tune open-source models for evaluation purposes is an important practice in real-world scenarios (He et al., 2023).Other techniques like (Peng et al., 2024(Peng et al., , 2023) ) remain to be further explored.\n\n4 Future Directions",
            "score": 0.46330707619825057,
            "section_title": "Indirect Evaluation",
            "char_start_offset": 25201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 21,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 573
                },
                {
                    "start": 575,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 960
                },
                {
                    "start": 960,
                    "end": 1126
                },
                {
                    "start": 1126,
                    "end": 1272
                },
                {
                    "start": 1272,
                    "end": 1439
                },
                {
                    "start": 1439,
                    "end": 1533
                },
                {
                    "start": 1535,
                    "end": 1554
                }
            ],
            "ref_mentions": [
                {
                    "start": 552,
                    "end": 572,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 905,
                    "end": 925,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1479,
                    "end": 1502,
                    "matchedPaperCorpusId": "261065007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423828125
        },
        {
            "corpus_id": "270216385",
            "title": "A Survey of Useful LLM Evaluation",
            "text": "Human language data is important and valuable in every subdomain in psychology.Because LLMs have the capability to understand and utilize multiple language, emotion detection and psychological measurement can be done by LLMs.Plenty of researches evaluated whether LLMs could complete these tasks with enough quality.Rathje et al. (2023) tested whether different versions of GPT (3.5 Turbo, 4, and 4 Turbo) can detect sentiment, discrete emotions, offensiveness, and moral foundations in text across 12 languages.They found that LLMs outperformed existing English-language dictionary analysis at detecting psychological constructs as judged by manual annotators.Lu et al. (2024) evaluated GPT-4V's performance in 5 crucial abilities for affective computing tasks.They used DISFA dataset (Mavadati et al., 2013) to assess GPT-4V's ability to action unit detection, RAF-DB dataset (Shan and Deng, 2018) for facial expression and compound emotion recognition (Du et al., 2014), CASME2 dataset (Yan et al., 2014) for Micro-expression Recognition (Zhao et al., 2023), and iMiGUE dataset (Liu et al., 2021a) for Microgesture Recognition.The results showed that GPT-4V could give satisfactory answers to action unit, compound emotion and Micro-gesture test samples, but failed to answer facial expression and Micro-expression test samples correctly.Regarding psychological measurement, Demszky et al. (2023) proposed 2 methods to evaluate the effects of features on human thought and behaviour: 1) Expert evaluation means trained research assistants and LLMs score the same texts for particular psychological construct, and then compute agreement between their scores.2) Impact evaluation means assessing the effect before and after the manipulation.For instance, Karinshak et al. (2023) used impact evaluation to measure participants' attitude to GPT-3-generated pro-vaccination messages.Demszky et al. (2023) additionally proposed that in assessing the capability of LLMs for psychological tasks, initial assessment could be conducted using expert evaluation for a manipulation check or a measure of construct validity.",
            "score": 0.46316921328228056,
            "section_title": "Psychology",
            "char_start_offset": 32053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 79,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 316
                },
                {
                    "start": 316,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1341
                },
                {
                    "start": 1341,
                    "end": 1660
                },
                {
                    "start": 1660,
                    "end": 1742
                },
                {
                    "start": 1742,
                    "end": 1881
                },
                {
                    "start": 1881,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 786,
                    "end": 809,
                    "matchedPaperCorpusId": "4246027"
                },
                {
                    "start": 878,
                    "end": 899,
                    "matchedPaperCorpusId": "52164815"
                },
                {
                    "start": 955,
                    "end": 972,
                    "matchedPaperCorpusId": "24675185"
                },
                {
                    "start": 989,
                    "end": 1007,
                    "matchedPaperCorpusId": "17756042"
                },
                {
                    "start": 1041,
                    "end": 1060,
                    "matchedPaperCorpusId": "259823625"
                },
                {
                    "start": 1081,
                    "end": 1100,
                    "matchedPaperCorpusId": "235694220"
                },
                {
                    "start": 1378,
                    "end": 1399,
                    "matchedPaperCorpusId": "264107446"
                },
                {
                    "start": 1756,
                    "end": 1779,
                    "matchedPaperCorpusId": "258171641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0537109375
        },
        {
            "corpus_id": "269448872",
            "title": "Advancing Healthcare Automation: Multi-Agent System for Medical Necessity Justification",
            "text": "Confidence Score (f ) Calculation Effect of LLM Choice: We conducted an evaluation of the Propagator Agent utilizing various LLMs, with a particular emphasis on metrics such as accuracy and latency.Opus and GPT-4 emerged as the top performers, achieving approximately 94-95% accuracy when CoT prompting was combined with ICL examples.GPT-3.5 is ranked second in terms of accuracy but presents significant benefits in reduced latency compared to GPT-4 and Opus, as depicted in Figure 11.Additionally, the operational costs associated with GPT-3.5 are substantially lower.Although selecting the optimal model involves a trade-off, GPT-3.5 stands out as the preferred option when considering a balance among cost, latency, and accuracy.Nonetheless, for scenarios where maximum accuracy is crucial, the larger models such as GPT-4 and Opus are more appropriate.",
            "score": 0.46307222675556814,
            "section_title": "Rule Set for No Information Items",
            "char_start_offset": 22082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 486
                },
                {
                    "start": 486,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007694244384765625
        },
        {
            "corpus_id": "270286247",
            "title": "On The Persona-based Summarization of Domain-Specific Documents",
            "text": "(C) Validation of finetuned model generated summaries: To further investigate the reliability of our finetuned model generated summaries, we choose 50 different WebMD articles and provide persona-based summaries for each persona (generated by GPT-4 in ground truth v/s Llama2-13b finetuned model generated) to two doctors to annotate: (i) whether finetuned generated summary is better, (ii) Both are Good, (iii) Ground Truth/GPT-4 summary is better and (iv) Both are bad.We find that for -20% cases Llama2-13b finetuned model summaries are better (i), for 50% cases both finetuned and ground truth generated summaries are good (ii) and rest 30% cases ground truth generated summaries are better (iii) and no instance is found where both performs bad (iv).(D) Different LLM Evaluators: We evaluate the fine-tuned model generated summaries in the test set with Gemini model 9 keeping the same prompts and criteria as used earlier for GPT-4 and the obtain 9 https://gemini.google.com/values of the same LLM based criteria are: .Thus, Gemini scores are also aligned with GPT-4 scores with a correlation coefficient of 0.808 (Gemini provides higher scores for all criteria except Criteria 3 -'Imp').This verifies that the GPT-4 based evaluation is impartial and robust.\n\n(E) Llama2-13b performance on Other data: We test our best performing Llama2-13b finetuned model on healthcare domain articles of OA-SUM (Yang et al., 2022) dataset which is publicly available.We select OASUM articles with aspects related to healthcare [Death, Diagnosis, Differential Diagnosis and Diagnosis Classification] and obtain 234 such documents.We perform GPTcritique based evaluation and observe that 82.77% of the summaries are labeled as good which signifies the robustness of our model in terms of generating high quality summaries.",
            "score": 0.46271996063507015,
            "section_title": "Evaluation and Results",
            "char_start_offset": 12692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 471
                },
                {
                    "start": 471,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 981
                },
                {
                    "start": 981,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1194
                },
                {
                    "start": 1194,
                    "end": 1264
                },
                {
                    "start": 1266,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1621
                },
                {
                    "start": 1621,
                    "end": 1812
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09234619140625
        },
        {
            "corpus_id": "267750150",
            "title": "NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization",
            "text": "Algorithm 1 MMLU Calculation using LLM Require: R (reference sentences), P (predicted sentences), model_path (LLM file path) Ensure: mmlu (Mean Modified Logarithm Units) \n\n\u25b7 where N is the number of elements in logits tensors 9: return mmlu_score For qualitative evaluation, we utilized the GPT4 Assistant API (\"gpt-4-1106-preview\") [18]and established the following evaluation criteria to assess the quality of generated summaries: \n\n\u2022 Accuracy: Measures how accurately the summary reflects the main content and contextual meaning of the input data. \n\n\u2022 (Information) Retention: Assesses the ability of the summary to retain key information and details from the input data. \n\n\u2022 Objectivity: Evaluates whether the summary maintains the objectivity of the input data, checking for the addition of information not present in the original that could distort the input data. \n\n\u2022 Structure: Examines the systematic organization and orderly arrangement of the summary. \n\n\u2022 Coherence: Focuses on the logical and consistent semantic structure of the summary, especially the connections between sentences. \n\n\u2022 Grammar: Checks the grammatical accuracy of the summary, including correct grammar, expressions, and punctuation. \n\n\u2022 Readability: Considers the clarity and readability of the summary, ensuring that it effectively conveys the content without unnecessary repetition or verbosity. \n\nTen DSs from the test set were randomly selected and evaluated using the GPT4 Assistant API. The sequential input dataset was provided as the first input, followed by the summaries generated by the SFT-T5 and NOTE, respectively. GPT4 then assigned each summary a score based on the seven evaluation criteria, with each criterion scored out of 10 points.",
            "score": 0.46243029206147745,
            "section_title": "Evaluation",
            "char_start_offset": 14461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1735
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.062225341796875
        },
        {
            "corpus_id": "267770450",
            "title": "Towards building multilingual language model for medicine",
            "text": "Evaluation Settings. To comprehensively assess the model's performance, we tested it in three different evaluation settings: zero-shot, parameter efficient fine-tuning (PEFT), and full fine-tuning. For the zero-shot setting, we directly test off-the-shelf LLMs with proper instruction, without any further exposure to the training part of MMedBench. In addition to zero-shot, to better evaluate the performance differences between models, we also try to fine-tune and then test the open-source models. There are two mainly used fine-tuning approaches: parameter efficient fine-tuning (PEFT), and full fine-tuning. In the former case, only a small subset of the model's parameters are trainable, representing the performance in a low computing resource available scenario. We adopt the most representative PEFT method LoRA [13] here. In the latter case, all parameters will be fine-tuned which is a more conventional practice. \n\nNext, we will introduce the baseline LLMs considered in our work for comparison: \n\n\u2022 GPT-4 [1], a groundbreaking multilingual large language model developed by OpenAI, stands as one of the most sophisticated LLMs to date. Due to the confidentiality of data and model details, it is uncertain for its detailed model scale. Though GPT-4 does not emphasize it is a multilingual LLM, its multilingual ability is still superior. Given that it is only accessible through API, we evaluate it in a zero-shot manner with proper instruction (Check Supplementary Material A for more details). \n\n\u2022 GPT-4 (5-shot, CoT) uses in context learning [5] and chain-of-thought [20] to further improve the performance of GPT-4, which represents the highest performance currently achievable. For implementation, we follow prompts used in MedPrompt [31]. Notice that, despite this approach can enhance the performance of different LLMs, it will take up more tokens and result in additional costs. \n\n\u2022 GPT-3.5 [33] is also developed by OpenAI.",
            "score": 0.46160259172139184,
            "section_title": "Comprehensive Multilingual Medical Benchmark",
            "char_start_offset": 40890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 822,
                    "end": 826,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1559,
                    "end": 1562,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1584,
                    "end": 1588,
                    "matchedPaperCorpusId": "258841149"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01358795166015625
        },
        {
            "corpus_id": "268857105",
            "title": "Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models",
            "text": "Furthermore, we experiment with a metric that evaluates the quality of LLM-generated distractors indirectly: we ask GPT-4 to answer MCQs in the test set under two scenarios: one using LLMgenerated distractors and the other using humanauthored ones.We then calculate and compare the solve rates.We found that LLM-generated distractors using kNN, the best-performing method under alignment-based metrics, are more difficult (71% solve rate) than the human-authored ones (72.5%).This result implies that the LLM-generated distractors are not placeholders, which would make it very easy for GPT-4 to select the key.However, solve rate cannot be used to evaluate the real quality of distractors and whether they reflect common errors or misconceptions among real students.",
            "score": 0.46137104602454104,
            "section_title": "Evaluation Metrics",
            "char_start_offset": 13816,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 611
                },
                {
                    "start": 611,
                    "end": 767
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057586669921875
        },
        {
            "corpus_id": "265213217",
            "title": "Fusion-Eval: Integrating Assistant Evaluators with LLMs",
            "text": "In both solutions, the prepared evaluation prompt template is used with each test example. This template is filled with the inputs, responses, and assistant evaluator scores for each test example. The executing LLM then processes this filled prompt, yielding Fusion-Eval's final evaluation scores as shown in the rightmost textbox in Figure 1. We found that the LLM generated evaluation scores in the correct format, so we did not need to do anything else to control the outputs. \n\nThe executing LLM processes the complete prompt and generates a numerical score for each evaluation dimension. The LLMs are configured to produce 8 predictions with temperatures Once the plan is obtained, it is concatenated with the template. The template placeholders are filled in for each test example along with its specific assistant evaluators' scores. This complete prompt is then used to obtain the Fusion-Eval evaluation score from the LLM. of 0.5 for PaLM2 and 0.1 for GPT-4. The final Fusion-Eval scores are the average of 8 predictions. We do this because we can't obtain log probabilities from the GPT API.",
            "score": 0.4606848636589663,
            "section_title": "Prompt Execution",
            "char_start_offset": 6890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1101
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203369140625
        },
        {
            "corpus_id": "265034254",
            "title": "GPTuner: A Manual-Reading Database Tuning System via GPT-Guided Bayesian Optimization",
            "text": "Effect of Error Correction Mechanisms. We measure the reliability of the two mechanisms with two manually prepared datasets. \n\n(1) Evaluating step 2 as a filter of step 1. Firstly, we utilize GPT-4 to generate domain knowledge for 150 knobs from PostgreSQL, and then conduct a survey with a team of five database experts to annotate the generated knowledge. Among all pieces of knowledge, 123 of them are consistent with the system_view of PostgreSQL and the remaining 27 are not. Next, we use GPT-4 to filter out noisy knowledge as discussed in Section 5.1. GPT-4 performs well in this task by identifying 20 pieces of noisy knowledge out of 27 pieces with a recall rate of 74%, and classifying 109 pieces of correct knowledge out of 123 pieces with a specificity rate of 88.6%. In summary, equipped with our step 2 as a filter of noisy knowledge, we improve the accuracy of input knowledge from 82% to 94%. \n\n(2) Evaluating step 4 as a corrector of step 3. At first, we prepare domain knowledge from GPT-4, manuals and web for 150 knobs from PostgreSQL. Next, we use GPT-3.5 and GPT-4 to complete step 3 and step 4, where they result in 7 and 4 inconsistencies respectively and rewrite all of them correctly. Since the number of inconsistencies is low and we want to further evaluate the LLM's reliability, we additionally prepare a dataset containing 50 summaries that are factual inconsistent with the original knowledge. Among the 50 incorrect pieces of knowledge, GPT-4 detects 39 of them are incorrect with an accuracy of 78%, and rewrite all of them correctly with an accuracy of 100%. The degradation of accuracy can be attributed to the complexity and diversity of the manually prepared knowledge, which pose challenges for LLM to process. Given that GPT-4 detects and corrects all inconsistencies in the real dataset, it is feasible to utilize GPT-4 as a corrector of step 3. Effect of Domain Knowledge Quality. We study how GPTuner's performance is affected by the quality of input domain knowledge. \n\n(1) Outdated Knowledge.",
            "score": 0.4606224848061101,
            "section_title": "Robustness Study",
            "char_start_offset": 43086,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 124
                },
                {
                    "start": 127,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2011
                },
                {
                    "start": 2014,
                    "end": 2037
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09912109375
        },
        {
            "corpus_id": "268247854",
            "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4",
            "text": "Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-sourced models, especially GPT-4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. While the fine-tuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this work, we conduct an empirical study of judge models. Our findings indicate that although the fine-tuned judge models achieve high performance on in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across several dimensions, including generalizability, fairness, aspect-specific evaluation, and scalability. We also reveal that the fine-tuned judge model inherently operates as a task-specific classifier, consequently imposing the limitations. Finally, we introduce a integrated method, leveraging GPT-4 to compensate for the limitations and improve the fine-tuned judges. Experiment results show our method achieves accuracy on par with GPT-4 with only 50% of the API expense.",
            "score": 0.4605059926614496,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1961669921875
        },
        {
            "corpus_id": "271957282",
            "title": "Towards Synthetic Trace Generation of Modeling Operations using In-Context Learning Approach",
            "text": "(iii) Q-GRAM: (GPT-4, GPT3.5) with p = 0.004; (GPT-4, Gemini) with p = 0.010; (GPT-4, Llama3) with p \u00ab 0.001. \n\nFinally, we used the Independent Samples T-Test to determine which of the LLMs considered in the previous combinations is the best in terms of the mean value. This analysis confirms that GPT-4 outperforms the other LLMs in terms of the considered similarity measures. \n\nHallucination: Automatic evaluation of non-realistic events have been implemented considering the Hallucination metric defined in Eq. 18 in Section 5.3. Table 2 shows the statistics related to the hallucination metrics. The Confidence Interval (CI) of the mean assumes sample means follow a t-distribution with N-1 degrees of freedom. Based on Table 2, GPT-4 is the only LLM with better values than the others, with a CI < 1 (CI-L = 0.874, CI-U = 0.95), an IQR equal to 0 (up to the 95th percentile all values are \u2264 1, minimizing hallucination effects), and with SE (0.0192), SD (0.352) and Variance (0.124) lower than all other LLMs. Applying the One Sample T-Test on GPT4 with the Null Hypothesis  0 : (  \u22124 > 1), we get a p-value \u00ab 0.001, allowing us to reject the  0 . This confirms that GPT-4, regarding the hallucination metric, is the best LLM among those considered. \n\nAnswer to RQ 1 : The results show that the traces generated by the LLMs are very similar to those generated by a human, especially considering GPT-4, which has the best statistical results and reduced hallucination effects. This demonstrates that LLMs can be used as tools to reliably emulate modeling operations that a human could perform. \n\nAddressing  2 . To answer this research question, we analyze two different aspects, i.e., the amount of training data and how synthetic data can be used to replace human-generated operations.",
            "score": 0.45992219828136754,
            "section_title": "RESULTS",
            "char_start_offset": 39828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 112,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1793
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034698486328125
        },
        {
            "corpus_id": "263152773",
            "title": "Disinformation Detection: An Evolving Challenge in the Age of LLMs",
            "text": "Input: Table 4 demonstrates ChatGPT's performance across various CoT prompts and datasets. Notably, GPT-4 consistently outperforms GPT-3.5 across all configurations. The misclassification rates for GPT-4 (all scale)\" are recorded at 4.7%, 11.9%, and 22.2% for D gpt std , D gpt mix , and D gpt cot , respectively. Critical elements to model performance are the event and time elements. Interestingly, \"GPT-4 (w/o person)\" and \"GPT-4 (w/o place)\" produce relatively good results on D gpt cot . We speculate this could be attributed to the retention of original person and place information in our LLMgenerated disinformation. This ablation study provides a deeper understanding of the importance of contextual elements for disinformation detection, suggesting that advanced prompts paired with LLMs hold the potential to effectively counter LLM-generated disinformation.",
            "score": 0.4598754325843143,
            "section_title": "Proposed Solution (RQ3).",
            "char_start_offset": 23135,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 869
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03289794921875
        },
        {
            "corpus_id": "274437466",
            "title": "Can Large Language Models Serve as Evaluators for Code Summarization?",
            "text": "We examine the impact of using different LLMs for evaluating code summarization, specifically the GPT-3.5 series models (i.e., text-davinci-003 and gpt-3.5-turbo) and GPT-4 (i.e., gpt-4). text-davinci-0031 provides high-quality outputs with reliable instruction-following ability, while gpt-3.5-turbo offers extended context length suitable for conversational applications. gpt-4 demonstrates high accuracy in complex problem-solving, making it effective for both interactive and traditional tasks. Experimental results across various LLMs in the reference-free scenario are presented in Figure 5. Obviously, the gpt-4 model exhibits a superior performance over the gpt-3.5 series of models. Moreover, the text-davinci-003 demonstrates well-rounded performance across overall evaluations. While gpt-4 outperforms the gpt-3.5 series, its higher API cost should be considered. Thus, we recommend choosing different LLMs as evaluators based on performance needs and budget considerations.",
            "score": 0.45982856592469656,
            "section_title": "D. RQ2: Influence of Evaluator Settings 1) Influence of Evaluator Types:",
            "char_start_offset": 26776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 985
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146728515625
        },
        {
            "corpus_id": "274280574",
            "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
            "text": "Evaluating harmlessness in text data is important for both content moderation and for creating or curating synthetic datasets. Given how expensive and time-consuming human labeling efforts are, and inspired by prior work that looks into moral beliefs embedded in LLMs (Scherrer et al., 2023), many recent works have investigated the use of LLMs to evaluate harmlessness. State-of-the-art LLMs are capable of being used effectively for content moderation, either off the shelf when guided with some policy guidelines3 , or when fine-tuned on safe/unsafe data (Inan et al., 2023;Zhang et al., 2024e) 4 . Ye et al. (2024b) explore the feasibility of using LLMs to evaluate harmlessness in a finegrained manner, among other attributes, and find that proprietary models perform much better than open source ones. Wang et al. (2024i) use Ope-nAI's GPT-4 to evaluate harmlessness and further compare the performance with that of a smaller pretrained language model fine-tuned for this specific task. Additionally, Bai et al. (2022) use principles to guide LLMs to make harmlessness evaluations for alignment purposes, a paradigm they call Constitutional AI. (Phute et al., 2023) use the same LLM to evaluate its responses as harmful, and provide insights and best practices for GPT-3.5 and Llama-2. Xie et al. (2024a) perform a comprehensive comparison of several LLMs on a benchmark of LLM safety refusals and find small LLMs are effective safety judges when used in fine-tuned settings. During inference time, (Li et al.) propose Rewindable Auto-regressive INference (RAIN), which allows LLMs to conduct self-evaluation and rewind for AI safety.",
            "score": 0.45951028184399884,
            "section_title": "Harmlessness",
            "char_start_offset": 10346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1640
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 291,
                    "matchedPaperCorpusId": "260164518"
                },
                {
                    "start": 558,
                    "end": 577,
                    "matchedPaperCorpusId": "266174345"
                },
                {
                    "start": 602,
                    "end": 619,
                    "matchedPaperCorpusId": "259991144"
                },
                {
                    "start": 808,
                    "end": 827,
                    "matchedPaperCorpusId": "268417319"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03759765625
        },
        {
            "corpus_id": "270559394",
            "title": "KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs",
            "text": "The KGPA framework uses the PRE module to filter out low-quality or altered prompts from the APGP module, ensuring high-quality adversarial samples.Figure 12 shows that with tau_llm be-  11: ASR comparison of LLMs on specialized domains with/without few-sample attack strategies low 0.9, ASR differences between GPT-3.5-turbo and GPT-4.0-turbo are minimal.Raising tau_llm above 0.9 widens the ASR gap, stabilizing ASR of GPT-3.5-turbo and decreasing GPT-4-turbo.Based on Figure 13, this occurs because removing lowquality prompts makes adversarial prompts of GPT-4-turbo closer to the original.Since the LLM used by the PRE module to calculate the LLMScore is the same LLM being evaluated for robustness, it reflects the corresponding LLM's robustness.Thus, tau_llm was set at 0.92 for most experiments.",
            "score": 0.45944315418454973,
            "section_title": "Experimental Analysis of KGPA Modules",
            "char_start_offset": 22267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 594
                },
                {
                    "start": 594,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 803
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04742431640625
        },
        {
            "corpus_id": "270391675",
            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
            "text": "This may involve tailoring the aspects of evaluation relevant to each field.For example, while evaluating legal documents, aspects such as legal accuracy and adherence to the judicial system are crucial (Cui et al., 2023b), which differ significantly from the aspects relevant in medical or financial texts.Therefore, to enhance the efficacy of LLMs as evaluators in specialized domains, there's a pressing need to develop models that are not only domain-aware but also equipped with the capability to evaluate based on domain-specific criteria.\n\nUnified Evaluation.LLMs have been expanded w.r.t their broad capabilities beyond traditional single-task focuses, encompassing complex instructions like coding and open-ended real-world requirements (OpenAI, 2023; Significant Gravitas).Consequently, there is an increasing demand for more comprehensive and flexible evaluation methods.However, traditional evaluation methods and most current LLM-based evaluators are limited to constrained tasks and evaluation aspects (cf.Table 2).Some promising attempts have been made in this direction.For instance, MT-Bench (Zheng et al., 2023) uses GPT-4 as an evaluator across multiple domains for multi-turn questions.Yet, this is too confined to a few evaluation aspects and limits dialogue to two turns only.Another model, Auto-J (Li et al., 2023b), approaches from a data construction perspective, training a 13B LLM on user queries and GPT-4 generated responses across a wide range of real-world scenarios.It accommodates diverse evaluation protocols and has been validated in 58 different scenarios, even outperforming many proprietary LLMs.In light of increasingly complex user queries, we advocate that developing a more unified and contemporaneous evaluation protocol is a promising direction.Additionally, constructing high-quality, comprehensive datasets to train unified models holds great potential.Such advancements could significantly contribute to more effective and universal evaluations of LLMs.",
            "score": 0.4592069904585303,
            "section_title": "Challenges and Open Problems",
            "char_start_offset": 43950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 76,
                    "end": 307
                },
                {
                    "start": 307,
                    "end": 545
                },
                {
                    "start": 547,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 783
                },
                {
                    "start": 783,
                    "end": 882
                },
                {
                    "start": 882,
                    "end": 1020
                },
                {
                    "start": 1020,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1086
                },
                {
                    "start": 1086,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1298
                },
                {
                    "start": 1298,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1634
                },
                {
                    "start": 1634,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 1899
                },
                {
                    "start": 1899,
                    "end": 2000
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464599609375
        },
        {
            "corpus_id": "269213992",
            "title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
            "text": "Of PE tools, only EvalLM offers a way to help users calculate the alignment of LLM evaluators with their expectations, but this feature is mentioned only in authors' Design section and is absent from their user study. 2 At best, users of PE tools inspect LLM-generated evaluator outputs manually to double-check; at worst, the tool hides individual scores entirely.Regardless of aligning implementations of metrics with user preferences, even identifying what metrics to evaluate for custom tasks remains challenging for LLM practitioners, as evidenced by a recent study [37].While many evaluation tools require users to declare metrics they care about, some prior work [45] and EvalGen employ LLMs to propose custom metrics based on prompts in the user's LLM pipelines.\n\nOver-trust and Over-generalization of LLM Behavior.That tools provide little assistance to validate evaluator quality is alarming, considering that other research shows people tend to over-rely and over-trust AI systems [3,28,31,50].For instance, in one highprofile incident, researchers from MIT posted a pre-print on arXiv claiming that GPT-4 could ace the MIT EECS exam.Within hours, work by Chowdhuri et al. debunked the study [5], citing problems arising from over-reliance on GPT-4 to grade itself.Other work has found further reasons to be cautious: LLMs asked to choose the best response from a set can be consistently biased by set ordering [30,51]; and LLMs can be highly sensitive to seemingly innocuous formatting changes [43].\n\nA related problem to over-reliance is over-generalization.Zamfirescu et al. [57] found that users unfamiliar with PE tend to overgeneralize from single failures (causing them to throw out potentially good prompts), rather than having a holistic view of the overall performance of a prompt or chain.This was despite the fact that the interface had support for systematic testing.",
            "score": 0.45909009826688596,
            "section_title": "MOTIVATION AND RELATED WORK",
            "char_start_offset": 3508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 365
                },
                {
                    "start": 365,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 770
                },
                {
                    "start": 772,
                    "end": 823
                },
                {
                    "start": 823,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1145
                },
                {
                    "start": 1145,
                    "end": 1276
                },
                {
                    "start": 1276,
                    "end": 1511
                },
                {
                    "start": 1513,
                    "end": 1571
                },
                {
                    "start": 1571,
                    "end": 1811
                },
                {
                    "start": 1811,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 995,
                    "end": 998,
                    "matchedPaperCorpusId": "263138444"
                },
                {
                    "start": 998,
                    "end": 1001,
                    "matchedPaperCorpusId": "248476449"
                },
                {
                    "start": 1001,
                    "end": 1004,
                    "matchedPaperCorpusId": "254591809"
                },
                {
                    "start": 1589,
                    "end": 1593,
                    "matchedPaperCorpusId": "258217984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18359375
        },
        {
            "corpus_id": "261397017",
            "title": "Large Language Models as Data Preprocessors",
            "text": "We use the datasets evaluated in [12]. We evaluate three LLMs: GPT-3.5-turbo-0301 (referred to as GPT-3.5), GPT-4-0314 (referred to as GPT-4), and GPT-4o-2024-05-13 (referred to as GPT-4o). The temperature parameter for these models is set at 0.35. For SM tasks, we use 3 few-shot examples, and for other tasks, we use 10. The default batch prompting method is random batching. The batch size ranges for GPT-3.5, GPT-4, and GPT-4o are [10,20], [10,15], and [5,10], respectively. As baselines, we employ GPT-3 (text-davinci-002) with the best settings [12] for all four tasks, and HoloClean [15] and HoloDetect [6] for ED, IPM [11] for DI, SMAT [19] for SM, and Magellan [8], Ditto [10], Unicorn/Unicorn ++ [17], and Table-GPT [9] for EM. As these baselines have been evaluated in [12], we use these results as a reference. Open LLMs like LLaMA are not considered here, as they are generally less competitive than close models [4].",
            "score": 0.45884457273194773,
            "section_title": "EXPERIMENTS 4.1 Experimental Setup",
            "char_start_offset": 10320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 930
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 37,
                    "matchedPaperCorpusId": "248965029"
                },
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "214743579"
                },
                {
                    "start": 444,
                    "end": 448,
                    "matchedPaperCorpusId": "214743579"
                },
                {
                    "start": 448,
                    "end": 451,
                    "matchedPaperCorpusId": "16301819"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "214743579"
                },
                {
                    "start": 551,
                    "end": 555,
                    "matchedPaperCorpusId": "248965029"
                },
                {
                    "start": 590,
                    "end": 594,
                    "matchedPaperCorpusId": "16301819"
                },
                {
                    "start": 610,
                    "end": 613,
                    "matchedPaperCorpusId": "102353360"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "235617644"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "237207055"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "214743579"
                },
                {
                    "start": 706,
                    "end": 710,
                    "matchedPaperCorpusId": "259077311"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "248965029"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0139007568359375
        },
        {
            "corpus_id": "273507427",
            "title": "Arabic Dataset for LLM Safeguard Evaluation",
            "text": "We use GPT-4o for automatic safety evaluation of the LLM responses above. To assess the reliability of GPT-4o as a judge, we compare its evaluations with human annotations and GPT-4 assessments. We randomly sampled 300 questions across five risk areas and three question types in English, Arabic, and Chinese, examining its evaluator ability in a multilingual setting. Each native-speaker author annotated one language. \n\nIn Table 6, GPT-4o outperforms GPT-4 in recall across the three datasets, especially for Chinese and Arabic, suggesting greater reliability of GPT-4o in identifying unsafe content. However, this comes at the cost of lower precision, particularly on Chinese, where precision is 12.0% using GPT-4o vs. 20.0% for GPT-4. We prioritize recall over precision in evaluating LLM safety to minimize the risk of missing unsafe content, even if it results in more false positives. The confusion matrices in Figure 7 reinforce the finding that GPT-4o captures more unsafe cases at the expense of slightly more false positives, which is generally acceptable in safetycritical applications where missing a risk could be more detrimental than overestimating it.",
            "score": 0.4584308206545211,
            "section_title": "Human Evaluation",
            "char_start_offset": 20227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1168
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.150390625
        },
        {
            "corpus_id": "266348592",
            "title": "Do LVLMs Understand Charts? Analyzing and Correcting Factual Errors in Chart Captioning",
            "text": "Our human assessments focus on comparing C2TFEC with GPT-4V by using the same annotation tasks detailed in \u00a72 for factual error identification, with the same annotators evaluating. We sampled 30 charts from each split of LVLM, LLM, and FT. For each chart, human judges are presented with a caption generated by one of the models. \n\nFigure 3 demonstrates C2TFECs superiority in multiple error categories, especially with a substantial decrease in Value Errors, over 20% better in the LVLM and LLM splits, and halving the overall error rate compared to GPT-4V. C2TFEC virtually eliminated Trend Errors, highlighting its strong error correction ability, particularly for axes-related errors like Label, Value, and Trend errors. A representative comparison is shown in Figure 4. GPT-4V's shortcomings seem to stem from its failure to accurately infer data point values from charts as evidenced in Figure 7. \n\nIn contrast, GPT-4V is better in addressing Outof-context Errors, involving information out of the chart's scope. However, GPT-4V seemed challenged in rectifying errors within captions generated by itself, particularly within the LVLM split. This observation echoes recent findings on LLMs' inability to self-correct (Huang et al., 2023a;Valmeekam et al., 2023), we find that LVLMs also cannot perform self-correction. More importantly, our human evaluation results, combined with our findings in Table 4 and Table 12, reflect that GPT-4V is subject to serious self-enhancement bias. Consequently, although GPT-4V's capabilities are formidable, we recommend not using them to assess their own outputs. \n\n7 Related Work",
            "score": 0.4581947677618158,
            "section_title": "Human Evaluation",
            "char_start_offset": 22535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1623
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03192138671875
        },
        {
            "corpus_id": "267499939",
            "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",
            "text": "We report the results of our review both quantitatively and qualitatively. Specifically, we report the number of works surveyed leaking data to GPT-3.5 or GPT-4 in such a way that it can be used by OpenAI to further improve the model (according to their data policy). In this paper we do not distinguish between works leaking data to GPT-3.5, GPT-4, or both. This is because indirect data leaking is caused by browser access, where both models are available through the ChatGPT Plus subscription. We also note that OpenAI confirmed that creating GPT-4 involved the use of ChatGPT to some extent. 13 For this reason, we estimate the data leakage to be effectively shared across the two models and for simplicity, we refer to both models as \"ChatGPT\" from now on. \n\nWe also document a series of evaluation practices emerging for the work reviewed that is problematic with respect to objectiveness and reproducibility. Finally, drawing upon our results, we present a series of best practices for researchers evaluating OpenAI's and other closedsource LLMs.",
            "score": 0.45785382797494956,
            "section_title": "Interpreting the findings",
            "char_start_offset": 14214,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1053
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014617919921875
        },
        {
            "corpus_id": "273901294",
            "title": "Subjective Topic meets LLMs: Unleashing Comprehensive, Reflective and Creative Thinking through the Negation of Negation",
            "text": "Therefore, we define six evaluation di-mensions denoted as Eval dim and the detailed requirements are provided in Table 10. Then, given Eval dim and a question Q, we generate specific scoring points by prompting the LLM: Sol point = M(Q, Eval dim ). By adhering to the criteria outlined by each evaluation dimension and considering the specific knowledge in question, the LLM will generate organized and reasonable scoring points that maintain both specificity and quality. Finally, we generate a complete solution, Sol comp = M(Q, Sol point , Eval dim ), that is comprehensive, profound, and innovative to serve as a more intuitive reference. We use GPT-4 (OpenAI, 2023) as the LLM for the whole data generation process, see prompt templates in Appendix A.3 and data examples in Appendix A.4.",
            "score": 0.45770395616674797,
            "section_title": "Data Generation",
            "char_start_offset": 8146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 793
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0723876953125
        },
        {
            "corpus_id": "262084217",
            "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset",
            "text": "In Table 5, we find GPT-4 wins 52% in Top-50 but only 22% in Bottom-50 against GPT-3.5-turbo, suggesting Top-50 prompt set is much more effective in benchmarking models. \n\nBased on this methodology, we identified the 200 most challenging prompts that get 9+ score agreed by GPT-3.5-Turbo, Claude-2, and GPT-4. Manual inspection confirms their superior quality (see examples in Appendix B.8). We then create a benchmark, Arena-Hard-200, to evaluate cutting-edge LLMs in the field. We score each model's answer with GPT-4 as judge approach (Zheng et al., 2023). In Figure 6, Arena-Hard-200 effectively ranks models and reveals larger performance gaps between open and proprietary models (e.g., GPT-4, Claude) than MT-Bench (Zheng et al., 2023), suggesting more rooms for open models to catch up in this challenging real-world task set. \n\nWe believe more research on LLM evaluation can be developed with this dataset (e.g., better categorization on user prompts, study selection bias of LLM graders) and leave them for future study.",
            "score": 0.4576845258216929,
            "section_title": "CREATING CHALLENGING BENCHMARK QUESTIONS",
            "char_start_offset": 17871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 1029
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06256103515625
        },
        {
            "corpus_id": "272826664",
            "title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
            "text": "However, all the experimented LLMs are far from understanding the nuances of scien-1 Please find the code and data at the github repo : llmscience-miscommunication tific rigor, particularly in relation to open problems (Findings #2). We investigate whether proprietary LLMs can successfully verify LLM-generated responses to these complex scientific questions (Contribution #3), revealing their shortcomings in verifying different aspects of the generated response (Findings #3). Finally, we perform a human evaluation of GPT-4 Turbo generated responses to a subset of questions from SCiPS-QA (Contribution #4). Alarmingly, the persuasive style of generation adopted by GPT-4 Turbo is enough to deceive human evaluators to trust the reasoning, particularly when answers are included in the response (Findings #4).",
            "score": 0.45722937896453697,
            "section_title": "Introduction",
            "char_start_offset": 3867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 813
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017303466796875
        },
        {
            "corpus_id": "268820220",
            "title": "Large Language Model Evaluation Via Multi AI Agents: Preliminary results",
            "text": "In this section of our research, we conducted an evaluation of code performance across various language models.This assessment was carried out by developing a sophisticated algorithm that utilized the API keys of the respective language models.The core objective was to methodically analyze and compare the output quality and efficiency of each model when tasked with generating code from high-level descriptions.In the pursuit of understanding the capabilities of various LLMs in the context of code generation, we conducted a comprehensive evaluation using the HumanEval benchmark to gauge the accuracy of the code produced by each model.Table 1 presents a synthesized view of this evaluation, showcasing the performance of different LLMs across identical input descriptions.\n\nUpon early testing and analysis, our results indicated that GPT-3.5 Turbo exhibited superior performance compared to other language models like GPT-4, GPT-4 Turbo, Google Bard, Hugging Face, and LLAMA.Specifically, GPT 3.5 Turbo provided accurate results for seven out of the ten inputs.This achievement marks a significant margin of excellence compared to its counterparts.In contrast, other LLM models in the study produced fewer accurate outputs, as can be observed from the comparative data in the Table 1.\n\nTable 1 enumerates seven different LLMs, each produced by renowned organizations with varying parameter sizes-from 355M by Hugging Face to 1.96 trillion by GPT-4 Turbo.Each model was provided with the same set of ten input descriptions to process and generate code.The primary focus of the evaluation was on the accuracy of the results, which refers to the functional correctness of the generated code as per the given description.Additionally, a quality rating, depicted with stars, provides a subjective assessment of the code based on criteria such as readability, efficiency, and adherence to best practices.\n\nThe data shows a varied performance across models.GPT-3.5 Turbo, with 154 billion parameters, leads with the highest number of accurate results, successfully generating correct code for seven out of ten descriptions.This is followed closely by GPT-4 Turbo, which each returned six accurate pieces of code.Notably, despite Google Bard's significantly larger parameter count, it did not outperform GPT-3.5 Turbo, indicating that larger model size does not necessarily equate to better performance in specific tasks such as code generation.",
            "score": 0.4569182585791772,
            "section_title": "PROPOSED MULTI AGENT MODEL (RQ1)",
            "char_start_offset": 23767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 244
                },
                {
                    "start": 244,
                    "end": 413
                },
                {
                    "start": 413,
                    "end": 640
                },
                {
                    "start": 640,
                    "end": 777
                },
                {
                    "start": 779,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1153
                },
                {
                    "start": 1153,
                    "end": 1289
                },
                {
                    "start": 1291,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1556
                },
                {
                    "start": 1556,
                    "end": 1722
                },
                {
                    "start": 1722,
                    "end": 1903
                },
                {
                    "start": 1905,
                    "end": 1955
                },
                {
                    "start": 1955,
                    "end": 2121
                },
                {
                    "start": 2121,
                    "end": 2210
                },
                {
                    "start": 2210,
                    "end": 2442
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0284423828125
        },
        {
            "corpus_id": "266999933",
            "title": "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step",
            "text": "The detailed experimental results are shown in Tab. 1. In this section, We aim to answer three research questions below. Q1: Which Model is Better at Tool Utilization? \n\nThe results in Tab. 1 show that GPT-4 achieves the highest score, with an overall score of 86.4, setting the pilot of the well-instructed and skillful tool-utilization LLMs. Apart from GPT-4, APIbased commercial LLMs, including GPT-3.5 and Claude2, get competitive scores on both string and JSON formats, indicating their strong abilities in acting as tool agents. \n\nAs for open-source models, we evaluate models with three different scales: around 7B, 13B, and 70B. It can be concluded from Fig. 3 that the performance of the model monotonically increases as the model scale increases. Among them, Qwen-7B gets the best of two worlds in terms of the model sizes and evaluation scores. With 7 billion parameters, Qwen-7B exhibits a competitive ability to understand complicated instructions and reply in First, open-source LLMs lack the instructionfollowing ability to respond with specific formats, which is the very first step to constructing wellregulated and high-usable tool agents. Without a legal response format, the system can not successfully extract the information generated by LLMs by a constant protocol, not to mention the correct tool executions. However, only a small amount of models achieve both high scores on INSTRUCT sub-set under string and JSON format. Besides, there are still large performance gaps between string and JSON evaluation protocols on other subsets. Considering that the understand ability of the Qwen-72B is comparable with GPT-4 evaluated in string format (84.5 vs 83.2), its JSON format result is more than 20 points lower, pinpointing the necessity to enhance the ability of open-source LLMs solving problems with specific formats. \n\nSecond, tool retrieval presents a relatively challenging task for most LLMs. Even the largest opensource model, Qwen-72B only reaches 65.0%, which is more than 20 points lower than GPT-3.5.",
            "score": 0.4564536377814213,
            "section_title": "Main Results",
            "char_start_offset": 15736,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1843
                },
                {
                    "start": 1846,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2035
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021942138671875
        },
        {
            "corpus_id": "265498312",
            "title": "Biomedical knowledge graph-optimized prompt generation for large language models",
            "text": "To conduct robust benchmarking, we curated datasets that underwent review by domain experts. \n\nGiven the swift progress in LLM research, we believe that such rigorously vetted datasets could serve as valuable resources not only for evaluating KG-RAG but also for assessing other ongoing LLM endeavors in biomedicine. In our benchmarking analysis, we found an enhancement in LLM performance as a function of the model size in terms of the number of parameters. \n\nIntriguingly, with the KG-RAG framework the performance of GPT-4 on the MCQ dataset, despite its model size, dropped significantly compared to that of the GPT-3.5-Turbo on the MCQ dataset. In fact, the performance of GPT-3.5-Turbo under KG-RAG framework was on par with that of the GPT-4 model on True/False datasets. These results suggest that at present, GPT-3.5 may be a better context listener than GPT-4. In fact, a recent study compared the March 2023 version of GPT-4 with the June 2023 version, shedding light on the drift in the LLM performance over time. (Chen, Zaharia and Zou 2023) The study revealed that, as time progressed, GPT-4 exhibited a reduced tendency to adhere to user instructions.(Chen, Zaharia and Zou 2023) In contrast, there was no consistent alteration observed in the instructionfollowing behavior of GPT-3.5 over time. (Chen, Zaharia and Zou 2023) When the proprietary GPT models were compared to the open-source Llama-2-13b model, they showed a narrow margin in performance on the biomedical True/False dataset. However, on the more challenging MCQ dataset, Llama-2 initially demonstrated lower performance compared to GPT models. Interestingly, the KG-RAG framework provided a substantial performance boost to Llama-2, improving its performance by ~71% from the baseline. Despite this boost that narrowed the performance gap, the performance of Llama-2 remained lower than that of the GPT models.",
            "score": 0.45636109267959557,
            "section_title": "Discussion",
            "char_start_offset": 10546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 95,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1891
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0289764404296875
        },
        {
            "corpus_id": "271769401",
            "title": "Using Large Language Models to Evaluate Biomedical Query-Focused Summarisation",
            "text": "Inspired by work by Shu et al. (2023), we experimented with the use of LLMs as meta-evaluators of 3 different evaluators. In particular: \n\n1. To aid the LLM in scoring repetition, we provided it with a \"Repscore\" which took the number of unique words in a response and divided it by the total number of words. \n\n2. Smog score (Laughlin, 1969), which looks at the number of polysyllabic words and sentences in a text, was used to aid the scoring of readability. \n\n3. Finally, the output from the fine-tuned GPT-3.5 model was also included to aid with the scoring of all dimensions. \n\nTesting was done primarily on GPT-4, since GPT-3.5 showed a very limited capability in reasoning with other scores.",
            "score": 0.4562735612702807,
            "section_title": "LLMs as Meta Evaluators",
            "char_start_offset": 11176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 309
                },
                {
                    "start": 312,
                    "end": 460
                },
                {
                    "start": 463,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 698
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 342,
                    "matchedPaperCorpusId": "9571753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1602783203125
        },
        {
            "corpus_id": "273403835",
            "title": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended Question Generation",
            "text": "In this section, we discuss the generation of questions via prompting GPT-3.5 Turbo (Section 4.1), the direct approach to evaluating the quality of the generated questions (Section 4.2), the feedback-based approach for evaluating generated question quality (Section 4.3), and the correlation between the best-performing LLM and human experts (Section 4.4). In our experiments, we used state-of-the-art LLMs, such as GPT-4 [Achiam et al., 2023], Gemini [Team et al., 2023], and Llama2-70b [Touvron et al., 2023], to assess both direct and feedback-based approaches.",
            "score": 0.4559508562550898,
            "section_title": "Methodology",
            "char_start_offset": 4233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 564
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09552001953125
        },
        {
            "corpus_id": "273345934",
            "title": "SensorBench: Benchmarking LLMs in Coding-Based Sensor Processing",
            "text": "We benchmark four leading LLMs -GPT-4o (Ope-nAI, 2023b), GPT-4 (Achiam et al., 2023), GPT-3.5-turbo (OpenAI, 2023a), and Llama-3.1-70b (Dubey et al., 2024) -on SensorBench to assess their capabilities in sensor data processing tasks. This evaluation aims to answer our first research question (Q1) about how well LLMs perform compared to human experts.",
            "score": 0.4557669945209941,
            "section_title": "Evaluating LLMs on Sensorbench",
            "char_start_offset": 12575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 352
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.022796630859375
        },
        {
            "corpus_id": "269293024",
            "title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models",
            "text": "Figure 10 shows cross-validation evaluation results. We change the model to generate reference answers and the model to serve as evaluators respectively. Results show while using GPT4 output as the reference answer for 8 scores, the order between the three models remains unchanged no matter the evaluation model. While using Qwen or GLM's output as both reference generator and evaluator, there exists a strong incline towards the model itself. \n\nWe also check the rating bias of different LLMs , r = 8 \n\n(1) where Rating A\u2212B (B) representing using LLM A as an evaluator and the output of LLM B as a reference answer to evaluate the performance of LLM B. r is the score for reference, in our cases, it is equal to 8. Note that in the experiments, reference answer and output awaiting score are two-time outputs of the same model, so if model B is stable and model A can score accurately, the Rating A\u2212B (B) should be r. We measure the Rating Bias to measure the calibrability of the evaluator model. As shown in the results, compared to Qwen-max and GLM-4, GPT-4 performs more accurately in this cross-validation experiments.",
            "score": 0.4556331592145816,
            "section_title": "D.4 Comparison with Different Evaluation Method",
            "char_start_offset": 31802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0830078125
        },
        {
            "corpus_id": "268033311",
            "title": "From RAGs to riches: Utilizing large language models to write documents for clinical trials.",
            "text": "Our assessment focused on GPT-4, one of the leading LLMs available today 12 and utilizing it to generate key sections of clinical trial protocols. The LLM output was subsequently assessed in terms of writing quality. Specifically we analyzed four dimensions: Clinical thinking and logic, which measures how closely recommendations from regulatory guidance documents were included in the generated section; Transparency and references, which verifies the presence and relevance of cited scientific sources in the generated text; Medical and clinical terminology, which assesses the use of appropriate jargon and scales of measurements; Content relevance and suitability, which measures, among others, whether the generated protocol section was specific to the disease and trial phase. An overview of the methodology is given in Figure 1, and a full description of both the generation and evaluation process, including the criteria and requirements used, is provided in the Supplementary Information. \n\nOur analysis does not make direct comparisons between LLM-written text and fully human-written text. This is because, from our experience in the field, there is often substantial variability between individual human writers. It is therefore challenging to establish a single, objective ''ground truth'' to compare against. Our evaluation framework, with its four dimensions described above, addresses this challenge by breaking down the assessment into discrete sub-dimensions which can be assessed objectively. Our assessment targeted two key sections of a clinical trial protocol document: the endpoints section and the eligibility criteria section. Two LLM models were evaluated: off-the-shelf GPT-4 via its commercially available application programming interface, 6,12 used as a baseline; and a retrieval-augmented generation (RAG) GPT-4 as an alternative to the off-the-shelf version (see Figure 1(b)). RAG is a methodology for incorporating knowledge from external databases, 13 and involves providing the LLM with external sources of knowledge to supplement the model's internal representation of information. 14 The RAG-augmented LLM was configured as follows: based on a user input query below, an LLM-powered decision agent automatically decided which tools to use to fetch relevant context and feed it to an LLM for final summarization and document generation.",
            "score": 0.4554844713290924,
            "section_title": "Methods",
            "char_start_offset": 2454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 2119,
                    "end": 2121,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1407470703125
        },
        {
            "corpus_id": "263829791",
            "title": "Generative Judge for Evaluating Alignment",
            "text": "It is universally known that the best way to evaluate LLMs is human judgment, but collecting human annotations can be costly, time-consuming, and laborious (Ouyang et al., 2022;Zheng et al., 2023). \n\nUsing strong LLMs (usually closed-source ones, e.g., GPT-4, Claude, ChatGPT) as an automated proxy for assessing LLMs has become a natural choice (Zhou et al., 2023). With appropriate prompt design, the quality of evaluation and agreement to human judgment can be promising (Dubois et al., 2023;Zheng et al., 2023;Zhang et al., 2023;Wang et al., 2023a). However, the cost concern still exists when calling the APIs of these proprietary models, especially when there is a frequent need for model validation on large-scale data. Moreover, closed-source evaluation leads to low reproducibility due to potential changes in models behind the API. Some recent works have started to make attempts for open-source alternatives. SelFee (Ye et al., 2023)",
            "score": 0.45529053559334154,
            "section_title": "EVALUATION OF LLMS",
            "char_start_offset": 7057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 200,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 944
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 177,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040771484375
        },
        {
            "corpus_id": "276249976",
            "title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "text": "Output-Based Methods. While the above methods usually align with human evaluation better than conventional metrics, they do not explain their scores or support certain closed-source LLMs that do not provide probabilities or representations. Output-based methods prompt LLMs to output the judgments, and do not require access to their internal implementations. G-Eval [31] utilizes Chain-of-Thought (CoT) [50] to request evaluation steps, samples multiple scores and then averages them as the final score. ChatEval [3] assigns different personas to several LLM agents, asking them to discuss and select a better response from two. \n\nSome researchers construct training sets to fine-tune LLMs instead of designing prompting or inference strategies. InstructScore [54] is fine-tuned on GPT-4-synthesized data to generate error reports of text from various domains. PandaLM [48] is fine-tuned on pairwise comparison results and reference answers generated by GPT-3.5, aiming at addressing subjective aspects including conciseness and clarity. X-Eval [30] has an extra training stage to learn the connections between fine-grained evaluation aspects, allowing evaluating from aspects not seen during training. \n\nHowever, these methods have not been tested on a sufficient number of challenging SE samples, leaving it unclear whether they achieve reliable human alignment for SE applications.",
            "score": 0.455275993513932,
            "section_title": "2.3.3",
            "char_start_offset": 13256,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1385
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "260887105"
                },
                {
                    "start": 761,
                    "end": 765,
                    "matchedPaperCorpusId": "258841553"
                },
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "259108266"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "265212670"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.287353515625
        },
        {
            "corpus_id": "271769784",
            "title": "To What Extent Are Large Language Models Capable of Generating Substantial Reflections for Motivational Interviewing Counseling Chatbots? A Human Evaluation",
            "text": "We mainly focus on comparing human-authored reflections to the reflections generated by GPT-4, as it is widely accepted as the current state-of-the-art, and adopted as the standard choice by many individuals. Although, the Open LLM Leaderboard1 serves as a benchmark for tracking progress of the LLM technology publicly and encourages the adoption of more open-source practices, Liesenfeld and Dingemanse (2024) highlight that the degree of openness of these LLMs in practice varies significantly. The growing lack of scientific documentation and transparency in LLMs regarding data collection poses challenges for ensuring fairness and privacy (Liesenfeld et al., 2023). In contrast, BLOOM (Scao et al., 2022) is a model developed by scientific community adhering to open-science principles and remains the most open model according to Opening up ChatGPT2 list. Hence, as an addition to GPT-4, we explore whether the openscience model, BLOOM, can generate substantial reflections to enrich the predetermined chatbots when applied with the current standards. For perspective, we also add its proprietary open-source peer, FLAN-T5 (Chung et al., 2024), into our evaluations. \n\nOur findings support that LLMs can enhance reflections in motivational interviewing chatbots. Moreover, we found that GPT-4 has the ability to produce reflections of a comparable quality as human-authored ones. Nevertheless, further analysis reveals that such applications should be approached with caution.",
            "score": 0.45526765901478683,
            "section_title": "Introduction",
            "char_start_offset": 4153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1483
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 411,
                    "matchedPaperCorpusId": "270286712"
                },
                {
                    "start": 645,
                    "end": 670,
                    "matchedPaperCorpusId": "259837343"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042388916015625
        },
        {
            "corpus_id": "273549733",
            "title": "From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical Integrity on Faulty Mathematical Problems",
            "text": "The primary task for LLMs in this research is to determine whether they can identify issues in given false math problems. However, directly asking this question would signal to the LLMs that the problem is false, thereby influencing their response. The same constraint applies to human annotators; we cannot explicitly ask whether the questions are false. Instead, we ask them to solve the problems without providing any hints. By analyzing their solutions, we can determine whether the falsehood was detected. For instance, if a solution mentions that the problem is solvable only with additional constraints, it indicates awareness of the issue. Consequently, evaluating the LLMs' responses to false math problems requires careful and nuanced assessment. \n\nTo tackle this challenge, we sought to develop an accurate automated evaluator using an LLM, designed to closely align with human assessments. We first established a gold standard by having two PhD-level reviewers evaluate responses from GPT-4-Turbo (2024-04-09 Update) on the dataset. The reviewers found that GPT-4-Turbo failed to identify issues in 300 of the 363 genuine false math problems, achieving a gold-standard accuracy of 17.36% (63 out of 363 identified). We then tested different auto-evaluation prompts with two other LLMs, GPT-4 and GPT-4o, comparing their performance against the gold standard. We use LLM s to denote the LLM to be evaluated (i.e., the problem solver) and LLM e to denote the LLM as an evaluator. \n\nAuto-Evaluation Prompts. For the automatic evaluation with LLM e , we tried the following prompting techniques: Inferred model: GPT-4-Turbo. Eval = the performance of GPT-4-Turbo using respective evaluation method. \n\n\u2022 Simple Prompt + Explanation: Insert \"Why Problem X is a False and Unsolvable Math: ['Simple Explanation']\" right after the LLM Y's Solution. \n\n\u2022 Chain-of-thought prompt: Provide one positive and one negative example, and explain step by step how the LLM Y identified or missed the falsehood of the math problem. Here, instead of answering \"yes\" or \"no\" directly, we prompted it to reason, and finally answer.",
            "score": 0.4546652155498494,
            "section_title": "EVALUATION METHOD",
            "char_start_offset": 12402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1851
                },
                {
                    "start": 1854,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2119
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11376953125
        },
        {
            "corpus_id": "272380956",
            "title": "Large Language Models for Mental Health Applications: Systematic Review",
            "text": "Evaluating ChatGPT at different stages separately and comparing its performance to that of humans can lead to varied conclusions. In the assessment of prognosis and treatment planning for depression using LLMs, GPT 3.5 demonstrated a distinctly pessimistic prognosis that differed significantly from those of GPT-4, Claude, Bard, and mental health professionals [128]. Therefore, continuous monitoring and evaluation are essential to fully understand and effectively use the advancements in LLM technologies.",
            "score": 0.45449285009307205,
            "section_title": "Limitations of the Selected Articles",
            "char_start_offset": 39253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 508
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 367,
                    "matchedPaperCorpusId": "266874600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0379638671875
        },
        {
            "corpus_id": "268531430",
            "title": "GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture",
            "text": "Inaccurate, biased, or incomplete data can significantly impact the model's outputs.Moreover, AI systems like ChatGPT can assist decision-making but are not a substitute for human intuition and experience in complex agricultural environments [16].Besides, Silva et al. [17] evaluate the capability of LLMs, including GPT-4, GPT-3.5, and Llama2, in responding to agriculturally-related queries.The queries were sourced from agricultural examinations and datasets from the United States, Brazil, and India.The study assessed the accuracy of answers produced by LLMs, the effectiveness of retrieval-augmented generation (RAG) and ensemble refinement (ER) techniques, and the comparative performance against human respondents.Silva et al. [17] discovered that in various tasks, GPT-4 performed better than GPT-3.5 and Llama2, achieving an impressive 93% accuracy rate in the certified crop adviser (CCA) exam.Additionally, in the study by Jiajun et al. [18], the application of LLMs, particularly GPT-4, in agriculture for pest and disease diagnosis is explored.Jiajun [18] introduces a novel approach that combines the deep logical reasoning capabilities of GPT-4 with the visual comprehension abilities of the You Only Look Once (YOLO) network.The paper evaluates the YOLO-PC, a new lightweight variant of YOLO, using metrics such as accuracy rate (94.5%) and reasoning accuracy (90% for agricultural diagnostic reports), assessing the quality of model-generated text in correlation with the recognized information [18].",
            "score": 0.45447420991614473,
            "section_title": "Application of LLMs",
            "char_start_offset": 5841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 84,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1518
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "257883762"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "261655632"
                },
                {
                    "start": 1065,
                    "end": 1069,
                    "matchedPaperCorpusId": "261655632"
                },
                {
                    "start": 1513,
                    "end": 1517,
                    "matchedPaperCorpusId": "261655632"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0863037109375
        },
        {
            "corpus_id": "270199552",
            "title": "FineRadScore: A Radiology Report Line-by-Line Evaluation Technique Generating Corrections with Severity Scores",
            "text": "Recent works have found LLMs, GPT-4 in particular, to be promising in evaluating NLG outputs. G-Eval, a GPT-4-based metric, evaluates NLG outputs using chain-of-thought prompting (Liu et al. (2023)). For a pair of candidate and ground-truth reports, G-Eval assigns a score based on the quality of the candidate report. The authors found that G-Eval is more aligned with human judgement than BLEU and BERTScore. In the medical domain, G-Rad is a similar technique, which prompts GPT-4 to identify the number of clinically significant and insignificant errors in a given report according to a set of error categories (Chaves et al. (2024)). Additionally, entailment-based prompts have been used on LLMs in the medical space to obtain a single score for each pair of reports (Zhu et al. (2023), Xie et al. (2023)). \n\nWe expand on these methods in our work by exploring whether GPT-4 can go beyond just identifying the number of errors to generating the corrections for those errors. Additionally, our work maintains the ability to detect the significance of each error by also producing a clinical severity rating for each error. Finally, we explore one of the Claude-3 models, which to the best of our knowledge, has not yet been examined in this task.",
            "score": 0.4541563225752792,
            "section_title": "GPT-4 as an Evaluator.",
            "char_start_offset": 6721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1250
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.320556640625
        },
        {
            "corpus_id": "268225777",
            "title": "A bilingual benchmark for evaluating large language models",
            "text": "Nori et al. (2023) evaluated GPT-4 on the United States Medical Licensing Examination (USMLE): a three-step examination that physicians must pass in order to obtain a medical license to practice medicine in the United States that is sponsored by the Federation of State Medical Boards (FSMB) and the National Board of Medical Examiners (NBME). Katz et al. (2023) investigated the performance of GPT-4 on legal language, which is often sophisticated and complex, by giving it (GPT-4) questions from the bar exam. \n\nThere has been some work on investigating LLMs' multilingual behavior. Lai et al. (2023) conducted an evaluation of ChatGPT across seven distinct tasks (part-of-speech tagging, named entity recognition, relation extraction, natural language inference, question answering, common sense reasoning, and summarization) and 37 diverse languages. Their findings indicate that ChatGPT's performance is suboptimal for NLP tasks across various languages, emphasizing the need for task-specific models to ensure optimal performance. Bang et al. (2023) research on the multilingual evaluation of ChatGPT demonstrates that its understanding of non-Latin scripts (Chinese and Korean) is superior to its ability to generate them. \n\nWhen it comes to Arabic-focused LLM evaluation, there is not much work in the area. We will discuss three related works and their limitations. First, in OpenAI's (OpenAI, 2023) GPT-4 report, the authors used machine translation to translate an English questions data set (MMLU (Hendrycks et al., 2021b;Hendrycks et al., 2021a)) to Arabic. While this can be thought of as a simple experiment to showcase GPT-4's performance in Arabic, it cannot be considered an accurate benchmark for the Arabic linguistic capabilities of an LLM. Second, Khondaker et al. ( 2023) evaluated ChatGPT's performance on many NLG and NLU tasks spanning 60 datasets.",
            "score": 0.4540363248701373,
            "section_title": "RELATED WORK",
            "char_start_offset": 11845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1874
                }
            ],
            "ref_mentions": [
                {
                    "start": 585,
                    "end": 602,
                    "matchedPaperCorpusId": "258079179"
                },
                {
                    "start": 1509,
                    "end": 1534,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1534,
                    "end": 1559,
                    "matchedPaperCorpusId": "220968818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014007568359375
        },
        {
            "corpus_id": "274234997",
            "title": "Can AI grade your essays? A comparative analysis of large language models and teacher ratings in multidimensional essay scoring",
            "text": "The correlation analysis revealed that GPT models exhibit higher inter-criteria correlation among evaluation categories compared to human raters. This phenomenon can be attributed to the autoregressive nature of LLMs, where each generated token is influenced by preceding outputs, thereby creating a more integrated and consistent reasoning process across different evaluation dimensions. Specifically, GPT-3.5 and GPT-4 showed strong correlations ( \u2265 0.86 resp.  \u2265 0.75) between criteria, suggesting that its assessments are highly dependent on prior ratings. While this may enhance consistency, it also raises concerns about the model's ability to independently evaluate each criterion based solely on the original text. In contrast, o1 demonstrated lower and more diverse correlations than GPT-3.5 and GPT-4, likely due to its larger parameter set and enhanced context understanding, which allow for a more balanced focus between the input text and prior evaluations. This results in a reasoning process that better reflects human evaluative practices, particularly in content-related categories. The open-source models LLaMA 3 and Mixtral, however, maintained weaker correlations across criteria, indicating a fragmented evaluation approach that lacks the cohesive reasoning seen in GPT models. These patterns suggest that the architectural and training differences between closed-source and open-source models significantly impact their evaluation strategies and reliability. \n\nWhile for humans content-related criteria like main part and plot logic have the highest influence on the overall rating, GPT-4 sets a greater focus on language aspects like expression and spelling. This discrepancy points, again, to a basic limitation where LLMs may prioritize surface-level linguistic features over deeper content analysis, potentially skewing the overall assessment towards aspects like spelling and punctuation rather than substantive content quality.",
            "score": 0.45387239689714454,
            "section_title": "Inter-Criteria Correlations emphasize Consistent Reasoning in GPT Models",
            "char_start_offset": 34138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1955
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0535888671875
        },
        {
            "corpus_id": "271065007",
            "title": "It Cannot Be Right If It Was Written by AI: On Lawyers' Preferences of Documents Perceived as Authored by an LLM vs a Human",
            "text": "Legal Reasoning The survey by Huang and Chang (2023) notes the significant progress in NLP caused by the introduction of LLMs. However, the authors point out that the extent to which LLMs can properly reason remains unclear. Blair-Stanek et al (2023) evaluated GPT-3 on the statutory-reasoning dataset SARA. The authors noted apparent limitations due to imperfect prior knowledge of U.S. statutes and poor performance on synthetic statutes not encountered during training. Studies by Nguyen et al (2023a,c,d) reported further limitations of using LLMs for legal reasoning. Additionally, Nguyen et al (2023b) analysed the performance of GPT-3.5 and GPT-4 on the COLIEE Task 4 dataset. The authors raised concerns about the GPT models' ability to generalize and learn adaptable rules for unknown cases. During the preliminary exploration of the ability of GPT-3.5 for reasoning about the FOIA requests, Baron et al (2023) reported ChatGPT performing below the level of an experienced FOIA reviewer. On the other hand, ChatGPT exhibited the ability to bring valuable recommendations accompanied by legal reasoning. Yu et al (2023) suggested that the reasoning capabilities of LLMs can be significantly improved by Chain-of-Thought prompting and fine-tuning with explanations. The research indicates that the best results are achieved using prompts directly derived from specific legal reasoning techniques (e.g., IRAC). Guha et al (2023) prepared a robust typology for organizing legal tasks and evaluated 20 LLMs from 11 different families to provide a benchmark for the legal reasoning capabilities of LLMs. Following the modified IRAC structure, the evaluation revealed diverging performance levels, with GPT-4 emerging as the most successful model. Generally, LLMs perform better on classification tasks than those focused on application. Kang et al (2023) evaluated GPT-3.5's ability to conduct IRAC analysis. The research found that powerful LLMs can provide reasonable answers but mostly fail to yield correct reasoning paths. Finally, Janatian et al (2023) suggest using GPT-4 to extract pathways from real-world legislation to support the development of legal expert systems.",
            "score": 0.4533157940421306,
            "section_title": "Use of LLMs in Legal Domain",
            "char_start_offset": 17443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 52,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 901,
                    "end": 919,
                    "matchedPaperCorpusId": "259373427"
                },
                {
                    "start": 1112,
                    "end": 1127,
                    "matchedPaperCorpusId": "259848886"
                },
                {
                    "start": 1417,
                    "end": 1434,
                    "matchedPaperCorpusId": "261064672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00926971435546875
        },
        {
            "corpus_id": "258436851",
            "title": "A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models",
            "text": "The XYZ protein plays a pivotal role in plant cellular signaling pathways. \n\nFigure 10: An illustration of personalized MT, adapting to user's domain-specific knowledge and language proficiency. \n\naccount the unique characteristics of LLM-based MT, such as the ability to generate fluent but inaccurate translations or the sensitivity to domainspecific knowledge. Possible approaches to a new evaluation paradigm include using specificallydesigned human evaluations (Graham et al., 2020;Ji et al., 2022) for such systems, or even directly employ LLMs to evaluate the translation output from LLMs (Kocmi and Federmann, 2023) -although studies show that LLMs would prefer the translation output from LLMs instead of other systems (Liu et al., 2023). Besides, using extrinsic evaluation is also feasible -the translation output can be used in other tasks and measure the corresponding performance instead of directly assessing the translation quality (Moghe et al., 2023). However, developing a new evaluation paradigm also poses several challenges. One challenge is how to balance the trade-off between evaluation efficiency and evaluation quality, as human evaluations can be time-consuming and expensive, and LLM-based evaluation can be biased. Another challenge is how to ensure the reliability and validity of the evaluation results, as different evaluators may have different subjective judgments or biases. An example of using GPT-4 to evaluate the translation output for a tweet from Elon Musk is shown in Figure 8. Although GPT-4 can analyze the text based on the standards it lists, there is a certain hallucination phenomenon, which means pointing out errors that do not exist in the translation text. Overcoming these challenges requires rigorous experimental design, statistical analysis, and transparency in reporting.",
            "score": 0.4531630472056474,
            "section_title": "Output:",
            "char_start_offset": 22490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 77,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1829
                }
            ],
            "ref_mentions": [
                {
                    "start": 466,
                    "end": 487,
                    "matchedPaperCorpusId": "226283480"
                },
                {
                    "start": 487,
                    "end": 503,
                    "matchedPaperCorpusId": "247411224"
                },
                {
                    "start": 948,
                    "end": 968,
                    "matchedPaperCorpusId": "254877237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11395263671875
        },
        {
            "corpus_id": "270391675",
            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
            "text": "Some works tune LLMs to provide quality ratings or labels for generated texts.Gekhman et al. (2023) employed FLAN-PaLM 540B (Chung et al., 2022) to annotate the quality of real model-generated summaries and utilized these annotated data as training data to tune a light-weight LLM (e.g., T5-11B) as a factual consistency summary evaluator, which predicts \"1\" if the summary demonstrates factual consistency and \"0\" otherwise.Yue et al. (2023) reused and repurposed the existing fact-checking, NLI, and summarization tasks datasets and obtained simulated data from open-domain QA datasets to tune lightweight LLMs for attribution evaluation, which generates attributable, extrapolatory or contradictory labels for the generated answer with given query and reference documents.Li et al. (2023a) created a dataset containing multiple scenarios and used GPT-4 (OpenAI, 2023) to generate evaluation judgments for each scenario as supervision signals to tune LLaMA as a generative evaluator, which can output overall quality rating for individual LLM-generated response in various scenarios.Wang et al. (2023a) repurposed existing datasets with proper anonymization and new personalized labels to tune LLaMA2 (Touvron et al., 2023) as a personalized story evaluation model which provides personalized evaluation for generated texts through outputting a grade in [1, 10] and detailed reviews.Kim et al. (2023a) prompted GPT-4 to construct training data, including reference answers and crafted diverse customized score rubrics, and used them to tune LLaMA to evaluate modelgenerated responses of given instruction, which is generalized to realistic user demands.Ke et al. (2023) instructed GPT-4 to collect referenced and reference-free training data with dialogue-based prompting, utilized to tune LLMs for evaluating the alignment of model-generated texts with human instructions through generating scores and explanations.(Liu et al., 2023a) constructed a referencefree instruction-tuning dataset tailored for multi-aspect evaluation across summarization, dialogue and data-to-text tasks.",
            "score": 0.4530904223012332,
            "section_title": "Tuning-based Evaluation",
            "char_start_offset": 23998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 78,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1385
                },
                {
                    "start": 1385,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1918
                },
                {
                    "start": 1918,
                    "end": 2084
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57275390625
        },
        {
            "corpus_id": "266191530",
            "title": "A Survey of Text Watermarking in the Era of Large Language Models",
            "text": "Since most LLMs are trained using the next word prediction paradigm, text completion is a capability that all LLMs possess. Therefore, the most common task for testing LLM capabilities is text completion. The specific approach is to provide the LLM with a text prefix as a prompt, have the LLM generate the subsequent text, and then evaluate the quality of the generated text. Currently, the evaluation of the quality of generated text typically leverages other LLMs, including PPL [26,33,34,42,54,56,57,59,60,84,105,107,118,124]based on the likelihood generated by LLMs, the P-SP [43,118] based on text similarity, and GPT-4-score [19], which uses the more powerful GPT-4 to directly score the text. \n\nThe calculation method for the PPL metric here is the same as the eq 11 mentioned in \u00a75.2.2. Generally, when testing PPL, larger LLMs than the current LLM are used for evaluation, with common LLMs including LLaMA-13B [57], LLaMA-70B [56], and GPT-3 [124]. Due to the general applicability and simplicity of PPL, calculating the PPL metric after the LLM performs a text completion task is currently the most widely adopted evaluation method. P-SP is more similar to a semantic score, used to evaluate the semantic similarity between two texts. Yoo et al. [118] used P-SP [106] to evaluate the semantic similarity between original human-written texts and watermarked texts generated by LLMs through the text completion task using these texts' prefixes. However, since the same text prefix can generate texts with different semantics, this evaluation has not been widely adopted. \n\nSince the PPL and P-SP can only evaluate text quality from certain perspectives, where PPL focuses on text coherence and P-SP on semantic similarity with the original text. Fairoze et al. [19] adopted  to assess the quality of text generated in the text completion task. Specifically, they designed a scoring prompt for GPT-4, enabling it to output an evaluation of text quality.",
            "score": 0.4529983753412915,
            "section_title": "Text Completion.",
            "char_start_offset": 50033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 482,
                    "end": 486,
                    "matchedPaperCorpusId": "266054907"
                },
                {
                    "start": 492,
                    "end": 495,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "267740518"
                },
                {
                    "start": 501,
                    "end": 504,
                    "matchedPaperCorpusId": "263830310"
                },
                {
                    "start": 504,
                    "end": 507,
                    "matchedPaperCorpusId": "267211556"
                },
                {
                    "start": 510,
                    "end": 513,
                    "matchedPaperCorpusId": "265213008"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "260334887"
                },
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "259308864"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "259095643"
                },
                {
                    "start": 632,
                    "end": 636,
                    "matchedPaperCorpusId": "264560541"
                },
                {
                    "start": 920,
                    "end": 924,
                    "matchedPaperCorpusId": "263830310"
                },
                {
                    "start": 952,
                    "end": 957,
                    "matchedPaperCorpusId": "259308864"
                },
                {
                    "start": 1273,
                    "end": 1278,
                    "matchedPaperCorpusId": "233476516"
                },
                {
                    "start": 1770,
                    "end": 1774,
                    "matchedPaperCorpusId": "264560541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382568359375
        },
        {
            "corpus_id": "272827470",
            "title": "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models",
            "text": "Qualitative comparison of human and LLM evaluations We examine cases where human and LLM evaluators either contradict or agree on the scores assigned to stories, providing insights into the differences in evaluations and preferences for various types of stories. Examples of these cases, highlighting instances of both agreement and disagreement between human and LLM evaluators, are presented in Appendix Figure 15. \n\nImage generation for personalized stories We explored the potential of incorporating images into stories to enhance engagement and representation. \n\nThe image generation and evaluation processes are detailed in Appendix Table 18. Notably, human evaluators show a high accuracy in identifying personalized elements in the images generated by DALL\u2022E 2 (Ramesh et al., 2022), with gender and interest being recognized with 100% and 95% accuracy, respectively (Appendix Figure 17). \n\nCorrelation between human and LLM evaluators Correlation analysis revealed a low to moderate alignment between human evaluators and GPT-4 in story evaluation metrics. GPT-4 aligned more closely with human evaluators on quality across all story types (correlations 0.22-0.47), but showed the weakest correlation in assessing personalness, particularly for personalized stories (as low as 0.08). This suggests that while GPT-4 is increasingly used for various evaluation tasks, its effectiveness in assessing subjective aspects of creative tasks is limited. A detailed analysis of these correlations and temperature variations is presented in Appendix A.4.2, Table 5 and Figure 16.",
            "score": 0.45295223160163256,
            "section_title": "Extended Analyses",
            "char_start_offset": 12083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1578
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037750244140625
        },
        {
            "corpus_id": "270226566",
            "title": "MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset",
            "text": "One concern regarding the MARS benchmark is the potential bias introduced by using GPT-series models, specifically ChatGPT, for dataset construction. Our approach to constructing MARS was guided by the need to balance scalability with quality. In pilot studies evaluating metaphysical reasoning across various models, GPT-series models consistently demonstrated the highest levels of creativity and reliability. Based on these findings, we selected GPT as the primary backbone for data generation. Constructing MARS, however, required extensive manual annotation, as LLMs often fail to provide accurate labels for complex reasoning tasks. This manual verification process made it impractical to create multiple versions of MARS using different backbone LLMs due to expensive human labors required. Thus, to address concerns about potential biases arising from reliance on ChatGPT, we conducted additional experiments by constructing two smaller versions of the MARS benchmark. These alternative benchmarks utilized data generated from two different LLMs, Claude-3.5-sonnet (Anthropic, 2024) andLLAMA 3.1-70B (Dubey et al., 2024), in each step, to obtain 200 evaluation data entries per task in MARS. All samples underwent expert annotation to collect ground-truth labels. We then evaluate GPT-4's zero-shot and few-shot performance on these alternative benchmarks alongside the original MARS. \n\nThe results are shown in Table 7. We observe that using different LLMs as backbones for MARS construction results in similar performance by GPT-4 across zero-shot and few-shot settings. Overall, the difficulty of the MARS benchmark remains robust and consistent, irrespective of the backbone LLM used during dataset generation. These experiments demonstrate that the reliance on ChatGPT for the original MARS construction does not compromise the benchmark's validity or difficulty. The results reinforce the reliability of MARS as a comprehensive test of metaphysical reasoning, with its complexity surpassing any potential biases introduced by the specific LLM used in data collection.",
            "score": 0.4527802319433418,
            "section_title": "E.4 Inherent Bias in MARS Construction",
            "char_start_offset": 58892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2081
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0546875
        },
        {
            "corpus_id": "270440146",
            "title": "EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries",
            "text": "These results highlight the efficacy of EHRNoteQA in evaluating LLM performance within the specified clinical context, compared to other benchmarks. \n\nLastly, we reconfirm the validity of our GPT-4-based evaluation method for EHRNoteQA by comparing the correlations between clinician-evaluated LLM scores and EHRNoteQA scores obtained through other evaluation methods. For open-ended questions, when evaluating with traditional methods such as BLEU [45], ROUGE-L [34], Exact Match, and cosine similarity (using Sentence-BERT [48] and ClinicalBERT [2]), the results show lower correlation compared to our GPT-4 based evaluation scores. For multi-choice questions, many studies employ probability-based scoring. Following this approach, we test two types of probability-based scoring methods on EHRNoteQA using LM-Evaluation Harness [17]: one measuring the probability of the answer index (e.g., A) and another measuring the probability of the correct answer choice text. However, both results show lower correlation compared to our GPT-4 based evaluation scores. No.",
            "score": 0.4527630071898989,
            "section_title": "The Reliability of EHRNoteQA as a Proxy for Clinician Evaluations",
            "char_start_offset": 23190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 547,
                    "end": 550,
                    "matchedPaperCorpusId": "102352093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04168701171875
        },
        {
            "corpus_id": "271310035",
            "title": "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering",
            "text": "To build RAG-QA ARENA on the LFRQA test set, we need a scalable evaluation method to benchmark various LLMs. We rely on model-based evaluation to achieve this goal. Before showing the final dashboard results, we check the quality of our selected evaluator (GPT-4-0125-PREVIEW) in Table 4. \n\nTo alleviate model bias, we use three LLMs' answers as benchmark data, and the query set is the same 700 subsample above. Row (3)-( 5) use GPT-4-0125-PREVIEW, MIXTRAL-8X22B-INSTRUCT and LLAMA-3-70B-INSTRUCT, respectively. All answers are generated based on the top 5 passages. \n\nWe observe that LLM evaluators' numbers align well with the average human scores, except that LLMs tend to predict less \"tie.\" Most importantly, all Pearson Correlation (Freedman et al., 2007) are above 0.52 (with p-values \u226a 0.01), and all Cohen's Kappa (Cohen, 1960) are above 0.43, both showing strong agreement between model and human judgments. In Appendix Table 8, we show correlation numbers using alternative LLMs as evaluators, but none of them works better than a single GPT-4-0125-PREVIEW model, which we select as our best quality evaluator for RAG-QA ARENA.",
            "score": 0.45276142952828347,
            "section_title": "Quality of Model-based Evaluator",
            "char_start_offset": 20075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1139
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06427001953125
        },
        {
            "corpus_id": "267406182",
            "title": "LLM-based NLG Evaluation: Current Status and Challenges",
            "text": "Zhang et al. (2023) find that under certain conditions, widening and deepening the network of LLMs can better align its evaluation with human judgments. Saha et al. (2024) propose a branch-solvemerge strategy, assigning LLMs the roles of decomposing problems, solving them, and aggregating answers, thereby improving the accuracy and reliability of evaluations. Wu et al. (2023) assume that different people such as politicians and the general public have different concerns about the quality of news summaries, use LLMs to play different roles in evaluation accordingly, and aggregate the results finally. \n\nDifferent LLMs. Different from having the same LLM play different roles, some research has used different LLMs (such as GPT-4 and Claude) in their studies. The use of a single LLM as evaluator may introduce bias, resulting in unfair evaluation results. In light of this, Bai et al. ( 2023) design a decentralized Peer-examination method, using different LLMs as evaluators and then aggregating the results. Further, Li, Patel, and Du (2023) let different LLMs serve as evaluators in pairwise comparisons and then have them go through a round of discussion to reach the final result. Additionally, Cohen et al. (2023) evaluate the factuality of texts through the interaction of two LLMs, where the LLM that generated the text acts as the examinee and the other LLM as the examiner.",
            "score": 0.4526489900667975,
            "section_title": "Role and Interaction",
            "char_start_offset": 27741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1389
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 171,
                    "matchedPaperCorpusId": "264591429"
                },
                {
                    "start": 362,
                    "end": 378,
                    "matchedPaperCorpusId": "257767249"
                },
                {
                    "start": 1206,
                    "end": 1225,
                    "matchedPaperCorpusId": "258833288"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49951171875
        },
        {
            "corpus_id": "264146937",
            "title": "Large Language Model-Aware In-Context Learning for Code Generation",
            "text": "We explore whether our retriever based on one LLM's feedback and specific dataset can be transferred to other LLMs or code generation datasets without further tuning. This is a significant research question since the retriever for each LLM and dataset needs to be trained in real applications. \n\n6.1.1 Transfer across LLMs. We consider transferring the retriever based on one LLM's feedback to another LLM. Specifically, we use a source LLM (e.g. CodeGen or GPT-3.5) to estimate examples for training a retriever and then apply the retriever to another target LLM (e.g. ChaTGPT) in generating programs. Table 6 shows the performance of ChaTGPT in three datasets. We surprisingly find our retriever based on CodeGen and GPT-3.5 can bring obvious improvements to ChaTGPT. In particular, in terms of Pass@1, ChaTGPT achieves 2.56% improvements from CodeGen's feedback and 4.31% enhancements from GPT-3.5's feedback compared to the SOTA baseline. The phenomenons demonstrate that our approach has satisfying transfer ability across different LLMs. Note that ChaT-GPT can not provide the prediction probability of ground truths in practice, thus LAIL is a quite meaningful approach, especially for LLMs whose parameters are unavailable. Besides, the performance of ChaTGPT from GPT-3.5's feedback is higher than the counterpart from CodeGen's feedback. The reason might be that GPT-3.5 and ChatGPT have comparable abilities in code generation, thus their preference is similar and GPT-3.5 can provide more proper examples as prompts. \n\nTo verify the transfer ability among LLMs with different sizes, we further evaluate the performance of CodeGen based on GPT-3.5's feedback and the results of GPT-3.5 from CodeGen's feedback, where the parameter size of CodeGen is much smaller than the counterpart of GPT-3.5.",
            "score": 0.45255927767580284,
            "section_title": "DISCUSSION 6.1 Transferability",
            "char_start_offset": 29278,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1528
                },
                {
                    "start": 1531,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1806
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.064208984375
        },
        {
            "corpus_id": "266899541",
            "title": "The Critique of Critique",
            "text": "Q4: Which evaluation methods can choose the critiques that humans prefer? \n\nSetup In this experiment, we utilize a range of scoring baselines to identify the better critique out of two critiques generated by LLMs. We calculate the agreement rate to evaluate the performance, defined as the consistency of the superior critique chosen by various methods with the gold standard critique determined by gold F1 score. \n\nResults Figure 3 illustrates the agreement rate of various methods. Our MetaCritique GPT4 -F1 achieves the best performance. Especially, it exceeds the Single GPT4 baseline by a considerable margin (44%). It implies that the scores of MetaCritique GPT4 -F1 are more comparable than Single GPT4 baseline. Even MetaCritique Open significantly outperforms GPT-4 powered baselines Single GPT4 and Pairwise GPT4 , confirming the effectiveness of our framework. Finally, precision and recall scores serve as complementary metrics for evaluating critiques, because MetaCritique GPT4 -P and MetaCritique GPT4 -R are somewhat less effective than MetaCritique GPT4 -F1. \n\n4.6 Exp V: Better Critique, Better Refinement Q5: Can critique evaluations improve the alignment of existing LLMs? \n\nSetup Critique is commonly applied to improve the quality of model outputs via refinement (Madaan et al., 2023). It is intuitive that superior critiques result in better refinements. To confirm this hypothesis, we conduct this experiment. Specifically, we instruct GPT-4 to refine the model outputs via the critique. Detailed instructions with demonstrations are presented in Table 18. Subsequently, we compare the refined outcomes to choose the better one. We conduct GPT-4 evalu- ation and human evaluation for this comparison. The prompt for GPT-4 evaluation is shown in Table 19, while the equivalent instruction for human evaluators is outlined in Table 20.",
            "score": 0.45240035805807777,
            "section_title": "Exp-IV: Pairwise Comparison",
            "char_start_offset": 19706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 76,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "265018956",
            "title": "DialogBench: Evaluating LLMs as Human-like Dialogue Systems",
            "text": "Zheng et al. (2023) evaluates LLMs' multiturn instruction-following abilities, which focuses on assessing its alignment with human preference, rather than LLMs as human-like dialogue systems. Recent researchers (Zhao et al., 2023b;Wang et al., 2023b;Rao et al., 2023;Ji et al., 2023;Wang et al., 2023a) also focus on human-like characters of GPT-4 or ChatGPT. However, our work holistically evaluates capabilities related to human likeness. \n\nLLMs for Data Generation. Many recent researches (Whitehouse et al., 2023;Yu et al., 2023;Tang et al., 2023;Xu et al., 2023a;Whitehouse et al., 2023) also leverage GPT-4 for data generation, mainly using several training instances as fewshot examples to prompt the generation of more training instances. In contrast, our work leverages GPT-4 to generate new evaluation instances for constructing benchmarks without few-shot examples.",
            "score": 0.45236376099940545,
            "section_title": "Related Work",
            "char_start_offset": 6128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 876
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055938720703125
        },
        {
            "corpus_id": "274301765",
            "title": "Qualitative metrics from the biomedical literature for evaluating large language models in clinical decision-making: a narrative review",
            "text": "The LLM used in each included paper was recorded and is presented in Table 1. The most frequently used LLM was GPT-3.5 (61%), [6] followed by GPT-4 (54%), [117] Bard (10%), [118] ChatGPT (unspecified)-where the model version was not included (8%), [119] and Bing Chat (7%) [120]. As 41 or 38% of the papers used multiple models (e.g. a comparison of GPT-3.5 with GPT-4), the total model count adds up to 173. One paper used 6 models [87], one paper used 5 models [99], five papers used 4 models [50,66,86,93,100], seven papers used 3 models [24,32,36,57,62,79,82], and 27 papers used 2 models [13, 17, 22, 23, 25, 29-31, 33, 38-41, 44, 53, 58, 61, 64, 76, 89, 91, 92, 101, 102, 104, 110, 111]. Overall, 24 different LLMs were used, and only one paper did not specify the LLM that was used in their study [21]. \n\nThe cumulative number of occurrences of the most frequently reported LLMs plotted against the month and year of the article's creation date in PubMed is shown in Fig. 2. Due to the wide variation in LLMs, the models based on LLaMa (Large Language Model Meta AI) [121] or Alpaca, which is fine-tuned from LLaMA [122], are grouped together for this analysis. Proprietary models, particularly GPT-3.5 and GPT-4.0, are used much more frequently than open-source models such as LLaMa and Alpaca. \n\nEach included article was analyzed to determine the criteria on which the LLM output was evaluated. Typically, these criteria are rated by clinicians, who review the LLM output and assign to the criteria a value based on a Likert scale. There is variability in the practices used to report results, as well as a lack of a standardized benchmark for how these studies should be analyzed and reported.",
            "score": 0.45197986131201046,
            "section_title": "Results",
            "char_start_offset": 5219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1302
                },
                {
                    "start": 1305,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1704
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "261046822"
                },
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "265277446"
                },
                {
                    "start": 499,
                    "end": 502,
                    "matchedPaperCorpusId": "265329112"
                },
                {
                    "start": 502,
                    "end": 505,
                    "matchedPaperCorpusId": "266279316"
                },
                {
                    "start": 505,
                    "end": 508,
                    "matchedPaperCorpusId": "268261645"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "268512122"
                },
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "260650520"
                },
                {
                    "start": 545,
                    "end": 548,
                    "matchedPaperCorpusId": "259208363"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "264139641"
                },
                {
                    "start": 554,
                    "end": 557,
                    "matchedPaperCorpusId": "266205103"
                },
                {
                    "start": 557,
                    "end": 560,
                    "matchedPaperCorpusId": "261047130"
                },
                {
                    "start": 560,
                    "end": 563,
                    "matchedPaperCorpusId": "267520288"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "261336909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00844573974609375
        },
        {
            "corpus_id": "268032396",
            "title": "If in a Crowdsourced Data Annotation Pipeline, a GPT-4",
            "text": "This paper evaluates GPT-4's labeling capabilities in contrast to a well-executed, ethical crowdsourcing pipeline for annotating unseen data. Utilizing the CODA-19 labeling scheme, we exhaustively tested various label-cleaning strategies, label-aggregation techniques, and interface designs on MTurk. Despite adhering to best crowdsourcing practices, the best-performing MTurk pipeline achieved an accuracy of 81.5%, slightly below GPT-4's 83.6%. Interestingly, by optimizing the combination of label aggregation techniques and interfaces, integrating GPT-4 labels with the MTurk aggregation process boosted accuracy to 87.5%. Moving forward, our research will focus on generating a smaller set of high-quality labels via MTurk, aiming to further enhance the labeling performance of already sophisticated LLMs like GPT-4. Additionally, we will delve deeper into the influence of worker interface design on label quality to further improve LLM performance. , where different labels had different colors. To simulate the practical scenario, we pretended that we had limited expert annotations during the experiment. As a result, some abstracts did not have expert annotation when we did the comparison.",
            "score": 0.4518849316063158,
            "section_title": "CONCLUSION AND FUTURE WORK",
            "char_start_offset": 46376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046539306640625
        },
        {
            "corpus_id": "273185876",
            "title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
            "text": "We observe that using automatic metrics (i) necessitates manual annotation of functions and pseudocodes each time, which significantly hampers the automation of the evaluation process. Moreover, evaluating the function and input5 separately falls short of flexible and comprehensive evaluation in a protocol manner. \n\nTo this end, we propose LLAM-EVAL, an automatic, flexible prompt-based framework to evaluate the quality of LLM responses. This framework requires three elements: two input texts (one serving as the baseline and the other as the target) and an evaluator LLM: Llama-36 . This method encompasses predefining a set of scores7 S = {s 1 , s 2 , ..., s n }, prompting Llama-3 to rate the outcomes of a target LLM with that of GPT-4 in the scale of S, calculating the probability of each score p(s i ), and calculating the final score as following. This method is inspired by G-Eval (Liu et al., 2023). \n\nLlama-3 is prompted to evaluate according to one criterion at a time. The original prompts targeting summarizing tasks are modified to perform evaluation on SPFT. In this work, we evaluate the pseudocode generated by the target LLM based on six criteria: the four original criteria used in G-Eval (Liu et al., 2023) (Coherence, Consistency, Fluency, and Relevance) and two criteria we propose (Precision, and Coverage), considering the context of SPFT. For example, the definition of Coherence is: \n\nCoherence (1-5) -the overall quality of all lines in the pseudocode. The target pseudocode should not be a rough overview but should provide a precise description of a baseline pseudocode. \n\nThe definitions of other criteria in prompts can be found at Appendix A.2. To automatically implement chain-of-thoughts (CoT) in the evaluation process, we instructed GPT-4 to create specific evaluation steps for each criterion. GPT is capable of producing these evaluation steps by itself (Liu et al., 2023). GPT-4 was given a task and evaluation criteria, then prompted to generate the evaluation steps using a form-filling paradigm.",
            "score": 0.4517369276274858,
            "section_title": "Metrics and Evaluation",
            "char_start_offset": 12083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2042
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36181640625
        },
        {
            "corpus_id": "272742848",
            "title": "David vs. Goliath: comparing conventional machine learning and a large language model for assessing students' concept use in a physics problem",
            "text": "This issue is exacerbated by the intransparency of the decisions made by LLMs (Chen et al., 2023;Manning, 2022). Intransparency in the decision-making process of an LLM may prevent researchers from understanding the logic behind a prediction, and thus hinder them from justifying their choices for certain LLMs. LLMs also exhibit human-like biases through imbalanced training data; and the extent to which LLMs truly extrapolate beyond their training data or merely mimic patterns-in the sense of \"stochastic parrots\" (Bender et al., 2021;Caliskan et al., 2017;Lake and Baroni, 2023)remains an open question. Many examples demonstrate that LLMs such as GPT-4 cannot sufficiently abstract and reason (Mitchell et al., 2023). Finally, the extensive use of LLMs significantly contributes to environmental concerns, particularly in terms of CO 2 emissions and expenditure of energy, both by training the foundation models and with every single request passed through the model (de Vries, 2023; Dodge et al., 2022). \n\nConsequently, we argue that the circumstances where machine learning (ML) and LLMs excel respectively should be critically evaluated to derive some guidance for researchers and practitioners. Conventional AI approaches (i.e., ML algorithms) are less complex and their decisions can commonly be explained using established procedures (Lundberg et al., 2019). Given their reduced complexity, conventional ML algorithms can be operated in a controlled manner and might not generate unanticipated outputs. For example, a trained binary classifier can by design only output two categories, whereas generative LLMs used in a binary classification problem might output the categories, however, it might also produce further textual output. Whether conventional ML or LLMs are used for solving a (research) problem in part depends on the complexity of the problem. In some contexts, e.g., fourth-grade mathematics, it was found that conventional ML can outperform LLMs on identifying incoherent student answers (Urrutia and Araya, 2023). However, this research considered the LLM GPT-3, which is now surpassed by GPT-4.",
            "score": 0.451709403282889,
            "section_title": "Introduction",
            "char_start_offset": 1643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 97,
                    "matchedPaperCorpusId": "257470943"
                },
                {
                    "start": 539,
                    "end": 561,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 561,
                    "end": 583,
                    "matchedPaperCorpusId": "264489248"
                },
                {
                    "start": 699,
                    "end": 722,
                    "matchedPaperCorpusId": "143821869"
                },
                {
                    "start": 990,
                    "end": 1009,
                    "matchedPaperCorpusId": "258999944"
                },
                {
                    "start": 1346,
                    "end": 1369,
                    "matchedPaperCorpusId": "152282481"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0066680908203125
        },
        {
            "corpus_id": "268385144",
            "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
            "text": "We design a rigorous singleanswer grading scheme with LLM-as-a-Judge where GPT-3.5-Turbo or GPT-4-Turbo evaluates a single LLM response at a time in a consistent and objective manner. This approach offers a more nuanced evaluation strategy than previous methods that depend on automated metrics and external tools by leveraging the advanced reasoning capabilities of LLMs. Their ability to discern intricacies and nuances in language allows for a more refined and context-sensitive evaluation of how well code aligns with coding preferences in a human-centric fashion. \n\nIn the initial part of this paper, we conduct an exploratory analysis of CodeUltraFeedback's annotations and demonstrate the strong judging capabilities of GPT-3.5-Turbo. GPT-3.5-Turbo effectively recognizes GPT-4-Turbo as a superior model to itself, demonstrating impartiality and fairness in its judgement while also discerning between responses of different quality. Moreover, our initial exploration also reveals a lack of alignment of 12 LLMs, including strong LLMs such as WizardCoder-33B [37] and DeepSeek-Coder-Instruct-33B [3]. \n\nIn the second part of this paper, we explore preference tuning of a small LLM, CodeLlama-7B-Instruct [2] using CodeUltraFeedback with supervised fine-tuning (SFT) and RLAIF with DPO [36], [38]. The method relies on using the AI feedback data from CodeUltraFeedback, where LLMs' responses with high and low ratings are selected for tuning and aligning CodeLlama-7B-Instruct to coding preferences with DPO. DPO encourages the LLM to favour highly rated LLMs' responses, enabling itself to generate more aligned content at inference. Moreover, we implement QLoRA [39] for efficient fine-tuning and show SFT and DPO can be achieved on a single RTX A5000 GPU (24GB). \n\nOur experiments validate the utility of CodeUltraFeedback for preference tuning.",
            "score": 0.45162499816202245,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 1854
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328857421875
        },
        {
            "corpus_id": "270067579",
            "title": "Aligning to Thousands of Preferences via System Message Generalization",
            "text": "Evaluator LLMs rate each model's generated responses on an absolute scale from 0 to 10, using the GPT-4-Turbo model as the evaluator. \n\nArena Hard Auto v0.1 Model performance is measured by comparing the win rate against responses generated by GPT-4-Turbo, scoring between 0 and 100 based on this comparison.",
            "score": 0.4506076373996985,
            "section_title": "MT-Bench",
            "char_start_offset": 43444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 136,
                    "end": 308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13623046875
        },
        {
            "corpus_id": "276249108",
            "title": "Using Large Language Models for Solving Thermodynamic Problems",
            "text": "Comparing the results for the other models on the simple problems, as shown in Table 3, it is clear that the GPT models not only have fewer penalties but also exhibit different error patterns. For the GPT models, the primary source of errors is wrong assumptions, while for Llama 3.1 and le Chat, numerical errors and wrong equations are most important. The reason for this is that even though both Llama 3.1 and le Chat generate code as part of their language modeling tasks, they do not have specialized code generation capabilities like GPT-4 and GPT-4o. The picture changes remarkably when going from the simple benchmark to the advanced benchmark: As expected, there is a significant increase in the number of penalties (see Table 4). However, the deterioration in performance is more important for the GPT models than for Llama 3.1 and le Chat, which leads to a leveling of the quality of the results. This also holds for the types of errors that occurred. For all LLMs, the most important errors are now context confusions and wrong assumptions. While numerical errors continue to be significant for Llama 3.1 and le Chat, they are only a minor issue for GPT-4 and GPT-4o, which aligns with the findings from the simple benchmark (see Table 3). However, if we consider a scenario in which all LLMs were capable of directly producing correct code and applying it, thereby eliminating the influ-ence of numerical errors, the differences between the LLMs would shrink, both for the simple and the advanced problems. \n\nThe high importance of context confusions that was found for all LLMs for the advanced problems is somehow similar to what was found for GPT-3.5 for the simple problems. This weakness of GPT-3.5 was overcome by the more recent versions GPT-4 and GPT-4o, which performed well on the simple problems. Whether or not this trend will also hold for future versions of GPT applied to the advanced problems remains to be seen. \n\nWe conclude this discussion with a few specific observations: GPT-3.5, GPT-4, and Llama 3.1 occasionally rely on fluid properties from undisclosed sources instead of using the provided data, which carries the risk of inducing avoidable errors.",
            "score": 0.45059826534463887,
            "section_title": "Error Types",
            "char_start_offset": 25458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1941
                },
                {
                    "start": 1944,
                    "end": 2187
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01337432861328125
        },
        {
            "corpus_id": "271769623",
            "title": "Simulating Expert Discussions with Multi-agent for Enhanced Scientific Problem Solving",
            "text": "Solution Quality In addition to evaluating the accuracy of the models, we also assess the quality of the generated solutions. We randomly sample 100 problems and evaluate the solutions using LLMs and human evaluation based on three criteria: (1) the correctness of the reasoning steps, (2) the clarity of the explanations, and (3) the appropriateness of the mathematical notations and symbols used. Each criterion was rated on a scale of 1 to 5, with 5 being the highest quality. \n\nFor the human evaluation, we employed three expert annotators. To ensure reliability, we calculated the inter-annotator agreement using Fleiss' kappa (Fleiss, 1971) for each of the three criteria: \n\n\u2022 Correctness of reasoning steps: \u03ba = 0.71 \n\n\u2022 Clarity of explanations: \u03ba = 0.62 \n\n\u2022 Appropriateness of mathematical notations and symbols: \u03ba = 0.55 \n\nThe overall average kappa value was 0.63, indicating substantial agreement among the annotators. \n\nThe detailed prompts for LLM evaluation and the specific guidelines for human evaluation are provided in Appendix B. Table 2 presents the average quality scores for solutions from GPT-4. Compared to baseline, SEDM consistently achieves higher quality scores in both LLM and human evaluations. The solutions generated by SEDM demonstrate clearer reasoning steps, more coherent explanations, and more precise use of mathematical notations. This suggests that the multi-expert discussion framework not only improves the accuracy of the solutions but also enhances their overall quality and readability. Table 1: The accuracy scores (%) of different baseline methods and our proposed SEDM approach across various scientific domains using GPT-3.5 and GPT-4 models under the setting of 2 experts and 2 discussion rounds. The best results for each subject are in bold. Table 2: The average quality score of solutions from GPT-4 evaluated by LLMs and humans.",
            "score": 0.45059160116364055,
            "section_title": "Further Analysis",
            "char_start_offset": 18524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 632,
                    "end": 646,
                    "matchedPaperCorpusId": "143544759"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45166015625
        },
        {
            "corpus_id": "263605445",
            "title": "Evaluating the Decency and Consistency of Data Validation Tests Generated by LLMs",
            "text": "We investigated whether large language models (LLMs) can develop data validation tests. We considered 96 conditions each for both GPT-3.5 and GPT-4, examining different prompt scenarios, learning modes, temperature settings, and roles. The prompt scenarios were: 1) Asking for expectations, 2) Asking for expectations with a given context, 3) Asking for expectations after requesting a data simulation, and 4) Asking for expectations with a provided data sample. The learning modes were: 1) zero-shot, 2) one-shot, and 3) few-shot learning. We also tested four temperature settings: 0, 0.4, 0.6, and 1. And the two distinct roles were: 1) helpful assistant, 2) expert data scientist. To gauge consistency, every setup was tested five times. The LLM-generated responses were benchmarked against a gold standard data validation suite, created by an experienced data scientist knowledgeable about the data in question. We find there are considerable returns to the use of few-shot learning, and that the more explicit the data setting can be the better, to a point. The best LLM configurations complement, rather than substitute, the gold standard results. This study underscores the value LLMs can bring to the data cleaning and preparation stages of the data science workflow, but highlights that they need considerable evaluation by experienced analysts.",
            "score": 0.45037416784958184,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "267522819",
            "title": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton",
            "text": "This section aims to evaluate the quality of generated dialogues using the DFA-RAG framework.We compare performance on major pretrained LLMs as the backbone model in generating dialogues: GPT-4 (OpenAI, 2023), GPT-3.5 (Ouyang et al., 2022).The concrete versions are gpt-4-1106-preview and gpt-3.5-turbo-1106respectively.\n\nEvaluation Technique.We leverage GPT-4 for the primary evaluation, given its superior performance metrics.According to Zheng et al. (2023), GPT-4 demonstrates an 85% agreement rate with human evaluators, signifying its reliability in assessing dialogue quality.We follow the methodology outlined in AlpacaEval (Dubois et al., 2023) with similar prompts and metrics as detailed in Appendix A.\n\nThe evaluation process involves GPT-4 comparing two dialogues to determine which one is closer to the ground truth.The dialogues completed directly by their respective LLMs serve as the target to be compared.The effectiveness of each method is quantified using the \"Win Rate\" score, which reflects how often a dialogue surpasses the quality of the competitor.For a qualitative comparison, we also present example dialogues with respective comments from the GPT-4 evaluator in Appendix C. Observations.Our observations from the evaluation in Table 1 reveal some notable trends and insights: (a) the approach of fine-tuning (FT-LLM) showed underwhelming performance, which can be attributed to the limited volume of domain-specific training data, such as NikeSupport; (b) RAG demonstrates stronger performance than RandSamp and BM25, which is likely due to its more accurate retrieval of relevant samples; (c) DFA-RAG outperforms the best baseline by 4% at win-rate.Note that DFA-RAG not only facilitates the generation of high-quality dialogues but also provides a clear and interpretable structure, making it easier for humans to understand and analyze the dialogue flow.",
            "score": 0.4503010701804505,
            "section_title": "Generation Quality Evaluation",
            "char_start_offset": 20966,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 307
                },
                {
                    "start": 307,
                    "end": 320
                },
                {
                    "start": 322,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 428
                },
                {
                    "start": 428,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 713
                },
                {
                    "start": 715,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 923
                },
                {
                    "start": 923,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1216
                },
                {
                    "start": 1216,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1886
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 239,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1055908203125
        },
        {
            "corpus_id": "257557820",
            "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
            "text": "The generative LLM used to generate passages for our dataset is GPT-3 (text-davinci-003), the stateof-the-art system at the time of creating and annotating the dataset. To obtain the main response, we set the temperature to 0.0 and use standard beam search decoding. For the stochastically generated samples, we set the temperature to 1.0 and generate 4 3-class refers to when selecting between accurate, minor inaccurate, major inaccurate. 2-class refers to when minor/major inaccuracies are combined into one label. N =20 samples. For the proxy LLM approach, we use LLaMA (Touvron et al., 2023), one of the bestperforming open-source LLMs currently available. For SelfCheckGPT-Prompt, we consider both GPT-3 (which is the same LLM that is used to generate passages) as well as the newly released ChatGPT (gpt-3.5-turbo). More details about the systems in SelfCheckGPT and results using other proxy LLMs can be found in the appendix.",
            "score": 0.4502483882528331,
            "section_title": "Experiments",
            "char_start_offset": 16487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0252227783203125
        },
        {
            "corpus_id": "278326847",
            "title": "SafeMate: A Modular RAG-Based Agent for Context-Aware Emergency Guidance",
            "text": "Figure 3 illustrates our evaluation framework, where GPT serves as an automated evaluator to assess the quality of SafeMate's generated responses (Liu et al., 2023). Given an emergency context that includes structured meta-information (e.g., disaster type, time, location) and supporting documents, a generator module synthesizes a question and a corresponding answer. SafeMate then generates its own response to the same question. Both responses are passed to an LLM-based evaluator (e.g., GPT-4), which scores the outputs across five dimensions: correctness, groundedness, completeness, relevance, and fluency. This evaluation loop enables scalable, consistent, and multicriteria assessment of SafeMate's response quality, particularly in settings where human evaluation is costly or infeasible. We compared three systems: \n\n\u2022 SafeMate (Ours): MCP-based RAG agent with RAPTOR and o3-mini-high. \n\n\u2022 GPT-4o: OpenAI's latest general-purpose LLM without retrieval. \n\n\u2022 GPT-3.5: Cost-efficient baseline LLM. \n\nEach model's answer was evaluated using GPT-4 based on five criteria: correctness, groundedness, completeness, relevance, and fluency (scale 0-5). \n\nCorrectness refers to the factual accuracy of the response. Groundedness measures whether the answer is supported by retrieved or provided evidence. Completeness evaluates whether all aspects of the question are sufficiently addressed. Relevance assesses whether the content stays focused on the user's query. Fluency pertains to the grammaticality, clarity, and coherence of the response. Each criterion is rated on a 0-5 scale, with higher scores indicating better performance.",
            "score": 0.44962415127592753,
            "section_title": "Evaluation and Verification",
            "char_start_offset": 19716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1153
                },
                {
                    "start": 1156,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1635
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257568359375
        },
        {
            "corpus_id": "267783118",
            "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
            "text": "In this work, we evaluate the ability of LLMs to write surveys on NLP concepts. We find that LLMs, particularly GPT-4, can author surveys following specific guidelines that rival the quality of human experts, even though there are shortcomings such as incomplete information. Our findings also indicate that GPT-4 may not be a perfect replacement for human judgment when evaluating human-composed texts, and certain biases exist when asking it to rate machine-generated texts. Nevertheless, the results imply that these advanced generative LLMs could play a transformative role in the realm of education. They hold the promise of effectively structuring domain-specific knowledge tailored to general learners. This adaptability could potentially lead to a more interactive and personalized learning experience, enabling students to engage in query-driven studies that cater directly to their unique curiosities and learning objectives.",
            "score": 0.4496137692086437,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 14399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 935
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06982421875
        },
        {
            "corpus_id": "271534006",
            "title": "Stochastic Parrots or ICU Experts? Large Language Models in Critical Care Medicine: A Scoping Review",
            "text": "Abdullahi et al. [84] used three popular LLMs, Bard, ChatGPT-3.5, and GPT-4, to explore their potential to aid the diagnosis of rare and complex diseases. The study utilized prompt engineering techniques on publicly available diagnostic cases, comparing the three LLMs' performance against human respondents and a medical-specific LLM, MedAlpaca. The findings revealed that the three studied LLMs outperformed the average human consensus and MedAlpaca in diagnostic accuracy through prompt engineering. \n\nTreatment Planning In treatment planning, LLMs show considerable promise in providing personalized treatment recommendations and optimizing clinical pathways. Savage et al. [73] developed and validated the LLM screening tool to selectively identify patients appropriate for deep vein thrombosis anticoagulation prophylaxis best practice alerts (BPAs) in EHRs. They found that the LLM screening tool improved the precision of BPAs, reducing the number of unnecessary alerts by 20% and increasing the applicability of alerts by 14.8%. Pham et al. [77] evaluated ChatGPT's performance in treating cardiac arrest and bradycardia simulations following the American Heart Association's Advanced Cardiovascular Life Support (ACLS) guidelines. Using the 2020 ACLS guidelines, ChatGPT's responses to two simulation scenarios were assessed for accuracy. The study found that ChatGPT had a median accuracy of 69% for cardiac arrest and 42% for bradycardia, with significant variability in its outputs, often missing critical actions, and having incorrect medication information. This study highlighted the need for consistent and reliable guidance to prevent potential medical errors and optimize the application of LLM to improve its reliability and effectiveness in clinical practice. \n\nPrognosis Prediction Amacher et al. [88] used GPT-4 to predict mortality and poor neurological outcomes at hospital discharge for adult cardiac arrest patients. The study involved prompting GPT-4 with sixteen prognostic parameters from established post-cardiac arrest scores. The findings showed that GPT-4 achieved an AUC of 0.85 for in-hospital mortality and 0.84 for poor neurological outcomes, comparable to traditional scoring systems. Despite these promising results, the study highlighted the need for human oversight due to instances of illogical answers provided by the model.",
            "score": 0.4495696246171625,
            "section_title": "Clinical Decision Support",
            "char_start_offset": 20656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1780
                },
                {
                    "start": 1783,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "266279316"
                },
                {
                    "start": 678,
                    "end": 682,
                    "matchedPaperCorpusId": "264462882"
                },
                {
                    "start": 1050,
                    "end": 1054,
                    "matchedPaperCorpusId": "268352359"
                },
                {
                    "start": 1819,
                    "end": 1823,
                    "matchedPaperCorpusId": "267999910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0259552001953125
        },
        {
            "corpus_id": "268032876",
            "title": "Dealing with Data for RE: Mitigating Challenges while using NLP and Generative AI",
            "text": "While a plethora of metrics exist to evaluate various NLP approaches, the integration of generative AI capabilities into SSNLPCores introduces fresh challenges when validating LLM output. The output necessitates evaluation against several parameters, the specifics of which depend upon the targeted task. When focusing on RE, the following parameters emerge as crucial based on our experience: \n\n-Factuality and Accuracy of Output: It's imperative that LLMs do not create or 'hallucinate' inaccurate text. -Information Retention: For certain RE tasks necessitating summarization or extraction of pertinent information, the generated output must not omit any critical details. -Societal Bias: The generated output must not infringe upon human rights and should safeguard the interests of all sensitive groups. \n\nCommonly used evaluation metrics such as accuracy, F-scores, BLEU, ROUGE, and perplexity are not specifically designed to cater to these parameters [13]. Additionally, most metrics assume the existence of ground truth for evaluation, which is frequently challenging to procure for specialized tasks, as previously discussed. Consequently, the expertise of domain experts is invaluable in validating the generated output. Their insights can be utilized to train models via Reinforcement Learning with Human Feedback (RLHF), a method being attributed as a major factor in the success of LLMs such as ChatGPT. \n\nBeyond utilizing domain experts to validate generated data, practitioners have also begun deploying LLMs themselves for evaluation purposes. Two predominant strategies for using LLMs as evaluators are as follows: \n\n-Self-Reflection [24]: Following the completion of a task by an LLM, the model is prompted to reconsider its generated output and identify any potential shortcomings. This method has demonstrated an enhancement in accuracy. \n\n-Engaging an Oracle-LLM for Evaluation [15]: This strategy involves the use of an auxiliary, typically more powerful, LLM for evaluation. For instance, the authors of Vicuna deployed GPT-4 as an 'oracle' LLM to compare and rank the outputs of various LLMs, including LLAMA, Alpaca, and Chat-GPT, thereby ascertaining the relative differences in generation quality.",
            "score": 0.44953722352019515,
            "section_title": "Validation of SSNLPCore Output",
            "char_start_offset": 43596,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "259360395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76708984375
        },
        {
            "corpus_id": "258822888",
            "title": "Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering",
            "text": "Output with the following format: Evaluation evidence: <your evaluation explanation here> Score of Assistant 1's response: <score> Score of Assistant 2's response: <score> Table 13: The prompt of the LLM evaluator generates an evaluation explanation first and then gives scores on two response candidates. exhibited a preference for the response located in the first position, resulting in inconsistent rankings between the two rounds of scoring. We introduce the concept of the score gap, which represents the absolute difference in scores between two responses within a single scoring round. Figure 4 shows the percentage of conflict and nonconflict cases when using GPT-3.5 and GPT-4 as evaluators, respectively. Note that each sample has two scoring rounds, and if a conflict arises between these two rounds, both rounds are labeled as conflict. Notably, the GPT-4 evaluator exhibits a significantly lower conflict rate compared to GPT-3.5. Then, we select GPT-4 as our preferred evaluator. Furthermore, we observe that conflicts mostly occur within a score gap range of 0-2. On the other hand, we calculate the conflict/non-conflict ratio for each score gap value (see Table 14). When the score gap is 0 or 1, the ratio indicates a high probability of conflict. Based on these observations, we propose a rule where we trust the LLM evaluator only when the score gap exceeds 1. Otherwise, we conduct two scoring rounds by switching response positions and rank them based on the average score of two rounds. This approach mitigates scoring conflicts and ensures a reliable and efficient evaluation process, primarily relying on a single scoring round for most cases.",
            "score": 0.44951156867407427,
            "section_title": "<LLM RESPONSE> {llama response} [User] {question}",
            "char_start_offset": 31288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07183837890625
        },
        {
            "corpus_id": "276775964",
            "title": "From Code to Courtroom: LLMs as the New Software Judges",
            "text": "Manuscript submitted to ACM Among existing studies, Zhuo et al. [88] and Weyssow et al. [66] utilize GPT-3.5 [43], and Xu et al. [70] use GPT-4 to annotate code snippets. CodeJudge [63] first leverages a detailed taxonomy of common programming errors to guide the LLM in analyzing the generated code. Then, CodeJudge summarizes the analysis report to produce its final judgment. Wang et al. [65] experiment with a series of general-purpose LLM-as-a-Judge methods from the natural language processing (NLP) domain in code generation evaluation, including BatchEval [79], GPTScore [15], and G-Eval [41]. Ahmed et al. [1] investigate LLM's ability to evaluate variable name-value inconsistency and function similarities. Gu et al. [21] focus on analyzing LLM's judgments on counterfeit code samples, i.e., incorrect programs generated by language models that pass weak but non-trivial correctness checks. Zhao et al. [84] empirically evaluate the performance of 12 LLMs on code generation evaluation. Patel et al. [47] hypothesis that a combination of multiple evaluators can approximate the optimal evaluation. Thus, they propose the AIME framework, which employs multiple LLMs to evaluate different aspects of the code, including code correctness, readability, runtime performance, etc. \n\nFurther, we summarize the common evaluation criteria used in the literature for code generation. We categorize them into two main criteria: Code Functionality and Code Quality. \n\nCode Functionality: This criterion assesses the overall effectiveness, maintainability, and clarity of the code beyond its functional correctness. It evaluates intrinsic code attributes. Key aspects include: \n\n-Execution Stability: LLM-as-a-Judge's evaluation process does not mandate code execution, meaning that running the code is not a required step. \n\n-Functional Correctness: Verifies whether the code produces the expected output according to the task description. \n\n-Fault Tolerance & Error Handling: Evaluates how the code manages edge cases, exceptions, invalid inputs, and unexpected failures.",
            "score": 0.4494628260883341,
            "section_title": "Code Generation",
            "char_start_offset": 6092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1821
                },
                {
                    "start": 1824,
                    "end": 1938
                },
                {
                    "start": 1941,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 185,
                    "matchedPaperCorpusId": "273097999"
                },
                {
                    "start": 728,
                    "end": 732,
                    "matchedPaperCorpusId": "268091229"
                },
                {
                    "start": 914,
                    "end": 918,
                    "matchedPaperCorpusId": "271909222"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1334228515625
        },
        {
            "corpus_id": "274192378",
            "title": "A Survey on Human-Centric LLMs",
            "text": "Analyzing LLM performance with these datasets has revealed significant insights into their reasoning abilities and limitations. For deductive reasoning, although LLMs like GPT-3 have made progress, their accuracy remains at 68.65% in tasks involving logical inference, which is significantly below the 90% human benchmark [37]. This gap indicates ongoing challenges in mastering complex logical structures, especially when multiple logical steps or intricate reasoning processes are required. LLMs like GPT-3.5, PaLM, and LLaMA perform well on simpler deductive reasoning tasks but struggle with more complex scenarios that involve chaining multiple logical premises together [45]. For inductive reasoning, on the other hand, GPT-4 shows improvements in rule application with up to 99.5% partial accuracy [46], yet struggles with larger problems and minimal examples. Even with Chain-of-Thought (CoT) prompting, GPT-4 and Davinci face difficulties in rule validation and integrating complex rules, with Davinci's accuracy declining to 51% in nuanced tasks [47]. In addition, Han et al. [47] evaluate GPT-3.5 and GPT-4 on property induction tasks, highlighting that while GPT-4 more closely aligns with human reasoning patterns, they still struggle to fully capture premise non-monotonicity, a critical element of human cognitive processing. \n\nCausal reasoning remains a significant challenge for LLMs like GPT-4 and Davinci, as it requires a deep understanding of cause-and-effect across various contexts. Although these models show reasonable proficiency in mathematical causal tasks, the CausalBench benchmark highlights their struggles with more complex text-based and coding-related causal problems [41]. Interpreting causal structures in narratives or code snippets often goes beyond simple data correlations, demanding robust reasoning to avoid producing misleading outputs. Even when GPT-4 initially performs well,  its reasoning capabilities frequently weaken when faced with flawed or conflicting arguments, raising concerns about its consistency in complex scenarios [48].",
            "score": 0.4493287743777382,
            "section_title": "Cognitive Skills",
            "char_start_offset": 11616,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 326,
                    "matchedPaperCorpusId": "259515154"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "258865898"
                },
                {
                    "start": 805,
                    "end": 809,
                    "matchedPaperCorpusId": "268417031"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "259138322"
                },
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "259138322"
                },
                {
                    "start": 1703,
                    "end": 1707,
                    "matchedPaperCorpusId": "271769521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0143890380859375
        },
        {
            "corpus_id": "270391675",
            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
            "text": "It takes into account a diverse range of factors, including coherence, relevance, and fluency, to offer a holistic understanding of the quality of the hypothesis text.Besides, some studies explore error-oriented evaluation which focused on examining and explaining the specific errors in the hypothesis text, offering insights into why a particular score is derived.This category delves into the fine-grained aspects of generated content to identify and justify evaluation outcomes.\n\nFor instance, Yue et al. (2023) first defined different types of attribution errors, and then explored prompting LLMs or fine-tuning smaller LLMs on simulated and repurposed data from related tasks such as question answering (QA), fact-checking, natural language inference (NLI), and summarization.Xu et al. (2023) utilized GPT-4 to construct fine-grained analysis data to tune LLaMA to generate error analysis for generated text compared with reference text, after which this work utilized real model-generated response-reference pairs to refine and self-train evaluator.Furthermore, Jiang et al. (2023) sampled data from diverse text generation datasets, including summarization, translation and data2text, whose system outputs included real-world system output and GPT-4 synthesis, and prompted GPT-4 to curate error analysis to tune LLaMA for fine-grained evaluation.",
            "score": 0.4489594309071214,
            "section_title": "Tuning-based Evaluation",
            "char_start_offset": 27964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 366
                },
                {
                    "start": 366,
                    "end": 482
                },
                {
                    "start": 484,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1355
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326171875
        },
        {
            "corpus_id": "265281389",
            "title": "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
            "text": "It's worth noting that the article's evaluation of LLMs is limited, and there is relatively little work that has tested LLMs using CodeApex. \n\nDu et al. [25] attempted to evaluate LLMs' class-level code generation capabilities and introduced a class-level code generation benchmark called ClassEval. The article also presents the performance of 11 state-of-the-art LLMs on ClassEval, as shown in Table 12. We can observe that GPT-4 achieves the best performance in almost all metrics, followed by GPT-3.5. Among the Code LLMs, WizardCoder achieves the best scores. \n\nYu et al. [106] introduce a benchmark called CoderEval to evaluate the performance of models in generating practical code. Compared to the HumanEval benchmark, CoderEval includes programming tasks from various open-source projects and provides full coverage testing to assess models' performance in practical code generation. The article does not conduct large-scale evaluation experiments on existing LLMs but compares the performance of CodeGen, Codex, and PanGu-Coder on CoderEval. The results indicate that Codex performs better in various testing scenarios. \n\nBased on the information provided, we can see that the Code-LLaMA series of LLMs performs well in several commonly used code generation benchmarks. Among them, Unnatural-Code-LLaMA-34B stands out with outstanding performance. For API-related code generation tasks, ToolCoder performs better. Additionally, GPT-4 and GPT-3.5 (GPT-3.5-turbo) also exhibit good performance.",
            "score": 0.4487797527508459,
            "section_title": "Model",
            "char_start_offset": 75476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 143,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1502
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0070648193359375
        },
        {
            "corpus_id": "260865902",
            "title": "Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?",
            "text": "In terms of the particular LLMs tested, three studies (Bommarito II and Katz, 2022;Savelka, 2023;Blair-Stanek et al, 2023) used the same text-davinci-003 model as we do in answering our first research question. Nay et al (2023) used various models, noting that GPT-4 (the state of the art at the time) performed best and that they actually used GPT-4 to help grade their answers. Yu et al (2022) used GPT-3.",
            "score": 0.44828448980457986,
            "section_title": "Models",
            "char_start_offset": 12999,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00536346435546875
        },
        {
            "corpus_id": "271891943",
            "title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
            "text": "Similar to the relevance-based quality criteria, we assume that to measure the quality for integrity-based criteria the LLMs-as-a-judge need to have an understanding of the task, but also the ability to solve the task itself. We hypothesize that, for task-specific evaluations, the underlying LLM-asa-judge actually needs to be able to solve the task itself to apply the correct score. As reported in (Lin et al. 2024), LLMs' ability to critic a task solution correlates with its ability to solve the task. We exemplify this by the evaluations for the integritybased evaluation criterion on the criterion logical correctness from the Flask dataset. This criterion reflects the correctness elements which are reflected in human annotations, which are more clearly scored as high (for logically correct) or low (for logically incorrect) scores. To select the appropriate scores, the LLM-as-a-judges need to know what is correct and what is wrong. Here, GPT-4 significantly outperforms all other models by a wide margin with a Pearson correlation of 0.68 with the human judgements in contrast to 0.34 for Llama3 70b and 0.33 for Phi3, for example. \n\nTo illustrate this, we plot the generated scores of the LLMs-as-a-judge (Fig. 6) and the human annotations. We observe that only GPT4 is able to generate lower scores to judge a task solution as \"bad.\" All other models predominantly give high scores, consistently grading bad logically incorrect responses as \"very good.\"",
            "score": 0.44808575179708354,
            "section_title": "Details on integrity-based criteria results",
            "char_start_offset": 26700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1468
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2073974609375
        },
        {
            "corpus_id": "267740374",
            "title": "Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement",
            "text": "Hyperparameter Settings When using GPT-4 to generate data in MORTISE, we set the temperature to 1, presence_penalty to 0.6, frequency_penalty to 0, and other parameters to default for the API parameters of GPT-4. When evaluating LLMs, the temperature is set to 0, the presence_penalty is set to 0.6, and the frequency_penalty is set to 0 for GPT-3.5 and GPT-4. We set the parameters to default and recommended configuration for other open-source and closed models. Additionally, the versions of GPT-3.5 and GPT-4 we use in our work are gpt-3.5-turbo-0613 and gpt-4-0314. We implement our code using Pytorch2 and Huggingface3 and experiment on A100 80GB GPUs, spending an average of 20 minutes to 2 hours on each evaluation.",
            "score": 0.4478655635756271,
            "section_title": "Evaluation Method Dimension Setting",
            "char_start_offset": 15886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 723
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01751708984375
        },
        {
            "corpus_id": "258833333",
            "title": "Lion: Adversarial Distillation of Proprietary Large Language Models",
            "text": "Therefore, the quality of the LLM is quite essential in the generation of new instructions. \n\nThe Evaluation Metrics Though automated evaluations leveraging GPT-4 have showcased promising prospects in appraising chatbot performance, the technique is yet to reach a level of maturity and accuracy, especially considering the propensity of large language models to generate non-existent or \"hallucinated\" information. Evaluating the efficacy of LLM across various tasks presents a considerable challenge since different tasks require quite different expertise (Wang et al., 2022). Therefore, the creation of a comprehensive, standardized evaluation system for chatbots is a prevailing research challenge that demands additional exploration and study.",
            "score": 0.4470233747035759,
            "section_title": "Limitations and Discussions",
            "char_start_offset": 27769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 94,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 748
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05706787109375
        },
        {
            "corpus_id": "270045144",
            "title": "Eliciting Informative Text Evaluations with Large Language Models",
            "text": "Furthermore, we employ the LLMs to create synthetic text reports based on a given item, simulating the scenario of the creation of fictitious academic peer reviews. We conduct the experiment on the ICLR2020 OpenReview dataset4 . Specifically, we provide the paper  as input to both GPT-3.5 and GPT-4 [Achiam et al., 2023, Brown et al., 2020], requesting them to generate comprehensive reviews following Liang et al. [2023]'s method. The prompt for this task is presented in Appendix D.4. \n\nWe compare three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generated review. Given that GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers, we hypothesize that we can utilize the LLM-generated reviews to simulate low-quality human reviews, thereby simulating three levels of quality. \n\nIt is worth noting that, unlike the degradations discussed in section 6.1, the LLM-generated review does not only depend on the agent's signal   since the reviewed paper is input into the LLMs to create reviews. It can not be regarded as untruthfully reporting the original signal. Thus, our focus remains on assessing the effectiveness of differentiating various quality levels of reports across different mechanisms.",
            "score": 0.4469823911306515,
            "section_title": "LLM-generated Reviews.",
            "char_start_offset": 48675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1311
                }
            ],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 341,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055938720703125
        },
        {
            "corpus_id": "272697317",
            "title": "Performance of Publicly Available Large Language Models on Internal Medicine Board-style Questions",
            "text": "Several recently released Large Language Models (LLMs) include GPT-3.5 and GPT-4.0 (OpenAI), LaMDA (Google) and Llama 2 (Meta), considered state-of-the-art due to their performance on common evaluation benchmarks (Table 1). The public chatbots ChatGPT, Bing Chat and Google Bard use GPT-3.5, GPT-4.0 and LaMDA, respectively [1]. There has been a surge in the proposed medical applications of LLMs, including medical note taking and consultations [2]. To that end, attempts have been made to benchmark LLM performance against practicing physicians' fund of knowledge. Research has evaluated their performance on the United States Medical Licensing Examination [3,4] as well as the ophthalmology [1], neonatal [5] and dermatology [6] board exams. Depending on the exam in question, LLMs performed below or above passing score rates and human respondents, raising the question of LLM performance on other subject matters. Additionally, the question of improving LLM performance with additional targeted augmentations also remains since these previous studies did not enhance the LLM systems with domain specific information. In this study, we investigate the Table 1. Large Language Model characteristics. A summary of model performance on common evaluation benchmarks along with their basic architecture and training setup, where available.",
            "score": 0.4469383809397966,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1338
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "259208363"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "259220760"
                },
                {
                    "start": 659,
                    "end": 662,
                    "matchedPaperCorpusId": "259842568"
                },
                {
                    "start": 662,
                    "end": 664,
                    "matchedPaperCorpusId": "254876189"
                },
                {
                    "start": 694,
                    "end": 697,
                    "matchedPaperCorpusId": "259208363"
                },
                {
                    "start": 708,
                    "end": 711,
                    "matchedPaperCorpusId": "259946878"
                },
                {
                    "start": 728,
                    "end": 731,
                    "matchedPaperCorpusId": "260523570"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01473236083984375
        },
        {
            "corpus_id": "270560307",
            "title": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary Empirical Study",
            "text": "Usually, when asking LLMs to select a score from 1-10 for evaluating answers to questions, providing reference answers may improve the evaluation quality of LLMs (Wang et al., 2023b;Zheng et al., 2023). In the zero-shot ICL regime, no in-context examples are provided for the GPT-4o evaluator, which selects scores only depending on itself. In the few-shot ICL regime, a few in-context examples are provided for the GPT-4o evaluator. In the many-shot ICL regime, many in-context examples are provided for the GPT-4o evaluator. The differences among the three regimes above are as follows: (1) zero-shot, which lacks any reference information, causes LLM to score entirely based on its preferences. (2) few-shot, due to providing only a small number of examples, may lead to evaluation results that lack diversity. (3) many-shot, by offering a larger number of examples, can ensure diversity and quality in evaluations. Therefore, we present the prompt template MSwR, which uses model-generated rationales as the in-context examples. Specifically, MSwR-4 indicates the prompt template of MSwR using 4 shots with reference as a demonstration, as shown in Table 3.",
            "score": 0.4462853184726029,
            "section_title": "Many-Shot with Reference",
            "char_start_offset": 7866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1161
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 201,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.430419921875
        },
        {
            "corpus_id": "263611057",
            "title": "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness",
            "text": "In open-domain tasks, it is challenging to evaluate the correctness/quality of answers (irrespective of whether these answers were generated by a LLM or human). Often one resorts to automated Preprint. \n\nevaluation using models like GPT-3.5-turbo or GPT-4 to assess the correctness of answers (Lin et al., 2023;Chen et al., 2023c;Taori et al., 2023;Chen et al., 2023b;Xu et al., 2023;Chen et al., 2023a). \n\nRecent instruction fine-tuning techniques such as Alpaca (Taori et al., 2023) and WizardLM (Xu et al., 2023) also utilize GPT-4 for automated evaluation of generated answers. Even when they are based on advanced LLMs like GPT-4, there remain questions about the reliability of these LLM-based evaluations. \n\nHere we outline two ways to boost the reliability of LLM-based evaluation: human-in-the-loop and fully automated. Both start by computing BSDetector confidence scores for each LLM-evaluation (these scores estimate not the trustworthiness of the generator of the answers, but rather the evaluator of their correctness). Let A denote the subset of answers where the corresponding LLM-evaluation had the lowest BSDetector confidence scores (indicating the automated evaluation for this answer is untrustworthy). The gold-standard for evaluating open-domain answers is human inspection, but this is costly. Under a limited labor budget, we can boost the reliability of LLM-based evaluation by having humans only inspect and provide evaluations for the answers in A. In settings where this human-in-the-loop approach is not possible, an alternative fully-automated way to boost the reliability of LLM-evaluation is to simply omit the answers in A entirely from the evaluation-set.",
            "score": 0.44622374668594855,
            "section_title": "APPLICATION: MORE RELIABLE LLM-BASED (AUTOMATED) EVALUATION",
            "char_start_offset": 16458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 201
                },
                {
                    "start": 204,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1690
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.294189453125
        },
        {
            "corpus_id": "271097661",
            "title": "Source Code Summarization in the Era of Large Language Models",
            "text": "LLM-based evaluation methods.Inspired by recent work in NLP [37]- [39], we also investigate the feasibility of employing LLMs as evaluators.Its advantage is that it does not rely on the quality of reference summaries, and the evaluation steps can be the same as human evaluation.Specifically, similar to human evaluation, when using LLMs as evaluators, for each sample, we input the code snippet to be summarized, the reference summary, and LLM-generated summaries, and ask LLMs to  rate each summary from 1 to 5 where a higher score represents a higher quality of the summary.The specific prompt when using LLMs as evaluators is shown in Figure 2.\n\nDatasets and Prompting Techniques.In this RQ, to reduce the workload of human evaluation volunteers, we randomly select 50 samples from the Java, Python, and C datasets, respectively, which means 150 samples in total.We employ few-shot prompting to adapt the four LLMs to generate summaries for code snippets as recent studies [25], [28], [30] have demonstrated the effectiveness of this prompting technique on code summarization tasks.\n\n2) Experimental Results.Human Evaluation Results.Table II shows the human evaluation scores for reference summaries and summaries generated by the four LLMs.Observe that the scores of reference summaries the three datasets are between 3 and 3.5 points, suggesting that the quality of the reference summaries is not very high.Therefore, evaluation methods based on summarysummary/code similarity may not accurately assess the quality of LLM-generated summaries.Among the four LLMs, GPT-4 has the highest scores on the Java and C datasets, and GPT-3.5 attains the highest score on the Python dataset.This suggests that the quality of summaries generated by GPT-3.5 and GPT-4 is relatively high.\n\n\u261e Finding \u25b6 According to human evaluation, the quality of reference summaries in the existing datasets is not particularly high.Summaries from general-purpose LLMs (e.g., GPT-3.5)",
            "score": 0.44608654608433596,
            "section_title": "IV. RESULTS AND FINDINGS",
            "char_start_offset": 24799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 29,
                    "end": 140
                },
                {
                    "start": 140,
                    "end": 279
                },
                {
                    "start": 279,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 648
                },
                {
                    "start": 650,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 1086
                },
                {
                    "start": 1088,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1137
                },
                {
                    "start": 1137,
                    "end": 1245
                },
                {
                    "start": 1245,
                    "end": 1413
                },
                {
                    "start": 1413,
                    "end": 1548
                },
                {
                    "start": 1548,
                    "end": 1686
                },
                {
                    "start": 1686,
                    "end": 1780
                },
                {
                    "start": 1782,
                    "end": 1910
                },
                {
                    "start": 1910,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 64,
                    "matchedPaperCorpusId": "257378627"
                },
                {
                    "start": 66,
                    "end": 70,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 977,
                    "end": 981,
                    "matchedPaperCorpusId": "250426482"
                },
                {
                    "start": 983,
                    "end": 987,
                    "matchedPaperCorpusId": "259108973"
                },
                {
                    "start": 989,
                    "end": 993,
                    "matchedPaperCorpusId": "258180059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39013671875
        },
        {
            "corpus_id": "271571554",
            "title": "Unlocking the Potential: Benchmarking Large Language Models in Water Engineering and Research",
            "text": "in \"DrinkingW\" in the five-shot setting. Gemini stood out significantly among other 6 LLMs in both zero-shot and five-shot settings, especially in \"WasteW\" and \"DrinkingW,\" where it scored a remarkable 40.4% and 43.5% in the five-shot setting, respectively, indicating its exceptional capability in pinpointing detailed research gaps. Llama3 exhibited moderate performance compared to GPT-4, with a strong 34.5% in \"DrinkingW\" in the five-shot setting. In contrast, GLM-4, QWEN, and ERNIE showed relatively lower capabilities compared to GPT-3.5. \n\nAlthough the evaluation scores obtained using automatic evaluation methods were relatively low (Table 2 and Table 3), this can be attributed to the irrelevant content generated by LLMs during title generation and research gap identification (Tables S3 and S4). Our evaluation suggested that LLMs, particularly Gemini, can generally be trusted for executing academic tasks, while users needed to carefully remove or revise irrelevant key information generated from some of LLMs (Tables S3 and   S4). Additionally, we found that LLMs exhibited varying capabilities across different types of academic tasks, such as generating titles or identifying research gaps, as well as across different water topics, including \"treatment processes for wastewaters\", \"environmental restoration\", \"drinking water treatment and distribution\", \"sanitation\", \"anaerobic digestion\" and \"contaminants and related water quality monitoring and assessment\". Therefore, further fine-tuning or retrieval-augmented generation (RAG) with targeted datasets was necessary to specifically develop a genuinely effective water research expert model.",
            "score": 0.4459875356059852,
            "section_title": "Water Research",
            "char_start_offset": 17468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1665
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0233306884765625
        },
        {
            "corpus_id": "270738200",
            "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
            "text": "Since most of the state-of-the-art LLMs can only be accessed via APIs, researchers start to automatically collect evaluation data by directly prompting GPT-4 and train their own evaluation models to provide stable and effective evaluations at a lower cost (Wang et al., 2024;Li et al., 2024;Kim et al., 2024).\n\nThe concurrent works similar to ours are the LLMs specially trained for evaluation tasks like PandaLM (Wang et al., 2024), JudgeLM (Zhu et al., 2023), and AUTO-J (Li et al., 2024).For comparison, our work is the first attempt to deal with the challenge of uninformative critique generation which commonly appears in recent LLMbased evaluation models especially without references.Instead of prompting GPT-4 directly, our proposed Eval-Instruct can fully utilize the connection among different evaluation tasks and settings to construct informative evaluation data, which are empirically shown to improve the quality of generated critiques.",
            "score": 0.4459348822399049,
            "section_title": "Related Work",
            "char_start_offset": 5855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 309
                },
                {
                    "start": 311,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 950
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 275,
                    "matchedPaperCorpusId": "259108266"
                },
                {
                    "start": 275,
                    "end": 291,
                    "matchedPaperCorpusId": "263829791"
                },
                {
                    "start": 413,
                    "end": 432,
                    "matchedPaperCorpusId": "259108266"
                },
                {
                    "start": 473,
                    "end": 490,
                    "matchedPaperCorpusId": "263829791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63037109375
        },
        {
            "corpus_id": "262138540",
            "title": "Prompt engineering of GPT-4 for chemical research: what can/cannot be done?",
            "text": "For the interactions with the large language model (LLM), we utilized the GPT-4 (ChatGPT May 24 Version) unless otherwise noted. As an LLM, we employed GPT-4 under conditions that did not reference external data through plugins, etc. Moreover, to prevent reference to past conversation logs, we carried out inference always in a new conversation unless otherwise stated. The response from GPT-4 slightly changed with each question. In this research, we asked the question only once and used that response. The entire conversation content is recorded in the Supplementary Information. \n\nWe clarify the limitations inherent in the current study. Our evaluation of GPT-4's abilities within the realm of chemical research is based on a select set of prompts and responses. Thus, the results showcased in this paper might not fully encapsulate GPT-4's overall performance when applied to chemical research. \n\nThis study was designed as a preliminary exploration, providing indicative insights rather than an exhaustive investigation. It seeks to offer potential use-cases and highlight certain limitations of applying large language models like GPT-4 to chemical research. Future research should aim to broaden the scope of evaluation prompts and investigate the performance of GPT-4 in diverse chemical research scenarios.",
            "score": 0.44512136089638066,
            "section_title": "Experimental",
            "char_start_offset": 5490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1318
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0191192626953125
        },
        {
            "corpus_id": "268247902",
            "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
            "text": "The generated QA pairs in MathScaleQA might be incorrect. Therefore, we introduce a separate validation step in Section 3.4. In this section, we design controlled experiment on 5K generated data from MathScaleQA and again using LLaMA-2 7B as our base model. \n\nGPT-4 v.s. GPT-3.5 Accuracy We manually annotate 100 randomly chosen generated data points and generate answers with GPT-3.5-Turbo and GPT-4. GPT-4 demonstrate an impressive accuracy of 87%, significantly outperforming the accuracy of 69% by GPT-3.5-Turbo. Therefore, we used GPT-4 to generate reference solutions and validate our synthetic solutions, replacing any incorrect solutions with the GPT-4 reference solutions. \n\nResults Within the 5K examples, 26% of the solutions are identified as incorrect by GPT-4 and are replaced. We have another two settings with either all GPT-3.5 solutions and GPT-4 solutions. The results are shown in Table 7 and we observe that using original 3.5-Turbo solutions lead to a similar results as using the validation step. \n\nThis observation is counter-intuitive. Maybe because training on synthetic data generated from GPT-3.5 is essential distillation. Even if some solutions are incorrect, they may still help to the open-source LLMs (e.g., LLaMA-2 or Mistral) to mimic the distirubtions of GPT-3.5. We also notice that in neural machine translation distillation, the step of validating incorrect translations is also ignored (Kim & Rush, 2016). Therefore, we opt to omit the validation and correction step from the final MathScale pipeline.",
            "score": 0.4449441560588402,
            "section_title": "On Validating Generated Data",
            "char_start_offset": 21186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1541
                }
            ],
            "ref_mentions": [
                {
                    "start": 1426,
                    "end": 1444,
                    "matchedPaperCorpusId": "8451212"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0618896484375
        },
        {
            "corpus_id": "264490974",
            "title": "Can large language models replace humans in systematic reviews? Evaluating GPT\u20104's efficacy in screening and extracting data from peer\u2010reviewed and grey literature in multiple languages",
            "text": "Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour\u2010intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre\u2010Trained Transformer (GPT)\u20104, the biggest LLM so far. This pre\u2010registered study uses a \u201chuman\u2010out\u2010of\u2010the\u2010loop\u201d approach to evaluate GPT\u20104's capability in title/abstract screening, full\u2010text review and data extraction across various literature types and languages. Although GPT\u20104 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full\u2010text literature using highly reliable prompts, GPT\u20104's performance was more robust, reaching \u201chuman\u2010like\u201d levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.",
            "score": 0.4448975020037399,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0938720703125
        },
        {
            "corpus_id": "270764369",
            "title": "Assessing the Effectiveness of LLMs in Android Application Vulnerability Analysis",
            "text": "Tables 1 and 2 recapitulate the results for each LLM.Particularly, each line of Table 1 indicates if the specific model Detected the vulnerability (denoted with the letter \"D\"), and if it explained the situation and provided a valid solution for Improving the code (denoted with the letter \"I\").Actually, the \"I\" aspect is Table 1.Vulnerability analysis results.The letters \"D\" and \"I\" stand for the number of vulnerable samples detected and the number of vulnerable samples for which the LLM suggested improvements, respectively.Top scores per vulnerability are in boldface.The asterisk exhibitor stands for Code Llama without RAG.a key factor in evaluating each LLM (also against each other), as this is the sole indicator of whether the LLM actually \"perceives\" the security issue.Overall, with reference to Table 1, the best performers in terms of total vulnerabilities detected, are Code Llama (81/100), GPT 4 (67/100), Nous Hermes Mixtral (62/100), Zephyr Beta (54/100), and Zephyr Alpha (53/100), followed by GPT 4 TURBO (50/100), GPT 3.5 (42/100), MistralOrca (37/100), and Llama 2 (30/100).On the other hand, the best performers, in terms of total code improvement suggestions, are GPT 4 (83/90), GPT 4 Turbo (66/90), Zephyr Alpha (58/90), Zephyr Beta (56/90), and Nous Hermes Mixtral (56/90), followed by Code Llama (44/90), MistralOrca (38/90), GPT 3.5 (37/90), and Llama 2 (31/90).Overall, GPT 4 poses as the top performer, considering a composite score of high \"D\" and high \"I\".On the other hand, LLMs like Code Llama, which do identify the correct vulnerability, but fail to provide corrections or suggestions regarding the problematic lines of code may indicate an insufficiently trained model for this type of analysis.",
            "score": 0.4443319834790458,
            "section_title": "Results",
            "char_start_offset": 22666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 53,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 331
                },
                {
                    "start": 331,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 784
                },
                {
                    "start": 784,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1735
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0127716064453125
        },
        {
            "corpus_id": "274762910",
            "title": "OpenForge: Probabilistic Metadata Integration",
            "text": "It heavily relies on prompting a LLM to construct the taxonomy from top to bottom in an iterative manner and leverages another smaller language model to reduce hallucinations of the LLM such as removing non-existent entities and parent-child relationships generated by the LLM. We compare OpenForge with Chain-of-Layer on datasets involving parent-child relationships. \n\nGPT-3.5/GPT-4. The GPT series of models have demonstrated that scaling up language models in size and data significantly improves task-agnostic performance with task and few-shot demonstrations [8,38]. In our experiments, we prompt GPT-3.5 and GPT-4 to predict the presence of a specified relationship between concept pairs using task descriptions and few-shot learning. We use the Ope-nAI API to interact with the models identified as gpt-3.5-turbo-0125 and gpt-4-turbo-2024-04-09. \n\nDue to class imbalance in the datasets, we evaluate the quality of all approaches using F1 score, precision, and recall. We report the best results of each approach. For example, LLMs can obtain higher scores without few-shot learning in some cases. Additionally, we report the runtime of our MRF modeling approach in seconds for efficiency and scalability analysis.",
            "score": 0.4442897146487398,
            "section_title": "Experimental Setup",
            "char_start_offset": 41250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1222
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01090240478515625
        },
        {
            "corpus_id": "270562306",
            "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
            "text": "It enables GLM-4 to process long context texts (up to 128K tokens) with performance comparable to that of Claude 2 and GPT-4 Turbo (1106). \u2022 ChatGLM-Math [48]: To improve math problem solving in LLMs, we introduced ChatGLM-Math that leverages self-critique rather than external models or manual annotations for data selection. \u2022 ChatGLM-RLHF [17]: To align LLMs with human feedback, we introduced ChatGLM-RLHFour practices of applying PPO and DPO into LLMs. \u2022 Self-Contrast [24]: To avoid the need for expensive human preference feedback data, we developed a feedback-free alignment strategy Self-Contrast. It utilizes the target LLM to self-generate massive negative samples for its RLHF alignment. \u2022 AgentTuning [52]: To improve LLMs' agent capabilities, we developed the AgentTurning framework with the AgentInstruct instruction-tuning dataset that includes high-quality interaction trajectories between agents and environment. \u2022 APAR [21]: To improve the inference speed of LLMs for responses with hierarchical structures, we presented an auto-parallel auto-regressive (APAR) generation approach. It leverages instruct tuning to train LLMs to plan their (parallel) generation process and execute APAR generation. \u2022 Benchmarks: We also developed several open LLM benchmarks, including AgentBench [25] for evaluating LLMs as agents, LongBench [2] for evaluating the long context handling performance of LLMs, AlignBench [1] to measure the alignment quality of ChatGLM with Chinese language content, HumanEval-X [58] to evaluate HumanEval [4] problems in programming languages beyond Python, as well as NaturalCodeBench (NCB) to measure models' capacities to solve practical programming tasks. \n\nGLM-4 All Tools. The latest ChatGLM models are GLM-4 and GLM-4 All Tools, both of which were trained and aligned by using the techniques above. GLM-4 All Tools is a model version further aligned to support intelligent agents and related tasks.",
            "score": 0.44424537075798043,
            "section_title": "ChatGLM Techniques",
            "char_start_offset": 13956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1694
                },
                {
                    "start": 1697,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1940
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040252685546875
        },
        {
            "corpus_id": "271097661",
            "title": "Source Code Summarization in the Era of Large Language Models",
            "text": "Evaluation.From Table III, it can also be observed that the average scores of reference summaries evaluated by the four LLM-based methods are mostly below 3 points.It means that similar to human evaluation, LLM-based evaluation methods also believe that the quality of the reference summaries is not very high.Besides, LLM-based evaluation methods are inclined to give higher scores to general-purpose LLMs GPT-3.5 and GPT-4, which is the same as human evaluation.\n\nBased on the above observations, we can reasonably speculate that compared to methods based on summary-summary text/semantic similarity and summary-code semantic similarity, LLM-based evaluation methods may be more suitable for evaluating the quality of summaries generated by LLMs.Therefore, we follow [9], [73] and calculate Spearman's correlation coefficient \u03c1 with the p-value between the results of each automated evaluation method and human evaluation, providing more convincing evidence for this speculation.The Spearman's correlation coefficient \u03c1 \u2208 [\u22121, 1] is suitable for judging the correlation between two sequences of discrete ordinal/continuous data, with a higher value representing a stronger correlation [77].\u22121 \u2264 \u03c1 < 0, \u03c1 = 0, and 0 < \u03c1 \u2264 1 respectively indicate the presence of negative correlation, no correlation, and positive correlation [78].The p-value helps determine whether the observed correlation is statistically significant or simply due to random chance.By comparing the p-value to a predefined significance level (typically 0.05), we can decide whether to reject the null hypothesis and conclude that the correlation is statistically significant.Table IV shows the statistical results of \u03c1 and p-value.It can be clearly observed that among all automated evaluation methods, there is a significant positive correlation (0.28 \u2264 \u03c1 \u2264 0.65) between the GPT-4-based evaluation method and human evaluation in scoring the quality of summaries generated by most LLMs, followed by the GPT-3.5-basedevaluation method.For other automated evaluation methods, in most cases, their correlation with human evaluation is negative or weakly positive.",
            "score": 0.44403418232116953,
            "section_title": "Correlation between Automated Evaluation and Human",
            "char_start_offset": 28671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 11,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 464
                },
                {
                    "start": 466,
                    "end": 748
                },
                {
                    "start": 748,
                    "end": 981
                },
                {
                    "start": 981,
                    "end": 1192
                },
                {
                    "start": 1192,
                    "end": 1331
                },
                {
                    "start": 1331,
                    "end": 1452
                },
                {
                    "start": 1452,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1701
                },
                {
                    "start": 1701,
                    "end": 1987
                },
                {
                    "start": 1987,
                    "end": 2005
                },
                {
                    "start": 2005,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 772,
                    "matchedPaperCorpusId": "246822923"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "236634281"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08538818359375
        },
        {
            "corpus_id": "277150883",
            "title": "Enhancing LLM Code Generation with Ensembles: A Similarity-Based Selection Approach",
            "text": "To evaluate the effectiveness of EnsLLM, we compare its performance against standalone LLMs on the HumanEval and LiveCodeBench datasets using the pass@1 metric. The goal is to determine whether EnsLLM improves accuracy over individual models by leveraging an ensemble-based selection strategy. \n\nTable 1 presents the results of individual LLMs as well as EnsLLM's performance on HumanEval and LiveCodeBench in 3 modes. The EnsLLM (All) mentioned the results of EnsLLM when using all the LLMs considered in this study. Achievable accuracy, mentioned in the braces with the accuracy of EnsLLM, refers to the maximum possible accuracy that the ensemble-based selection approach can attain, assuming an optimal selection strategy. It is defined as the proportion of problems for which at least one of the individual LLMs in the ensemble produces a correct solution. Since EnsLLM does not generate new program but rather selects the best candidate from multiple LLM outputs, its upper bound performance is inherently constrained by the presence of correct solutions among the generated candidates. Mathematically, if an ensemble of N LLMs is used, and for a given benchmark dataset, there are M total problems and C is the number of problems where at least one LLM generated a correct solution the achievable accuracy is calculated by:  This metric provides a theoretical limit on how well EnsLLM can perform. In our evaluation, this upper bound is 90.9% for HumanEval and 53.8% for LiveCodeBench, meaning that even with a perfect selection mechanism, the system cannot exceed these accuracy values since no correct solution exists for the remaining problems in the set of LLM-generated outputs. \n\nThe best-performing standalone models on HumanEval are GPT-4o (83.5%), followed closely by GPT-4 (82.3%) and GPT-3.5 (76.8%). Similarly, for LiveCodeBench, the highest accuracy among standalone models is GPT-4o (43.4%), with GPT-4 (42.2%) and CoPilot (41.6%) ranking next.",
            "score": 0.44402791376033995,
            "section_title": "RQ1: How does EnsLLM perform compared to standalone LLMs?",
            "char_start_offset": 32853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1690
                },
                {
                    "start": 1693,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1965
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0167236328125
        },
        {
            "corpus_id": "264426138",
            "title": "A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions",
            "text": "a) Questionable Reliability of Using LLMs: Several works have examined the feasibility of utilizing LLMs as detectors to discern text generated by either themselves or other LLMs. This approach was first broached by Zellers et al. [67], wherein the text generation model Grover [67] was noted to produce disinformation that was remarkably deceptive due to its inherently controllable nature. Subsequent exploratory analyses by Zellers et al. [67] engaging various architectural models like GPT-2 [87] and BERT [142] revealed that Grover's most effective countermeasure was Grover itself, boasting an accuracy rate of 92%, while other detector types experienced a decline in accuracy to approximately 70% as Grover's size increased. A recent reevaluation conducted by Bhattacharjee and Liu [116] on more recent LLMs like ChatGPT and GPT-4 yielded that neither could reliably identify text generated by various LLMs. During the observations, it was noted that ChatGPT and GPT-4 exhibited contrasting tendencies. \n\nChatGPT tended classify text generated by LLMs as if it were written by humans, with a misclassification probability of about 50%. While GPT-4 leaned towards labeling humanwritten text as if it were generated by LLMs, and about 95% of human-written texts are misclassified as LLM-generated texts. ArguGPT [70] further attested to the lackluster performance of GPT-3.5-Turbo in detecting text generated by LLMs, with accuracy rates languishing below 50% across zero-shot, oneshot, and two-shot settings. These findings collectively demonstrate the diminishing reliability of employing LLMs for direct self-generated text detection, particularly when compared to statistical and neural network methods. This is particularly evident in light of the increasing complexity of LLMs. \n\nb) ICL: A Powerful Technique for LLM-Based Detection: Despite the unreliability issues associated with using LLMs for direct detection of LLM-generated text, recent empirical investigations highlight the potential efficacy of ICL in augmenting LLMs' detection capabilities.",
            "score": 0.44351422990568384,
            "section_title": "E. LLMs as Detectors",
            "char_start_offset": 62989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 235,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 278,
                    "end": 282,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 442,
                    "end": 446,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 510,
                    "end": 515,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 789,
                    "end": 794,
                    "matchedPaperCorpusId": "260379022"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "258179137"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01233673095703125
        },
        {
            "corpus_id": "267499939",
            "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",
            "text": "We review 255 papers and carefully detail data leakage emerging from them. We focus primarily on the models accessible through OpenAI's ChatGPT, 2 (GPT-3.5 and GPT-4 3 ) as these are the most frequently used commercial LLMs in NLP research. By considering OpenAI's data usage policy, we assess how much data was reported to be sent to the models in a way that it could be used for further training, hence giving the models an unfair advantage during evaluation. We also report a series of emergent evaluation malpractices, including lack of comparison with other approaches, differences in the evaluation scale (e.g., evaluating open models on entire benchmarks while comparing to proprietary LLMs evaluated on samples only), lack of code and data access, or data leakage even in situations where it could be avoided. To our knowledge, this work is the most comprehensive and extensive quantification of the data leakage issue in LLMs to date. \n\nIn short, our contributions are as follows: \n\n(1) We systematically analyse 255 papers evaluating OpenAI's GPT-3.5 and GPT-4 on a variety of tasks in NLP and other domains (Section 4). \n\n(2) For each paper, we estimate the amount of data leaked in such a way that it could be used for further model training. Overall, we conclude that \u223c42% of the reviewed papers leaked data to GPT-3.5 and GPT-4, for a total of \u223c4.7M benchmark samples across 263 benchmarks (Section 5.1). \n\n(3) We further analyse the evaluation protocols of the selected papers, and we reveal some critical malpractices limiting both the experiments' reproducibility and fairness (Sections 5.2 and 5.3). \n\n(4) Based on our findings, we propose a list of suggested practices for the evaluation of closed-source LLMs (Section 6). \n\nWe believe that our work can contribute to ongoing efforts on quantifying LLM data contamination by pointing out which datasets are worthy of further investigation. We release our survey results as a collaborative repository, in the form of a webpage at https://leak-llm.github.io/.",
            "score": 0.44345659786489494,
            "section_title": "Introduction",
            "char_start_offset": 2209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1741
                },
                {
                    "start": 1744,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0279541015625
        },
        {
            "corpus_id": "273482644",
            "title": "UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models",
            "text": "In the experiments, GPT-4o is used as the user simulator to generate queries and simulate realworld conversations. Claude-3.5-Sonnet serves as   the evaluator to compare model responses, ensuring a clear separation between testing and evaluation to minimize bias. For dialogue simulations, the temperature is set to 0.5 with no token limit. We tested financialspecific LLMs (7B to 70B parameters) along with their backbone models and included generalpurpose models like GPT-4o and GPT-4o-mini, accessed via APIs. Table 4 lists all models used. \n\nTo mitigate positional bias in LLM evaluations (Li et al., 2023c), we shuffled the input order during dialogue comparisons. To further minimize evaluator bias, such as misinformation or cognitive bias (Talboy and Fuller, 2023), we designed the evaluation prompts based on two key criteria: \u2022 Evaluation Standards: Focused on User \n\nThe backbone model of Tongyi-Finance-14B-Chat is Qwen-14B. Model Card: https: //modelscope.cn/models/TongyiFinance/Tongyi-Finance-14B/summary The full evaluation prompt is available in Appendix E.",
            "score": 0.4434411771004224,
            "section_title": "Experimental Settings",
            "char_start_offset": 16364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0219879150390625
        },
        {
            "corpus_id": "268063814",
            "title": "PRSA: Prompt Stealing Attacks against Real-World Prompt Services",
            "text": "While prompt marketplaces often reveal the target LLMs for their prompts, such as those based on GPT-3.5 or GPT-4, other real-world scenarios, like LLM applications, may not reveal their target LLMs. This lack of model disclosure underscores the importance of evaluating the transferability of attack methods to unknown target LLMs. Table 3 presents a comparison of the average transferability of attack methods across three similarity metrics (further details in Appendix F). \n\nFor target prompts designed for GPT-4, using only GPT-3.5 as the generative model to generate surrogate prompts proves to be the least effective approach. However, the performance of surrogate prompts generated by GPT-3.5 + PRSA shows a significant improvement, highlighting PRSA's role in enhancing the transferability of GPT-3.5, especially in semantic similarity, where scores increase from 0.404 to 0.716. The performance difference between GPT-3.5 + PRSA and GPT-4 + PRSA remains below 10.2%. When targeting GPT-3.5-based prompts with GPT-4, the transferability of GPT-4 + PRSA remains stable, and the attack performance exceeds that of GPT-4 itself. These findings demonstrate that PRSA effectively improves the transferability of attack methods across different LLMs. \n\n7 Attack Performance on Interactive Prompt Services",
            "score": 0.44341819669616267,
            "section_title": "Transferability",
            "char_start_offset": 44752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1307
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01312255859375
        },
        {
            "corpus_id": "268856493",
            "title": "METAL: Towards Multilingual Meta-Evaluation",
            "text": "LLMs also show position bias or scale region bias and are unable to distinguish between candidates that are close to each other (Golchin and Surdeanu, 2023).LLMs are sensitive to instructions and their capabilities vary for different metrics (Skopek et al., 2023;Wang et al., 2023b;Shen et al., 2023).Another significant challenge when using LLMs as evaluators is a limited assessment of their abilities in multilingual settings.Studies have shown that LLMs have inferior performance even on some high-resource languages and cannot be assessed extensively on low-resource languages due to a lack of benchmarks (Ahuja et al., 2023a).Therefore, it is still unclear if LLMs can replace human evaluations in multilingual settings.\n\nIn this paper, we introduce the METAL framework for a robust assessment of LLMs as evaluators in multilingual scenarios.Figure 1 shows an outline of our framework.The METAL framework is an end-to-end pipeline, that starts with first creating a rich meta-evaluation dataset that contains a variety of samples across the metrics of interest.We do this by systematically prompting GPT-4 to generate a wide range of sample data points, that are then evaluated by native speakers.In the next step, we compare LLM judgments with human judgments.For this, we draw on our previous work (Hada et al., 2024) to prompt LLMs for evaluations and subsequently compare the scores with human judgments.In particular, for the task of summarization we create a dataset of 1000 summaries covering 10 languages, with human ratings across 5 metrics. 1 Next, we obtain LLM evaluations from GPT-3.5-Turbo,GPT-4, and PaLM2 across these 1000 summaries and 5 metrics.\n\nOur findings show that the GPT-3.5-Turbo-basedevaluator does not perform well across languages and metrics, while evaluators based on GPT-4 and PaLM2 perform better.We find that the evaluation ability of LLMs significantly across languages, motivating the creation of a meticulously crafted metaevaluation dataset covering all target languages before using LLM-based evaluators.",
            "score": 0.4433726687350698,
            "section_title": "Introduction",
            "char_start_offset": 1550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 726
                },
                {
                    "start": 728,
                    "end": 848
                },
                {
                    "start": 848,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1610
                },
                {
                    "start": 1610,
                    "end": 1669
                },
                {
                    "start": 1671,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1836
                },
                {
                    "start": 1836,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 263,
                    "matchedPaperCorpusId": "263909080"
                },
                {
                    "start": 610,
                    "end": 631,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 1306,
                    "end": 1325,
                    "matchedPaperCorpusId": "261822638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26025390625
        },
        {
            "corpus_id": "272826664",
            "title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
            "text": "Our experiments on SCiPS-QA with a diverse array of LLMs using a comprehensive evaluation strategy reveal several key insights. Firstly, existing LLMs, whether open-access or proprietary, demonstrate a limited understanding of scientific methodologies required to serve as reliable assistants. While the parameter scaling law holds within each LLM family, models of similar size across different families are not directly comparable. For instance, Meta Llama-3 70B models emerge as formidable competitors to much larger GPT models, frequently outperforming GPT-4 Turbo in our evaluations. This reiterates earlier findings that parameter scaling alone does not reflect the capabilities of LLMs and current models, along with their training methodology, are underperforming their 'true' potential (Hoffmann et al., 2022). \n\nEchoing Huang et al. ( 2024)'s findings, we observe that powerful LLMs such as GPT-4 Turbo and GPT-3.5 Turbo struggle to reliably verify their responses. Hallucination detection techniques like SelfCheckGPT also prove ineffective in detecting incorrect reasoning posed by strong LLMs like GPT-4 Turbo in complex questions within SCiPS-QA. In fact, we notice a counterintuitive trend where GPT-3.5 Turbo assigns lower scores to incorrect responses compared to the stronger GPT-4 Turbo. \n\nHowever, the most concerning finding of this paper revolves around how human evaluators perceive LLM-generated scientific reasoning. When tasked with evaluating the convincingness of reasoning explanations generated by GPT-4 Turbo, human evaluators tend to assign higher ratings to a significant majority of incorrect answers. This aligns with the concern raised by Dutta and Chakraborty (2023) that current LLM-based AI assistants have the potential to propagate widespread scientific misunderstandings if left unchecked. \n\nImplications for future research. We hope that our proposed dataset, SCiPS-QA, along with the evaluation suit we design in this work, will serve as a valuable benchmark for future LLM research. Given the growing popularity of generalist as well as domain-specific AI assistants, we envision a positive future focus in building reliable scientific assistants. Finally, our findings with human evaluation calls upon further focus in trustworthy AI research.",
            "score": 0.4432129690955182,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 25589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2289
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0162811279296875
        },
        {
            "corpus_id": "271270236",
            "title": "AI AI Bias: Large Language Models Favor Their Own Generated Content",
            "text": "To complement our studies on LLM bias, we conducted an initial experiment to gauge human preferences in similar decision contexts (See Table 5 and Tabel 6 for results).It is important to note that these preferences were collected by research assistants rather than actual users.This study serves as a preliminary investigation with a small sample size and best-effort human baseline, and the findings are not definitive.We used the product details and scientific paper abstracts generated by both GPT-3.5 and GPT-4, and presented them to a group of human evaluators.These participants were asked to choose which descriptions they preferred without knowing whether they were written by a human or an LLM.They also had the option to state that they had no preference between the two presented texts.\n\nParticipants were presented with pairs of descriptions: one generated by an LLM and one written by a human.Each participant evaluated a randomized set of pairs to mitigate any order or preference bias.The results in Table 5 and Table 6 indicate that human evaluators generally preferred human-generated product descriptions over those generated by LLMs, with GPT-3.5 having a preference ratio of 0.29 and GPT-4 at 0.28.For scientific paper abstracts, the preferences leaned more towards LLM-generated texts, with GPT-4 abstracts being preferred at a ratio of 0.60 compared to GPT-3.5 at 0.46.",
            "score": 0.4424536892142759,
            "section_title": "Preferences of Humans",
            "char_start_offset": 14486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 703
                },
                {
                    "start": 703,
                    "end": 797
                },
                {
                    "start": 799,
                    "end": 906
                },
                {
                    "start": 906,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1391
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0227203369140625
        },
        {
            "corpus_id": "260887765",
            "title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
            "text": "ML as a scientific field focuses on the former, but most LLM leaderboards present apple-to-orange comparisons. \n\nFinally, for both few-shot and fine-tuned evaluation of LLMs, most of the reported results should be taken with a grain of salt because of test data contamination. For example, GPT-4 received a lot of press coverage due to the claim of achieving a score that falls in the top 10% of test takers on a simulated bar exam (Katz et al., 2023) -but that result was soon questioned on grounds of improper evaluation and possible data contamination (Mart\u00ednez, 2023). \n\nIn the GPT-3 report, OpenAI itself documented how hard it is to avoid benchmark contamination (Brown et al., 2020). By now, multiple studies presented evidence of the presence of common NLP benchmarks in multiple datasets used for training LLMs (Dodge et al., 2021;Magar & Schwartz, 2022;Blevins & Zettlemoyer, 2022), which can inflate LLM performance in certain tasks and datasets. The LM Contamination Index5 , a collaborative effort to document benchmark contamination, currently has 375 entries for various benchmarks and models across different tasks. Furthermore, a recent study has documented the effect where GPTs score higher on the \"old\" benchmarks than on the new ones (Liu et al., 2023a), which strongly suggests that the previously reported evaluation results may be inflated.",
            "score": 0.4422135098954807,
            "section_title": "Claim: (Few-shot) LLMs Are State-of-the-Art",
            "char_start_offset": 18176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 113,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1364
                }
            ],
            "ref_mentions": [
                {
                    "start": 669,
                    "end": 689,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 863,
                    "end": 891,
                    "matchedPaperCorpusId": "252780005"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.009521484375
        },
        {
            "corpus_id": "263608642",
            "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
            "text": "We ran a user study with 14 software engineering researchers, who evaluated GPT-4-generated assumptions and analysis plans. We then performed a manual analysis of the GPT-4-generated code. We find that GPT-4 surfaces mostly correct assumptions, but struggles to generate ones that apply common but implicit knowledge about software engineering data (e.g., pull requests showing the lines of code changed). We also observe that GPT-4 can generate analysis plans that correctly outline the modules for replication, but is limited by the quality and detail of the methodology as written in the original research paper. Finally, we find that the GPT-4-generated replication code contains the correct high-level logic, but has many small implementation-level errors (e.g., using incorrect tables in a database). Our findings have implications for leveraging LLMs for software engineering research, such as teaching GPT-4 software engineering domain knowledge.",
            "score": 0.44206568909678234,
            "section_title": "60:3",
            "char_start_offset": 4234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.052337646484375
        },
        {
            "corpus_id": "268856493",
            "title": "METAL: Towards Multilingual Meta-Evaluation",
            "text": "We presented the first framework for end-to-end evaluation of LLMs as evaluators in multilingual scenarios.We created a dataset of 1000 summaries across 10 languages rated by native speakers on 5 different metrics.Our dataset covers a range of summaries in terms of linguistic acceptability, output quality, task quality, and others.We do this by systematically prompting GPT-4 to generate summaries of varying quality.The human ratings obtained for these summaries are of high quality with \u03ba > 0.6 and F 1 > 0.75.We plan to make the METALdataset available to the research community.Using our dataset, we investigate the capabilities of three LLMs as evaluators: GPT-3.5-Turbo,GPT-4, and PaLM2 using two prompting strategies and compare their evaluation with the METALhuman evaluations.Our results show that GPT-4 with detailed instructions performs closest to humans, while GPT-3.5-Turbo is not a suitable multilingual evaluator but surprisingly does better than GPT-4 and PaLM2 in some metrics for Bengali.We also show that GPT-4 with detailed instructions does best when there is disagreement amongst human annotators.We compare the overlapping summaries between SEAHORSE and METAL and show how our metrics and prompting methods can be used to compare generations from different models.Finally, we analyze human and LLM reasoning and observe that LLMs often provide incorrect justifications for their scores, thus showing that more research is needed to be able to use LLM-based evaluators with confidence in the multilingual setting.",
            "score": 0.4419938286353787,
            "section_title": "Conclusion",
            "char_start_offset": 28580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 677
                },
                {
                    "start": 677,
                    "end": 786
                },
                {
                    "start": 786,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1485595703125
        },
        {
            "corpus_id": "257805032",
            "title": "Improving large language models for clinical named entity recognition via prompt engineering",
            "text": ".901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed. Discussion The study\u2019s findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings. Conclusion While direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.",
            "score": 0.44169474735771863,
            "section_title": "abstract",
            "char_start_offset": 1835,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0244293212890625
        },
        {
            "corpus_id": "270562487",
            "title": "AI-Assisted Human Evaluation of Machine Translation",
            "text": "Despite the advantages in lower costs per error span of the presented setup, we urge practitioners to not use this approach when metrics evaluation is one of the expected tasks due to the particular bias to the used metric in the setup. The intended application of this pipeline is purely a more efficient evaluation of machine translation system quality. \n\nBoth ESA AI and GEMBA rank GPT-4-5shot as the best system, a system that uses the same LLM to translate sentences as we use to generate for GEMBA. This indicated a weakness that our approach is biased towards systems build on top of the same underlying LLM. Liu et al. (2023) described this phenomena when the same system used for generating output should not be used to also evaluate them. This issue could be mitigated by using two different LLMs to generate error spans. \n\nLastly, we use GEMBA, a GPT4-based system, for the quality estimation and work with WMT 2023 data. Unfortunately, we can not exclude the possibility of the QE system being trained on this data, though the texts and scores are kept in two separate large files with non-linear mappings.",
            "score": 0.4415591893746056,
            "section_title": "Limitations",
            "char_start_offset": 13098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1118
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 633,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07879638671875
        },
        {
            "corpus_id": "258841126",
            "title": "On Learning to Summarize with Large Language Models as References",
            "text": "Proprietary LLMs, such as GPT-4, can be more capable than open-source LLMs but are less costefficient. Therefore, we focus on a cost-effective low-resource setting using contrastive learning where the LLMs are used as both summary generators and evaluators. Automatic Evaluation For reference-based evaluation, we report the ROUGE-1/2 F1 scores between the system outputs and the reference summaries generated by the reference LLM. For reference-free evaluation, we use either GPTScore (Fu et al., 2023) or GPTRank ( \u00a72.2.2). In particular, for GPTScore we report both the un-normalized and normalized sum of log-probability.",
            "score": 0.4414824487837872,
            "section_title": "Learning under Low Resource Settings",
            "char_start_offset": 8843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 625
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146484375
        },
        {
            "corpus_id": "275920895",
            "title": "Do LLMs Have Visualization Literacy? An Evaluation on Modified Visualizations to Test Generalization in Data Interpretation",
            "text": "As LLMs continue to show robustness and cost-efficiency, researchers are evaluating their performance across various domains to expand their applications [9]. One prominent area of study is programming, where researchers assessed LLMs' ability to generate code [10,27,42]. Additionally, LLMs have been tested in fields such as planning [43], computational social science [48], and psychology [15]. In the humancomputer interaction field, H\u00e4m\u00e4l\u00e4inen et al. [18] conducted studies to assess whether LLMs can simulate humans to provide open-ended text responses. The results indicated that they can provide human-like answers, but the answers would potentially be affected by malicious inputs. Chiang et al. [12] investigated the evaluation ability of LLMs to see whether they can replace human evaluation. These studies underscore the need for further research into the use of LLMs in various applications, including visualizations. \n\nIn the realm of visualization, researchers have been utilizing LLMs to create or enhance visualizations. For example, Liew and Mueller [26] have applied LLMs to generate meaningful captions for visualizations. Feng et al. [17] developed a system to enable people to visually conduct prompt engineering in order to generate an ideal image from texts. ChartSpark [47] employs LLMs to integrate semantic context into charts through a novel text-to-image generative model. Such works demonstrate the power of LLMs for creating visualizations; however, evaluating the quality of the generated enhancements remains challenging [35]. Kim et al. [23] evaluated the capabilities of ChatGPT in giving advice on visualization design with the questions from the VisGuide forum. Chen et al. [11] evaluated the performance of the GPT model in a data visualization course. In their experiments, they used SVG files and JavaScript code as input to test the GPT-4 model's ability to interpret visualizations. However, SVG files and JavaScript code are not commonly encountered in everyday contexts, such as news websites. While the results of their experiments suggest that GPT has the potential to complete diverse visualization tasks, GPT may simply be analyzing the provided code for the answer rather than truly interpreting, understanding, and reasoning about the charts.",
            "score": 0.4414385531871784,
            "section_title": "Evaluating LLMs",
            "char_start_offset": 9219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "258437095"
                },
                {
                    "start": 268,
                    "end": 271,
                    "matchedPaperCorpusId": "247255943"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "260440590"
                },
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "264107446"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "258218228"
                },
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "258461287"
                },
                {
                    "start": 1068,
                    "end": 1072,
                    "matchedPaperCorpusId": "253892340"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "259950893"
                },
                {
                    "start": 1294,
                    "end": 1298,
                    "matchedPaperCorpusId": "258418257"
                },
                {
                    "start": 1554,
                    "end": 1558,
                    "matchedPaperCorpusId": "258584102"
                },
                {
                    "start": 1711,
                    "end": 1715,
                    "matchedPaperCorpusId": "259076029"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042388916015625
        },
        {
            "corpus_id": "258833685",
            "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
            "text": "A natural question to ask is whether such aforementioned limitations are resolved with an stronger LLM. In this section, we conduct similar analyses on GPT-4 (OpenAI, 2023) with the RTS method. We present the GPT-4 results in the last rows of Table 4 and 5. The results suggest that a stronger LLM does not necessarily translate to a stronger LLM evaluator, although Table 4 does show that GPT-4 outperforms ChatGPT in terms of human correlation consistently across most dimensions. Unfortunately, GPT-4 still suffers from the same limitations as ChatGPT. It appears to be both candidate-dependent and dimension-dependent, as demonstrated by the large spreads with varying median values across dimensions in Figure 5 and the significantly negative meta-correlations out of 3 dimensions (Table 6). However, GPT-4 is less dimension-dependant as compared to ChatGPT, as the medians in the box plots in Figure 5 are more aligned than those in Figure 1. \n\nIn addition, there is a notable enhancement in the meta-correlation for consistency, which we attribute to a significant reduction in reported hallucinations with GPT-4 (OpenAI, 2023). It is possible that with much more instruction training to avoid hallucinations, GPT-4 is much better aligned with humans to detect inconsistencies (i.e. hallucina- tions) in summaries. \n\nNevertheless, GPT-4 exhibits a much worse negative meta-correlation in the relevance dimension, which, interestingly, seems to reflect the challenges of maintaining both \"truthfulness\" and \"informativeness\" (Ouyang et al., 2022). This is because a model could be easily made more truthful if allowed to provide less relevant information (for instance, by refusing to answer the users' questions). It is possible that with reduced capability in the informativeness dimension, the model is less capable of differentiating the nuances of less relevant summaries when the summary quality is generally high. Nevertheless, we leave it to future work to determine whether GPT-4's more negative metacorrelation in the relevance dimension could be related to its stronger performance in consistency. We provide more details on the GPT-4 evaluator in Appendix H.",
            "score": 0.4413322631744675,
            "section_title": "GPT-4 Evaluator",
            "char_start_offset": 25329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2176
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044769287109375
        },
        {
            "corpus_id": "269157210",
            "title": "Spiral of Silence: How is Large Language Model Killing Information Retrieval? - A Case Study on Open Domain Question Answering",
            "text": "The EM llm for the correct answers and specific incorrect answers, as generated by the CTRLGEN method containing misinformation, are presented in Table 7.\n\nMoreover, we substantiate the rationality of employing EM as a QA evaluation metric in the experiments of Section 4 by calculating the Pearson correlation coefficient between EM llm and EM based on the experimental results in this section.We observe that the EM llm values for the texts generated by the other four LLMs, as verified by GPT-3.5-Turbo, have an average correlation exceeding 0.5 across four datasets when compared to the EM values obtained through direct string matching, as shown in Table 8.This demonstrates a significant correlation between the two metrics.For incorrect answers, the correlation is relatively lower, indicating the necessity of using GPT-3.5-Turbo to further filter texts in the exploration of misinformation.Considering the higher efficiency of direct EM calculation over EM llm , we use EM to evaluate the QA quality of the RAG system for experiments in other sections.",
            "score": 0.441331493781138,
            "section_title": "A.7 Effects of Misinformation",
            "char_start_offset": 36638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 156,
                    "end": 395
                },
                {
                    "start": 395,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1061
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037384033203125
        },
        {
            "corpus_id": "259274653",
            "title": "Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese",
            "text": "Nunes et al. (2023) explored the use of GPT-3.5 and GPT-4 to answer questions for the Exame Nacional do Ensino M\u00e9dio (ENEM), an entrance examination used by many Brazilian universities. They tested different prompt strategies, including using Chain-of-Thought (CoT) to generate explanations for answers, and found that GPT-4 with CoT was the best-performing approach, achieving an accuracy of 87% on the 2022 exam. Wu et al. (2023) evaluated the performance of different models for GEC, including Grammarly, GECToR, and ChatGPT (authors did not specify whether they used GPT-3.5 or GPT-4), and found that automatic evaluation methods result in worse numbers for ChatGPT than other GEC methods. In contrast, human evaluation showed that Chat-GPT produces fewer under-corrections or miscorrections and more overcorrections, indicating not only the potential of LLMs for GEC but also the limitation of automatic metrics to evaluate GEC tools. Fang et al. (2023) investigated GPT-3.5's potential for GEC using zero-and few-shot chain-of-thought settings. The model was evaluated in English, German, and Chinese, showcasing its multilingual capabilities. The study found that GPT-3.5 exhibited strong error detection and generated fluent sentences but led to over-corrections. \n\nDespite their outstanding performance on many tasks, LLMs may not be the silver bullet for NLP in multi-lingual settings. Lai et al. (2023)",
            "score": 0.44122636350086947,
            "section_title": "Related Work",
            "char_start_offset": 2060,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1271
                },
                {
                    "start": 1274,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.018768310546875
        },
        {
            "corpus_id": "258212638",
            "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent",
            "text": "Table 3 illustrates the evaluation results on our newly collected NovelEval, a test set containing 21 novel questions and 420 passages that GPT-4 had not learned. The results show that GPT-4 performs well on these questions, significantly outperforming the previous best-supervised method, monoT5 (3B). Additionally, ChatGPT achieves a performance level comparable to that of monoBERT. This outcome implies that LLMs possess the capability to effectively re-rank unfamiliar information. Table 5: Ablation study on TREC-DL19. We use gpt-3.5-turbo with permutation generation with different configuration. \n\nin the OpenAI API3 : curie-001 -GPT-3 model with about 6.7 billion parameters (Brown et al., 2020); davinci-003 -GPT-3.5 model trained with RLHF and about 175 billion parameters (Ouyang et al., 2022); gpt-3.5-turbo -The underlying model of ChatGPT (OpenAI, 2022); gpt-4 -GPT-4 model (OpenAI, 2023). \n\nThe results are listed in Table 4. From the results, we can see that: (i) The proposed PG method outperforms both QG and RG methods in instructing LLMs to re-rank passages. We suggest two explanations: First, from the result that PG has significantly higher top-1 accuracy compared to other methods, we infer that LLMs can explicitly compare multiple passages with PG, allowing subtle differences between passages to be discerned. Second, LLMs gain a more comprehensive understanding of the query and passages by reading multiple passages with potentially complementary information, thus improving the model's ranking ability. (ii) With PG, ChatGPT performs comparably to GPT-4 on nDCG@1, but lags behind it on nDCG@10. The Davinci model (text-davinci-003) performs poorly compared to ChatGPT and GPT-4. This may be because of the generation stability of Davinci and ChatGPT trails that of GPT-4.",
            "score": 0.4410177041352853,
            "section_title": "Results on NovelEval",
            "char_start_offset": 17855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1803
                }
            ],
            "ref_mentions": [
                {
                    "start": 684,
                    "end": 704,
                    "matchedPaperCorpusId": "199134224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051849365234375
        },
        {
            "corpus_id": "270738200",
            "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
            "text": "To investigate whether the critiques generated by our model can serve as feedback to improve the quality of LLM-generated texts, we employ Chat-GPT, GPT-4, and CRITIQUELLM to provide critiques for the generated texts of ChatGPT in the reference-free setting.Then, we instruct ChatGPT to modify its original generation based on the critiques.Finally, we use GPT-4 to perform referenced evaluations on the original texts and the modified texts generated by ChatGPT, respectively.\n\nThe results in Table 4 show that the critiques from CRITIQUELLM can serve as positive feedback whose contributed improvement on the overall score is close to that from the GPT-4's critiques.This further verifies the utility of CRITIQUELLM to provide informative critiques as scalable feedback that can guide LLMs towards better generation.We also notice that the critiques from Chat-GPT itself have a negative impact on the overall quality of its generated texts.This phenomenon is consistent with recent works that doubt the selfcorrection ability of LLMs without external inputs (Huang et al., 2023a;Stechly et al., 2023;Valmeekam et al., 2023).\n\nWe also report the evaluation scores before and after the critique-based modification across different tasks in Table 4.It is notable that the critiques from CRITIQUELLM can help enhance the quality of generated texts in a majority of tasks.However, in the tasks of logical reasoning, mathematics, and advanced Chinese understanding which are mostly hard tasks involving reasoning, the critiques from CRITIQUELLM seem to degrade the performance.We manually checked error cases and found that our model obtained misleading critiques on the reasoning process of generated texts.Since the evaluation of reasoning chains remains a challenging task (Golovneva et al., 2023) even for GPT-4, we leave further investigation in these tasks as future work.\n\nSince our experiment is a preliminary step towards utilizing critiques as feedback, we additionally have some findings which may inspire future research.",
            "score": 0.4410060123142076,
            "section_title": "Analysis of Critique as Feedback",
            "char_start_offset": 22325,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 258,
                    "end": 341
                },
                {
                    "start": 341,
                    "end": 477
                },
                {
                    "start": 479,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 818
                },
                {
                    "start": 818,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1126
                },
                {
                    "start": 1128,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1369
                },
                {
                    "start": 1369,
                    "end": 1573
                },
                {
                    "start": 1573,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1874
                },
                {
                    "start": 1876,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 1772,
                    "end": 1796,
                    "matchedPaperCorpusId": "254685985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439697265625
        },
        {
            "corpus_id": "271859987",
            "title": "A Perspective on Large Language Models, Intelligent Machines, and Knowledge Acquisition",
            "text": "In this paper, empirical evaluation of LLM capabilities is focused on understanding of abstract concepts and reasoning, which are specific to human intelligence. This allows us to underscore the differences between human and LLM mechanisms of learning. We use GPT-4 (version gpt-4-0125), because its performance is similar or better than other LLM implementations 19,20 . We adopt the Turing test setting for evaluation of LLM capabilities, similar to other studies 11,19,21,22 . However, rather than using the benchmarking approach [23][24][25][26] , we adopt an experimental design setting. That is, we present carefully designed questions that require understanding of abstract concepts in various areas, such as common-sense reasoning, simple math and machine learning. Specifically, we present several questions that require understanding of the same concept. Then analysis of GPT-4 responses may reveal the difference between LLM and human learning. That is, humans who understand a particular concept typically provide correct answers to all questions (based on this concept). In contrast, GPT-4 may provide inconsistent responses, suggesting that it lacks understanding. \n\nUnderstanding of abstract concepts is specific to human intelligence, because: -human use of abstract concepts for generalization from empirical data, -abstract concepts provide robustness with respect to variations of empirical data, and to minor variations in wording of presented questions. \n\nSince LLM's generalization is statistical in nature, the quality of its responses depends on the training data, and on specific wording of questions. \n\nOverall, our analysis of GPT-4 responses indicates that it lacks understanding and reasoning abilities, even though it may successfully imitate human intelligence. In such cases, correct answers are generated by a trained LLM model, rather than by conceptual understanding 27 . This is a brief summary of our findings: -GPT-4 struggles with understanding abstract concepts, especially when it requires combination of several concepts. Since GPT-4 lacks understanding, it can be easily tricked into providing wrong and nonsensical responses. \n\n-GPT-4 may generate inconsistent answers to similar questions. \n\n-GPT-4 often generates verbose and non-specific answers to well-posed questions. \n\n-GPT-4 cannot confidently qualify its answers as correct or wrong or 'don't know'.",
            "score": 0.44081044607634867,
            "section_title": "Empirical Evaluation of LLM Capabilities",
            "char_start_offset": 5291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1474
                },
                {
                    "start": 1477,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2169
                },
                {
                    "start": 2172,
                    "end": 2234
                },
                {
                    "start": 2237,
                    "end": 2317
                },
                {
                    "start": 2320,
                    "end": 2402
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "258866000"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "251196636"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "258866000"
                },
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 545,
                    "end": 549,
                    "matchedPaperCorpusId": "264491155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0245208740234375
        },
        {
            "corpus_id": "267616740",
            "title": "Rethinking Data Selection for Supervised Fine-Tuning",
            "text": "Another crucial potential bias is verbosity bias, meaning LLM judges usually prefer longer responses. However, GPT-4 as judge mitigates this bias to large extent: Zheng et al. (2023) shows that GPT-4 is robust to repetitive attack with failure rate of only 8%; whereas other LLM judges such as ChatGPT and Claude (Anthropic, 2023) fail more than 90% of the time. Following Chen et al. (2023b), we use the evaluation prompt shown in Appendix Figure 4) and winning score of #Win\u2212#Lose #Testset + 1 where #Testset = #Win + #Tie + #Lose as evaluation metric.",
            "score": 0.44077312123681045,
            "section_title": "Baselines.",
            "char_start_offset": 6887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 554
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 182,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04931640625
        },
        {
            "corpus_id": "267751098",
            "title": "FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence",
            "text": "Prior work has also shown that LLMs themselves can be good evaluators for factuality in summarization (Luo et al., 2023;Wang et al., 2023;Tang et al., 2023a;Tian et al., 2023). FACTPICO judgments are finer-grained. Therefore, we prompt LLMs with instructions emulating the questions asked of human evaluators in FACTPICO. In addition to ratings, the LLMs also generate rationales for their scores, which we analyze in Section 6. \n\nThe implementation details and the prompts used for this task are in Appendix H. Prior work found that LLM evaluations may be biased in that a sys-tem may 'prefer' its own outputs (Liu et al., 2023), but we did not observe this here (Table 5). Full-Text Evaluation. As input, we provide LLMs the full-text of a complex medical abstract and corresponding LLM-generated plain language summary. We instruct the evaluator LLM to find PICO elements in the text and evaluate them according to the provided criteria. In addition to PICO elements, we also evaluate Evidence Inference outputs; here the reference results span (annotated in prior work) is compared against the full text of the plain language summary. We evaluate four LLMs in this way: GPT-4 (OpenAI, 2023), Llama-2-Chat (7B) (Touvron et al., 2023), Alpaca (7B) (Taori et al., 2023), and Mistral (7B-Instruct-v0.1;Jiang et al. 2023). \n\nPICO-R Extraction Pipeline. We also evaluate LLM scorers explicitly informed of the PICO elements and results inferred from evidence (PICO-R). We adopt this two-stage pipeline using GPT-4. We first extract PICO-R from both the original abstract and the plain language summary. 4 For Evidence Inference, this extraction is only necessary from the summary. Next, extracted elements are passed to GPT-4 along with an evaluation prompt.",
            "score": 0.4406760545759101,
            "section_title": "LLM Evaluators",
            "char_start_offset": 18666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 138,
                    "matchedPaperCorpusId": "257378627"
                },
                {
                    "start": 138,
                    "end": 157,
                    "matchedPaperCorpusId": "249062579"
                },
                {
                    "start": 157,
                    "end": 175,
                    "matchedPaperCorpusId": "265158181"
                },
                {
                    "start": 611,
                    "end": 628,
                    "matchedPaperCorpusId": "263334039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392333984375
        },
        {
            "corpus_id": "267750439",
            "title": "How Susceptible are Large Language Models to Ideological Manipulation?",
            "text": "No, I believe that arming teachers is not the solution to school safety. Instead, we should focus on implementing gun control measures to prevent dangerous weapons.... We choose GPT-4 as the ideology evaluator for several reasons. First, the majority of responses in IDEOINST are generated by GPT-4 itself. Second, identifying political ideology within textual responses is a complex task that often demands domain-specific knowledge, making it impractical for general crowdworkers for accurate ideological assessment, nor economically feasible to recruit subject matter experts. Therefore, using GPT-4 for the task streamlines and expedites the evaluation timeframe significantly. Nevertheless, as a further quality check, we recruit three human annotators and use two LLMs (Llama-2-70B and Claude-3sonnet) to cross validate GPT-4 as a feasible ideology evaluator. We compare the predictions of GPT-4 to (1) human annotations, and (2) the predictions of two other LLMs, which can be found in Appendix B.2. The high agreement of GPT-4 to both humans and other LLMs indicates the reliability of using GPT-4 for ideology classification. \n\nBuilding upon the findings of Santurkar et al. (2023), which highlight the left-leaning bias of LMs in response to multi-choice survey questions, our study extends the examination of ideological biases to the open-ended responses of LLMs. We focus our analysis on four prominent LLMs: Llama-2-7B (Touvron et al., 2023), GPT-3.5-turbo (Ouyang et al., 2022), Alpaca-7B (Taori et al., 2023), and Mistral-7B (Jiang et al., 2023), utilizing IDEOINST to assess their outputs without ideological manipulation. The results serve as a baseline for the subsequent ideological manipulation in \u00a75.   5 Manipulating LLMs' Ideologies",
            "score": 0.4406507560271744,
            "section_title": "LLM",
            "char_start_offset": 9424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1756
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0207977294921875
        },
        {
            "corpus_id": "271946793",
            "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
            "text": "Since measuring length bias requires dividing all the data cases into two conditions: whether longer responses are preferred by humans or not, we leverage the stratified sampling to preserve the same ratio of these two conditions as in the entire dataset. \n\nOverall, both datasets used in our experiments contain 200 distinct samples for each split, which results in 1000 samples in total. The summarization and HH-RLHF-Helpfulness datasets have a stratified ratio (# of humans prefer longer responses: # of humans prefer shorter responses) of 115:85 and 111:89 respectively. \n\nLLM Judges Our LLM judges integrate a range of varied commercial large language models and prompt templates. Particularly, we assess GPT-4o, GPT-4o-mini and GPT-3.5-turbo with 8 templates on the summarization dataset and 10 templates on the HH-RLHF-Helpfulness dataset. Thus, there are 3\u00d78 = 24 LLM judges for the summarization dataset and 3\u00d710 = 30 LLM judges for the HH-RLHF-Helpfulness dataset. \n\nGPT-4o is one of the most advanced models which has the latest checkpoint on 08/06/2024, GPT-4omini is the most cost-efficient model, while GPT-3.5-turbo is from the last OpenAI model generation and serves as the baseline in our experiments. Our preliminary studies suggest that GPT-4o exhibits comparable performance to GPT-4 in judging decision-making, but at a cost that is 4 to 6 times lower. Due to limited budget, we select GPT-4o for evaluation over GPT-4 from the list of commercial LLMs, despite GPT-4 being the most widely-used model in LLM alignment studies before the release of GPT-4o. \n\nAll the considered templates were actually used in the pairwise comparison tasks to evaluate different LLM alignment algorithms by papers of year 2023 and 2024, and we make sure they all have dissimilar prompt formats.",
            "score": 0.4404728034329486,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 18615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 258,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1797
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09124755859375
        },
        {
            "corpus_id": "265281389",
            "title": "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
            "text": "NumpyEval PandasEval TorchDataEval pass@1 pass@10 pass@1 pass@10 pass@1 pass@10 Table 9. Pass rates of models on private library benchmarks in [113]. benchmark. The misuse rate refers to the proportion of misuse cases among executable cases, exec. The sample represents the proportion of executable cases among all questions, and the overall misuse percentage is the proportion of misuse cases among all questions. However, from Table 10, it is difficult to determine which LLM performs better. We can only conclude that LLMs commonly exhibit API misuse issues, even when generating code that is executable and aligns with the user's intent. Tu et al. [24] introduce and define the problem of error code completion, where given a problem statement and partially coded program with potential errors, the task is to complete the coding Table 11. Pass@1 of completion methods on buggy-HumanEval and buggy-FixEval datasets in [24]. Table 12. Pass@k with Nucleus Sampling on ClassEval in [25]. \n\nModel Class-level Method-level Pass@1 Pass@3 Pass@5 Pass@1 Pass@3 Pass@5 GPT-4 According to the experimental data provided in Table 11, CodeGen-2B consistently achieves better results across various metrics. However, it's important to note that the article only compares InCoder and CodeGen, which limits the scope of the results presented in the article. Fu et al. [27] introduce CodeApex, a benchmark focused on evaluating LLMs' programming understanding and code generation capabilities. In terms of programming understanding, GPT-3.5turbo consistently ranks first in almost all tasks, followed by InternLM-Chat-7B. The experimental conclusions for code generation are similar, with GPT-3.5-turbo performing the best. Additionally, WizardCoder-15B also achieves very good results. It's worth noting that the article's evaluation of LLMs is limited, and there is relatively little work that has tested LLMs using CodeApex.",
            "score": 0.44043097156308364,
            "section_title": "Model",
            "char_start_offset": 73701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1915
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005619049072265625
        },
        {
            "corpus_id": "265281389",
            "title": "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
            "text": "However, CodeT5Mix was not directly compared to general-purpose LLMs in these metrics. In some metrics, the smallerscale CodeT5Mix outperformed larger models like LaMDA and GPT-2. Unfortunately, the article did not provide a comparison between CodeT5Mix and GPT-3.5/GPT-4. On the other hand, WizardCoder proposed by Luo et al. [57] was compared to GPT-3.5 and GPT-4 on HumanEval (pass@1(%)) and MBPP. The experimental results showed that WizardCoder outperformed GPT-3.5 but was inferior to GPT-4. However, it still performed significantly better than larger models like PaLM-540B and PaLM-Coder-540B. \n\nLi et al. [52] also demonstrated impressive performance in leapfrogging, performing well on Common Sense Reasoning Benchmarks and surpassing general LLMs like Llama2-7B. However, it was outperformed by general LLMs on Language Understanding and Knowledge Benchmarks. The article presents results that make it difficult to assess the superiority of Code LLMs over general LLMs. It challenges the popular notion that the ability of LLMs is solely determined by their scale, indicating that data quality plays a more important role than previously believed. The article further demonstrates the feasibility of achieving high-level functionality in smaller LLMs. Perhaps the results presented in the article are far more important than assessing the performance of Code LLMs compared to general LLMs. \n\nSome recent Code LLMs have shown better performance. OpenCoderPlus [96] claims to achieve 102.5% of ChatGPT score on Vicuna GPT-4 evaluation and a 78.7 win rate on AlpacaEval. CodeFuse-CodeLlama-34B [62] also outperformed GPT-4 in HumanEval (Pass@1). Additionally, MFTCoder [62] published the performance of WizardCoder-Python-34B-V1.0 in HumanEval (Pass@1), which is slightly better than GPT-4. CodeGeeX2-6B [93] also surpassed LLaMA2-70B in HumanEval (Pass@1).",
            "score": 0.4402614905108313,
            "section_title": "DO CODE LLMS REALLY PERFORM BETTER THAN GENERAL LLMS IN SOFTWARE ENGINEERING TASKS?",
            "char_start_offset": 62548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1865
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0150146484375
        },
        {
            "corpus_id": "269157574",
            "title": "Reasoning on Efficient Knowledge Paths: Knowledge Graph Guides Large Language Model for Domain Question Answering",
            "text": "We compare with different QA methods based on LLM, including GPT-3.5, MindMap and Document embedding retrieval.Table 2 presents the BERTScore and GPT-4 ranking of GenMedGPT-5k and CMCQA these two datasets.BERTScore mainly measures the semantic similarity between the generated answer and reference answer.Since the generated answer is a sentence rather than an entity, its length may affect the final result.It can be seen from Table 2 that the BERTScore of these methods are relatively close.Since the final generated results are dependent on the LLM, there will be some similar template words in the answers.To better evaluate the generated answers, we use GPT-4 average Ranking and key entities average match accuracy.Specifically, we leveraged GPT-4 to generate an accuracy ranking of the answers for the four methods and then calculated the average Ranking.In the GenMedGPT-5k dataset, The GPT-4 average Ranking of RoK shows a significant improvement compared to the vanilla baseline GPT-3.5.Compared with other retrieval enhancement methods the GPT-3.5 had the worst answer accuracy.As can be seen in , most of the medical knowledge in the GPT 3.5 responses was correct, but the knowledge was not well established logically thus leading to a final incorrect answer.This demonstrates that LLMs may have a sufficiently rich corpus of domain knowledge and they can memorize the knowledge well, but they do not sufficiently learn the logical relations between the knowledge.\n\nMindMap, also belonging to retrieval enhancement methods, performs much better than embedded retrieval.This demonstrates that compared to using large sections of documents as prompts, structured reasoning paths as prompts can better stimulate the understanding and reasoning abilities of LLMs.Since using GPT-4 as a discriminator is not the most objective evaluation method, we introduce a new metric approach.As we can see in Figure 6 and Figure 7, the accuracy of the answer mainly depends on the key entities.Therefore, we also measured the key entity matching accuracy as shown in Table 3.It can be seen from Table 3, RoK achieved the best performance.By further analyzing the results, we find that the obtained reasoning path contains most of the correct entities.",
            "score": 0.44024549396595447,
            "section_title": "Comparison to Other Methods",
            "char_start_offset": 21704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 493
                },
                {
                    "start": 493,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 721
                },
                {
                    "start": 721,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 997
                },
                {
                    "start": 997,
                    "end": 1089
                },
                {
                    "start": 1089,
                    "end": 1271
                },
                {
                    "start": 1271,
                    "end": 1476
                },
                {
                    "start": 1478,
                    "end": 1581
                },
                {
                    "start": 1581,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1888
                },
                {
                    "start": 1888,
                    "end": 1990
                },
                {
                    "start": 1990,
                    "end": 2071
                },
                {
                    "start": 2071,
                    "end": 2134
                },
                {
                    "start": 2134,
                    "end": 2247
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.086181640625
        },
        {
            "corpus_id": "268032681",
            "title": "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",
            "text": "To assess the quality of the analysis constructed by GPT-4, we sample 200 data points from our training set and manually check for reasonability, label consistency, unsafe type consistency, controversial type consistency, and rule consistency in each analysis. Reasonability refers to whether the analysis provides a reasonable explanation for why the LLM's response is deemed safe, unsafe or controversial. Consistency refers to whether the information involved in the analysis conforms to that in human annotation. According to the statistics, the reasonability, label consistency, unsafe type consistency, controversial type consistency and rule consistency score are 94.5%, 98.0%, 97.5%, 92.5%, and 98.8%, respectively. This result demonstrates that the analysis constructed by GPT-4 is of pre-mium quality as training data.",
            "score": 0.4402173171601634,
            "section_title": "D Quality Evaluation of the Analysis Constructed by GPT-4",
            "char_start_offset": 29114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 828
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29931640625
        },
        {
            "corpus_id": "258558106",
            "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
            "text": "Then we translate the responses given by X-LLM into English for comparison with GPT-4. GPT-4 makes reference predictions based on the question, ground truth bounding boxes, and captions, marking the upper limit. After obtaining the responses from the two models, we provide the question, visual information (in the form of captions and bounding boxes), and generated responses from both assistants to ChatGPT. ChatGPT evaluates the usefulness, Each image is associated one short question, one detailed question, and one complex reasoning question, resulting in a total of 90 questions. We prompt ChatGPT with the answers from our model outputs and the answers by GPT-4 (text-only), and let it compare between both responses and give a rating with an explanation. \"w/ 4M\" denotes that we train the image interface only using 4M image-text pairs. \"w/ 4M no init\" denotes that we train the image interface only using 4M image-text pairs and without using the parameters of pretrained BLIP2. \n\nrelevance, accuracy, and level of detail of the assistants' responses and gives an overall score from 1 to 10, with higher scores indicating better overall performance. ChatGPT is also required to provide a comprehensive evaluation explanation for a better understanding of the model. LLaVA used GPT-4 as a teacher to evaluate the quality of the responses generated by LLaVA and GPT-4, while we believe that using a non-GPT-4 evaluation model (i.e. using ChatGPT) will be more objective (Also because we do not have GPT-4 API.). Examples of test questions can be found in Appendix A.2. \n\nWe show the results in Table 1. Although different evaluation models are used (LLaVA uses GPT-4, X-LLM uses ChatGPT), we are able to make rough comparisons. The results show that X-LLM yields a performance of 84.5% nearly GPT-4. X-LLM outperforms LLaVA in terms of conversation and detail description but is inferior in complex reasoning. There are two reasons for this.",
            "score": 0.4401703762258019,
            "section_title": "Multimodal Chat",
            "char_start_offset": 22171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 1948
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03436279296875
        },
        {
            "corpus_id": "273234268",
            "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
            "text": "HolisticBias [36] Toxicity Score (\u2191) \n\n, where f \u03b8 is the Perspective API. Fairness implications vary across task scenarios and model quality. Fig. 2 and first two sub-figures in Fig. 3 also reveal that fairness degradation patterns differ between LLMs, even within the same task. For instance, GPT series LLMs outperform Llama series LLMs in the generation task (Holistic). However, in the classification task (PISA) and the question-answering task (BBQ), Llama series LLMs exhibit superior fairness across all unfairness rates. This is unexpected, given that GPT series LLMs are typically regarded as more advanced, with better alignment to trustworthiness. To explore this further, we analyzed the accuracy results, as shown in last two sub-figures in Fig. 3. The findings reveal that Llama models perform significantly worse in terms of accuracy compared to GPT series LLMs. On BBQ, Llama series LLMs achieve less than 50% accuracy, performing not much better than random guessing. This suggests that the apparent fairness advantage in Llama series LLMs might stem from their inability to properly understand the questions, leading to random responses rather than informed, fairness-aware decision. Moreover, as shown in Fig. 4, Llama series LLMs are notably more cautious than GPT series LLMs, often refusing to answer a higher proportion of questions. For instance, Llama7B refuses to answer 10% of questions, even without using RAG. We believe this hypercautious behaviour contributes to their perceived fairness, as refusing to answer reduces the chances of generating unfair or biased content. However, this also comes at the cost of user experience. Considering accuracy, response rate, and fairness, we recommend using GPT series LLMs in practice, as they strike a better balance across these metrics. \n\nSensitivity to different bias categories. The BBQ dataset, which includes samples from various bias categories, allows us to examine fairness performance across these different categories. Specifically, we compare the fairness degradation of GPT series LLMs on BBQ, contrasting the No-RAG baseline with RAG-based LLMs that utilize unfair data (unfairness rate of 1.0) as shown in Fig. 5.",
            "score": 0.44015761715424806,
            "section_title": "Generation",
            "char_start_offset": 21125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 39,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1812
                },
                {
                    "start": 1815,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2202
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036346435546875
        },
        {
            "corpus_id": "268820220",
            "title": "Large Language Model Evaluation Via Multi AI Agents: Preliminary results",
            "text": "In comparison, other models like GPT-4 and GPT-4 Turbo showed varying levels of performance, but none matched the consistency of GPT-3.5 Turbo.\n\nThis evaluation highlights the effectiveness of GPT-3.5 Turbo in code generation tasks, particularly in its ability to understand and translate complex task descriptions into functional code.The use of HumanEval benchmark and the 'pass@k' metric provided a robust framework for this assessment, allowing for a fair and comprehensive comparison across different LLMs.",
            "score": 0.44000756322586754,
            "section_title": "INITIAL EVALUATION FRAMEWORK (RQ2)",
            "char_start_offset": 29058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 145,
                    "end": 336
                },
                {
                    "start": 336,
                    "end": 511
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0182342529296875
        },
        {
            "paperId": "5f730b597ffa937c31abcef49347f102a41df402",
            "corpusId": 274306693,
            "title": "TMU-HIT's Submission for the WMT24 Quality Estimation Shared Task: Is GPT-4 a Good Evaluator for Machine Translation?",
            "venue": "Conference on Machine Translation",
            "year": 2024,
            "referenceCount": 16,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.wmt-1.38?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.wmt-1.38, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308481664",
                    "name": "Ayako Sato"
                },
                {
                    "authorId": "2308475290",
                    "name": "Kyotaro Nakajima"
                },
                {
                    "authorId": "1768158656",
                    "name": "Hwichan Kim"
                },
                {
                    "authorId": "2144174417",
                    "name": "Zhousi Chen"
                },
                {
                    "authorId": "2253596383",
                    "name": "Mamoru Komachi"
                },
                {
                    "authorId": "2290402489",
                    "name": "Hugo Touvron"
                },
                {
                    "authorId": "2249724552",
                    "name": "Louis Martin"
                },
                {
                    "authorId": "2282542714",
                    "name": "Kevin R. Stone"
                },
                {
                    "authorId": "2282542430",
                    "name": "Peter Al-bert"
                },
                {
                    "authorId": "2634674",
                    "name": "Amjad Almahairi"
                },
                {
                    "authorId": "2223764353",
                    "name": "Yasmine Babaei"
                },
                {
                    "authorId": "2223756247",
                    "name": "Niko-lay Bashlykov"
                },
                {
                    "authorId": "47505161",
                    "name": "Soumya Batra"
                },
                {
                    "authorId": "51229603",
                    "name": "Prajjwal Bhargava"
                },
                {
                    "authorId": "2116473",
                    "name": "Shruti Bhosale"
                },
                {
                    "authorId": "2265492068",
                    "name": "Daniel M. Bikel"
                },
                {
                    "authorId": "2040305955",
                    "name": "Lukas Blecher"
                },
                {
                    "authorId": "66286536",
                    "name": "Cris-tian Cant\u00f3n Ferrer"
                },
                {
                    "authorId": "2108267192",
                    "name": "Moya Chen"
                },
                {
                    "authorId": "7153363",
                    "name": "Guillem Cucurull"
                },
                {
                    "authorId": "71039937",
                    "name": "David Esiobu"
                },
                {
                    "authorId": "2166312768",
                    "name": "Jude Fernandes"
                },
                {
                    "authorId": "2223974989",
                    "name": "J. Fu"
                },
                {
                    "authorId": "2223742000",
                    "name": "Wenyin Fu"
                },
                {
                    "authorId": "2223748737",
                    "name": "Brian Fuller"
                },
                {
                    "authorId": "2294721061",
                    "name": "Cynthia Gao"
                },
                {
                    "authorId": "28554843",
                    "name": "Vedanuj Goswami"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "2325255500",
                    "name": "A. Hartshorn"
                },
                {
                    "authorId": "2268759462",
                    "name": "S. Hosseini"
                },
                {
                    "authorId": "2266467782",
                    "name": "Rui Hou"
                },
                {
                    "authorId": "2065277797",
                    "name": "Hakan Inan"
                },
                {
                    "authorId": "2059886128",
                    "name": "Marcin Kardas"
                },
                {
                    "authorId": "2190957318",
                    "name": "Viktor Kerkez"
                },
                {
                    "authorId": "2072010",
                    "name": "Madian Khabsa"
                },
                {
                    "authorId": "2207049",
                    "name": "Isabel M. Kloumann"
                },
                {
                    "authorId": "2282542614",
                    "name": "Punit Artem Korenev"
                },
                {
                    "authorId": "2257006866",
                    "name": "Singh Koura"
                },
                {
                    "authorId": "114952298",
                    "name": "M. Lachaux"
                },
                {
                    "authorId": "46183616",
                    "name": "Thibaut Lavril"
                },
                {
                    "authorId": "2223749565",
                    "name": "Jenya Lee"
                },
                {
                    "authorId": "2145259939",
                    "name": "Diana Liskovich"
                },
                {
                    "authorId": "2282552151",
                    "name": "Yinghai Lu"
                },
                {
                    "authorId": "2272672481",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "2282542625",
                    "name": "Xavier Mar-tinet"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "2282543499",
                    "name": "Pushkar Mishra"
                },
                {
                    "authorId": "2282542598",
                    "name": "Igor Moly-bog"
                },
                {
                    "authorId": "2282534534",
                    "name": "Yixin Nie"
                },
                {
                    "authorId": "2282542314",
                    "name": "Andrew Poulton"
                },
                {
                    "authorId": "2282542912",
                    "name": "Jeremy Reizen-stein"
                },
                {
                    "authorId": "150282885",
                    "name": "Rashi Rungta"
                },
                {
                    "authorId": "1859294",
                    "name": "Kalyan Saladi"
                }
            ],
            "abstract": "In machine translation quality estimation (QE), translation quality is evaluated automatically without the need for reference translations. This paper describes our contribution to the sentence-level subtask of Task 1 at the Ninth Machine Translation Conference (WMT24), which predicts quality scores for neural MT outputs without reference translations. We fine-tune GPT-4o mini, a large-scale language model (LLM), with limited data for QE. We report results for the direct assessment (DA) method for four language pairs: English-Gujarati (En-Gu), English-Hindi (En-Hi), English-Tamil (En-Ta), and English-Telugu (En-Te). Experiments under zero-shot, few-shot prompting, and fine-tuning settings revealed significantly low performance in the zero-shot, while fine-tuning achieved accuracy comparable to last year\u2019s best scores. Our sys-tem demonstrated the effectiveness of this approach in low-resource language QE, securing 1st place in both En-Gu and En-Hi, and 4th place in En-Ta and En-Te. The code used in our experiments is available at the following URL 1 .",
            "corpus_id": "274306693",
            "text": "In machine translation quality estimation (QE), translation quality is evaluated automatically without the need for reference translations. This paper describes our contribution to the sentence-level subtask of Task 1 at the Ninth Machine Translation Conference (WMT24), which predicts quality scores for neural MT outputs without reference translations. We fine-tune GPT-4o mini, a large-scale language model (LLM), with limited data for QE. We report results for the direct assessment (DA) method for four language pairs: English-Gujarati (En-Gu), English-Hindi (En-Hi), English-Tamil (En-Ta), and English-Telugu (En-Te). Experiments under zero-shot, few-shot prompting, and fine-tuning settings revealed significantly low performance in the zero-shot, while fine-tuning achieved accuracy comparable to last year\u2019s best scores. Our sys-tem demonstrated the effectiveness of this approach in low-resource language QE, securing 1st place in both En-Gu and En-Hi, and 4th place in En-Ta and En-Te. The code used in our experiments is available at the following URL 1 .",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0869140625
        },
        {
            "paperId": "281af3686dda55dfe5d3c732c55f36640317b13b",
            "corpusId": 272551696,
            "title": "An Empirical Evaluation of the GPT-4 Multimodal Language Model on Visualization Literacy Tasks",
            "venue": "IEEE Transactions on Visualization and Computer Graphics",
            "year": 2024,
            "referenceCount": 81,
            "citationCount": 24,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TVCG.2024.3456155?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TVCG.2024.3456155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2320487918",
                    "name": "Alexander Bendeck"
                },
                {
                    "authorId": "2263801606",
                    "name": "John Stasko"
                }
            ],
            "abstract": "Large Language Models (LLMs) like GPT-4 which support multimodal input (i.e., prompts containing images in addition to text) have immense potential to advance visualization research. However, many questions exist about the visual capabilities of such models, including how well they can read and interpret visually represented data. In our work, we address this question by evaluating the GPT-4 multimodal LLM using a suite of task sets meant to assess the model's visualization literacy. The task sets are based on existing work in the visualization community addressing both automated chart question answering and human visualization literacy across multiple settings. Our assessment finds that GPT-4 can perform tasks such as recognizing trends and extreme values, and also demonstrates some understanding of visualization design best-practices. By contrast, GPT-4 struggles with simple value retrieval when not provided with the original dataset, lacks the ability to reliably distinguish between colors in charts, and occasionally suffers from hallucination and inconsistency. We conclude by reflecting on the model's strengths and weaknesses as well as the potential utility of models like GPT-4 for future visualization research. We also release all code, stimuli, and results for the task sets at the following link: https://doi.org/10.17605/OSF.IO/F39J6",
            "corpus_id": "272551696",
            "text": "Large Language Models (LLMs) like GPT-4 which support multimodal input (i.e., prompts containing images in addition to text) have immense potential to advance visualization research. However, many questions exist about the visual capabilities of such models, including how well they can read and interpret visually represented data. In our work, we address this question by evaluating the GPT-4 multimodal LLM using a suite of task sets meant to assess the model's visualization literacy. The task sets are based on existing work in the visualization community addressing both automated chart question answering and human visualization literacy across multiple settings. Our assessment finds that GPT-4 can perform tasks such as recognizing trends and extreme values, and also demonstrates some understanding of visualization design best-practices. By contrast, GPT-4 struggles with simple value retrieval when not provided with the original dataset, lacks the ability to reliably distinguish between colors in charts, and occasionally suffers from hallucination and inconsistency. We conclude by reflecting on the model's strengths and weaknesses as well as the potential utility of models like GPT-4 for future visualization research. We also release all code, stimuli, and results for the task sets at the following link: https://doi.org/10.17605/OSF.IO/F39J6",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0239105224609375
        },
        {
            "paperId": "efefe39ab81f821a864a74c6f7f7ae0c7c482539",
            "corpusId": 270369813,
            "title": "Docimological Quality Analysis of LLM-Generated Multiple Choice Questions in Computer Science and Medicine",
            "venue": "SN Computer Science",
            "year": 2024,
            "referenceCount": 5,
            "citationCount": 9,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s42979-024-02963-6.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s42979-024-02963-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s42979-024-02963-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2285162092",
                    "name": "Christian Gr\u00e9visse"
                },
                {
                    "authorId": "2305593254",
                    "name": "M. A. S. Pavlou"
                },
                {
                    "authorId": "2305624482",
                    "name": "Jochen G Schneider"
                }
            ],
            "abstract": "Assessment is an essential part of education, both for teachers who assess their students as well as learners who may evaluate themselves. Multiple-choice questions (MCQ) are one of the most popular types of knowledge assessment, e.g., in medical education, as they can be automatically graded and can cover a wide range of learning items. However, the creation of high-quality MCQ items is a time-consuming task. The recent advent of Large Language Models (LLM), such as Generative Pre-trained Transformer (GPT), caused a new momentum for automatic question generation solutions. Still, evaluating generated questions according to the best practices for MCQ item writing is needed to ensure docimological quality. In this article, we propose an analysis of the quality of LLM-generated MCQs. We employ zero-shot approaches in two domains, namely computer science and medicine. In the former, we make use of 3 GPT-based services to generate MCQs. In the latter, we developed a plugin for the Moodle learning management system that generates MCQs based on learning material. We compare the generated MCQs against common multiple-choice item writing guidelines. Among the major challenges, we determined that while LLMs are certainly useful in generating MCQs more efficiently, they sometimes create broad items with ambiguous keys or implausible distractors. Human oversight is also necessary to ensure instructional alignment between generated items and course contents. Finally, we propose solutions for AQG developers.",
            "corpus_id": "270369813",
            "text": "Assessment is an essential part of education, both for teachers who assess their students as well as learners who may evaluate themselves. Multiple-choice questions (MCQ) are one of the most popular types of knowledge assessment, e.g., in medical education, as they can be automatically graded and can cover a wide range of learning items. However, the creation of high-quality MCQ items is a time-consuming task. The recent advent of Large Language Models (LLM), such as Generative Pre-trained Transformer (GPT), caused a new momentum for automatic question generation solutions. Still, evaluating generated questions according to the best practices for MCQ item writing is needed to ensure docimological quality. In this article, we propose an analysis of the quality of LLM-generated MCQs. We employ zero-shot approaches in two domains, namely computer science and medicine. In the former, we make use of 3 GPT-based services to generate MCQs. In the latter, we developed a plugin for the Moodle learning management system that generates MCQs based on learning material. We compare the generated MCQs against common multiple-choice item writing guidelines. Among the major challenges, we determined that while LLMs are certainly useful in generating MCQs more efficiently, they sometimes create broad items with ambiguous keys or implausible distractors. Human oversight is also necessary to ensure instructional alignment between generated items and course contents. Finally, we propose solutions for AQG developers.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.017578125
        },
        {
            "paperId": "8f038be5aab804cc08ecebe8065e3e0485356e3a",
            "corpusId": 273377417,
            "title": "Evaluating LLM-Generated Topics from Survey Responses: Identifying Challenges in Recruiting Participants through Crowdsourcing",
            "venue": "IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments",
            "year": 2024,
            "referenceCount": 19,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/VL/HCC60511.2024.00064?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/VL/HCC60511.2024.00064, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "7935911",
                    "name": "Reham Al Tamime"
                },
                {
                    "authorId": "2734912",
                    "name": "Joni O. Salminen"
                },
                {
                    "authorId": "1861541",
                    "name": "Soon-gyo Jung"
                },
                {
                    "authorId": "2238056461",
                    "name": "Bernard J. Jansen"
                }
            ],
            "abstract": "The evolution of generative artificial intelligence (AI) technologies, particularly large language models (LLMs), has lead to consequences for the field of Human-Computer Interaction (HCI) in areas such as personalization, predictive analytics, automation, and data analysis. This research aims to evaluate LLM-generated topics derived from survey responses in comparison with topics suggested by humans, particularly participants recruited through a crowdsourcing experiment. We present an evaluation results to compare LLM-generated topics with human-generated topics in terms of Quality, Usefulness, Accuracy, Interestingness, and Completeness. This involves three stages: (1) Design and Generate Topics with an LLM (OpenAI\u2019s GPT-4); (2) Crowdsourcing Human-Generated Topics; and (3) Evaluation of Human-Generated Topics and LLM-Generated Topics. However, a feasibility study with 33 crowdworkers indicated challenges in using participants for LLM evaluation, particularly in inviting humans participants to suggest topics based on open-ended survey answers. We highlight several challenges in recruiting crowdsourcing participants for generating topics from survey responses. We recommend using well-trained human experts rather than crowdsourcing to generate human baselines for LLM evaluation.",
            "corpus_id": "273377417",
            "text": "The evolution of generative artificial intelligence (AI) technologies, particularly large language models (LLMs), has lead to consequences for the field of Human-Computer Interaction (HCI) in areas such as personalization, predictive analytics, automation, and data analysis. This research aims to evaluate LLM-generated topics derived from survey responses in comparison with topics suggested by humans, particularly participants recruited through a crowdsourcing experiment. We present an evaluation results to compare LLM-generated topics with human-generated topics in terms of Quality, Usefulness, Accuracy, Interestingness, and Completeness. This involves three stages: (1) Design and Generate Topics with an LLM (OpenAI\u2019s GPT-4); (2) Crowdsourcing Human-Generated Topics; and (3) Evaluation of Human-Generated Topics and LLM-Generated Topics. However, a feasibility study with 33 crowdworkers indicated challenges in using participants for LLM evaluation, particularly in inviting humans participants to suggest topics based on open-ended survey answers. We highlight several challenges in recruiting crowdsourcing participants for generating topics from survey responses. We recommend using well-trained human experts rather than crowdsourcing to generate human baselines for LLM evaluation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.10211181640625
        },
        {
            "paperId": "91b2b47cabd800ef658b65bfe1f52b7293a740c3",
            "corpusId": 258840943,
            "title": "LLM-powered Data Augmentation for Enhanced Crosslingual Performance",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 43,
            "citationCount": 74,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.14288",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2161240241",
                    "name": "Chenxi Whitehouse"
                },
                {
                    "authorId": "143990839",
                    "name": "M. Choudhury"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                }
            ],
            "abstract": "This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.",
            "corpus_id": "258840943",
            "text": "This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.102294921875
        },
        {
            "paperId": "dc2c70379ef46ec5ce28a088e212a25e994914eb",
            "corpusId": 259991228,
            "title": "Abductive Reasoning with the GPT-4 Language Model: Case studies from criminal investigation, medical practice, scientific research",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 17,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.10250",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.10250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2617531",
                    "name": "R. Pareschi"
                }
            ],
            "abstract": "This study evaluates the GPT-4 Large Language Model's abductive reasoning in complex fields like medical diagnostics, criminology, and cosmology. Using an interactive interview format, the AI assistant demonstrated reliability in generating and selecting hypotheses. It inferred plausible medical diagnoses based on patient data and provided potential causes and explanations in criminology and cosmology. The results highlight the potential of LLMs in complex problem-solving and the need for further research to maximize their practical applications.",
            "corpus_id": "259991228",
            "text": "This study evaluates the GPT-4 Large Language Model's abductive reasoning in complex fields like medical diagnostics, criminology, and cosmology. Using an interactive interview format, the AI assistant demonstrated reliability in generating and selecting hypotheses. It inferred plausible medical diagnoses based on patient data and provided potential causes and explanations in criminology and cosmology. The results highlight the potential of LLMs in complex problem-solving and the need for further research to maximize their practical applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00948333740234375
        },
        {
            "paperId": "965a6de65b8cebef8901ed4dd4fb38995f710dd3",
            "corpusId": 272631299,
            "title": "The Implementation of Multimodal Large Language Models for Hydrological Applications: A Comparative Study of GPT-4 Vision, Gemini, LLaVa, and Multimodal-GPT",
            "venue": "Hydrology",
            "year": 2024,
            "referenceCount": 31,
            "citationCount": 17,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/hydrology11090148",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/hydrology11090148?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/hydrology11090148, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303319768",
                    "name": "L. Kadiyala"
                },
                {
                    "authorId": "2295243395",
                    "name": "O. Mermer"
                },
                {
                    "authorId": "2305101388",
                    "name": "Dinesh Jackson Samuel"
                },
                {
                    "authorId": "79541664",
                    "name": "Y. Sermet"
                },
                {
                    "authorId": "2281708559",
                    "name": "I. Demir"
                }
            ],
            "abstract": "Large Language Models (LLMs) combined with visual foundation models have demonstrated significant advancements, achieving intelligence levels comparable to human capabilities. This study analyzes the latest Multimodal LLMs (MLLMs), including Multimodal-GPT, GPT-4 Vision, Gemini, and LLaVa, with a focus on hydrological applications such as flood management, water level monitoring, agricultural water discharge, and water pollution management. We evaluated these MLLMs on hydrology-specific tasks, testing their response generation and real-time suitability in complex real-world scenarios. Prompts were designed to enhance the models\u2019 visual inference capabilities and contextual comprehension from images. Our findings reveal that GPT-4 Vision demonstrated exceptional proficiency in interpreting visual data, providing accurate assessments of flood severity and water quality. Additionally, MLLMs showed potential in various hydrological applications, including drought prediction, streamflow forecasting, groundwater management, and wetland conservation. These models can optimize water resource management by predicting rainfall, evaporation rates, and soil moisture levels, thereby promoting sustainable agricultural practices. This research provides valuable insights into the potential applications of advanced AI models in addressing complex hydrological challenges and improving real-time decision-making in water resource management",
            "corpus_id": "272631299",
            "text": "Large Language Models (LLMs) combined with visual foundation models have demonstrated significant advancements, achieving intelligence levels comparable to human capabilities. This study analyzes the latest Multimodal LLMs (MLLMs), including Multimodal-GPT, GPT-4 Vision, Gemini, and LLaVa, with a focus on hydrological applications such as flood management, water level monitoring, agricultural water discharge, and water pollution management. We evaluated these MLLMs on hydrology-specific tasks, testing their response generation and real-time suitability in complex real-world scenarios. Prompts were designed to enhance the models\u2019 visual inference capabilities and contextual comprehension from images. Our findings reveal that GPT-4 Vision demonstrated exceptional proficiency in interpreting visual data, providing accurate assessments of flood severity and water quality. Additionally, MLLMs showed potential in various hydrological applications, including drought prediction, streamflow forecasting, groundwater management, and wetland conservation. These models can optimize water resource management by predicting rainfall, evaporation rates, and soil moisture levels, thereby promoting sustainable agricultural practices. This research provides valuable insights into the potential applications of advanced AI models in addressing complex hydrological challenges and improving real-time decision-making in water resource management",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0137939453125
        },
        {
            "paperId": "faa0a869a538f22784801ab703a1beedc2e32298",
            "corpusId": 259308858,
            "title": "Harnessing LLMs in Curricular Design: Using GPT-4 to Support Authoring of Learning Objectives",
            "venue": "LLM@AIED",
            "year": 2023,
            "referenceCount": 29,
            "citationCount": 46,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.17459",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.17459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2127991914",
                    "name": "Pragnya Sridhar"
                },
                {
                    "authorId": "48431853",
                    "name": "A. Doyle"
                },
                {
                    "authorId": "2152116534",
                    "name": "Arav Agarwal"
                },
                {
                    "authorId": "2065313146",
                    "name": "Chris Bogart"
                },
                {
                    "authorId": "2313931",
                    "name": "Jarom\u00edr \u0160avelka"
                },
                {
                    "authorId": "145470743",
                    "name": "M. Sakr"
                }
            ],
            "abstract": "We evaluated the capability of a generative pre-trained transformer (GPT-4) to automatically generate high-quality learning objectives (LOs) in the context of a practically oriented university course on Artificial Intelligence. Discussions of opportunities (e.g., content generation, explanation) and risks (e.g., cheating) of this emerging technology in education have intensified, but to date there has not been a study of the models' capabilities in supporting the course design and authoring of LOs. LOs articulate the knowledge and skills learners are intended to acquire by engaging with a course. To be effective, LOs must focus on what students are intended to achieve, focus on specific cognitive processes, and be measurable. Thus, authoring high-quality LOs is a challenging and time consuming (i.e., expensive) effort. We evaluated 127 LOs that were automatically generated based on a carefully crafted prompt (detailed guidelines on high-quality LOs authoring) submitted to GPT-4 for conceptual modules and projects of an AI Practitioner course. We analyzed the generated LOs if they follow certain best practices such as beginning with action verbs from Bloom's taxonomy in regards to the level of sophistication intended. Our analysis showed that the generated LOs are sensible, properly expressed (e.g., starting with an action verb), and that they largely operate at the appropriate level of Bloom's taxonomy, respecting the different nature of the conceptual modules (lower levels) and projects (higher levels). Our results can be leveraged by instructors and curricular designers wishing to take advantage of the state-of-the-art generative models to support their curricular and course design efforts.",
            "corpus_id": "259308858",
            "text": "We evaluated the capability of a generative pre-trained transformer (GPT-4) to automatically generate high-quality learning objectives (LOs) in the context of a practically oriented university course on Artificial Intelligence. Discussions of opportunities (e.g., content generation, explanation) and risks (e.g., cheating) of this emerging technology in education have intensified, but to date there has not been a study of the models' capabilities in supporting the course design and authoring of LOs. LOs articulate the knowledge and skills learners are intended to acquire by engaging with a course. To be effective, LOs must focus on what students are intended to achieve, focus on specific cognitive processes, and be measurable. Thus, authoring high-quality LOs is a challenging and time consuming (i.e., expensive) effort. We evaluated 127 LOs that were automatically generated based on a carefully crafted prompt (detailed guidelines on high-quality LOs authoring) submitted to GPT-4 for conceptual modules and projects of an AI Practitioner course. We analyzed the generated LOs if they follow certain best practices such as beginning with action verbs from Bloom's taxonomy in regards to the level of sophistication intended. Our analysis showed that the generated LOs are sensible, properly expressed (e.g., starting with an action verb), and that they largely operate at the appropriate level of Bloom's taxonomy, respecting the different nature of the conceptual modules (lower levels) and projects (higher levels). Our results can be leveraged by instructors and curricular designers wishing to take advantage of the state-of-the-art generative models to support their curricular and course design efforts.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00905609130859375
        },
        {
            "paperId": "e71538cece90df6b710397773addaf2f85c7e666",
            "corpusId": 268349658,
            "title": "Using GPT-4 to Provide Tiered, Formative Code Feedback",
            "venue": "Technical Symposium on Computer Science Education",
            "year": 2024,
            "referenceCount": 39,
            "citationCount": 12,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3626252.3630960",
                "status": "HYBRID",
                "license": "CCBYND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3626252.3630960?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3626252.3630960, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2240931020",
                    "name": "Ha Nguyen"
                },
                {
                    "authorId": "2290828611",
                    "name": "Vicki Allan"
                }
            ],
            "abstract": "Large language models (LLMs) have shown promise in generating sensible code explanation and feedback in programming exercises. In this experience report, we discuss the process of using one of these models (OpenAI's GPT-4) to generate individualized feedback for students' Java code and pseudocode. We instructed GPT-4 to generate feedback for 113 submissions to four programming problems in an Algorithms and Data Structures class. We prompted the model with example feedback (few-shot learning) and instruction to (1) give feedback on conceptual understanding, syntax, and time complexity, and (2) suggest follow-up actions based on students' code or provide guiding questions. Overall, GPT-4 provided accurate feedback and successfully built on students' ideas in most submissions. Human evaluators (computer science instructors and tutors) rated GPT-4's hints as useful in guiding students' next steps. Model performance varied with programming problems but not submission quality. We reflect on where the model performed well and fell short, and discuss the potential of integrating LLM-generated, individualized feedback into computer science instruction.",
            "corpus_id": "268349658",
            "text": "Large language models (LLMs) have shown promise in generating sensible code explanation and feedback in programming exercises. In this experience report, we discuss the process of using one of these models (OpenAI's GPT-4) to generate individualized feedback for students' Java code and pseudocode. We instructed GPT-4 to generate feedback for 113 submissions to four programming problems in an Algorithms and Data Structures class. We prompted the model with example feedback (few-shot learning) and instruction to (1) give feedback on conceptual understanding, syntax, and time complexity, and (2) suggest follow-up actions based on students' code or provide guiding questions. Overall, GPT-4 provided accurate feedback and successfully built on students' ideas in most submissions. Human evaluators (computer science instructors and tutors) rated GPT-4's hints as useful in guiding students' next steps. Model performance varied with programming problems but not submission quality. We reflect on where the model performed well and fell short, and discuss the potential of integrating LLM-generated, individualized feedback into computer science instruction.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0225372314453125
        },
        {
            "paperId": "ec9feb7b0259e60befbb3ae1012c38bc898ce9dc",
            "corpusId": 274610256,
            "title": "Generating Knowledge Graphs from Large Language Models: A Comparative Study of GPT-4, LLaMA 2, and BERT",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 2,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.07412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2330184226",
                    "name": "Ahan Bhatt"
                },
                {
                    "authorId": "2330588162",
                    "name": "Nandan Vaghela"
                },
                {
                    "authorId": "2334569989",
                    "name": "Kush Dudhia"
                }
            ],
            "abstract": "Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a form of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks requiring structured reasoning and semantic understanding. However, creating KGs for GraphRAGs remains a significant challenge due to accuracy and scalability limitations of traditional methods. This paper introduces a novel approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and BERT to generate KGs directly from unstructured data, bypassing traditional pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit Distance, and Semantic Similarity, we evaluate the models' ability to generate high-quality KGs. Results demonstrate that GPT-4 achieves superior semantic fidelity and structural accuracy, LLaMA 2 excels in lightweight, domain-specific graphs, and BERT provides insights into challenges in entity-relationship modeling. This study underscores the potential of LLMs to streamline KG creation and enhance GraphRAG accessibility for real-world applications, while setting a foundation for future advancements.",
            "corpus_id": "274610256",
            "text": "Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a form of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks requiring structured reasoning and semantic understanding. However, creating KGs for GraphRAGs remains a significant challenge due to accuracy and scalability limitations of traditional methods. This paper introduces a novel approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and BERT to generate KGs directly from unstructured data, bypassing traditional pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit Distance, and Semantic Similarity, we evaluate the models' ability to generate high-quality KGs. Results demonstrate that GPT-4 achieves superior semantic fidelity and structural accuracy, LLaMA 2 excels in lightweight, domain-specific graphs, and BERT provides insights into challenges in entity-relationship modeling. This study underscores the potential of LLMs to streamline KG creation and enhance GraphRAG accessibility for real-world applications, while setting a foundation for future advancements.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.03839111328125
        },
        {
            "paperId": "ea0d5ac5e9e6c0f855311db9da5607d02dbb397c",
            "corpusId": 273654696,
            "title": "Architectural Flaw Detection in Civil Engineering Using GPT-4",
            "venue": "Ubiquitous Computing, Electronics & Mobile Communication Conference",
            "year": 2024,
            "referenceCount": 11,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2410.20036",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.20036, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310654424",
                    "name": "Saket Kumar"
                },
                {
                    "authorId": "2284079039",
                    "name": "Abul Ehtesham"
                },
                {
                    "authorId": "2109435730",
                    "name": "Aditi Singh"
                },
                {
                    "authorId": "2418738",
                    "name": "T. T. Khoei"
                }
            ],
            "abstract": "The application of artificial intelligence (AI) in civil engineering presents a transformative approach to enhancing design quality and safety. This paper investigates the potential of the advanced LLM GPT4 Turbo vision model in detecting architectural flaws during the design phase, with a specific focus on identifying missing doors and windows. The study evaluates the model's performance through metrics such as precision, recall, and F 1 score, demonstrating AI's effectiveness in accurately detecting flaws compared to humanverified data. Additionally, the research explores AI's broader capabilities, including identifying load-bearing issues, material weaknesses, and ensuring compliance with building codes. The findings highlight how AI can significantly improve design accuracy, reduce costly revisions, and support sustainable practices, ultimately revolutionizing the civil engineering field by ensuring safer, more efficient, and aesthetically optimized structures.",
            "corpus_id": "273654696",
            "text": "The application of artificial intelligence (AI) in civil engineering presents a transformative approach to enhancing design quality and safety. This paper investigates the potential of the advanced LLM GPT4 Turbo vision model in detecting architectural flaws during the design phase, with a specific focus on identifying missing doors and windows. The study evaluates the model's performance through metrics such as precision, recall, and F 1 score, demonstrating AI's effectiveness in accurately detecting flaws compared to humanverified data. Additionally, the research explores AI's broader capabilities, including identifying load-bearing issues, material weaknesses, and ensuring compliance with building codes. The findings highlight how AI can significantly improve design accuracy, reduce costly revisions, and support sustainable practices, ultimately revolutionizing the civil engineering field by ensuring safer, more efficient, and aesthetically optimized structures.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.01971435546875
        },
        {
            "paperId": "1d83d2512bd9c7ceb8de1fa25ac7b8c4c00b5573",
            "corpusId": 268032638,
            "title": "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "referenceCount": 51,
            "citationCount": 19,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "9358635",
                    "name": "Rifki Afina Putri"
                },
                {
                    "authorId": "2287841290",
                    "name": "Faiz Ghifari Haznitrama"
                },
                {
                    "authorId": "2210201739",
                    "name": "Dea Adhista"
                },
                {
                    "authorId": "2283768962",
                    "name": "Alice Oh"
                }
            ],
            "abstract": "Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in 4.5K questions per language (9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally \u2018deep\u2019 as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.",
            "corpus_id": "268032638",
            "text": "Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in 4.5K questions per language (9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally \u2018deep\u2019 as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.05706787109375
        },
        {
            "paperId": "5dc4f7effff9ec37bfa916abe9a921fd273623b8",
            "corpusId": 273375142,
            "title": "Learning to Predict Usage Options of Product Reviews with LLM-Generated Labels",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 37,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12470, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326115629",
                    "name": "Leo Kohlenberg"
                },
                {
                    "authorId": "2326115694",
                    "name": "Leonard Horns"
                },
                {
                    "authorId": "2326114291",
                    "name": "Frederic Sadrieh"
                },
                {
                    "authorId": "2326114493",
                    "name": "Nils Kiele"
                },
                {
                    "authorId": "2326113309",
                    "name": "Matthis Clausen"
                },
                {
                    "authorId": "2326115457",
                    "name": "Konstantin Ketterer"
                },
                {
                    "authorId": "2326113978",
                    "name": "Avetis Navasardyan"
                },
                {
                    "authorId": "2326115617",
                    "name": "Tamara Czinczoll"
                },
                {
                    "authorId": "144608002",
                    "name": "Gerard de Melo"
                },
                {
                    "authorId": "2326114271",
                    "name": "Ralf Herbrich"
                }
            ],
            "abstract": "Annotating large datasets can be challenging. However, crowd-sourcing is often expensive and can lack quality, especially for non-trivial tasks. We propose a method of using LLMs as few-shot learners for annotating data in a complex natural language task where we learn a standalone model to predict usage options for products from customer reviews. We also propose a new evaluation metric for this scenario, HAMS4, that can be used to compare a set of strings with multiple reference sets. Learning a custom model offers individual control over energy efficiency and privacy measures compared to using the LLM directly for the sequence-to-sequence task. We compare this data annotation approach with other traditional methods and demonstrate how LLMs can enable considerable cost savings. We find that the quality of the resulting data exceeds the level attained by third-party vendor services and that GPT-4-generated labels even reach the level of domain experts. We make the code and generated labels publicly available.",
            "corpus_id": "273375142",
            "text": "Annotating large datasets can be challenging. However, crowd-sourcing is often expensive and can lack quality, especially for non-trivial tasks. We propose a method of using LLMs as few-shot learners for annotating data in a complex natural language task where we learn a standalone model to predict usage options for products from customer reviews. We also propose a new evaluation metric for this scenario, HAMS4, that can be used to compare a set of strings with multiple reference sets. Learning a custom model offers individual control over energy efficiency and privacy measures compared to using the LLM directly for the sequence-to-sequence task. We compare this data annotation approach with other traditional methods and demonstrate how LLMs can enable considerable cost savings. We find that the quality of the resulting data exceeds the level attained by third-party vendor services and that GPT-4-generated labels even reach the level of domain experts. We make the code and generated labels publicly available.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.10577392578125
        },
        {
            "paperId": "923e48642bca55d1f4f9fa282c2db847110335b1",
            "corpusId": 272004486,
            "title": "Leveraging Generative Large Language Models for Optimizing Sales Arguments Creation: An Evaluation of GPT-4 Capabilities",
            "venue": "International Journal of Intelligent Engineering and Systems",
            "year": 2024,
            "referenceCount": 66,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.22266/ijies2024.1031.59?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.22266/ijies2024.1031.59, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319166118",
                    "name": "Mustapha Elhissoufi"
                },
                {
                    "authorId": "3330938",
                    "name": "E. Nfaoui"
                },
                {
                    "authorId": "2319165700",
                    "name": "Lhoussaine Alla"
                },
                {
                    "authorId": "2319164925",
                    "name": "Jawad Elghalfiki"
                }
            ],
            "abstract": ": A new era in Generative Artificial Intelligence has begun with the release of powerful Large Language Models (LLMs). These models have shown significant potential in marketing. Essentially, they have been redefining sales practices and changing sales funnel steps. Despite their promise and persuasive capabilities, there remains a dearth of comprehensive understanding regarding the capabilities of LLMs in generating sales arguments, which is a task traditionally characterized by high costs, time consumption, subjective biases, and the need for considerable expertise and skills. In this study, we take the first step towards exploring the ability of LLMs to generate sales arguments. To this end, we evaluated GPT-4 as one of the most capable LLMs to date in performing zero-shot sales argument creation from product features. We created a dataset containing textual descriptions of features extracted from brochures, catalogs, and technical data sheets of various products of a global company specializing in the manufacturing and retailing of furniture, appliances, and home accessories. We conducted a human evaluation with five experts, covering three main criteria of argument quality, namely coherence, persuasiveness, and relevance. The experimental results revealed the remarkable ability of GPT-4 to generate high-quality and well-structured sales arguments according to the Feature-Advantage-Benefit method. Over 98% of the evaluated arguments were coherent and persuasive. Regarding relevance, the model exhibited an accuracy of 91.53% to align the generated arguments to customers' purchase motives, namely security, vanity, novelty, comfort, money, and likability. These findings could lead to significant time and cost reductions, allowing the sales force to focus on higher-value tasks. We posit that this study heralds a novel avenue for exploring LLMs' capabilities in other steps of the sales funnel process.",
            "corpus_id": "272004486",
            "text": ": A new era in Generative Artificial Intelligence has begun with the release of powerful Large Language Models (LLMs). These models have shown significant potential in marketing. Essentially, they have been redefining sales practices and changing sales funnel steps. Despite their promise and persuasive capabilities, there remains a dearth of comprehensive understanding regarding the capabilities of LLMs in generating sales arguments, which is a task traditionally characterized by high costs, time consumption, subjective biases, and the need for considerable expertise and skills. In this study, we take the first step towards exploring the ability of LLMs to generate sales arguments. To this end, we evaluated GPT-4 as one of the most capable LLMs to date in performing zero-shot sales argument creation from product features. We created a dataset containing textual descriptions of features extracted from brochures, catalogs, and technical data sheets of various products of a global company specializing in the manufacturing and retailing of furniture, appliances, and home accessories. We conducted a human evaluation with five experts, covering three main criteria of argument quality, namely coherence, persuasiveness, and relevance. The experimental results revealed the remarkable ability of GPT-4 to generate high-quality and well-structured sales arguments according to the Feature-Advantage-Benefit method. Over 98% of the evaluated arguments were coherent and persuasive. Regarding relevance, the model exhibited an accuracy of 91.53% to align the generated arguments to customers' purchase motives, namely security, vanity, novelty, comfort, money, and likability. These findings could lead to significant time and cost reductions, allowing the sales force to focus on higher-value tasks. We posit that this study heralds a novel avenue for exploring LLMs' capabilities in other steps of the sales funnel process.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.010986328125
        },
        {
            "paperId": "b0fbc776cb4f9a09504f0720ff1e36910bdc5bc5",
            "corpusId": 279325573,
            "title": "Revisiting SWE-Bench: On the Importance of Data Quality for LLM-Based Code Models",
            "venue": "2025 IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)",
            "year": 2025,
            "referenceCount": 11,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSE-Companion66252.2025.00075?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSE-Companion66252.2025.00075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115012783",
                    "name": "Reem Aleithan"
                }
            ],
            "abstract": "The use of Large Language Models (LLMs) for code generation has emerged as a rapidly growing field, gaining substantial traction within software engineering. However, ensuring the reliability and accuracy of generated code requires robust evaluation frameworks. To address this gap, Carlos et al. introduced the SWE-bench dataset, which consists of 2,294 GitHub issues paired with their corresponding pull requests, collected from 12 prominent Python repositories. This dataset has become a key benchmark for evaluating code generation models, with resolution rates prominently featured on the SWE-bench leaderboard. Despite its widespread adoption, the dataset has yet to undergo a systematic reliability assessment. Motivated by this gap, we conducted the first empirical study aimed at evaluating the reliability of the SWE-Bench dataset to ensure it provides meaningful and realistic model evaluations. We centered our analysis on the highest-performing model reported on the leaderboard at the time of the study: SWE-Agent + GPT-4. A thorough investigation was conducted by comparing the model-generated patches with the corresponding pull requests from the dataset. Our findings revealed two key issues: (1) 32.67% of successful cases were influenced by solution leakage, and (2) 31.08% succeeded due to weak test cases. When these problematic instances were excluded, the resolution rate of SWE-Agent + GPT-4 dropped from 12.47% to 3.97%.",
            "corpus_id": "279325573",
            "text": "The use of Large Language Models (LLMs) for code generation has emerged as a rapidly growing field, gaining substantial traction within software engineering. However, ensuring the reliability and accuracy of generated code requires robust evaluation frameworks. To address this gap, Carlos et al. introduced the SWE-bench dataset, which consists of 2,294 GitHub issues paired with their corresponding pull requests, collected from 12 prominent Python repositories. This dataset has become a key benchmark for evaluating code generation models, with resolution rates prominently featured on the SWE-bench leaderboard. Despite its widespread adoption, the dataset has yet to undergo a systematic reliability assessment. Motivated by this gap, we conducted the first empirical study aimed at evaluating the reliability of the SWE-Bench dataset to ensure it provides meaningful and realistic model evaluations. We centered our analysis on the highest-performing model reported on the leaderboard at the time of the study: SWE-Agent + GPT-4. A thorough investigation was conducted by comparing the model-generated patches with the corresponding pull requests from the dataset. Our findings revealed two key issues: (1) 32.67% of successful cases were influenced by solution leakage, and (2) 31.08% succeeded due to weak test cases. When these problematic instances were excluded, the resolution rate of SWE-Agent + GPT-4 dropped from 12.47% to 3.97%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.174560546875
        },
        {
            "paperId": "e20265f636e7787c0379bb7abdb86562adb104c6",
            "corpusId": 276611183,
            "title": "Quality assurance and validity of AI-generated single best answer questions",
            "venue": "BMC Medical Education",
            "year": 2025,
            "referenceCount": 31,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1186/s12909-025-06881-w",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11854382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323905790",
                    "name": "Ayla Ahmed"
                },
                {
                    "authorId": "2347282695",
                    "name": "Ellen Kerr"
                },
                {
                    "authorId": "2323819626",
                    "name": "Andrew O'Malley"
                }
            ],
            "abstract": "Background Recent advancements in generative artificial intelligence (AI) have opened new avenues in educational methodologies, particularly in medical education. This study seeks to assess whether generative AI might be useful in addressing the depletion of assessment question banks, a challenge intensified during the Covid-era due to the prevalence of open-book examinations, and to augment the pool of formative assessment opportunities available to students. While many recent publications have sought to ascertain whether AI can achieve a passing standard in existing examinations, this study investigates the potential for AI to generate the exam itself. Summary of work This research utilized a commercially available AI large language model (LLM), OpenAI GPT-4, to generate 220 single best answer (SBA) questions, adhering to Medical Schools Council Assessment Alliance guidelines the and a selection of Learning Outcomes (LOs) of the Scottish Graduate-Entry Medicine (ScotGEM) program. All questions were assessed by an expert panel for accuracy and quality. A total of 50 AI-generated and 50 human-authored questions were used to create two 50-item formative SBA examinations for Year 1 and Year 2 ScotGEM students. Each exam, delivered via the Speedwell eSystem, comprised 25 AI-generated and 25 human-authored questions presented in random order. Students completed the online, closed-book exams on personal devices under exam conditions that reflected summative examinations. The performance of both AI-generated and human-authored questions was evaluated, focusing on facility and discrimination index as key metrics. Summary of results The screening process revealed that 69% of AI-generated SBAs were fit for inclusion in the examinations with little or no modifications required. Modifications, when necessary, were predominantly due to reasons such as the inclusion of \"all of the above\" options, usage of American English spellings, and non-alphabetized answer choices. 31% of questions were rejected for inclusion in the examinations, due to factual inaccuracies and non-alignment with students\u2019 learning. When included in an examination, post hoc statistical analysis indicated no significant difference in performance between the AI- and human- authored questions in terms of facility and discrimination index. Discussion and conclusion The outcomes of this study suggest that AI LLMs can generate SBA questions that are in line with best-practice guidelines and specific LOs. However, a robust quality assurance process is necessary to ensure that erroneous questions are identified and rejected. The insights gained from this research provide a foundation for further investigation into refining AI prompts, aiming for a more reliable generation of curriculum-aligned questions. LLMs show significant potential in supplementing traditional methods of question generation in medical education. This approach offers a viable solution to rapidly replenish and diversify assessment resources in medical curricula, marking a step forward in the intersection of AI and education.",
            "corpus_id": "276611183",
            "text": "Background Recent advancements in generative artificial intelligence (AI) have opened new avenues in educational methodologies, particularly in medical education. This study seeks to assess whether generative AI might be useful in addressing the depletion of assessment question banks, a challenge intensified during the Covid-era due to the prevalence of open-book examinations, and to augment the pool of formative assessment opportunities available to students. While many recent publications have sought to ascertain whether AI can achieve a passing standard in existing examinations, this study investigates the potential for AI to generate the exam itself. Summary of work This research utilized a commercially available AI large language model (LLM), OpenAI GPT-4, to generate 220 single best answer (SBA) questions, adhering to Medical Schools Council Assessment Alliance guidelines the and a selection of Learning Outcomes (LOs) of the Scottish Graduate-Entry Medicine (ScotGEM) program. All questions were assessed by an expert panel for accuracy and quality. A total of 50 AI-generated and 50 human-authored questions were used to create two 50-item formative SBA examinations for Year 1 and Year 2 ScotGEM students. Each exam, delivered via the Speedwell eSystem, comprised 25 AI-generated and 25 human-authored questions presented in random order. Students completed the online, closed-book exams on personal devices under exam conditions that reflected summative examinations. The performance of both AI-generated and human-authored questions was evaluated, focusing on facility and discrimination index as key metrics. Summary of results The screening process revealed that 69% of AI-generated SBAs were fit for inclusion in the examinations with little or no modifications required. Modifications, when necessary, were predominantly due to reasons such as the inclusion of \"all of the above\" options, usage of American English spellings, and non-alphabetized answer choices. 31% of questions were rejected for inclusion in the examinations, due to factual inaccuracies and non-alignment with students\u2019 learning. When included in an examination, post hoc statistical analysis indicated no significant difference in performance between the AI- and human- authored questions in terms of facility and discrimination index. Discussion and conclusion The outcomes of this study suggest that AI LLMs can generate SBA questions that are in line with best-practice guidelines and specific LOs. However, a robust quality assurance process is necessary to ensure that erroneous questions are identified and rejected. The insights gained from this research provide a foundation for further investigation into refining AI prompts, aiming for a more reliable generation of curriculum-aligned questions. LLMs show significant potential in supplementing traditional methods of question generation in medical education. This approach offers a viable solution to rapidly replenish and diversify assessment resources in medical curricula, marking a step forward in the intersection of AI and education.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.018157958984375
        }
    ],
    "quotes": {
        "cost": 0.036144,
        "quotes": [
            {
                "idx": 0,
                "key": "[266690837 | Zhang et al. | 2023 | Citations: 2]",
                "snippets": "We utilized the LLMs' ability in medical questions reasoning to evaluate the performance of collected conversation. We randomly selected 100 instances from the 876 generated based on MADE and combined them with 1000 test and 1000 validation data generated from MIMIC-III for LLM evaluation (totally 2100 cases). The LLMs used for evaluation were GPT-3.5-Turbo (referred to as Turbo in this section) and GPT-4. We aggregated the criteria into an evaluation prompt Table 4 to guide the LLMs in scoring each conversation. The results are shown in the Table 2. From the figure, it is evident that Turbo received higher scores than GPT-4, particularly in the Turbo NIP Explanation task. This discrepancy can be attributed to the strict evaluation criteria outlined in our evaluation prompt. Turbo, while slightly less capable than GPT-4, did not fully meet these criteria. Additionally, we observed that data generated by GPT-4 NIP, whether in the Q&A task or the Explanation task, exhibited more stable scores across both Turbo and GPT-4 evaluations. Notably, GPT-4 received a noticeably lower score in the Turbo NIP Explanation task when compared to Turbo.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM Evaluation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1153,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 115
                            },
                            {
                                "start": 116,
                                "end": 311
                            },
                            {
                                "start": 312,
                                "end": 359
                            },
                            {
                                "start": 360,
                                "end": 409
                            },
                            {
                                "start": 410,
                                "end": 518
                            },
                            {
                                "start": 519,
                                "end": 681
                            },
                            {
                                "start": 682,
                                "end": 785
                            },
                            {
                                "start": 786,
                                "end": 867
                            },
                            {
                                "start": 868,
                                "end": 1046
                            },
                            {
                                "start": 1047,
                                "end": 1153
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We utilized the LLMs' ability in medical questions reasoning to evaluate the performance of collected conversation. We randomly selected 100 instances from the 876 generated based on MADE and combined them with 1000 test and 1000 validation data generated from MIMIC-III for LLM evaluation (totally 2100 cases). The LLMs used for evaluation were GPT-3.5-Turbo (referred to as Turbo in this section) and GPT-4. We aggregated the criteria into an evaluation prompt Table 4 to guide the LLMs in scoring each conversation. The results are shown in the Table 2. From the figure, it is evident that Turbo received higher scores than GPT-4, particularly in the Turbo NIP Explanation task. This discrepancy can be attributed to the strict evaluation criteria outlined in our evaluation prompt. Turbo, while slightly less capable than GPT-4, did not fully meet these criteria. Additionally, we observed that data generated by GPT-4 NIP, whether in the Q&A task or the Explanation task, exhibited more stable scores across both Turbo and GPT-4 evaluations. Notably, GPT-4 received a noticeably lower score in the Turbo NIP Explanation task when compared to Turbo."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[268032876 | Ghaisas et al. | 2024 | Citations: 0]",
                "snippets": "Beyond utilizing domain experts to validate generated data, practitioners have also begun deploying LLMs themselves for evaluation purposes. Two predominant strategies for using LLMs as evaluators are as follows: \n\n-Self-Reflection [24]: Following the completion of a task by an LLM, the model is prompted to reconsider its generated output and identify any potential shortcomings. This method has demonstrated an enhancement in accuracy. \n\n-Engaging an Oracle-LLM for Evaluation [15]: This strategy involves the use of an auxiliary, typically more powerful, LLM for evaluation. For instance, the authors of Vicuna deployed GPT-4 as an 'oracle' LLM to compare and rank the outputs of various LLMs, including LLAMA, Alpaca, and Chat-GPT, thereby ascertaining the relative differences in generation quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Validation of SSNLPCore Output",
                        "pdf_hash": "",
                        "start": 1420,
                        "end": 2225,
                        "sentence_offsets": [
                            {
                                "start": 1420,
                                "end": 1560
                            },
                            {
                                "start": 1561,
                                "end": 1632
                            },
                            {
                                "start": 1635,
                                "end": 1801
                            },
                            {
                                "start": 1802,
                                "end": 1858
                            },
                            {
                                "start": 1861,
                                "end": 1998
                            },
                            {
                                "start": 1999,
                                "end": 2225
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Beyond utilizing domain experts to validate generated data, practitioners have also begun deploying LLMs themselves for evaluation purposes. Two predominant strategies for using LLMs as evaluators are as follows: \n\n-Self-Reflection [24]: Following the completion of a task by an LLM, the model is prompted to reconsider its generated output and identify any potential shortcomings. This method has demonstrated an enhancement in accuracy. \n\n-Engaging an Oracle-LLM for Evaluation [15]: This strategy involves the use of an auxiliary, typically more powerful, LLM for evaluation. For instance, the authors of Vicuna deployed GPT-4 as an 'oracle' LLM to compare and rank the outputs of various LLMs, including LLAMA, Alpaca, and Chat-GPT, thereby ascertaining the relative differences in generation quality."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[270391675 | Li et al. | 2024 | Citations: 15]",
                "snippets": "LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;(Ouyang et al., 2022) to ensure more balanced and comprehensive assessments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215548699 | Sellam et al. | 2020 | Citations: 1505]": "Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.",
                    "[246426909 | Ouyang et al. | 2022 | Citations: 13203]": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                },
                "metadata": [
                    {
                        "section_title": "Challenges and Open Problems",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1185,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 169
                            },
                            {
                                "start": 169,
                                "end": 372
                            },
                            {
                                "start": 372,
                                "end": 543
                            },
                            {
                                "start": 543,
                                "end": 692
                            },
                            {
                                "start": 692,
                                "end": 877
                            },
                            {
                                "start": 877,
                                "end": 1185
                            }
                        ],
                        "ref_mentions": [
                            "215548699",
                            "246426909"
                        ],
                        "quote": "LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;(Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[270738200 | Ke et al. | 2023 | Citations: 34]",
                "snippets": "Since the NLP community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Since the NLP community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting.",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.015486,
        "cot": "Looking at the user query, they are asking about best practices for using LLMs (specifically mentioning GPT-4) to evaluate the quality of data generated by GPT-4. They're wondering whether to use GPT-4 itself as the evaluator or a different LLM.\n\nFor this query, I'll organize the information into these dimensions:\n\n1. \"Background on LLMs as Evaluators\" - This will be a synthesis section providing context about how LLMs are used to evaluate generated content. This serves as an introduction.\n\n2. \"Approaches to LLM-based Evaluation\" - This will be a list format section outlining the different methodologies used when employing LLMs as evaluators, including self-reflection and using oracle LLMs.\n\n3. \"Using GPT-4 as an Evaluator: Benefits and Limitations\" - This will be a synthesis section that specifically addresses the query's concern about using GPT-4 to evaluate its own outputs, discussing both advantages and potential biases.\n\n4. \"Best Practices for LLM Evaluation\" - This will be a synthesis section that provides recommendations for evaluating LLM-generated data, including alternatives to using the same model as generator and evaluator.\n\nThe quotes cover various aspects of using LLMs (particularly GPT-4) as evaluators. Quote [0] discusses a specific experiment using both GPT-3.5 and GPT-4 as evaluators. Quote [1] outlines two common strategies for LLM evaluation. Quote [2] discusses potential biases when using GPT-4 to evaluate its own outputs and suggests alternative approaches. Quote [3] mentions that many works use GPT-4's direct prompting for evaluation data labeling.\n\nI'll organize these quotes into the dimensions accordingly to provide a comprehensive response.",
        "plan": {
            "Background on LLMs as Evaluators (synthesis)": [],
            "Approaches to LLM-based Evaluation (list)": [
                1
            ],
            "Using GPT-4 as an Evaluator: Benefits and Limitations (synthesis)": [
                0,
                2,
                3
            ],
            "Best Practices for LLM Evaluation (synthesis)": [
                2
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Background on LLMs as Evaluators",
                "tldr": "Large Language Models (LLMs) have emerged as powerful tools for evaluating text quality, including their own outputs. They can serve as automated judges that assess various aspects of generated content like coherence, factuality, and creativity. (LLM Memory)",
                "text": "\nLarge Language Models (LLMs) like GPT-4 are increasingly being used not only to generate content but also to evaluate the quality of text. This practice has become particularly important as organizations seek efficient ways to assess LLM-generated outputs at scale. LLMs can be prompted to act as evaluators that judge various dimensions of text quality, including factual accuracy, coherence, relevance, creativity, and adherence to instructions. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe use of LLMs as evaluators stems from their ability to understand context, detect nuances in language, and apply complex criteria when assessing text. This capability has created a new paradigm where machines can provide human-like feedback on content quality without the need for extensive human evaluation, which is often costly and time-consuming. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nLLMs can evaluate text through various methods, including direct scoring (e.g., rating on a scale of 1-10), comparative judgments (deciding which of two texts is better), or detailed qualitative feedback. Researchers have found that when properly prompted, advanced LLMs can achieve evaluation results that correlate strongly with human judgments across many tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nHowever, using LLMs to evaluate their own or other LLMs' outputs presents both opportunities and challenges. While it offers a scalable evaluation solution, concerns exist about potential biases, the \"self-rater\" problem (where models may be biased toward their own outputs), and the reliability of such evaluations for highly specialized or novel content. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Approaches to LLM-based Evaluation",
                "tldr": "LLMs can evaluate text quality through several established approaches, with self-reflection and using separate \"oracle\" LLMs being two primary strategies. (1 source)",
                "text": "\nResearchers and practitioners have developed several methods for using LLMs as evaluators of text quality:\n\n1. **Self-Reflection**: In this approach, after generating content, the same LLM is prompted to reconsider and critique its own output to identify potential shortcomings. This technique has been shown to improve accuracy by enabling the model to catch and correct its own errors. <Paper corpusId=\"268032876\" paperTitle=\"(Ghaisas et al., 2024)\" isShortName></Paper>\n\n2. **Oracle-LLM Evaluation**: This strategy employs a separate, often more powerful LLM as an external evaluator or \"oracle.\" For example, GPT-4 has been used as an oracle to compare and rank outputs from various other models like LLAMA, Alpaca, and ChatGPT, effectively measuring relative differences in generation quality. <Paper corpusId=\"268032876\" paperTitle=\"(Ghaisas et al., 2024)\" isShortName></Paper>\n\n3. **Comparative Judgments**: LLMs can be tasked with comparing two or more text samples and determining which one is superior based on specific criteria such as coherence, accuracy, or relevance. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n4. **Rubric-Based Evaluation**: Models can be provided with detailed rubrics or criteria to systematically evaluate text against specific dimensions of quality. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n5. **Numerical Scoring**: LLMs can assign numerical scores to text samples based on predefined quality metrics, creating quantifiable evaluation results. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThese approaches can be used individually or in combination to create robust evaluation frameworks that leverage the linguistic understanding capabilities of LLMs while attempting to mitigate potential biases.",
                "citations": [
                    {
                        "id": "(Ghaisas et al., 2024)",
                        "snippets": [
                            "Beyond utilizing domain experts to validate generated data, practitioners have also begun deploying LLMs themselves for evaluation purposes. Two predominant strategies for using LLMs as evaluators are as follows: \n\n-Self-Reflection [24]: Following the completion of a task by an LLM, the model is prompted to reconsider its generated output and identify any potential shortcomings. This method has demonstrated an enhancement in accuracy. \n\n-Engaging an Oracle-LLM for Evaluation [15]: This strategy involves the use of an auxiliary, typically more powerful, LLM for evaluation. For instance, the authors of Vicuna deployed GPT-4 as an 'oracle' LLM to compare and rank the outputs of various LLMs, including LLAMA, Alpaca, and Chat-GPT, thereby ascertaining the relative differences in generation quality."
                        ],
                        "paper": {
                            "corpus_id": 268032876,
                            "title": "Dealing with Data for RE: Mitigating Challenges while using NLP and Generative AI",
                            "authors": [
                                {
                                    "authorId": "2107155",
                                    "name": "S. Ghaisas"
                                },
                                {
                                    "authorId": "2120316520",
                                    "name": "Anmol Singhal"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.76708984375
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Using GPT-4 as an Evaluator: Benefits and Limitations",
                "tldr": "GPT-4 is widely used as an evaluator due to its advanced capabilities, but researchers have identified significant limitations including potential bias when evaluating its own outputs and the \"evaluator-generator\" capability paradox. (5 sources)",
                "text": "\nGPT-4 has emerged as a popular choice for text evaluation tasks due to its advanced capabilities and reasoning abilities. Researchers frequently utilize GPT-4 as an evaluator, leveraging its status as one of the most sophisticated LLMs currently available <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This preference is reflected in the growing body of research that uses GPT-4's judgments as the standard for assessing the quality of text generation, including in specialized domains like medical question answering <Paper corpusId=\"266690837\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nHowever, using GPT-4 as an evaluator presents several notable limitations. One significant concern is bias when GPT-4 evaluates outputs generated by itself or models with comparable capabilities. This creates what researchers describe as a \"chicken-and-egg dilemma\" - developing better LLMs requires robust evaluation methods, but the most capable evaluators are themselves advanced LLMs <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. GPT-4 has been observed to exhibit \"egocentric bias,\" potentially favoring its own generated responses over those from other models <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe evaluation landscape is further complicated by the challenge of impartiality. When the evaluating model (LLM-as-evaluator) has similar capabilities to the model being evaluated (LLM-as-generator), the objectivity of the assessment becomes questionable <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This has led to calls for more diverse evaluation approaches that go beyond single-model judgments.\n\nTo address these limitations, researchers recommend using a broader spectrum of evaluation methods, including various benchmarks, diverse evaluation criteria <Paper corpusId=\"215548699\" paperTitle=\"(Sellam et al., 2020)\" isShortName></Paper>, and human feedback <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>. The NLP community has also begun exploring approaches that train specialized critique generation models based on evaluation data labeled through GPT-4's direct prompting, creating a more standardized evaluation framework <Paper corpusId=\"270738200\" paperTitle=\"(Ke et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;(Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."
                        ],
                        "paper": {
                            "corpus_id": 270391675,
                            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                            "authors": [
                                {
                                    "authorId": "2145256331",
                                    "name": "Zhen Li"
                                },
                                {
                                    "authorId": "2279658967",
                                    "name": "Xiaohan Xu"
                                },
                                {
                                    "authorId": "2279548827",
                                    "name": "Tao Shen"
                                },
                                {
                                    "authorId": "2284826718",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2308241851",
                                    "name": "Jia-Chen Gu"
                                },
                                {
                                    "authorId": "2308073132",
                                    "name": "Yuxuan Lai"
                                },
                                {
                                    "authorId": "2287928517",
                                    "name": "Chongyang Tao"
                                },
                                {
                                    "authorId": "2307142498",
                                    "name": "Shuai Ma"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 15
                        },
                        "score": 0.57275390625
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "We utilized the LLMs' ability in medical questions reasoning to evaluate the performance of collected conversation. We randomly selected 100 instances from the 876 generated based on MADE and combined them with 1000 test and 1000 validation data generated from MIMIC-III for LLM evaluation (totally 2100 cases). The LLMs used for evaluation were GPT-3.5-Turbo (referred to as Turbo in this section) and GPT-4. We aggregated the criteria into an evaluation prompt Table 4 to guide the LLMs in scoring each conversation. The results are shown in the Table 2. From the figure, it is evident that Turbo received higher scores than GPT-4, particularly in the Turbo NIP Explanation task. This discrepancy can be attributed to the strict evaluation criteria outlined in our evaluation prompt. Turbo, while slightly less capable than GPT-4, did not fully meet these criteria. Additionally, we observed that data generated by GPT-4 NIP, whether in the Q&A task or the Explanation task, exhibited more stable scores across both Turbo and GPT-4 evaluations. Notably, GPT-4 received a noticeably lower score in the Turbo NIP Explanation task when compared to Turbo."
                        ],
                        "paper": {
                            "corpus_id": 266690837,
                            "title": "EHR Interaction Between Patients and AI: NoteAid EHR Interaction",
                            "authors": [
                                {
                                    "authorId": "2277238665",
                                    "name": "Xiaocheng Zhang"
                                },
                                {
                                    "authorId": "1576489304",
                                    "name": "Zonghai Yao"
                                },
                                {
                                    "authorId": "2261455807",
                                    "name": "Hong Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.55810546875
                    },
                    {
                        "id": "(Sellam et al., 2020)",
                        "snippets": [
                            "Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."
                        ],
                        "paper": {
                            "corpus_id": 215548699,
                            "title": "BLEURT: Learning Robust Metrics for Text Generation",
                            "authors": [
                                {
                                    "authorId": "145450400",
                                    "name": "Thibault Sellam"
                                },
                                {
                                    "authorId": "143790066",
                                    "name": "Dipanjan Das"
                                },
                                {
                                    "authorId": "144729897",
                                    "name": "Ankur P. Parikh"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1505
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ouyang et al., 2022)",
                        "snippets": [
                            "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                        ],
                        "paper": {
                            "corpus_id": 246426909,
                            "title": "Training language models to follow instructions with human feedback",
                            "authors": [
                                {
                                    "authorId": "31793034",
                                    "name": "Long Ouyang"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2115903168",
                                    "name": "Xu Jiang"
                                },
                                {
                                    "authorId": "2061137049",
                                    "name": "Diogo Almeida"
                                },
                                {
                                    "authorId": "2064084601",
                                    "name": "Carroll L. Wainwright"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": null,
                                    "name": "Chong Zhang"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "2117680841",
                                    "name": "Katarina Slama"
                                },
                                {
                                    "authorId": "2064770039",
                                    "name": "Alex Ray"
                                },
                                {
                                    "authorId": "47971768",
                                    "name": "John Schulman"
                                },
                                {
                                    "authorId": "2052366271",
                                    "name": "Jacob Hilton"
                                },
                                {
                                    "authorId": "2151735262",
                                    "name": "Fraser Kelton"
                                },
                                {
                                    "authorId": "2142365973",
                                    "name": "Luke E. Miller"
                                },
                                {
                                    "authorId": "2151735251",
                                    "name": "Maddie Simens"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2930640",
                                    "name": "P. Welinder"
                                },
                                {
                                    "authorId": "145791315",
                                    "name": "P. Christiano"
                                },
                                {
                                    "authorId": "2990741",
                                    "name": "Jan Leike"
                                },
                                {
                                    "authorId": "49407415",
                                    "name": "Ryan J. Lowe"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 13203
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ke et al., 2023)",
                        "snippets": [
                            "Since the NLP community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting."
                        ],
                        "paper": {
                            "corpus_id": 270738200,
                            "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
                            "authors": [
                                {
                                    "authorId": "1886879",
                                    "name": "Pei Ke"
                                },
                                {
                                    "authorId": "2122225897",
                                    "name": "Bosi Wen"
                                },
                                {
                                    "authorId": "2316634875",
                                    "name": "Andrew Feng"
                                },
                                {
                                    "authorId": "2308072332",
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "2181283109",
                                    "name": "Xuanyu Lei"
                                },
                                {
                                    "authorId": "2308160059",
                                    "name": "Jiale Cheng"
                                },
                                {
                                    "authorId": "2151486382",
                                    "name": "Shengyuan Wang"
                                },
                                {
                                    "authorId": "2051712753",
                                    "name": "Aohan Zeng"
                                },
                                {
                                    "authorId": "2243402027",
                                    "name": "Yuxiao Dong"
                                },
                                {
                                    "authorId": "2253869803",
                                    "name": "Hongning Wang"
                                },
                                {
                                    "authorId": "2260595820",
                                    "name": "Jie Tang"
                                },
                                {
                                    "authorId": "2254009342",
                                    "name": "Minlie Huang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 34
                        },
                        "score": 0.63037109375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Best Practices for LLM Evaluation",
                "tldr": "To mitigate inherent biases when using LLMs as evaluators, practitioners should implement multiple evaluation methods and employ diverse benchmark datasets. Using separate models for generation and evaluation helps reduce egocentric bias and improve assessment reliability. (3 sources)",
                "text": "\nTo address the challenges associated with LLM-based evaluation, researchers have identified several best practices that can help ensure more reliable and unbiased assessments:\n\n1. **Use diverse evaluation methods**: Rather than relying solely on a single LLM as an evaluator, employ a broader spectrum of evaluation approaches including various benchmarks, different evaluation criteria, and human feedback <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215548699\" paperTitle=\"(Sellam et al., 2020)\" isShortName></Paper>. This multi-method approach helps mitigate the limitations of any single evaluation technique.\n\n2. **Separate generation from evaluation**: When possible, use different models for content generation and evaluation to reduce the \"egocentric bias\" problem where models favor their own outputs. This separation helps address the fundamental \"chicken-and-egg dilemma\" in LLM evaluation <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n3. **Incorporate human judgment**: Human feedback remains a crucial component of effective evaluation frameworks. Models fine-tuned with human feedback, like InstructGPT, have demonstrated improved alignment with user intent and reduced toxic output generation <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>.\n\n4. **Define clear evaluation criteria**: Providing well-defined rubrics and specific evaluation dimensions helps guide the LLM evaluator toward more consistent and targeted assessments. This approach allows for more structured feedback on particular aspects of text quality <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n5. **Consider model capability alignment**: Be aware of the relative capabilities between the evaluator and generator models. The most reliable evaluations typically come from using more capable models to evaluate less capable ones, rather than having models of similar capabilities evaluate each other <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n6. **Validate evaluation results**: Cross-validate LLM evaluations against established benchmarks and, when possible, human judgments to ensure the evaluations align with recognized quality standards <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nBy implementing these best practices, practitioners can develop more robust evaluation frameworks that leverage the strengths of LLMs while minimizing their inherent limitations as evaluators.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;(Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."
                        ],
                        "paper": {
                            "corpus_id": 270391675,
                            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                            "authors": [
                                {
                                    "authorId": "2145256331",
                                    "name": "Zhen Li"
                                },
                                {
                                    "authorId": "2279658967",
                                    "name": "Xiaohan Xu"
                                },
                                {
                                    "authorId": "2279548827",
                                    "name": "Tao Shen"
                                },
                                {
                                    "authorId": "2284826718",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2308241851",
                                    "name": "Jia-Chen Gu"
                                },
                                {
                                    "authorId": "2308073132",
                                    "name": "Yuxuan Lai"
                                },
                                {
                                    "authorId": "2287928517",
                                    "name": "Chongyang Tao"
                                },
                                {
                                    "authorId": "2307142498",
                                    "name": "Shuai Ma"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 15
                        },
                        "score": 0.57275390625
                    },
                    {
                        "id": "(Sellam et al., 2020)",
                        "snippets": [
                            "Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."
                        ],
                        "paper": {
                            "corpus_id": 215548699,
                            "title": "BLEURT: Learning Robust Metrics for Text Generation",
                            "authors": [
                                {
                                    "authorId": "145450400",
                                    "name": "Thibault Sellam"
                                },
                                {
                                    "authorId": "143790066",
                                    "name": "Dipanjan Das"
                                },
                                {
                                    "authorId": "144729897",
                                    "name": "Ankur P. Parikh"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1505
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ouyang et al., 2022)",
                        "snippets": [
                            "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                        ],
                        "paper": {
                            "corpus_id": 246426909,
                            "title": "Training language models to follow instructions with human feedback",
                            "authors": [
                                {
                                    "authorId": "31793034",
                                    "name": "Long Ouyang"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2115903168",
                                    "name": "Xu Jiang"
                                },
                                {
                                    "authorId": "2061137049",
                                    "name": "Diogo Almeida"
                                },
                                {
                                    "authorId": "2064084601",
                                    "name": "Carroll L. Wainwright"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": null,
                                    "name": "Chong Zhang"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "2117680841",
                                    "name": "Katarina Slama"
                                },
                                {
                                    "authorId": "2064770039",
                                    "name": "Alex Ray"
                                },
                                {
                                    "authorId": "47971768",
                                    "name": "John Schulman"
                                },
                                {
                                    "authorId": "2052366271",
                                    "name": "Jacob Hilton"
                                },
                                {
                                    "authorId": "2151735262",
                                    "name": "Fraser Kelton"
                                },
                                {
                                    "authorId": "2142365973",
                                    "name": "Luke E. Miller"
                                },
                                {
                                    "authorId": "2151735251",
                                    "name": "Maddie Simens"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2930640",
                                    "name": "P. Welinder"
                                },
                                {
                                    "authorId": "145791315",
                                    "name": "P. Christiano"
                                },
                                {
                                    "authorId": "2990741",
                                    "name": "Jan Leike"
                                },
                                {
                                    "authorId": "49407415",
                                    "name": "Ryan J. Lowe"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 13203
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.058326
    }
}