{
    "query": "What are the main differences in compression ratio, performance impact, and hardware compatibility between recent unstructured and structured pruning methods for large language models?",
    "user_id": "lib_user",
    "task_id": "15eb238e-78dc-4283-9b6b-11dc42846485",
    "timestamp": "2025-06-24T01:11:01.857454",
    "n_retrieval": 256,
    "n_retrieved": 261,
    "n_candidates": 45,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.633891,
    "decomposed_query": {
        "rewritten_query": "Main differences in compression ratio, performance impact, and hardware compatibility between unstructured and structured pruning methods for large language models.",
        "keyword_query": "compression ratio performance impact hardware compatibility unstructured structured pruning large language models",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009936,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Efficient self-attention with smart pruning for sustainable large language models",
            "venue": "Scientific Reports",
            "year": 2025,
            "reference_count": 45,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1038/s41598-025-92586-5",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11933332, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "102804035",
                    "name": "S. Belhaouari"
                },
                {
                    "authorId": "2292003273",
                    "name": "Insaf Kraidia"
                }
            ],
            "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence by enabling multitasking across diverse fields. However, their high computational demands result in significant environmental impacts, particularly in terms of energy and water consumption. This paper addresses these issues by proposing an innovative compression approach to reducing LLM sizes. We focus on compressing the internal transformer layers, which are critical contributors to LLMs\u2019 computational complexity. Our approach combines new mathematical and structural key methods for model compression. We begin by applying Forward Propagation Pruning (FPP) to compress the embedding and feed-forward layers, utilizing a weight freezing and zeroing technique for suspected unused parameters. This reduces the number of trainable parameters, accelerating the overall training process and enabling faster convergence. Second, the Weight Matrix Folding method is introduced to efficiently prune the self-attention layer matrices in a simple and efficient mathematical model. This method integrates Identical Row Compression (IRC) to optimize the compression of the Query and Key matrices, alongside Diagonal Weight Compression (DWC), which reformulates the Value matrix into a diagonal structure. Consequently, this technique significantly diminishes parameter variability across the three metrics, enhancing consistency and performance while simplifying complexity. The compression approach is evaluated on three language modeling datasets and eight widely used classification datasets, comparing it to various pruning methods. Our method successfully compresses transformer layers by 99% and linear layers by 70%, resulting in an overall model compression of around 70%, while maintaining nearly the same accuracy. Notably, with moderate compression rates of 20% to 40%, model performance not only remained stable but even improved. This leads to substantial reductions in memory usage and computational demands, making LLMs more resource-efficient and highlighting the potential to optimize them for a more sustainable AI future.",
            "corpus_id": 277275922,
            "sentences": [
                {
                    "corpus_id": "277275922",
                    "title": "Efficient self-attention with smart pruning for sustainable large language models",
                    "text": "Structure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility 25 . Furthermore, this method can achieve a different level of sparsity than unstructured pruning, limiting its ability to compress models efficiently without retraining or redesigning the model architecture 26 . On the other hand, unstructured pruning operates at the level of individual weights in a neural network, meaning it can remove any specific weight, regardless of its position 27 . This allows for more fine-grained control over which weights to prune. This can target only the least essential weights, leading to a more refined reduction in model size while retaining critical model capacity 28 . It can achieve higher sparsity levels without compromising model performance because it is not constrained by the rigid structures to which structured pruning must adhere. \n\nDespite preserving model performance, unstructured pruning often results in sparse weight matrices, which may not fully utilize hardware efficiency, as specialized hardware (such as GPUs) is typically optimized for dense matrix operations. To address the limitations of unstructured pruning in efficiently utilizing hardware resources, we propose a novel approach that combines Identical Row Compression, Diagonal Weight Compression for self-attention mechanisms, and linear weight freezing for the feed-forward (FF) and embedding layers. By integrating these methods, we create a model that retains performance and enhances computational efficiency. The combination of IRC and DWC allows for a reduction in the number of active parameters while maintaining a structure that can be efficiently processed on hardware optimized for dense operations. Meanwhile, linear weight freezing stabilizes the training process and reduces computational overhead in the feed-forward and embedding layers. Overall, this multi-faceted approach effectively addresses the challenges associated with unstructured pruning while enhancing hardware efficiency and model performance.",
                    "score": 0.49961313701303994,
                    "section_title": "Related work",
                    "char_start_offset": 6960,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 358
                        },
                        {
                            "start": 359,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 754
                        },
                        {
                            "start": 755,
                            "end": 926
                        },
                        {
                            "start": 929,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1579
                        },
                        {
                            "start": 1580,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 2089
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 146,
                            "end": 148,
                            "matchedPaperCorpusId": "204009154"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96142578125
                },
                {
                    "corpus_id": "277275922",
                    "title": "Efficient self-attention with smart pruning for sustainable large language models",
                    "text": "The compression of language models has gained significant attention, leading to the development of various methods, including network pruning 12 , knowledge distillation, and quantization 13 . LLM-Pruner 14 and FLAP 15 aim to reduce network width by pruning coupled structures. Sheared-LLaMA 16 takes a more comprehensive approach, reducing network width and depth by removing entire layers. While methods that address both width and depth aspects exist 17,18 , there is still a need for in-depth analysis comparing the impact of these factors on LLM inference efficiency. Applying traditional pruning techniques to LLMs presents unique challenges due to their vast number of parameters and substantial computational requirements 19 . Pruning methods for LLMs can be broadly categorized into unstructured and structured approaches. \n\nStructured pruning methods 20 focus on removing entire groups of parameters, maintaining dense weight matrices, and improving hardware efficiency. Techniques such as LLM-Pruner 14 and LoRAPrune 11 emphasize efficient deployment and inference acceleration. Sheared-LLaMA 16 aims to prune models to a target architecture and train them dynamically. The work of Tao introduces a novel compression technique called QuantGPT. This method focused on quantization and module adaptation, allowing for systematic compression while maintaining the integrity of the model's architecture and performance 25 . LightPAFF focuses on transferring knowledge from a more prominent (teacher) model to a smaller (student) model. This process inherently involves structuring the model to retain important features learned by the teacher rather than simply removing individual weights, as seen in unstructured pruning 21 . Unstructured pruning methods 22 target individual weights, maintaining performance. Notable examples include SparseGPT 23 , which employs sophisticated weight updates and pruning without retraining. Edalati et al. developed KnGPT2 for compressing the linear mappings of the GPT-2 model, focusing on reducing the number of parameters flexibly without drastically altering the overall architecture. This technique allows for representing weight matrices in a more compact form while maintaining performance, which aligns with the characteristics of unstructured pruning 24 . \n\nStructure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility 25 .",
                    "score": 0.577265962792162,
                    "section_title": "Related work",
                    "char_start_offset": 4650,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 192
                        },
                        {
                            "start": 193,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 734
                        },
                        {
                            "start": 735,
                            "end": 831
                        },
                        {
                            "start": 834,
                            "end": 980
                        },
                        {
                            "start": 981,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1430
                        },
                        {
                            "start": 1431,
                            "end": 1542
                        },
                        {
                            "start": 1543,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 1933
                        },
                        {
                            "start": 1934,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2307
                        },
                        {
                            "start": 2310,
                            "end": 2460
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 216,
                            "end": 218,
                            "matchedPaperCorpusId": "266362404"
                        },
                        {
                            "start": 457,
                            "end": 459,
                            "matchedPaperCorpusId": "256662263"
                        },
                        {
                            "start": 730,
                            "end": 732,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 861,
                            "end": 863,
                            "matchedPaperCorpusId": "235899080"
                        },
                        {
                            "start": 1426,
                            "end": 1428,
                            "matchedPaperCorpusId": "204009154"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.802734375
                }
            ],
            "relevance_judgement": 0.96142578125,
            "relevance_judgment_input_expanded": "# Title: Efficient self-attention with smart pruning for sustainable large language models\n# Venue: Scientific Reports\n# Authors: S. Belhaouari, Insaf Kraidia\n## Abstract\nLarge Language Models (LLMs) have revolutionized artificial intelligence by enabling multitasking across diverse fields. However, their high computational demands result in significant environmental impacts, particularly in terms of energy and water consumption. This paper addresses these issues by proposing an innovative compression approach to reducing LLM sizes. We focus on compressing the internal transformer layers, which are critical contributors to LLMs\u2019 computational complexity. Our approach combines new mathematical and structural key methods for model compression. We begin by applying Forward Propagation Pruning (FPP) to compress the embedding and feed-forward layers, utilizing a weight freezing and zeroing technique for suspected unused parameters. This reduces the number of trainable parameters, accelerating the overall training process and enabling faster convergence. Second, the Weight Matrix Folding method is introduced to efficiently prune the self-attention layer matrices in a simple and efficient mathematical model. This method integrates Identical Row Compression (IRC) to optimize the compression of the Query and Key matrices, alongside Diagonal Weight Compression (DWC), which reformulates the Value matrix into a diagonal structure. Consequently, this technique significantly diminishes parameter variability across the three metrics, enhancing consistency and performance while simplifying complexity. The compression approach is evaluated on three language modeling datasets and eight widely used classification datasets, comparing it to various pruning methods. Our method successfully compresses transformer layers by 99% and linear layers by 70%, resulting in an overall model compression of around 70%, while maintaining nearly the same accuracy. Notably, with moderate compression rates of 20% to 40%, model performance not only remained stable but even improved. This leads to substantial reductions in memory usage and computational demands, making LLMs more resource-efficient and highlighting the potential to optimize them for a more sustainable AI future.\n## Related work\nThe compression of language models has gained significant attention, leading to the development of various methods, including network pruning 12 , knowledge distillation, and quantization 13 . LLM-Pruner 14 and FLAP 15 aim to reduce network width by pruning coupled structures. Sheared-LLaMA 16 takes a more comprehensive approach, reducing network width and depth by removing entire layers. While methods that address both width and depth aspects exist 17,18 , there is still a need for in-depth analysis comparing the impact of these factors on LLM inference efficiency. Applying traditional pruning techniques to LLMs presents unique challenges due to their vast number of parameters and substantial computational requirements 19 . Pruning methods for LLMs can be broadly categorized into unstructured and structured approaches. \n\nStructured pruning methods 20 focus on removing entire groups of parameters, maintaining dense weight matrices, and improving hardware efficiency. Techniques such as LLM-Pruner 14 and LoRAPrune 11 emphasize efficient deployment and inference acceleration. Sheared-LLaMA 16 aims to prune models to a target architecture and train them dynamically. The work of Tao introduces a novel compression technique called QuantGPT. This method focused on quantization and module adaptation, allowing for systematic compression while maintaining the integrity of the model's architecture and performance 25 . LightPAFF focuses on transferring knowledge from a more prominent (teacher) model to a smaller (student) model. This process inherently involves structuring the model to retain important features learned by the teacher rather than simply removing individual weights, as seen in unstructured pruning 21 . Unstructured pruning methods 22 target individual weights, maintaining performance. Notable examples include SparseGPT 23 , which employs sophisticated weight updates and pruning without retraining. Edalati et al. developed KnGPT2 for compressing the linear mappings of the GPT-2 model, focusing on reducing the number of parameters flexibly without drastically altering the overall architecture. This technique allows for representing weight matrices in a more compact form while maintaining performance, which aligns with the characteristics of unstructured pruning 24 . \n\nStructure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility 25 .\n...\nStructure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility 25 . Furthermore, this method can achieve a different level of sparsity than unstructured pruning, limiting its ability to compress models efficiently without retraining or redesigning the model architecture 26 . On the other hand, unstructured pruning operates at the level of individual weights in a neural network, meaning it can remove any specific weight, regardless of its position 27 . This allows for more fine-grained control over which weights to prune. This can target only the least essential weights, leading to a more refined reduction in model size while retaining critical model capacity 28 . It can achieve higher sparsity levels without compromising model performance because it is not constrained by the rigid structures to which structured pruning must adhere. \n\nDespite preserving model performance, unstructured pruning often results in sparse weight matrices, which may not fully utilize hardware efficiency, as specialized hardware (such as GPUs) is typically optimized for dense matrix operations. To address the limitations of unstructured pruning in efficiently utilizing hardware resources, we propose a novel approach that combines Identical Row Compression, Diagonal Weight Compression for self-attention mechanisms, and linear weight freezing for the feed-forward (FF) and embedding layers. By integrating these methods, we create a model that retains performance and enhances computational efficiency. The combination of IRC and DWC allows for a reduction in the number of active parameters while maintaining a structure that can be efficiently processed on hardware optimized for dense operations. Meanwhile, linear weight freezing stabilizes the training process and reduces computational overhead in the feed-forward and embedding layers. Overall, this multi-faceted approach effectively addresses the challenges associated with unstructured pruning while enhancing hardware efficiency and model performance.",
            "reference_string": "[277275922 | Belhaouari et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 137,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352948034",
                    "name": "Ziyang Ma"
                },
                {
                    "authorId": "2274202084",
                    "name": "Zuchao Li"
                },
                {
                    "authorId": "2269488794",
                    "name": "Lefei Zhang"
                },
                {
                    "authorId": "2343636012",
                    "name": "Gui-Song Xia"
                },
                {
                    "authorId": "2306994733",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2268745050",
                    "name": "Liangpei Zhang"
                },
                {
                    "authorId": "2275194788",
                    "name": "Dacheng Tao"
                }
            ],
            "abstract": "Large language models (LLMs) demonstrate strong performance across natural language processing tasks, yet undergo significant performance degradation when modified for deployment through quantization, pruning, or decoding strategy adjustments. We define this phenomenon as model hemorrhage - performance decline caused by parameter alterations and architectural changes. Through systematic analysis of various LLM frameworks, we identify key vulnerability patterns: layer expansion frequently disrupts attention mechanisms, compression techniques induce information loss cascades, and decoding adjustments amplify prediction divergences. Our investigation reveals transformer architectures exhibit inherent robustness thresholds that determine hemorrhage severity across modification types. We propose three mitigation strategies: gradient-aware pruning preserves critical weight pathways, dynamic quantization scaling maintains activation integrity, and decoding calibration aligns generation trajectories with original model distributions. This work establishes foundational metrics for evaluating model stability during adaptation, providing practical guidelines for maintaining performance while enabling efficient LLM deployment. Our findings advance understanding of neural network resilience under architectural transformations, particularly for large-scale language models.",
            "corpus_id": 277452419,
            "sentences": [
                {
                    "corpus_id": "277452419",
                    "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                    "text": "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks [27], while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]. However, unstructured pruning often results in irregular sparse patterns in the weight matrix, necessitating specialized hardware accelerators (e.g., sparse matrix multiplication units) to efficiently handle sparse matrix computations and fully exploit the benefits of sparsity in terms of storage and computation. Among various unstructured pruning methods, Magnitude Pruning is the most basic, directly removing weights with small magnitudes. While simple to implement, it does not account for the contextual importance of weights. SparseGPT [29], on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda [27] simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA [30] introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM [31] builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers.",
                    "score": 0.5835271244969228,
                    "section_title": "Unstructured Pruning",
                    "char_start_offset": 21938,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 610
                        },
                        {
                            "start": 611,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1808
                        },
                        {
                            "start": 1809,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2165
                        },
                        {
                            "start": 2166,
                            "end": 2295
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 438,
                            "end": 442,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 1155,
                            "end": 1159,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1384,
                            "end": 1388,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 1813,
                            "end": 1817,
                            "matchedPaperCorpusId": "271745835"
                        },
                        {
                            "start": 1964,
                            "end": 1968,
                            "matchedPaperCorpusId": "266818263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94482421875
                }
            ],
            "relevance_judgement": 0.94482421875,
            "relevance_judgment_input_expanded": "# Title: Model Hemorrhage and the Robustness Limits of Large Language Models\n# Venue: arXiv.org\n# Authors: Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, Dacheng Tao\n## Abstract\nLarge language models (LLMs) demonstrate strong performance across natural language processing tasks, yet undergo significant performance degradation when modified for deployment through quantization, pruning, or decoding strategy adjustments. We define this phenomenon as model hemorrhage - performance decline caused by parameter alterations and architectural changes. Through systematic analysis of various LLM frameworks, we identify key vulnerability patterns: layer expansion frequently disrupts attention mechanisms, compression techniques induce information loss cascades, and decoding adjustments amplify prediction divergences. Our investigation reveals transformer architectures exhibit inherent robustness thresholds that determine hemorrhage severity across modification types. We propose three mitigation strategies: gradient-aware pruning preserves critical weight pathways, dynamic quantization scaling maintains activation integrity, and decoding calibration aligns generation trajectories with original model distributions. This work establishes foundational metrics for evaluating model stability during adaptation, providing practical guidelines for maintaining performance while enabling efficient LLM deployment. Our findings advance understanding of neural network resilience under architectural transformations, particularly for large-scale language models.\n## Unstructured Pruning\nUnstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks [27], while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]. However, unstructured pruning often results in irregular sparse patterns in the weight matrix, necessitating specialized hardware accelerators (e.g., sparse matrix multiplication units) to efficiently handle sparse matrix computations and fully exploit the benefits of sparsity in terms of storage and computation. Among various unstructured pruning methods, Magnitude Pruning is the most basic, directly removing weights with small magnitudes. While simple to implement, it does not account for the contextual importance of weights. SparseGPT [29], on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda [27] simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA [30] introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM [31] builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers.",
            "reference_string": "[277452419 | Ma et al. | 2025 | Citations: 1]"
        },
        {
            "title": "AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 62,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.03745, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325996847",
                    "name": "Yanbiao Liang"
                },
                {
                    "authorId": "30984015",
                    "name": "Huihong Shi"
                },
                {
                    "authorId": "2146641923",
                    "name": "Haikuo Shao"
                },
                {
                    "authorId": "2275509567",
                    "name": "Zhongfeng Wang"
                }
            ],
            "abstract": "Recently, large language models (LLMs) have achieved huge success in the natural language processing (NLP) field, driving a growing demand to extend their deployment from the cloud to edge devices. However, deploying LLMs on resource-constrained edge devices poses significant challenges, including (1) intensive computations and huge model sizes, (2) great memory and bandwidth demands introduced by the autoregressive generation process, and (3) limited scalability for handling long sequences. To address these challenges, we propose AccLLM, a comprehensive acceleration framework that enables efficient and fast long-context LLM inference through algorithm and hardware co-design. At the algorithmic level, we integrate (1) pruning, (2) {\\Lambda}-shaped attention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and 4-bit KV cache) quantization scheme, thus effectively reducing memory and bandwidth requirements while facilitating LLMs' long-sequence generation. At the hardware level, we design a dedicated FPGA-based accelerator with a reconfigurable computing engine to effectively and flexibly accommodate diverse operations arising from our compression algorithm, thereby fully translating the algorithmic innovations into tangible hardware efficiency. We validate AccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency and a 2.98x throughput compared to the state-of-the-art work FlightLLM.",
            "corpus_id": 278367624,
            "sentences": [
                {
                    "corpus_id": "278367624",
                    "title": "AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design",
                    "text": "Pruning boosts model compactness at the architectural level by removing redundant parameters, ranging from individual weights (unstructured pruning [32]- [34]) to entire channels or layers (structured pruning [35]- [37]). Although unstructured pruning can achieve significant compression ratios, the resulting irregular sparsity is not conducive to hardware implementation [38]. In contrast, structured pruning is more compatible with hardware acceleration but often results in model accuracy degradation and limited sparsity [39]. To balance model accuracy and hardware efficiency, N :M semi-structured pruning [17], [18], where N out of every M elements are pruned, is commonly adopted in prevalent LLMs [16], [40]. For example, SparseGPT [17] effectively prunes GPT-family models [3] to achieve 2:4 and 4:8 sparsity in a one-shot manner without any retraining. Moreover, Wanda [18] leverages the product of weights and input activations to achieve 2:4 semi-structured pruning, demonstrating improved perplexity in certain cases, such as Llama-2-13B [4].",
                    "score": 0.5285495852796835,
                    "section_title": "A. Pruning for LLMs",
                    "char_start_offset": 5562,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 717
                        },
                        {
                            "start": 718,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1056
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 148,
                            "end": 152,
                            "matchedPaperCorpusId": "264146174"
                        },
                        {
                            "start": 154,
                            "end": 158,
                            "matchedPaperCorpusId": "268032346"
                        },
                        {
                            "start": 209,
                            "end": 213,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 215,
                            "end": 219,
                            "matchedPaperCorpusId": "259203385"
                        },
                        {
                            "start": 373,
                            "end": 377,
                            "matchedPaperCorpusId": "16167970"
                        },
                        {
                            "start": 526,
                            "end": 530,
                            "matchedPaperCorpusId": "20157893"
                        },
                        {
                            "start": 612,
                            "end": 616,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 618,
                            "end": 622,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 706,
                            "end": 710,
                            "matchedPaperCorpusId": "266844224"
                        },
                        {
                            "start": 712,
                            "end": 716,
                            "matchedPaperCorpusId": "233296249"
                        },
                        {
                            "start": 741,
                            "end": 745,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 783,
                            "end": 786,
                            "matchedPaperCorpusId": "248496292"
                        },
                        {
                            "start": 880,
                            "end": 884,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 1052,
                            "end": 1055,
                            "matchedPaperCorpusId": "259950998"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.939453125
                }
            ],
            "relevance_judgement": 0.939453125,
            "relevance_judgment_input_expanded": "# Title: AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design\n# Venue: arXiv.org\n# Authors: Yanbiao Liang, Huihong Shi, Haikuo Shao, Zhongfeng Wang\n## Abstract\nRecently, large language models (LLMs) have achieved huge success in the natural language processing (NLP) field, driving a growing demand to extend their deployment from the cloud to edge devices. However, deploying LLMs on resource-constrained edge devices poses significant challenges, including (1) intensive computations and huge model sizes, (2) great memory and bandwidth demands introduced by the autoregressive generation process, and (3) limited scalability for handling long sequences. To address these challenges, we propose AccLLM, a comprehensive acceleration framework that enables efficient and fast long-context LLM inference through algorithm and hardware co-design. At the algorithmic level, we integrate (1) pruning, (2) {\\Lambda}-shaped attention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and 4-bit KV cache) quantization scheme, thus effectively reducing memory and bandwidth requirements while facilitating LLMs' long-sequence generation. At the hardware level, we design a dedicated FPGA-based accelerator with a reconfigurable computing engine to effectively and flexibly accommodate diverse operations arising from our compression algorithm, thereby fully translating the algorithmic innovations into tangible hardware efficiency. We validate AccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency and a 2.98x throughput compared to the state-of-the-art work FlightLLM.\n## A. Pruning for LLMs\nPruning boosts model compactness at the architectural level by removing redundant parameters, ranging from individual weights (unstructured pruning [32]- [34]) to entire channels or layers (structured pruning [35]- [37]). Although unstructured pruning can achieve significant compression ratios, the resulting irregular sparsity is not conducive to hardware implementation [38]. In contrast, structured pruning is more compatible with hardware acceleration but often results in model accuracy degradation and limited sparsity [39]. To balance model accuracy and hardware efficiency, N :M semi-structured pruning [17], [18], where N out of every M elements are pruned, is commonly adopted in prevalent LLMs [16], [40]. For example, SparseGPT [17] effectively prunes GPT-family models [3] to achieve 2:4 and 4:8 sparsity in a one-shot manner without any retraining. Moreover, Wanda [18] leverages the product of weights and input activations to achieve 2:4 semi-structured pruning, demonstrating improved perplexity in certain cases, such as Llama-2-13B [4].",
            "reference_string": "[278367624 | Liang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.03373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283405371",
                    "name": "Hanyu Hu"
                },
                {
                    "authorId": "2283433779",
                    "name": "Xiaoming Yuan"
                }
            ],
            "abstract": "The deployment of large language models (LLMs) is often constrained by their substantial computational and memory demands. While structured pruning presents a viable approach by eliminating entire network components, existing methods suffer from performance degradation, reliance on heuristic metrics, or expensive finetuning. To address these challenges, we propose SPAP (Structured Pruning via Alternating Optimization and Penalty Methods), a novel and efficient structured pruning framework for LLMs grounded in optimization theory. SPAP formulates the pruning problem through a mixed-integer optimization model, employs a penalty method that effectively makes pruning decisions to minimize pruning errors, and introduces an alternating minimization algorithm tailored to the splittable problem structure for efficient weight updates and performance recovery. Extensive experiments on OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over state-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at 30% sparsity) and proportional memory reductions. Our work offers a practical, optimization-driven solution for pruning LLMs while preserving model performance.",
            "corpus_id": 278338989,
            "sentences": [
                {
                    "corpus_id": "278338989",
                    "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods",
                    "text": "The rapid advancement of large language models (LLMs) has revolutionized natural language processing (Zhang et al., 2022;OpenAI, 2023;Touvron et al., 2023a,b;Meta AI, 2023;Gemini Team et al., 2023;Grattafiori et al., 2024;Apple Inc, 2024;DeepSeek-AI, 2025;Qwen Team, 2025), yet their enormous size poses significant challenges for practical deployment. Pruning techniques have emerged as a cornerstone for LLM compression (Frantar and Alistarh, 2023;Sun et al., 2023;Ma et al., 2023;Shen et al., 2024;Fang et al., 2024), offering a promising solution to reduce model size and computational requirements while preserving performance. While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware Middle: We propose a penalty method to decide which parts of the weight matrices should be pruned. Right: We develop an alternating minimization algorithm to efficiently update the remaining weights to restore model performance. \n\nor sparse computations. These advantages make structured pruning a robust and hardware-agnostic strategy for LLM compression.",
                    "score": 0.6959867323037338,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1369
                        },
                        {
                            "start": 1370,
                            "end": 1748
                        },
                        {
                            "start": 1749,
                            "end": 1878
                        },
                        {
                            "start": 1881,
                            "end": 1904
                        },
                        {
                            "start": 1905,
                            "end": 2006
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 422,
                            "end": 450,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 660,
                            "end": 688,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 960,
                            "end": 981,
                            "matchedPaperCorpusId": "240288854"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.93359375,
            "relevance_judgment_input_expanded": "# Title: SPAP: Structured Pruning via Alternating Optimization and Penalty Methods\n# Venue: arXiv.org\n# Authors: Hanyu Hu, Xiaoming Yuan\n## Abstract\nThe deployment of large language models (LLMs) is often constrained by their substantial computational and memory demands. While structured pruning presents a viable approach by eliminating entire network components, existing methods suffer from performance degradation, reliance on heuristic metrics, or expensive finetuning. To address these challenges, we propose SPAP (Structured Pruning via Alternating Optimization and Penalty Methods), a novel and efficient structured pruning framework for LLMs grounded in optimization theory. SPAP formulates the pruning problem through a mixed-integer optimization model, employs a penalty method that effectively makes pruning decisions to minimize pruning errors, and introduces an alternating minimization algorithm tailored to the splittable problem structure for efficient weight updates and performance recovery. Extensive experiments on OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over state-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at 30% sparsity) and proportional memory reductions. Our work offers a practical, optimization-driven solution for pruning LLMs while preserving model performance.\n## Introduction\nThe rapid advancement of large language models (LLMs) has revolutionized natural language processing (Zhang et al., 2022;OpenAI, 2023;Touvron et al., 2023a,b;Meta AI, 2023;Gemini Team et al., 2023;Grattafiori et al., 2024;Apple Inc, 2024;DeepSeek-AI, 2025;Qwen Team, 2025), yet their enormous size poses significant challenges for practical deployment. Pruning techniques have emerged as a cornerstone for LLM compression (Frantar and Alistarh, 2023;Sun et al., 2023;Ma et al., 2023;Shen et al., 2024;Fang et al., 2024), offering a promising solution to reduce model size and computational requirements while preserving performance. While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware Middle: We propose a penalty method to decide which parts of the weight matrices should be pruned. Right: We develop an alternating minimization algorithm to efficiently update the remaining weights to restore model performance. \n\nor sparse computations. These advantages make structured pruning a robust and hardware-agnostic strategy for LLM compression.",
            "reference_string": "[278338989 | Hu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290611525",
                    "name": "Jiayu Qin"
                },
                {
                    "authorId": "2326256572",
                    "name": "Jianchao Tan"
                },
                {
                    "authorId": "2326248013",
                    "name": "Kefeng Zhang"
                },
                {
                    "authorId": "2326248599",
                    "name": "Xunliang Cai"
                },
                {
                    "authorId": "2338695871",
                    "name": "Wei Wang"
                }
            ],
            "abstract": "The remarkable performance of large language models (LLMs) in various language tasks has attracted considerable attention. However, the ever-increasing size of these models presents growing challenges for deployment and inference. Structured pruning, an effective model compression technique, is gaining increasing attention due to its ability to enhance inference efficiency. Nevertheless, most previous optimization-based structured pruning methods sacrifice the uniform structure across layers for greater flexibility to maintain performance. The heterogeneous structure hinders the effective utilization of off-the-shelf inference acceleration techniques and impedes efficient configuration for continued training. To address this issue, we propose a novel masking learning paradigm based on minimax optimization to obtain the uniform pruned structure by optimizing the masks under sparsity regularization. Extensive experimental results demonstrate that our method can maintain high performance while ensuring the uniformity of the pruned model structure, thereby outperforming existing SOTA methods.",
            "corpus_id": 276482745,
            "sentences": [
                {
                    "corpus_id": "276482745",
                    "title": "MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures",
                    "text": "Unstructured pruning compresses models by removing individual parameters, resulting in sparse weight matrices that consume less memory. However, without dedicated hardware support, the updated models do not achieve faster inference, thereby still imposing computational burdens during the inference process. Semi-structured pruning offers some speed improvements, but these are limited compared to those achieved by structured pruning. Structured pruning adopts a more modular approach to remove modules from models, typically targeting attention heads, embedding dimensions, FFN intermediate dimensions, experts in Mixture-of-Experts (MoE) networks, or layers. After structured pruning, the weight matrices of the models remain dense, and their reduced dimensions typically lead to greater inference acceleration. However, the coarser granularity of this pruning method makes it more challenging to preserve model capabilities after pruning. Currently, most pruning techniques employ metricbased methods, which determine the modules to be pruned by introducing specific pruning metrics. These metrics are usually designed heuristically and often perform poorly at high pruning rates. Moreover, a single metric cannot fully capture the importance of model weights, making it difficult to identify superior local optimal solutions. In contrast, optimization-based pruning methods determine which weights to prune by learning a pruning mask, thereby avoiding the performance degradation associated with manually designed metrics. This paper primarily focuses on optimization-based pruning methods. \n\nGiven the large scale of Large Language Models (LLMs), existing optimization-based pruning methods employ structured pruning, wherein a single mask prunes entire modules of the model. Methods such as CoFi (Xia et al., 2022), Compresso (Guo et al., 2023) and NutePrune (Li et al., 2024) follow the L 0 regularization (Louizos et al., 2018) training paradigm during the training of pruning masks, learning masks by setting a total sparsity without additional constraints. This approach results in a lack of uniformity between layers during training, causing each layer to have a different number of attention heads and FFN intermediate dimensions, as illustrated in Figure 1, which leads to suboptimal inference speed.",
                    "score": 0.594204205290018,
                    "section_title": "Introduction",
                    "char_start_offset": 1761,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 435
                        },
                        {
                            "start": 436,
                            "end": 661
                        },
                        {
                            "start": 662,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1330
                        },
                        {
                            "start": 1331,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1595
                        },
                        {
                            "start": 1598,
                            "end": 1781
                        },
                        {
                            "start": 1782,
                            "end": 2067
                        },
                        {
                            "start": 2068,
                            "end": 2314
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1803,
                            "end": 1821,
                            "matchedPaperCorpusId": "247922354"
                        },
                        {
                            "start": 1914,
                            "end": 1936,
                            "matchedPaperCorpusId": "30535508"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92919921875
                }
            ],
            "relevance_judgement": 0.92919921875,
            "relevance_judgment_input_expanded": "# Title: MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures\n# Venue: arXiv.org\n# Authors: Jiayu Qin, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Wei Wang\n## Abstract\nThe remarkable performance of large language models (LLMs) in various language tasks has attracted considerable attention. However, the ever-increasing size of these models presents growing challenges for deployment and inference. Structured pruning, an effective model compression technique, is gaining increasing attention due to its ability to enhance inference efficiency. Nevertheless, most previous optimization-based structured pruning methods sacrifice the uniform structure across layers for greater flexibility to maintain performance. The heterogeneous structure hinders the effective utilization of off-the-shelf inference acceleration techniques and impedes efficient configuration for continued training. To address this issue, we propose a novel masking learning paradigm based on minimax optimization to obtain the uniform pruned structure by optimizing the masks under sparsity regularization. Extensive experimental results demonstrate that our method can maintain high performance while ensuring the uniformity of the pruned model structure, thereby outperforming existing SOTA methods.\n## Introduction\nUnstructured pruning compresses models by removing individual parameters, resulting in sparse weight matrices that consume less memory. However, without dedicated hardware support, the updated models do not achieve faster inference, thereby still imposing computational burdens during the inference process. Semi-structured pruning offers some speed improvements, but these are limited compared to those achieved by structured pruning. Structured pruning adopts a more modular approach to remove modules from models, typically targeting attention heads, embedding dimensions, FFN intermediate dimensions, experts in Mixture-of-Experts (MoE) networks, or layers. After structured pruning, the weight matrices of the models remain dense, and their reduced dimensions typically lead to greater inference acceleration. However, the coarser granularity of this pruning method makes it more challenging to preserve model capabilities after pruning. Currently, most pruning techniques employ metricbased methods, which determine the modules to be pruned by introducing specific pruning metrics. These metrics are usually designed heuristically and often perform poorly at high pruning rates. Moreover, a single metric cannot fully capture the importance of model weights, making it difficult to identify superior local optimal solutions. In contrast, optimization-based pruning methods determine which weights to prune by learning a pruning mask, thereby avoiding the performance degradation associated with manually designed metrics. This paper primarily focuses on optimization-based pruning methods. \n\nGiven the large scale of Large Language Models (LLMs), existing optimization-based pruning methods employ structured pruning, wherein a single mask prunes entire modules of the model. Methods such as CoFi (Xia et al., 2022), Compresso (Guo et al., 2023) and NutePrune (Li et al., 2024) follow the L 0 regularization (Louizos et al., 2018) training paradigm during the training of pruning masks, learning masks by setting a total sparsity without additional constraints. This approach results in a lack of uniformity between layers during training, causing each layer to have a different number of attention heads and FFN intermediate dimensions, as illustrated in Figure 1, which leads to suboptimal inference speed.",
            "reference_string": "[276482745 | Qin et al. | 2025 | Citations: 0]"
        },
        {
            "title": "You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning",
            "venue": "International Conference on Learning Representations",
            "year": 2025,
            "reference_count": 27,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.15296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34920835",
                    "name": "Ayan Sengupta"
                },
                {
                    "authorId": "2261280847",
                    "name": "Siddhant Chaudhary"
                },
                {
                    "authorId": "2249914540",
                    "name": "Tanmoy Chakraborty"
                }
            ],
            "abstract": "The ever-increasing size of large language models (LLMs) presents significant challenges for deployment due to their heavy computational and memory requirements. Current model pruning techniques attempt to alleviate these issues by relying heavily on external calibration datasets to determine which parameters to prune or compress, thus limiting their flexibility and scalability across different compression ratios. Moreover, these methods often cause severe performance degradation, particularly in downstream tasks, when subjected to higher compression rates. In this paper, we propose PruneNet, a novel model compression method that addresses these limitations by reformulating model pruning as a policy learning process. PruneNet decouples the pruning process from the model architecture, eliminating the need for calibration datasets. It learns a stochastic pruning policy to assess parameter importance solely based on intrinsic model properties while preserving the spectral structure to minimize information loss. PruneNet can compress the LLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its zero-shot performance with a 30% compression ratio, outperforming existing methods that retain only 75% performance. Furthermore, on complex multitask language understanding tasks, PruneNet demonstrates its robustness by preserving up to 80% performance of the original model, proving itself a superior alternative to conventional structured compression techniques.",
            "corpus_id": 275921475,
            "sentences": [
                {
                    "corpus_id": "275921475",
                    "title": "You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning",
                    "text": "Model pruning is a widely-used technique to reduce the number of parameters in a model, enhancing both speed and efficiency. It can be broadly categorized into two classes -unstructured and structured pruning. Unstructured pruning removes individual weights, as seen in SparseGPT (Frantar & Alistarh, 2023), which leverages Hessian matrix inversion to identify and eliminate less critical weights. However, unstructured pruning often requires hardware-specific optimizations and may not always result in significant computational gains (Yang et al., 2024;Wang et al., 2024b). In contrast, structured pruning removes entire channels or components, making it more compatible with various hardware setups. For example, LLM-Pruner (Ma et al., 2023b) evaluates weight group importance and uses LoRA fine-tuning to recover lost accuracy. While structured pruning is more hardware-friendly, it can lead to greater accuracy loss at higher compression ratios. Methods like Layer Collapse (Yang et al., 2024) take a layer-wise approach, merging parameters of adjacent lay-ers to achieve up to 50% compression without extensive retraining. Recent advancements in posttraining compression methods, such as SliceGPT (Ashkboos et al., 2024) and SVD-LLM (Wang et al., 2024b), aim to maintain performance while reducing model size. SliceGPT is a structured pruning method that compresses LLMs by slicing off entire rows and columns of weight matrices, using orthogonal transformations to reduce the embedding dimensions. SVD-LLM, on the other hand, applies singular value decomposition with layer-wise updates, ensuring minimal accuracy loss even under high compression, outperforming previous methods like ASVD (Yuan et al., 2023) and FWSVD (Hsu et al., 2022). \n\nMajority of these structured pruning methods rely heavily on external calibration datasets, making them sensitive to data quality.",
                    "score": 0.5510358091239363,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 4717,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 950
                        },
                        {
                            "start": 951,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1745
                        },
                        {
                            "start": 1748,
                            "end": 1878
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 727,
                            "end": 745,
                            "matchedPaperCorpusId": "258823276"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.923828125,
            "relevance_judgment_input_expanded": "# Title: You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning\n# Venue: International Conference on Learning Representations\n# Authors: Ayan Sengupta, Siddhant Chaudhary, Tanmoy Chakraborty\n## Abstract\nThe ever-increasing size of large language models (LLMs) presents significant challenges for deployment due to their heavy computational and memory requirements. Current model pruning techniques attempt to alleviate these issues by relying heavily on external calibration datasets to determine which parameters to prune or compress, thus limiting their flexibility and scalability across different compression ratios. Moreover, these methods often cause severe performance degradation, particularly in downstream tasks, when subjected to higher compression rates. In this paper, we propose PruneNet, a novel model compression method that addresses these limitations by reformulating model pruning as a policy learning process. PruneNet decouples the pruning process from the model architecture, eliminating the need for calibration datasets. It learns a stochastic pruning policy to assess parameter importance solely based on intrinsic model properties while preserving the spectral structure to minimize information loss. PruneNet can compress the LLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its zero-shot performance with a 30% compression ratio, outperforming existing methods that retain only 75% performance. Furthermore, on complex multitask language understanding tasks, PruneNet demonstrates its robustness by preserving up to 80% performance of the original model, proving itself a superior alternative to conventional structured compression techniques.\n## RELATED WORK\nModel pruning is a widely-used technique to reduce the number of parameters in a model, enhancing both speed and efficiency. It can be broadly categorized into two classes -unstructured and structured pruning. Unstructured pruning removes individual weights, as seen in SparseGPT (Frantar & Alistarh, 2023), which leverages Hessian matrix inversion to identify and eliminate less critical weights. However, unstructured pruning often requires hardware-specific optimizations and may not always result in significant computational gains (Yang et al., 2024;Wang et al., 2024b). In contrast, structured pruning removes entire channels or components, making it more compatible with various hardware setups. For example, LLM-Pruner (Ma et al., 2023b) evaluates weight group importance and uses LoRA fine-tuning to recover lost accuracy. While structured pruning is more hardware-friendly, it can lead to greater accuracy loss at higher compression ratios. Methods like Layer Collapse (Yang et al., 2024) take a layer-wise approach, merging parameters of adjacent lay-ers to achieve up to 50% compression without extensive retraining. Recent advancements in posttraining compression methods, such as SliceGPT (Ashkboos et al., 2024) and SVD-LLM (Wang et al., 2024b), aim to maintain performance while reducing model size. SliceGPT is a structured pruning method that compresses LLMs by slicing off entire rows and columns of weight matrices, using orthogonal transformations to reduce the embedding dimensions. SVD-LLM, on the other hand, applies singular value decomposition with layer-wise updates, ensuring minimal accuracy loss even under high compression, outperforming previous methods like ASVD (Yuan et al., 2023) and FWSVD (Hsu et al., 2022). \n\nMajority of these structured pruning methods rely heavily on external calibration datasets, making them sensitive to data quality.",
            "reference_string": "[275921475 | Sengupta et al. | 2025 | Citations: 4]"
        },
        {
            "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 62,
            "citation_count": 26,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.04089, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40992614",
                    "name": "Eldar Kurtic"
                },
                {
                    "authorId": "1502248377",
                    "name": "Elias Frantar"
                },
                {
                    "authorId": "3311387",
                    "name": "Dan Alistarh"
                }
            ],
            "abstract": "The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and pruning techniques, making it a cost-effective approach for generating an entire family of smaller, faster, and highly accurate models, guaranteed to meet the desired inference specifications. In particular, ZipLM outperforms all prior BERT-base distillation and pruning techniques, such as CoFi, MiniLM, and TinyBERT. Moreover, it matches the performance of the heavily optimized MobileBERT model, obtained via extensive architecture search, by simply pruning the baseline BERT-large model. When compressing GPT2, ZipLM outperforms DistilGPT2 while being 60% smaller and 30% faster. Our code is available at: https://github.com/IST-DASLab/ZipLM.",
            "corpus_id": 256662263,
            "sentences": [
                {
                    "corpus_id": "256662263",
                    "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
                    "text": "The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning [2], quantization [3], and distillation [4]. In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy. In addition, structural compression significantly complicates the use of knowledge distillation [5], which is usually done via manual or dynamic layer mapping [6,7]. On the practical side, another challenge is that most existing techniques do not provide runtime speedup guarantees: the model is pruned to a fixed sparsity or FLOPS target, and then must be evaluated in the target inference environment. If the pruned model fails to meet the target inference specifications, the whole process must be repeated from scratch. \n\nOverview. In this paper, we resolve these issues and provide a novel structured pruning approach called ZipLM, which achieves state-of-the-art performance, both in the post-training/one-shot setting, where retraining is not desirable, as well as in the popular gradual compression setting, where retraining is possible. We accomplish this via an inference-aware algorithm, which successfully balances the loss-runtime trade-off at each pruning step. By taking runtime into account, we avoid removing components that do not bring significant speedup gains. Additionally, our algorithm provides speedup guarantees for compressed models, a highly-desirable property in practical applications. \n\nWe summarize our contributions as follows:",
                    "score": 0.5145846601366911,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 198
                        },
                        {
                            "start": 199,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1584
                        },
                        {
                            "start": 1587,
                            "end": 1596
                        },
                        {
                            "start": 1597,
                            "end": 1906
                        },
                        {
                            "start": 1907,
                            "end": 2036
                        },
                        {
                            "start": 2037,
                            "end": 2142
                        },
                        {
                            "start": 2143,
                            "end": 2276
                        },
                        {
                            "start": 2279,
                            "end": 2321
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 305,
                            "end": 308,
                            "matchedPaperCorpusId": "231740691"
                        },
                        {
                            "start": 345,
                            "end": 348,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1220,
                            "end": 1223,
                            "matchedPaperCorpusId": "202719327"
                        },
                        {
                            "start": 1223,
                            "end": 1225,
                            "matchedPaperCorpusId": "247922354"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90576171875
                }
            ],
            "relevance_judgement": 0.90576171875,
            "relevance_judgment_input_expanded": "# Title: ZipLM: Inference-Aware Structured Pruning of Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Eldar Kurtic, Elias Frantar, Dan Alistarh\n## Abstract\nThe breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and pruning techniques, making it a cost-effective approach for generating an entire family of smaller, faster, and highly accurate models, guaranteed to meet the desired inference specifications. In particular, ZipLM outperforms all prior BERT-base distillation and pruning techniques, such as CoFi, MiniLM, and TinyBERT. Moreover, it matches the performance of the heavily optimized MobileBERT model, obtained via extensive architecture search, by simply pruning the baseline BERT-large model. When compressing GPT2, ZipLM outperforms DistilGPT2 while being 60% smaller and 30% faster. Our code is available at: https://github.com/IST-DASLab/ZipLM.\n## Introduction\nThe high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning [2], quantization [3], and distillation [4]. In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy. In addition, structural compression significantly complicates the use of knowledge distillation [5], which is usually done via manual or dynamic layer mapping [6,7]. On the practical side, another challenge is that most existing techniques do not provide runtime speedup guarantees: the model is pruned to a fixed sparsity or FLOPS target, and then must be evaluated in the target inference environment. If the pruned model fails to meet the target inference specifications, the whole process must be repeated from scratch. \n\nOverview. In this paper, we resolve these issues and provide a novel structured pruning approach called ZipLM, which achieves state-of-the-art performance, both in the post-training/one-shot setting, where retraining is not desirable, as well as in the popular gradual compression setting, where retraining is possible. We accomplish this via an inference-aware algorithm, which successfully balances the loss-runtime trade-off at each pruning step. By taking runtime into account, we avoid removing components that do not bring significant speedup gains. Additionally, our algorithm provides speedup guarantees for compressed models, a highly-desirable property in practical applications. \n\nWe summarize our contributions as follows:",
            "reference_string": "[256662263 | Kurtic et al. | 2023 | Citations: 26]"
        },
        {
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 33,
            "citation_count": 61,
            "influential_citation_count": 14,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.11983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2167834971",
                    "name": "Yongqi An"
                },
                {
                    "authorId": "2118489444",
                    "name": "Xu Zhao"
                },
                {
                    "authorId": "40418746",
                    "name": "Tao Yu"
                },
                {
                    "authorId": "2113727378",
                    "name": "Ming Tang"
                },
                {
                    "authorId": "2241943585",
                    "name": "Jinqiao Wang"
                }
            ],
            "abstract": "Network Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive\nStructured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP.",
            "corpus_id": 266362404,
            "sentences": [
                {
                    "corpus_id": "266362404",
                    "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
                    "text": "Network Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive\nStructured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP.",
                    "score": 0.5319784462385577,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                },
                {
                    "corpus_id": "266362404",
                    "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
                    "text": "Network pruning is a model compression technique that identifies and eliminates redundancy in the structure or parameters of a neural network, based on specific pruning metrics, and incorporates methods to recover model performance (LeCun, Denker, and Solla 1989;Hassibi, Stork, and Wolff 1993;Han et al. 2015). Pruning methods fall into two categories: unstructured pruning and structured pruning. Unstructured pruning is performed at the individual weight level, allowing for a large sparsity but failing to achieve real inference acceleration or storage reduction (Zafrir et al. 2021;Han, Mao, and Dally 2016). Within unstructured pruning, there exists a specialized variant known as semi-structured pruning. This approach enforces exactly N non-zero values in each block of M consecutive weights (Zhou et al. 2021). This approach has gained traction recently, particularly with support on newer NVIDIA hardware (Mishra et al. 2021). Structured pruning, by contrast, operates on entire rows or columns of weights, providing a more hardware-friendly solution that reduces storage requirements and enhances inference speed (Xia, Zhong, and Chen 2022;Molchanov et al. 2017). However, conventional structured pruning methods typically rely on retraining (sometimes iteratively) to regain the performance of the pruned model (Han et al. 2015;Tan and Motani 2020;Han, Mao, and Dally 2016). Such methods pose scalability challenges for billion-scale LLMs due to constraints on memory and computational resources. Therefore a retraining-free structured pruning method for LLMs is very critical.",
                    "score": 0.5121273017549195,
                    "section_title": "Related Works Network Pruning Methods",
                    "char_start_offset": 4505,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 311
                        },
                        {
                            "start": 312,
                            "end": 398
                        },
                        {
                            "start": 399,
                            "end": 613
                        },
                        {
                            "start": 614,
                            "end": 711
                        },
                        {
                            "start": 712,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1589
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 263,
                            "end": 294,
                            "matchedPaperCorpusId": "2238772"
                        },
                        {
                            "start": 294,
                            "end": 310,
                            "matchedPaperCorpusId": "2238772"
                        },
                        {
                            "start": 587,
                            "end": 612,
                            "matchedPaperCorpusId": "2134321"
                        },
                        {
                            "start": 915,
                            "end": 935,
                            "matchedPaperCorpusId": "17240902"
                        },
                        {
                            "start": 1124,
                            "end": 1151,
                            "matchedPaperCorpusId": "247922354"
                        },
                        {
                            "start": 1323,
                            "end": 1340,
                            "matchedPaperCorpusId": "2238772"
                        },
                        {
                            "start": 1340,
                            "end": 1360,
                            "matchedPaperCorpusId": "221082307"
                        },
                        {
                            "start": 1360,
                            "end": 1385,
                            "matchedPaperCorpusId": "2134321"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90283203125
                },
                {
                    "corpus_id": "266362404",
                    "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
                    "text": "Large Language Models (LLMs) (Brown et al. 2020;Touvron et al. 2023;Zhang et al. 2022;Scao et al. 2022) have recently achieved outstanding performance across various language benchmarks in NLP (Bommarito and Katz 2022;Bubeck et al. 2023;Wei et al. 2022), spurring a large number of open-source applications (Taori et al. 2023;Anand et al. 2023;Richards 2023). These remarkable capabilities typically come with a huge-scale model size with high inference costs. This makes it harder for more people to benefit from LLMs. Due to the computational resource constraints, most of the model compression methods in the pre-LLM era are no longer feasible for LLMs. Model compression methods for LLMs to date focus on model quantization (Dettmers et al. 2022;Xiao et al. 2023;Frantar et al. 2023;Dettmers et al. 2023) and unstructured pruning (Sun et al. 2023;Frantar and Alistarh 2023). \n\nStructured pruning (He and Xiao 2023), which prunes entire rows or columns of weights, offers a promising solution to the deployment challenges of LLMs. Unlike unstructured pruning, structured pruning reduces both parameters and inference time without relying on specific hardware, making it more widely applicable (Anwar, Hwang, and Sung 2017). For effective structured pruning, it's crucial to have a metric that captures the collective significance of an entire row or column. However, current unstructured pruning techniques for LLMs, as seen in methods like (Sun et al. 2023;Frantar and Alistarh 2023), primarily focus on the importance of individual elements of each row in isolation. This absence of structured metrics that evaluate entire rows or columns makes them less suitable for structured pruning. The recent LLM-Pruner (Ma, Fang, and Wang 2023) attempted structured pruning for LLMs, but its dependence on LoRA finetuning (Hu et al. 2021) creates a tough trade-off between high computation and effective pruning, limiting its use in larger models.",
                    "score": 0.7265949506355132,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 460
                        },
                        {
                            "start": 461,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 878
                        },
                        {
                            "start": 881,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1360
                        },
                        {
                            "start": 1361,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1943
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 48,
                            "end": 68,
                            "matchedPaperCorpusId": "221082307"
                        },
                        {
                            "start": 237,
                            "end": 253,
                            "matchedPaperCorpusId": "249674500"
                        },
                        {
                            "start": 307,
                            "end": 326,
                            "matchedPaperCorpusId": "221082307"
                        },
                        {
                            "start": 750,
                            "end": 767,
                            "matchedPaperCorpusId": "253708271"
                        },
                        {
                            "start": 1196,
                            "end": 1225,
                            "matchedPaperCorpusId": "7333079"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76513671875
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: Fluctuation-based Adaptive Structured Pruning for Large Language Models\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Yongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao Wang\n## Abstract\nNetwork Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive\nStructured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP.\n## Introduction\nLarge Language Models (LLMs) (Brown et al. 2020;Touvron et al. 2023;Zhang et al. 2022;Scao et al. 2022) have recently achieved outstanding performance across various language benchmarks in NLP (Bommarito and Katz 2022;Bubeck et al. 2023;Wei et al. 2022), spurring a large number of open-source applications (Taori et al. 2023;Anand et al. 2023;Richards 2023). These remarkable capabilities typically come with a huge-scale model size with high inference costs. This makes it harder for more people to benefit from LLMs. Due to the computational resource constraints, most of the model compression methods in the pre-LLM era are no longer feasible for LLMs. Model compression methods for LLMs to date focus on model quantization (Dettmers et al. 2022;Xiao et al. 2023;Frantar et al. 2023;Dettmers et al. 2023) and unstructured pruning (Sun et al. 2023;Frantar and Alistarh 2023). \n\nStructured pruning (He and Xiao 2023), which prunes entire rows or columns of weights, offers a promising solution to the deployment challenges of LLMs. Unlike unstructured pruning, structured pruning reduces both parameters and inference time without relying on specific hardware, making it more widely applicable (Anwar, Hwang, and Sung 2017). For effective structured pruning, it's crucial to have a metric that captures the collective significance of an entire row or column. However, current unstructured pruning techniques for LLMs, as seen in methods like (Sun et al. 2023;Frantar and Alistarh 2023), primarily focus on the importance of individual elements of each row in isolation. This absence of structured metrics that evaluate entire rows or columns makes them less suitable for structured pruning. The recent LLM-Pruner (Ma, Fang, and Wang 2023) attempted structured pruning for LLMs, but its dependence on LoRA finetuning (Hu et al. 2021) creates a tough trade-off between high computation and effective pruning, limiting its use in larger models.\n\n## Related Works Network Pruning Methods\nNetwork pruning is a model compression technique that identifies and eliminates redundancy in the structure or parameters of a neural network, based on specific pruning metrics, and incorporates methods to recover model performance (LeCun, Denker, and Solla 1989;Hassibi, Stork, and Wolff 1993;Han et al. 2015). Pruning methods fall into two categories: unstructured pruning and structured pruning. Unstructured pruning is performed at the individual weight level, allowing for a large sparsity but failing to achieve real inference acceleration or storage reduction (Zafrir et al. 2021;Han, Mao, and Dally 2016). Within unstructured pruning, there exists a specialized variant known as semi-structured pruning. This approach enforces exactly N non-zero values in each block of M consecutive weights (Zhou et al. 2021). This approach has gained traction recently, particularly with support on newer NVIDIA hardware (Mishra et al. 2021). Structured pruning, by contrast, operates on entire rows or columns of weights, providing a more hardware-friendly solution that reduces storage requirements and enhances inference speed (Xia, Zhong, and Chen 2022;Molchanov et al. 2017). However, conventional structured pruning methods typically rely on retraining (sometimes iteratively) to regain the performance of the pruned model (Han et al. 2015;Tan and Motani 2020;Han, Mao, and Dally 2016). Such methods pose scalability challenges for billion-scale LLMs due to constraints on memory and computational resources. Therefore a retraining-free structured pruning method for LLMs is very critical.",
            "reference_string": "[266362404 | An et al. | 2023 | Citations: 61]"
        },
        {
            "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 9,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.18110, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336957585",
                    "name": "Gui Ling"
                },
                {
                    "authorId": "2337041453",
                    "name": "Ziyang Wang"
                },
                {
                    "authorId": "2314344679",
                    "name": "Yuliang Yan"
                },
                {
                    "authorId": "2337389064",
                    "name": "Qingwen Liu"
                }
            ],
            "abstract": "Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results.",
            "corpus_id": 274992799,
            "sentences": [
                {
                    "corpus_id": "274992799",
                    "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
                    "text": "Large Language Models (LLMs) [1,2,3] have made significant strides in various natural language processing tasks, leading to the emergence of novel applications such as AI agents [4]. One of the factors contributing to the exceptional capabilities of LLMs is their massive parameter scales. However, these extensive parameters also introduce increased inference costs and deployment challenges, hindering the widespread application and adoption of LLMs. Accelerating inference for LLMs has become a focal point of current research. Model compression [5], as one of the strategies for inference acceleration, including techniques like pruning and quantization [6,7], has been extensively researched. Nevertheless, earlier model compression techniques, particularly model pruning, typically rely on heavy post-training to recover the model's capabilities, which typically involves retraining with the entire training dataset. Given the constraints of current computational resources, the above approaches are not feasible for LLMs. \n\nIn the domain of LLM pruning, recent studies have largely focused on unstructured (or semistructured) pruning [8], a method that shrinks models by selectively zeroing out weights considered non-critical. Despite its advancements, unstructured pruning falls short in substantially reducing parameter count, which is crucial for accelerating LLM inference as it is often bottlenecked on memory bandwidth and communication [9]. To accelerate inference speed, unstructured pruning models are often paired with specialized frameworks or hardware solutions. Conversely, structured pruning [10,11] effectively decreases the model's parameter count by systematically eliminating columns or rows from weight matrices, enabling significant improvements in inference speed, and reduce deployment cost on conventional hardware. Yet, structured pruning often entails more pronounced compromises in model performance, which poses a greater challenge. \n\nRecently, researchers have applied the classic Optimal Brain Surgeon (OBS) framework to the compression of LLMs. This approach includes parameter compensation which can mitigate the loss incurred during compression and reduce the dependence on post-training. The OBS framework is currently applied in the areas of unstructured pruning [12] and quantization [13] for LLMs. However, there exist some challenges in its application to structured pruning:",
                    "score": 0.5377063307875437,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 289
                        },
                        {
                            "start": 290,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 697
                        },
                        {
                            "start": 698,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 1028
                        },
                        {
                            "start": 1031,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1846
                        },
                        {
                            "start": 1847,
                            "end": 1967
                        },
                        {
                            "start": 1970,
                            "end": 2082
                        },
                        {
                            "start": 2083,
                            "end": 2228
                        },
                        {
                            "start": 2229,
                            "end": 2341
                        },
                        {
                            "start": 2342,
                            "end": 2420
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 549,
                            "end": 552,
                            "matchedPaperCorpusId": "211062209"
                        },
                        {
                            "start": 658,
                            "end": 661,
                            "matchedPaperCorpusId": "231740691"
                        },
                        {
                            "start": 661,
                            "end": 663,
                            "matchedPaperCorpusId": "232352683"
                        },
                        {
                            "start": 1451,
                            "end": 1454,
                            "matchedPaperCorpusId": "254096365"
                        },
                        {
                            "start": 1614,
                            "end": 1618,
                            "matchedPaperCorpusId": "7333079"
                        },
                        {
                            "start": 2305,
                            "end": 2309,
                            "matchedPaperCorpusId": "255372747"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90185546875
                },
                {
                    "corpus_id": "274992799",
                    "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
                    "text": "However, there exist some challenges in its application to structured pruning: \n\n\u2022 The OBS is a fine-grained compression framework that compresses one parameter at each iteration, whereas structured pruning has a minimum granularity of either a column or head. Directly applying the OBS framework will result in high numerical errors, impairing model performance. \u2022 The OBS is essentially a layer-wise compression method. It focuses on each individual layer, thus failing to allocate pruning ratios for each layer rationally using global information (such as global gradients). This is crucial for LLM structured pruning, which relies on a non-uniform strategy to reduce the impact on performance. \n\nTo address these issues, we propose a new structured pruning method for LLMs. We introduce Batched Greedy Pruning to achieve low-cost and rapid pruning for LLMs. Specifically, for attention heads, we propose grouped Cholesky decomposition to select nearly optimal heads for pruning in each iteration, thereby maintaining an approximately locally optimal pruning result. For Feed-Forward Networks (FFNs), we achieve near-optimal and efficient pruning results through Dynamic Group Size. Furthermore, since the OBS is essentially a layer-wise compression framework, we investigate the error accumulation phenomenon in layer-wise pruning and propose pruning by Incremental Pruning Ratio, a straightforward non-uniform strategy to control the pruning rate of each layer, further mitigating performance loss under a given overall pruning ratio. \n\nContribution. In this paper, we propose SlimGPT, a layer-wise pruning approach that extends the classical OBS framework to structured pruning for LLMs. The characteristics of SlimGPT can be summarized as follows: (i) Task-agnostic pruning scheme. Only a random sample of data from generic pre-training corpora is needed as a calibration set, and we can obtain a compressed model with most performance preserved; (ii) Low-cost, low-resource, and time-efficient compression scheme. The model can be compressed using just a single GPU, a few hundred of calibration data, and about one hour; (iii) A universal pruning method for Transformer-based models.",
                    "score": 0.554386606456799,
                    "section_title": "Introduction",
                    "char_start_offset": 2357,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 81,
                            "end": 260
                        },
                        {
                            "start": 261,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 697
                        },
                        {
                            "start": 700,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 861
                        },
                        {
                            "start": 862,
                            "end": 1069
                        },
                        {
                            "start": 1070,
                            "end": 1185
                        },
                        {
                            "start": 1186,
                            "end": 1539
                        },
                        {
                            "start": 1542,
                            "end": 1555
                        },
                        {
                            "start": 1556,
                            "end": 1693
                        },
                        {
                            "start": 1694,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 2021
                        },
                        {
                            "start": 2022,
                            "end": 2192
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83203125
                }
            ],
            "relevance_judgement": 0.90185546875,
            "relevance_judgment_input_expanded": "# Title: SlimGPT: Layer-wise Structured Pruning for Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Gui Ling, Ziyang Wang, Yuliang Yan, Qingwen Liu\n## Abstract\nLarge language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results.\n## Introduction\nLarge Language Models (LLMs) [1,2,3] have made significant strides in various natural language processing tasks, leading to the emergence of novel applications such as AI agents [4]. One of the factors contributing to the exceptional capabilities of LLMs is their massive parameter scales. However, these extensive parameters also introduce increased inference costs and deployment challenges, hindering the widespread application and adoption of LLMs. Accelerating inference for LLMs has become a focal point of current research. Model compression [5], as one of the strategies for inference acceleration, including techniques like pruning and quantization [6,7], has been extensively researched. Nevertheless, earlier model compression techniques, particularly model pruning, typically rely on heavy post-training to recover the model's capabilities, which typically involves retraining with the entire training dataset. Given the constraints of current computational resources, the above approaches are not feasible for LLMs. \n\nIn the domain of LLM pruning, recent studies have largely focused on unstructured (or semistructured) pruning [8], a method that shrinks models by selectively zeroing out weights considered non-critical. Despite its advancements, unstructured pruning falls short in substantially reducing parameter count, which is crucial for accelerating LLM inference as it is often bottlenecked on memory bandwidth and communication [9]. To accelerate inference speed, unstructured pruning models are often paired with specialized frameworks or hardware solutions. Conversely, structured pruning [10,11] effectively decreases the model's parameter count by systematically eliminating columns or rows from weight matrices, enabling significant improvements in inference speed, and reduce deployment cost on conventional hardware. Yet, structured pruning often entails more pronounced compromises in model performance, which poses a greater challenge. \n\nRecently, researchers have applied the classic Optimal Brain Surgeon (OBS) framework to the compression of LLMs. This approach includes parameter compensation which can mitigate the loss incurred during compression and reduce the dependence on post-training. The OBS framework is currently applied in the areas of unstructured pruning [12] and quantization [13] for LLMs. However, there exist some challenges in its application to structured pruning:\n...\nHowever, there exist some challenges in its application to structured pruning: \n\n\u2022 The OBS is a fine-grained compression framework that compresses one parameter at each iteration, whereas structured pruning has a minimum granularity of either a column or head. Directly applying the OBS framework will result in high numerical errors, impairing model performance. \u2022 The OBS is essentially a layer-wise compression method. It focuses on each individual layer, thus failing to allocate pruning ratios for each layer rationally using global information (such as global gradients). This is crucial for LLM structured pruning, which relies on a non-uniform strategy to reduce the impact on performance. \n\nTo address these issues, we propose a new structured pruning method for LLMs. We introduce Batched Greedy Pruning to achieve low-cost and rapid pruning for LLMs. Specifically, for attention heads, we propose grouped Cholesky decomposition to select nearly optimal heads for pruning in each iteration, thereby maintaining an approximately locally optimal pruning result. For Feed-Forward Networks (FFNs), we achieve near-optimal and efficient pruning results through Dynamic Group Size. Furthermore, since the OBS is essentially a layer-wise compression framework, we investigate the error accumulation phenomenon in layer-wise pruning and propose pruning by Incremental Pruning Ratio, a straightforward non-uniform strategy to control the pruning rate of each layer, further mitigating performance loss under a given overall pruning ratio. \n\nContribution. In this paper, we propose SlimGPT, a layer-wise pruning approach that extends the classical OBS framework to structured pruning for LLMs. The characteristics of SlimGPT can be summarized as follows: (i) Task-agnostic pruning scheme. Only a random sample of data from generic pre-training corpora is needed as a calibration set, and we can obtain a compressed model with most performance preserved; (ii) Low-cost, low-resource, and time-efficient compression scheme. The model can be compressed using just a single GPU, a few hundred of calibration data, and about one hour; (iii) A universal pruning method for Transformer-based models.",
            "reference_string": "[274992799 | Ling et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?",
            "venue": "",
            "year": 2025,
            "reference_count": 34,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.07289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2360373404",
                    "name": "Stanislas Laborde"
                },
                {
                    "authorId": "2360359994",
                    "name": "Martin Cousseau"
                },
                {
                    "authorId": "40605834",
                    "name": "Antoun Yaacoub"
                },
                {
                    "authorId": "2266474578",
                    "name": "Lionel Prevost"
                }
            ],
            "abstract": "The exponential growth in Large Language Model (LLM) deployment has intensified the need for efficient model compression techniques to reduce computational and memory costs. While pruning and quantization have shown promise, their combined potential remains largely unexplored. In this paper, we examine joint compression and how strategically combining pruning and quantization could yield superior performance-to-compression ratios compared to single-method approaches. Recognizing the challenges in accurately assessing LLM performance, we address key limitations of previous evaluation frameworks and introduce the Semantic Retention Compression Rate (SrCr), a novel metric that quantifies the trade-off between model compression and semantic preservation, facilitating the optimization of pruning-quantization configurations. Experiments demonstrate that our recommended combination achieves, on average, a 20% performance increase compared to an equivalent quantization-only model at the same theoretical compression rate.",
            "corpus_id": 278501529,
            "sentences": [
                {
                    "corpus_id": "278501529",
                    "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?",
                    "text": "While training-aware pruning traditionally achieved better results, its computational demands proved impractical for billion-parameter models [3]. This led to efficient post-training approaches like SparseGPT [8], which enables one-shot pruning through sparse regression. Unstructured pruning offers maximum theoretical compression by removing individual weights, but often results in irregular sparsity patterns that are challenging to accelerate on current hardware. Semi-structured approaches, like N:M sparsity patterns [10], balance compression rates with hardware efficiency by enforcing regular pruning patterns, where N out of every M consecutive weights are pruned. Structured pruning [11] takes this further by removing entire structures, channels, or attention heads, with recent work showing that up to 50% of attention layers in large models can be removed while preserving performance [12].",
                    "score": 0.5670426122199868,
                    "section_title": "A. Pruning",
                    "char_start_offset": 4253,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 468
                        },
                        {
                            "start": 469,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 904
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 142,
                            "end": 145,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 209,
                            "end": 212,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 524,
                            "end": 528,
                            "matchedPaperCorpusId": "231847094"
                        },
                        {
                            "start": 694,
                            "end": 698,
                            "matchedPaperCorpusId": "258823276"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89892578125
                }
            ],
            "relevance_judgement": 0.89892578125,
            "relevance_judgment_input_expanded": "# Title: Semantic Retention and Extreme Compression in LLMs: Can We Have Both?\n# Venue: \n# Authors: Stanislas Laborde, Martin Cousseau, Antoun Yaacoub, Lionel Prevost\n## Abstract\nThe exponential growth in Large Language Model (LLM) deployment has intensified the need for efficient model compression techniques to reduce computational and memory costs. While pruning and quantization have shown promise, their combined potential remains largely unexplored. In this paper, we examine joint compression and how strategically combining pruning and quantization could yield superior performance-to-compression ratios compared to single-method approaches. Recognizing the challenges in accurately assessing LLM performance, we address key limitations of previous evaluation frameworks and introduce the Semantic Retention Compression Rate (SrCr), a novel metric that quantifies the trade-off between model compression and semantic preservation, facilitating the optimization of pruning-quantization configurations. Experiments demonstrate that our recommended combination achieves, on average, a 20% performance increase compared to an equivalent quantization-only model at the same theoretical compression rate.\n## A. Pruning\nWhile training-aware pruning traditionally achieved better results, its computational demands proved impractical for billion-parameter models [3]. This led to efficient post-training approaches like SparseGPT [8], which enables one-shot pruning through sparse regression. Unstructured pruning offers maximum theoretical compression by removing individual weights, but often results in irregular sparsity patterns that are challenging to accelerate on current hardware. Semi-structured approaches, like N:M sparsity patterns [10], balance compression rates with hardware efficiency by enforcing regular pruning patterns, where N out of every M consecutive weights are pruned. Structured pruning [11] takes this further by removing entire structures, channels, or attention heads, with recent work showing that up to 50% of attention layers in large models can be removed while preserving performance [12].",
            "reference_string": "[278501529 | Laborde et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Iterative Activation-based Structured Pruning",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1995855",
                    "name": "Kaiqi Zhao"
                },
                {
                    "authorId": "101682296",
                    "name": "Animesh Jain"
                },
                {
                    "authorId": "2152527896",
                    "name": "Ming Zhao"
                }
            ],
            "abstract": "Deploying complex deep learning models on edge devices is challenging because they have substantial compute and memory resource requirements, whereas edge devices' resource budget is limited. To solve this problem, extensive pruning techniques have been proposed for compressing networks. Recent advances based on the Lottery Ticket Hypothesis (LTH) show that iterative model pruning tends to produce smaller and more accurate models. However, LTH research focuses on unstructured pruning, which is hardware-inefficient and difficult to accelerate on hardware platforms. In this paper, we investigate iterative pruning in the context of structured pruning because structurally pruned models map well on commodity hardware. We find that directly applying a structured weight-based pruning technique iteratively, called iterative L1-norm based pruning (ILP), does not produce accurate pruned models. To solve this problem, we propose two activation-based pruning methods, Iterative Activation-based Pruning (IAP) and Adaptive Iterative Activation-based Pruning (AIAP). We observe that, with only 1% accuracy loss, IAP and AIAP achieve 7.75X and 15.88$X compression on LeNet-5, and 1.25X and 1.71X compression on ResNet-50, whereas ILP achieves 4.77X and 1.13X, respectively.",
            "corpus_id": 246276158,
            "sentences": [
                {
                    "corpus_id": "246276158",
                    "title": "Iterative Activation-based Structured Pruning",
                    "text": "Like other compression techniques, pruning has a performance-accuracy tradeoff, i.e, as we compress the model more aggressively, we achieve better hardware performance (higher reductions in computation and memory requirements) but suffer higher accuracy losses. In this paper, we measure performance across two dimensions. First, memory footprint, i.e., memory space used to store parameters, and second, inference latency, i.e., time to run one inference. Pruning can be broadly categorized into two classes, namely, unstructured pruning and structured pruning. The two techniques take different approaches on memory footprint reduction, and impact inference latency and accuracy differently. \n\nUnstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al. 2018), e.g, (Hill et al. 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al. 2018). \n\ncan choose either structured or unstructured pruning techniques.",
                    "score": 0.619585882014117,
                    "section_title": "Unstructured vs. Structured Pruning",
                    "char_start_offset": 915,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 322
                        },
                        {
                            "start": 323,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 693
                        },
                        {
                            "start": 696,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 1011
                        },
                        {
                            "start": 1012,
                            "end": 1419
                        },
                        {
                            "start": 1422,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 1969
                        },
                        {
                            "start": 1972,
                            "end": 2036
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1250,
                            "end": 1265,
                            "matchedPaperCorpusId": "52048008"
                        },
                        {
                            "start": 1273,
                            "end": 1291,
                            "matchedPaperCorpusId": "13876417"
                        },
                        {
                            "start": 1952,
                            "end": 1968,
                            "matchedPaperCorpusId": "52048008"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89111328125
                }
            ],
            "relevance_judgement": 0.89111328125,
            "relevance_judgment_input_expanded": "# Title: Iterative Activation-based Structured Pruning\n# Venue: arXiv.org\n# Authors: Kaiqi Zhao, Animesh Jain, Ming Zhao\n## Abstract\nDeploying complex deep learning models on edge devices is challenging because they have substantial compute and memory resource requirements, whereas edge devices' resource budget is limited. To solve this problem, extensive pruning techniques have been proposed for compressing networks. Recent advances based on the Lottery Ticket Hypothesis (LTH) show that iterative model pruning tends to produce smaller and more accurate models. However, LTH research focuses on unstructured pruning, which is hardware-inefficient and difficult to accelerate on hardware platforms. In this paper, we investigate iterative pruning in the context of structured pruning because structurally pruned models map well on commodity hardware. We find that directly applying a structured weight-based pruning technique iteratively, called iterative L1-norm based pruning (ILP), does not produce accurate pruned models. To solve this problem, we propose two activation-based pruning methods, Iterative Activation-based Pruning (IAP) and Adaptive Iterative Activation-based Pruning (AIAP). We observe that, with only 1% accuracy loss, IAP and AIAP achieve 7.75X and 15.88$X compression on LeNet-5, and 1.25X and 1.71X compression on ResNet-50, whereas ILP achieves 4.77X and 1.13X, respectively.\n## Unstructured vs. Structured Pruning\nLike other compression techniques, pruning has a performance-accuracy tradeoff, i.e, as we compress the model more aggressively, we achieve better hardware performance (higher reductions in computation and memory requirements) but suffer higher accuracy losses. In this paper, we measure performance across two dimensions. First, memory footprint, i.e., memory space used to store parameters, and second, inference latency, i.e., time to run one inference. Pruning can be broadly categorized into two classes, namely, unstructured pruning and structured pruning. The two techniques take different approaches on memory footprint reduction, and impact inference latency and accuracy differently. \n\nUnstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al. 2018), e.g, (Hill et al. 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al. 2018). \n\ncan choose either structured or unstructured pruning techniques.",
            "reference_string": "[246276158 | Zhao et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Large Language Model Compression via the Nested Activation-Aware Decomposition",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.17101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316872781",
                    "name": "Jun Lu"
                },
                {
                    "authorId": "2351716503",
                    "name": "Tianyi Xu"
                },
                {
                    "authorId": "2316639822",
                    "name": "Bill Ding"
                },
                {
                    "authorId": "2316673285",
                    "name": "David Li"
                },
                {
                    "authorId": "2316671224",
                    "name": "Yu Kang"
                }
            ],
            "abstract": "In this paper, we tackle the critical challenge of compressing large language models (LLMs) to facilitate their practical deployment and broader adoption. We introduce a novel post-training compression paradigm that focuses on low-rank decomposition of LLM weights. Our analysis identifies two main challenges in this task: the variability in LLM activation distributions and handling unseen activations from different datasets and models. To address these challenges, we propose a nested activation-aware framework (NSVD) for LLMs, a training-free approach designed to enhance the accuracy of low-rank decompositions by managing activation outliers through transforming the weight matrix based on activation distribution and the original weight matrix. This method allows for the absorption of outliers into the transformed weight matrix, improving decomposition accuracy. Our comprehensive evaluation across eight datasets and six models from three distinct LLM families demonstrates the superiority of NSVD over current state-of-the-art methods, especially at medium to large compression ratios or in multilingual and multitask settings.",
            "corpus_id": 277244079,
            "sentences": [
                {
                    "corpus_id": "277244079",
                    "title": "Large Language Model Compression via the Nested Activation-Aware Decomposition",
                    "text": "However, the irregular sparsification achieved through unstructured pruning often fails to achieve the desired speedup or memory savings. Structured pruning, in contrast, removes entire channels or components from LLMs, making it easier to implement on hardware but potentially leading to significant accuracy degradation, especially under high compression ratios as seen with LLM-Pruner (Ma et al., 2023). \n\nQuantization techniques reduce the precision of weight matrices, offering limited compression options typically ranging from 3 to 8 bits, which may not fully utilize available memory budgets. GPTQ is a notable example that uses layer-wise quantization and updates weights with inverse Hessian information (Frantar et al., 2022). \n\n\u25b6 Low-rank approximation for LLMs. Despite the popularity of low-rank factorization as a neural network compression technique, its application in LLMs remains relatively underexplored. This gap is addressed through the introduction of novel low-rank decomposition methods designed specifically for LLMs, such as ASVD (Yuan et al., 2023). To be more specific, standard SVD focuses on compressing original weight matrices without considering parameter importance, which can result in larger compression errors. To address this, FWSVD incorporates Fisher information to weigh parameter importance, ASVD scales the weight matrix by a diagonal matrix representing input channel impact to account for activation distribution effects, while SVD-LLM further scales the weight matrix by a Cholesky decomposition of the activation matrix to upper bound the reconstruction error (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024). However, all of these methods face challenges in balancing compression efficiency and accuracy, especially under medium to high compression scenarios for datasets with different languages or tasks. \n\nDespite advancements such as the introduction of rank-adaptive methods like FWSVD, ASVD, and SVD-LLM, which aim to optimize compression efficiency and accuracy by considering parameter importance and activation distribution effects, significant challenges remain. These methods struggle with severe accuracy degradation under medium to high compression ratios due to unawareness of the discrepancy between different datasets.",
                    "score": 0.5158140781148913,
                    "section_title": "Related Work",
                    "char_start_offset": 5237,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 406
                        },
                        {
                            "start": 409,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 737
                        },
                        {
                            "start": 740,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1248
                        },
                        {
                            "start": 1249,
                            "end": 1663
                        },
                        {
                            "start": 1664,
                            "end": 1861
                        },
                        {
                            "start": 1864,
                            "end": 2127
                        },
                        {
                            "start": 2128,
                            "end": 2289
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89013671875
                }
            ],
            "relevance_judgement": 0.89013671875,
            "relevance_judgment_input_expanded": "# Title: Large Language Model Compression via the Nested Activation-Aware Decomposition\n# Venue: arXiv.org\n# Authors: Jun Lu, Tianyi Xu, Bill Ding, David Li, Yu Kang\n## Abstract\nIn this paper, we tackle the critical challenge of compressing large language models (LLMs) to facilitate their practical deployment and broader adoption. We introduce a novel post-training compression paradigm that focuses on low-rank decomposition of LLM weights. Our analysis identifies two main challenges in this task: the variability in LLM activation distributions and handling unseen activations from different datasets and models. To address these challenges, we propose a nested activation-aware framework (NSVD) for LLMs, a training-free approach designed to enhance the accuracy of low-rank decompositions by managing activation outliers through transforming the weight matrix based on activation distribution and the original weight matrix. This method allows for the absorption of outliers into the transformed weight matrix, improving decomposition accuracy. Our comprehensive evaluation across eight datasets and six models from three distinct LLM families demonstrates the superiority of NSVD over current state-of-the-art methods, especially at medium to large compression ratios or in multilingual and multitask settings.\n## Related Work\nHowever, the irregular sparsification achieved through unstructured pruning often fails to achieve the desired speedup or memory savings. Structured pruning, in contrast, removes entire channels or components from LLMs, making it easier to implement on hardware but potentially leading to significant accuracy degradation, especially under high compression ratios as seen with LLM-Pruner (Ma et al., 2023). \n\nQuantization techniques reduce the precision of weight matrices, offering limited compression options typically ranging from 3 to 8 bits, which may not fully utilize available memory budgets. GPTQ is a notable example that uses layer-wise quantization and updates weights with inverse Hessian information (Frantar et al., 2022). \n\n\u25b6 Low-rank approximation for LLMs. Despite the popularity of low-rank factorization as a neural network compression technique, its application in LLMs remains relatively underexplored. This gap is addressed through the introduction of novel low-rank decomposition methods designed specifically for LLMs, such as ASVD (Yuan et al., 2023). To be more specific, standard SVD focuses on compressing original weight matrices without considering parameter importance, which can result in larger compression errors. To address this, FWSVD incorporates Fisher information to weigh parameter importance, ASVD scales the weight matrix by a diagonal matrix representing input channel impact to account for activation distribution effects, while SVD-LLM further scales the weight matrix by a Cholesky decomposition of the activation matrix to upper bound the reconstruction error (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024). However, all of these methods face challenges in balancing compression efficiency and accuracy, especially under medium to high compression scenarios for datasets with different languages or tasks. \n\nDespite advancements such as the introduction of rank-adaptive methods like FWSVD, ASVD, and SVD-LLM, which aim to optimize compression efficiency and accuracy by considering parameter importance and activation distribution effects, significant challenges remain. These methods struggle with severe accuracy degradation under medium to high compression ratios due to unawareness of the discrepancy between different datasets.",
            "reference_string": "[277244079 | Lu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 63,
            "influential_citation_count": 9,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.07378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2272325489",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2149514527",
                    "name": "Yu Zheng"
                },
                {
                    "authorId": "2240538849",
                    "name": "Zhongwei Wan"
                },
                {
                    "authorId": "144661656",
                    "name": "Mi Zhang"
                }
            ],
            "abstract": "The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitates LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weights after SVD truncation. In this work, we propose SVD-LLM, a SVD-based post-training LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening technique to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a parameter update with sequential low-rank approximation to compensate for the accuracy degradation after SVD compression. We evaluate SVD-LLM on 10 datasets and seven models from three different LLM families at three different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios. Our code is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM",
            "corpus_id": 268364208,
            "sentences": [
                {
                    "corpus_id": "268364208",
                    "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression",
                    "text": "Large Language Model Compression: LLMs in general contain billion-scale parameters. Applying conventional model compression methods for LLMs is unfeasible as they necessitate resourceintensive retraining. Given that, post-training methods that avoid retraining in the compression process have been proposed. In general, these methods can be grouped into four categories: unstructured pruning, structured pruning, quantization, and low-rank approximation. Specifically, unstructured pruning methods (Frantar and Alistarh, 2023) set the individual weights of an LLM to zero without changing its shape. However, irregular sparsification of unstructured pruning is difficult to achieve the desired speedup or memory saving. Unlike unstructured pruning, structured pruning methods (Ma et al., 2023;Ashkboos et al., 2024;Zhong et al., 2024) remove entire channels or other structured components from LLMs, making them easier to implement on hardware. One notable contribution is LLM-Pruner (Ma et al., 2023), which groups weight matrices based on their dependency and assigns the pruning ratio to each group based on the estimated importance. Quantization methods (Lin et al., 2024) compress models by reducing the precision of weight matrices of the LLM. However, similar to unstructured pruning, quantization is also difficult to achieve the desired inference speedup due to the lack of hardware support and efficient kernels for low-precision computation (Lin et al., 2024). Recent studies including PB-LLM (Yuan et al., 2024) and BiLLM (Huang et al., 2024) push the frontier to 1-bit quantization. Nevertheless, these approaches often lead to severe accuracy degradation. \n\nSVD for Language Model Compression: Singular Value Decomposition (SVD) is a widely used low-rank approximation technique to reduce matrix size by approximating a matrix with two smaller low-ranking matrices (Golub et al., 1987). Given that, SVD is commonly used for model compression. \n\nFor instance, DRONE (Chen et al., 2021) achieves optimal SVD compression for small language models such as BERT.",
                    "score": 0.6588999979346214,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 4739,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 83
                        },
                        {
                            "start": 84,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 454
                        },
                        {
                            "start": 455,
                            "end": 599
                        },
                        {
                            "start": 600,
                            "end": 719
                        },
                        {
                            "start": 720,
                            "end": 944
                        },
                        {
                            "start": 945,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1669
                        },
                        {
                            "start": 1672,
                            "end": 1900
                        },
                        {
                            "start": 1901,
                            "end": 1956
                        },
                        {
                            "start": 1959,
                            "end": 2071
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 498,
                            "end": 526,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 793,
                            "end": 815,
                            "matchedPaperCorpusId": "267301573"
                        },
                        {
                            "start": 1504,
                            "end": 1523,
                            "matchedPaperCorpusId": "263333921"
                        },
                        {
                            "start": 1534,
                            "end": 1554,
                            "matchedPaperCorpusId": "267523201"
                        },
                        {
                            "start": 1879,
                            "end": 1899,
                            "matchedPaperCorpusId": "121324775"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8857421875
                }
            ],
            "relevance_judgement": 0.8857421875,
            "relevance_judgment_input_expanded": "# Title: SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression\n# Venue: arXiv.org\n# Authors: Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang\n## Abstract\nThe advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitates LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weights after SVD truncation. In this work, we propose SVD-LLM, a SVD-based post-training LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening technique to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a parameter update with sequential low-rank approximation to compensate for the accuracy degradation after SVD compression. We evaluate SVD-LLM on 10 datasets and seven models from three different LLM families at three different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios. Our code is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM\n## RELATED WORK\nLarge Language Model Compression: LLMs in general contain billion-scale parameters. Applying conventional model compression methods for LLMs is unfeasible as they necessitate resourceintensive retraining. Given that, post-training methods that avoid retraining in the compression process have been proposed. In general, these methods can be grouped into four categories: unstructured pruning, structured pruning, quantization, and low-rank approximation. Specifically, unstructured pruning methods (Frantar and Alistarh, 2023) set the individual weights of an LLM to zero without changing its shape. However, irregular sparsification of unstructured pruning is difficult to achieve the desired speedup or memory saving. Unlike unstructured pruning, structured pruning methods (Ma et al., 2023;Ashkboos et al., 2024;Zhong et al., 2024) remove entire channels or other structured components from LLMs, making them easier to implement on hardware. One notable contribution is LLM-Pruner (Ma et al., 2023), which groups weight matrices based on their dependency and assigns the pruning ratio to each group based on the estimated importance. Quantization methods (Lin et al., 2024) compress models by reducing the precision of weight matrices of the LLM. However, similar to unstructured pruning, quantization is also difficult to achieve the desired inference speedup due to the lack of hardware support and efficient kernels for low-precision computation (Lin et al., 2024). Recent studies including PB-LLM (Yuan et al., 2024) and BiLLM (Huang et al., 2024) push the frontier to 1-bit quantization. Nevertheless, these approaches often lead to severe accuracy degradation. \n\nSVD for Language Model Compression: Singular Value Decomposition (SVD) is a widely used low-rank approximation technique to reduce matrix size by approximating a matrix with two smaller low-ranking matrices (Golub et al., 1987). Given that, SVD is commonly used for model compression. \n\nFor instance, DRONE (Chen et al., 2021) achieves optimal SVD compression for small language models such as BERT.",
            "reference_string": "[268364208 | Wang et al. | 2024 | Citations: 63]"
        },
        {
            "title": "Reconstruct the Pruned Model without Any Retraining",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 63,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.13331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267730543",
                    "name": "Pingjie Wang"
                },
                {
                    "authorId": "9069836",
                    "name": "Ziqing Fan"
                },
                {
                    "authorId": "2176837980",
                    "name": "Shengchao Hu"
                },
                {
                    "authorId": "2255052564",
                    "name": "Zhe Chen"
                },
                {
                    "authorId": "2238120662",
                    "name": "Yanfeng Wang"
                },
                {
                    "authorId": "2257318223",
                    "name": "Yu Wang"
                }
            ],
            "abstract": "Structured pruning is a promising hardware-friendly compression technique for large language models (LLMs), which is expected to be retraining-free to avoid the enormous retraining cost. This retraining-free paradigm involves (1) pruning criteria to define the architecture and (2) distortion reconstruction to restore performance. However, existing methods often emphasize pruning criteria while using reconstruction techniques that are specific to certain modules or criteria, resulting in limited generalizability. To address this, we introduce the Linear Interpolation-based Adaptive Reconstruction (LIAR) framework, which is both efficient and effective. LIAR does not require back-propagation or retraining and is compatible with various pruning criteria and modules. By applying linear interpolation to the preserved weights, LIAR minimizes reconstruction error and effectively reconstructs the pruned output. Our evaluations on benchmarks such as GLUE, SQuAD, WikiText, and common sense reasoning show that LIAR enables a BERT model to maintain 98% accuracy even after removing 50% of its parameters and achieves top performance for LLaMA in just a few minutes.",
            "corpus_id": 271270022,
            "sentences": [
                {
                    "corpus_id": "271270022",
                    "title": "Reconstruct the Pruned Model without Any Retraining",
                    "text": "Network pruning is a widely applicable compression technique, whose key point is to remove the redundant weight or modules from the original network [18] and reserve the salient ones [11,12].It is broadly categorized from the granularity aspect into structured and unstructured pruning.Unstructured pruning [19,20,21] performs at the individual weight level, which brings about larger sparsity but fails to accelerate the model and reduce the storage cost without requiring additional hardware support.By contrast, structured pruning [22,23,24] removes a group of weights, such as an entire channel, head, layer, and so on, therefore providing a more hardware-friendly solution, enhancing the lower inference latency and memory demands, so we focus on structured pruning in this paper.\n\nThe conventional retraining-based paradigm involves compressing the original model using various criteria followed by retraining to restore performance [25,26,27,28].However, as the size and complexity of LLMs rapidly increase [29,30,31], this conventional approach becomes impractical and costly, prompting the need for retraining-free compression techniques.Recent developments in this area have primarily centered around quantization [32,33,34] and have expanded to include pruning methods [13,15,14] that eliminate the need for retraining.In this paper, our work targets enhancing the performance of the retraining-free pruning paradigm, which can reduce the model size, lower the memory consumption, accelerate the inference, and be orthogonal and compatible with quantization for further compression simultaneously.",
                    "score": 0.5339086214858795,
                    "section_title": "Network Pruning for Language Models",
                    "char_start_offset": 4907,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 191,
                            "end": 286
                        },
                        {
                            "start": 286,
                            "end": 502
                        },
                        {
                            "start": 502,
                            "end": 785
                        },
                        {
                            "start": 787,
                            "end": 953
                        },
                        {
                            "start": 953,
                            "end": 1147
                        },
                        {
                            "start": 1147,
                            "end": 1330
                        },
                        {
                            "start": 1330,
                            "end": 1608
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 183,
                            "end": 187,
                            "matchedPaperCorpusId": "7785881"
                        },
                        {
                            "start": 187,
                            "end": 190,
                            "matchedPaperCorpusId": "61815367"
                        },
                        {
                            "start": 314,
                            "end": 317,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1014,
                            "end": 1018,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1280,
                            "end": 1284,
                            "matchedPaperCorpusId": "248266822"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8798828125
                }
            ],
            "relevance_judgement": 0.8798828125,
            "relevance_judgment_input_expanded": "# Title: Reconstruct the Pruned Model without Any Retraining\n# Venue: arXiv.org\n# Authors: Pingjie Wang, Ziqing Fan, Shengchao Hu, Zhe Chen, Yanfeng Wang, Yu Wang\n## Abstract\nStructured pruning is a promising hardware-friendly compression technique for large language models (LLMs), which is expected to be retraining-free to avoid the enormous retraining cost. This retraining-free paradigm involves (1) pruning criteria to define the architecture and (2) distortion reconstruction to restore performance. However, existing methods often emphasize pruning criteria while using reconstruction techniques that are specific to certain modules or criteria, resulting in limited generalizability. To address this, we introduce the Linear Interpolation-based Adaptive Reconstruction (LIAR) framework, which is both efficient and effective. LIAR does not require back-propagation or retraining and is compatible with various pruning criteria and modules. By applying linear interpolation to the preserved weights, LIAR minimizes reconstruction error and effectively reconstructs the pruned output. Our evaluations on benchmarks such as GLUE, SQuAD, WikiText, and common sense reasoning show that LIAR enables a BERT model to maintain 98% accuracy even after removing 50% of its parameters and achieves top performance for LLaMA in just a few minutes.\n## Network Pruning for Language Models\nNetwork pruning is a widely applicable compression technique, whose key point is to remove the redundant weight or modules from the original network [18] and reserve the salient ones [11,12].It is broadly categorized from the granularity aspect into structured and unstructured pruning.Unstructured pruning [19,20,21] performs at the individual weight level, which brings about larger sparsity but fails to accelerate the model and reduce the storage cost without requiring additional hardware support.By contrast, structured pruning [22,23,24] removes a group of weights, such as an entire channel, head, layer, and so on, therefore providing a more hardware-friendly solution, enhancing the lower inference latency and memory demands, so we focus on structured pruning in this paper.\n\nThe conventional retraining-based paradigm involves compressing the original model using various criteria followed by retraining to restore performance [25,26,27,28].However, as the size and complexity of LLMs rapidly increase [29,30,31], this conventional approach becomes impractical and costly, prompting the need for retraining-free compression techniques.Recent developments in this area have primarily centered around quantization [32,33,34] and have expanded to include pruning methods [13,15,14] that eliminate the need for retraining.In this paper, our work targets enhancing the performance of the retraining-free pruning paradigm, which can reduce the model size, lower the memory consumption, accelerate the inference, and be orthogonal and compatible with quantization for further compression simultaneously.",
            "reference_string": "[271270022 | Wang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 10,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17481, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "150110431",
                    "name": "Gongfan Fang"
                },
                {
                    "authorId": "1989015",
                    "name": "Hongxu Yin"
                },
                {
                    "authorId": "31225166",
                    "name": "Saurav Muralidharan"
                },
                {
                    "authorId": "2273650910",
                    "name": "Greg Heinrich"
                },
                {
                    "authorId": "2322991313",
                    "name": "Jeff Pool"
                },
                {
                    "authorId": "2273651410",
                    "name": "Jan Kautz"
                },
                {
                    "authorId": "2824500",
                    "name": "Pavlo Molchanov"
                },
                {
                    "authorId": "2322993154",
                    "name": "Xinchao Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at https://github.com/NVlabs/MaskLLM.",
            "corpus_id": 272910976,
            "sentences": [
                {
                    "corpus_id": "272910976",
                    "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
                    "text": "Pruning Large Language Models. Network Pruning [15,30,17,18,41] have been proven an efficient approach to compress pre-trained language models via the removal of redundant parameters. \n\nAccording to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning [26,43,24], Unstructured Pruning [17,15], and Semi-Structured Pruning [12,38,29,32,33]. Structured pruning physically eliminates substructures like attention heads [26], embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters [26,43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy [12,38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32,33,12]. \n\nIn this study, we focus on N:M semi-structured sparsity within Large Language Models and present a learnable framework to obtain high-quality masks via end-to-end training. \n\nLearnable Semi-Structured Sparsity. On another hand, a burgeoning interest exists in developing learnable masks [49,25,47], especially in the field of vision models. Markedly contrasted with traditional one-shot pruning methods that rely on a predetermined metric of importance, learnable sparsity can fully leverage the rich information in training data, enabling the identification of more effective sparsity masks.",
                    "score": 0.5938160519062461,
                    "section_title": "Related Works",
                    "char_start_offset": 5105,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 30
                        },
                        {
                            "start": 31,
                            "end": 183
                        },
                        {
                            "start": 186,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 761
                        },
                        {
                            "start": 762,
                            "end": 965
                        },
                        {
                            "start": 966,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1476
                        },
                        {
                            "start": 1479,
                            "end": 1651
                        },
                        {
                            "start": 1654,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1819
                        },
                        {
                            "start": 1820,
                            "end": 2071
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 51,
                            "end": 54,
                            "matchedPaperCorpusId": "195657904"
                        },
                        {
                            "start": 54,
                            "end": 57,
                            "matchedPaperCorpusId": "2238772"
                        },
                        {
                            "start": 57,
                            "end": 60,
                            "matchedPaperCorpusId": "20157893"
                        },
                        {
                            "start": 60,
                            "end": 63,
                            "matchedPaperCorpusId": "204009154"
                        },
                        {
                            "start": 304,
                            "end": 308,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 311,
                            "end": 314,
                            "matchedPaperCorpusId": "260815690"
                        },
                        {
                            "start": 337,
                            "end": 341,
                            "matchedPaperCorpusId": "2238772"
                        },
                        {
                            "start": 374,
                            "end": 378,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 387,
                            "end": 390,
                            "matchedPaperCorpusId": "245002847"
                        },
                        {
                            "start": 468,
                            "end": 472,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 748,
                            "end": 752,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 948,
                            "end": 952,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1469,
                            "end": 1472,
                            "matchedPaperCorpusId": "245002847"
                        },
                        {
                            "start": 1472,
                            "end": 1475,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1770,
                            "end": 1773,
                            "matchedPaperCorpusId": "256503843"
                        },
                        {
                            "start": 1773,
                            "end": 1776,
                            "matchedPaperCorpusId": "249642429"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8720703125
                }
            ],
            "relevance_judgement": 0.8720703125,
            "relevance_judgment_input_expanded": "# Title: MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov, Xinchao Wang\n## Abstract\nLarge Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at https://github.com/NVlabs/MaskLLM.\n## Related Works\nPruning Large Language Models. Network Pruning [15,30,17,18,41] have been proven an efficient approach to compress pre-trained language models via the removal of redundant parameters. \n\nAccording to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning [26,43,24], Unstructured Pruning [17,15], and Semi-Structured Pruning [12,38,29,32,33]. Structured pruning physically eliminates substructures like attention heads [26], embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters [26,43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy [12,38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32,33,12]. \n\nIn this study, we focus on N:M semi-structured sparsity within Large Language Models and present a learnable framework to obtain high-quality masks via end-to-end training. \n\nLearnable Semi-Structured Sparsity. On another hand, a burgeoning interest exists in developing learnable masks [49,25,47], especially in the field of vision models. Markedly contrasted with traditional one-shot pruning methods that rely on a predetermined metric of importance, learnable sparsity can fully leverage the rich information in training data, enabling the identification of more effective sparsity masks.",
            "reference_string": "[272910976 | Fang et al. | 2024 | Citations: 10]"
        },
        {
            "title": "A Review of Neural Network Lightweighting Techniques",
            "venue": "Innovation &amp; Technology Advances",
            "year": 2024,
            "reference_count": 136,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.bergersci.com/index.php/jta/article/download/36/23",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.61187/ita.v1i2.36?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.61187/ita.v1i2.36, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280410840",
                    "name": "Ziyi Gong"
                },
                {
                    "authorId": "2280726529",
                    "name": "Huifu Zhang"
                },
                {
                    "authorId": "2280414619",
                    "name": "Hao Yang"
                },
                {
                    "authorId": "2280959623",
                    "name": "Fangjun Liu"
                },
                {
                    "authorId": "2280411835",
                    "name": "Fan Luo"
                }
            ],
            "abstract": "The application of portable devices based on deep learning has become increasingly widespread, which has made the deployment of complex neural networks on embedded devices a hot research topic. Neural network lightweighting is one of the key technologies for applying neural networks to embedded devices. This paper elaborates and analyzes neural network lightweighting techniques from two aspects: model pruning and network structure design. For model pruning, a comparison of methods from different periods is conducted, highlighting their advantages and limitations. Regarding network structure design, the principles of four classical lightweight network designs are described from a mathematical perspective, and the latest optimization methods for these networks are reviewed. Finally, potential research directions for lightweight neural network pruning and structure design optimization are discussed.",
            "corpus_id": 267103825,
            "sentences": [
                {
                    "corpus_id": "267103825",
                    "title": "A Review of Neural Network Lightweighting Techniques",
                    "text": "By analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67]. \n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation.",
                    "score": 0.5886747204199544,
                    "section_title": "Analysis and discussion",
                    "char_start_offset": 17510,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 476
                        },
                        {
                            "start": 477,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1186
                        },
                        {
                            "start": 1187,
                            "end": 1263
                        },
                        {
                            "start": 1264,
                            "end": 1419
                        },
                        {
                            "start": 1420,
                            "end": 1559
                        },
                        {
                            "start": 1562,
                            "end": 1724
                        },
                        {
                            "start": 1725,
                            "end": 1877
                        },
                        {
                            "start": 1878,
                            "end": 1944
                        },
                        {
                            "start": 1945,
                            "end": 2053
                        },
                        {
                            "start": 2054,
                            "end": 2142
                        },
                        {
                            "start": 2143,
                            "end": 2254
                        },
                        {
                            "start": 2255,
                            "end": 2410
                        },
                        {
                            "start": 2411,
                            "end": 2503
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1534,
                            "end": 1538,
                            "matchedPaperCorpusId": "202558809"
                        },
                        {
                            "start": 1538,
                            "end": 1542,
                            "matchedPaperCorpusId": "15663507"
                        },
                        {
                            "start": 1546,
                            "end": 1550,
                            "matchedPaperCorpusId": "5423629"
                        },
                        {
                            "start": 1550,
                            "end": 1554,
                            "matchedPaperCorpusId": "256390345"
                        },
                        {
                            "start": 1554,
                            "end": 1558,
                            "matchedPaperCorpusId": "249205107"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8623046875
                }
            ],
            "relevance_judgement": 0.8623046875,
            "relevance_judgment_input_expanded": "# Title: A Review of Neural Network Lightweighting Techniques\n# Venue: Innovation &amp; Technology Advances\n# Authors: Ziyi Gong, Huifu Zhang, Hao Yang, Fangjun Liu, Fan Luo\n## Abstract\nThe application of portable devices based on deep learning has become increasingly widespread, which has made the deployment of complex neural networks on embedded devices a hot research topic. Neural network lightweighting is one of the key technologies for applying neural networks to embedded devices. This paper elaborates and analyzes neural network lightweighting techniques from two aspects: model pruning and network structure design. For model pruning, a comparison of methods from different periods is conducted, highlighting their advantages and limitations. Regarding network structure design, the principles of four classical lightweight network designs are described from a mathematical perspective, and the latest optimization methods for these networks are reviewed. Finally, potential research directions for lightweight neural network pruning and structure design optimization are discussed.\n## Analysis and discussion\nBy analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67]. \n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation.",
            "reference_string": "[267103825 | Gong et al. | 2024 | Citations: 0]"
        },
        {
            "title": "On-Device Language Models: A Comprehensive Review",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 178,
            "citation_count": 35,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.00088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316519813",
                    "name": "Jiajun Xu"
                },
                {
                    "authorId": "2294674012",
                    "name": "Zhiyuan Li"
                },
                {
                    "authorId": "2294845809",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2316514278",
                    "name": "Qun Wang"
                },
                {
                    "authorId": "2319809164",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2364055424",
                    "name": "Qi Cai"
                },
                {
                    "authorId": "2319410023",
                    "name": "Ziyuan Ling"
                }
            ],
            "abstract": "The advent of large language models (LLMs) revolutionized natural language processing applications, and running LLMs on edge devices has become increasingly attractive for reasons including reduced latency, data localization, and personalized user experiences. This comprehensive review examines the challenges of deploying computationally expensive LLMs on resource-constrained devices and explores innovative solutions across multiple domains. The paper investigates the development of on-device language models, their efficient architectures, including parameter sharing and modular designs, as well as state-of-the-art compression techniques like quantization, pruning, and knowledge distillation. Hardware acceleration strategies and collaborative edge-cloud deployment approaches are analyzed, highlighting the intricate balance between performance and resource utilization. Case studies of on-device language models from major mobile manufacturers demonstrate real-world applications and potential benefits. The review also addresses critical aspects such as adaptive learning, multi-modal capabilities, and personalization. By identifying key research directions and open challenges, this paper provides a roadmap for future advancements in on-device language models, emphasizing the need for interdisciplinary efforts to realize the full potential of ubiquitous, intelligent computing while ensuring responsible and ethical deployment. For a comprehensive review of research work and educational resources on on-device large language models (LLMs), please visit https://github.com/NexaAI/Awesome-LLMs-on-device. To download and run on-device LLMs, visit https://www.nexaai.com/models.",
            "corpus_id": 272368391,
            "sentences": [
                {
                    "corpus_id": "272368391",
                    "title": "On-Device Language Models: A Comprehensive Review",
                    "text": "Pruning in neural networks involves selectively removing weights or neurons to reduce complexity and enhance computational efficiency without significantly compromising performance. This process targets the less crucial components of a model, focusing on efficiency and functional integrity. \n\n1. Structured Pruning: This approach removes entire subsets of parameters like layers, channels, or filters, which is beneficial for hardware optimization due to more regular memory access patterns and simplified computations. The 'LLM-Pruner' (Kaddour et al., 2023) employs structured pruning to eliminate non-essential groups based on gradient data, thus maintaining critical functionalities. It also facilitates performance recovery through techniques such as LoRA, allowing efficient restoration with minimal data. 2. Unstructured Pruning: Unlike structured pruning, unstructured pruning removes individual weights across the model, offering finer granularity and potentially higher compression rates (Li et al., 2023a). However, this method typically results in sparse matrices, which can be less compatible with traditional hardware architectures, compromising computational efficiency. It is most suitable where maximum compression is needed without constraints on structural preservation. 3. Contextual Pruning: This advanced method prunes based on the operational context of the model, targeting weights or neurons that are only relevant under specific conditions or for particular tasks. Contextual pruning ensures that reductions align dynamically with the model's operational needs, thereby preserving performance where it matters most.",
                    "score": 0.5261804646117343,
                    "section_title": "Pruning",
                    "char_start_offset": 36780,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 291
                        },
                        {
                            "start": 294,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1186
                        },
                        {
                            "start": 1187,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1491
                        },
                        {
                            "start": 1492,
                            "end": 1642
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85986328125
                }
            ],
            "relevance_judgement": 0.85986328125,
            "relevance_judgment_input_expanded": "# Title: On-Device Language Models: A Comprehensive Review\n# Venue: arXiv.org\n# Authors: Jiajun Xu, Zhiyuan Li, Wei Chen, Qun Wang, Xin Gao, Qi Cai, Ziyuan Ling\n## Abstract\nThe advent of large language models (LLMs) revolutionized natural language processing applications, and running LLMs on edge devices has become increasingly attractive for reasons including reduced latency, data localization, and personalized user experiences. This comprehensive review examines the challenges of deploying computationally expensive LLMs on resource-constrained devices and explores innovative solutions across multiple domains. The paper investigates the development of on-device language models, their efficient architectures, including parameter sharing and modular designs, as well as state-of-the-art compression techniques like quantization, pruning, and knowledge distillation. Hardware acceleration strategies and collaborative edge-cloud deployment approaches are analyzed, highlighting the intricate balance between performance and resource utilization. Case studies of on-device language models from major mobile manufacturers demonstrate real-world applications and potential benefits. The review also addresses critical aspects such as adaptive learning, multi-modal capabilities, and personalization. By identifying key research directions and open challenges, this paper provides a roadmap for future advancements in on-device language models, emphasizing the need for interdisciplinary efforts to realize the full potential of ubiquitous, intelligent computing while ensuring responsible and ethical deployment. For a comprehensive review of research work and educational resources on on-device large language models (LLMs), please visit https://github.com/NexaAI/Awesome-LLMs-on-device. To download and run on-device LLMs, visit https://www.nexaai.com/models.\n## Pruning\nPruning in neural networks involves selectively removing weights or neurons to reduce complexity and enhance computational efficiency without significantly compromising performance. This process targets the less crucial components of a model, focusing on efficiency and functional integrity. \n\n1. Structured Pruning: This approach removes entire subsets of parameters like layers, channels, or filters, which is beneficial for hardware optimization due to more regular memory access patterns and simplified computations. The 'LLM-Pruner' (Kaddour et al., 2023) employs structured pruning to eliminate non-essential groups based on gradient data, thus maintaining critical functionalities. It also facilitates performance recovery through techniques such as LoRA, allowing efficient restoration with minimal data. 2. Unstructured Pruning: Unlike structured pruning, unstructured pruning removes individual weights across the model, offering finer granularity and potentially higher compression rates (Li et al., 2023a). However, this method typically results in sparse matrices, which can be less compatible with traditional hardware architectures, compromising computational efficiency. It is most suitable where maximum compression is needed without constraints on structural preservation. 3. Contextual Pruning: This advanced method prunes based on the operational context of the model, targeting weights or neurons that are only relevant under specific conditions or for particular tasks. Contextual pruning ensures that reductions align dynamically with the model's operational needs, thereby preserving performance where it matters most.",
            "reference_string": "[272368391 | Xu et al. | 2024 | Citations: 35]"
        },
        {
            "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.01731, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359207803",
                    "name": "Chuan Sun"
                },
                {
                    "authorId": "2148706587",
                    "name": "Han Yu"
                },
                {
                    "authorId": "2313694394",
                    "name": "Li-zhen Cui"
                },
                {
                    "authorId": "2283747425",
                    "name": "Xiaoxiao Li"
                }
            ],
            "abstract": "Pruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the Shapley Value-based Non-Uniform Pruning (SV-NUP) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, SV-NUP achieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70% sparsity.",
            "corpus_id": 278327238,
            "sentences": [
                {
                    "corpus_id": "278327238",
                    "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
                    "text": "Large language models (LLMs) have emerged as a transformative technology, demonstrating remarkable capabilities in natural language understanding and generation tasks [1]. LLMs have showcased significant adaptability through fine-tuning, enabling their deployment in highly specialized applications. These advantages underscore the critical role of LLMs in solving real-world challenges [2]. Despite these capabilities, the deployment of LLMs has been hindered by their immense computational demands. Modern LLMs often consist of billions or even trillions of parameters [3], as seen in models like GPT-3 (175 billion parameters) and PaLM (540 billion parameters). The immense scale incurs significant memory, storage and power costs, making it challenging to run these models on resource-constrained devices. To address these issues, researchers have increasingly turned to model compression techniques, particularly pruning, to reduce the size and computational requirements of LLMs, while retaining their performance [4]. Pruning [5; 6; 7] is one of the most prominent model compression techniques. It aims to remove redundant or less important parameters from neural networks, thereby reducing their sizes and computational complexity. Recent advances in pruning have progressed from unstructured sparsity (i.e., individual weight removal) to structured sparsity (i.e., eliminating entire neurons, heads or layers), thereby enabling hardware-efficient implementation [4; 8; 9; 10]. For LLMs, several stateof-the-art methods have been proposed, including magnitude pruning, lottery ticket hypothesis approaches, and structured pruning based on attention mechanisms. These methods demonstrate the potential of pruning to enable LLM deployment on resource-constrained devices without causing substantial performance degradation. \n\nExisting pruning methods often use a layer-wise strategy that applies uniform sparsity across all layers, ignoring their varying importance. While simple to implement, this approach overlooks the inherent differences in the contributions of different layers to the overall performance of the model. Thus, it can only find the local optimal pruning solution, but not the global optimal solution. Empirical evidence also suggests that certain layers are more critical than others, and uniformly pruning across all layers may lead to the removal of essential parameters, ultimately impairing the pruned model's performance [11; 12].",
                    "score": 0.5116534570184916,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 171
                        },
                        {
                            "start": 172,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 500
                        },
                        {
                            "start": 501,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1239
                        },
                        {
                            "start": 1240,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1668
                        },
                        {
                            "start": 1669,
                            "end": 1829
                        },
                        {
                            "start": 1832,
                            "end": 1972
                        },
                        {
                            "start": 1973,
                            "end": 2130
                        },
                        {
                            "start": 2131,
                            "end": 2226
                        },
                        {
                            "start": 2227,
                            "end": 2461
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 167,
                            "end": 170,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 571,
                            "end": 574,
                            "matchedPaperCorpusId": "253265387"
                        },
                        {
                            "start": 1020,
                            "end": 1023,
                            "matchedPaperCorpusId": "255372747"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84228515625
                }
            ],
            "relevance_judgement": 0.84228515625,
            "relevance_judgment_input_expanded": "# Title: Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models\n# Venue: arXiv.org\n# Authors: Chuan Sun, Han Yu, Li-zhen Cui, Xiaoxiao Li\n## Abstract\nPruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the Shapley Value-based Non-Uniform Pruning (SV-NUP) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, SV-NUP achieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70% sparsity.\n## Introduction\nLarge language models (LLMs) have emerged as a transformative technology, demonstrating remarkable capabilities in natural language understanding and generation tasks [1]. LLMs have showcased significant adaptability through fine-tuning, enabling their deployment in highly specialized applications. These advantages underscore the critical role of LLMs in solving real-world challenges [2]. Despite these capabilities, the deployment of LLMs has been hindered by their immense computational demands. Modern LLMs often consist of billions or even trillions of parameters [3], as seen in models like GPT-3 (175 billion parameters) and PaLM (540 billion parameters). The immense scale incurs significant memory, storage and power costs, making it challenging to run these models on resource-constrained devices. To address these issues, researchers have increasingly turned to model compression techniques, particularly pruning, to reduce the size and computational requirements of LLMs, while retaining their performance [4]. Pruning [5; 6; 7] is one of the most prominent model compression techniques. It aims to remove redundant or less important parameters from neural networks, thereby reducing their sizes and computational complexity. Recent advances in pruning have progressed from unstructured sparsity (i.e., individual weight removal) to structured sparsity (i.e., eliminating entire neurons, heads or layers), thereby enabling hardware-efficient implementation [4; 8; 9; 10]. For LLMs, several stateof-the-art methods have been proposed, including magnitude pruning, lottery ticket hypothesis approaches, and structured pruning based on attention mechanisms. These methods demonstrate the potential of pruning to enable LLM deployment on resource-constrained devices without causing substantial performance degradation. \n\nExisting pruning methods often use a layer-wise strategy that applies uniform sparsity across all layers, ignoring their varying importance. While simple to implement, this approach overlooks the inherent differences in the contributions of different layers to the overall performance of the model. Thus, it can only find the local optimal pruning solution, but not the global optimal solution. Empirical evidence also suggests that certain layers are more critical than others, and uniformly pruning across all layers may lead to the removal of essential parameters, ultimately impairing the pruned model's performance [11; 12].",
            "reference_string": "[278327238 | Sun et al. | 2025 | Citations: 3]"
        },
        {
            "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 21,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.15996",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.15996, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48599077",
                    "name": "Ziqing Yang"
                },
                {
                    "authorId": "3043830",
                    "name": "Yiming Cui"
                },
                {
                    "authorId": "2156610145",
                    "name": "Zhigang Chen"
                }
            ],
            "abstract": "Pre-trained language models have been prevailed in natural language processing and become the backbones of many NLP tasks, but the demands for computational resources have limited their applications. In this paper, we introduce TextPruner, an open-source model pruning toolkit designed for pre-trained language models, targeting fast and easy model compression. TextPruner offers structured post-training pruning methods, including vocabulary pruning and transformer pruning, and can be applied to various models and tasks. We also propose a self-supervised pruning method that can be applied without the labeled data. Our experiments with several NLP tasks demonstrate the ability of TextPruner to reduce the model size without re-training the model.",
            "corpus_id": 247794014,
            "sentences": [
                {
                    "corpus_id": "247794014",
                    "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models",
                    "text": "Large pre-trained language models (PLMs) (Devlin et al., 2019;Liu et al., 2019) have achieved great success in a variety of NLP tasks. However, it is difficult to deploy them for real-world applications where computation and memory resources are limited. Reducing the pre-trained model size and speeding up the inference have become a critical issue.\n\nPruning is a common technique for model compression. It identifies and removes redundant or less important neurons from the networks. From the view of the model structure, pruning methods can be categorized into unstructured pruning and structured pruning. In the unstructured pruning, each model parameter is individually removed if it reaches some criteria based on the magnitude or importance score (Han et al., 2015;Zhu and Gupta, 2018;. The unstructured pruning results in sparse matrices and allows for significant model compression, but the inference speed can hardly be improved without specialized devices. While in the structured pruning, rows or columns of the parameters are removed from the weight matrices (McCarley, 2019;Michel et al., 2019;Voita et al., 2019;Lagunas et al., 2021;Hou et al., 2020). Thus, the resulting model speeds up on the common CPU and GPU devices.\n\nPruning methods can also be classified into optimization-free methods (Michel et al., 2019) and the ones that involve optimization (Frankle and Carbin, 2019;Lagunas et al., 2021). The latter usually achieves higher performance, but the former runs faster and is more convenient to use.\n\nPruning PLMs has been of growing interest. Most of the works focus on reducing transformer size while ignoring the vocabulary (Abdaoui et al., 2020). Pruning vocabulary can greatly reduce the model size for multilingual PLMs.\n\nIn this paper, we present TextPruner, a model pruning toolkit for PLMs. It combines both transformer pruning and vocabulary pruning. The purpose of TextPruner is to offer a universal, fast, and easy-to-use tool for model compression. We expect it can be accessible to users with little model training experience. Therefore, we implement the structured optimization-free",
                    "score": 0.5756872739559102,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 41,
                            "end": 62,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 772,
                            "end": 792,
                            "matchedPaperCorpusId": "27494814"
                        },
                        {
                            "start": 1088,
                            "end": 1108,
                            "matchedPaperCorpusId": "166227946"
                        },
                        {
                            "start": 1108,
                            "end": 1127,
                            "matchedPaperCorpusId": "162183964"
                        },
                        {
                            "start": 1127,
                            "end": 1148,
                            "matchedPaperCorpusId": "237485472"
                        },
                        {
                            "start": 1148,
                            "end": 1165,
                            "matchedPaperCorpusId": "215415863"
                        },
                        {
                            "start": 1309,
                            "end": 1330,
                            "matchedPaperCorpusId": "166227946"
                        },
                        {
                            "start": 1370,
                            "end": 1396,
                            "matchedPaperCorpusId": "53388625"
                        },
                        {
                            "start": 1396,
                            "end": 1417,
                            "matchedPaperCorpusId": "237485472"
                        },
                        {
                            "start": 1652,
                            "end": 1674,
                            "matchedPaperCorpusId": "222291680"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83837890625
                }
            ],
            "relevance_judgement": 0.83837890625,
            "relevance_judgment_input_expanded": "# Title: TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Ziqing Yang, Yiming Cui, Zhigang Chen\n## Abstract\nPre-trained language models have been prevailed in natural language processing and become the backbones of many NLP tasks, but the demands for computational resources have limited their applications. In this paper, we introduce TextPruner, an open-source model pruning toolkit designed for pre-trained language models, targeting fast and easy model compression. TextPruner offers structured post-training pruning methods, including vocabulary pruning and transformer pruning, and can be applied to various models and tasks. We also propose a self-supervised pruning method that can be applied without the labeled data. Our experiments with several NLP tasks demonstrate the ability of TextPruner to reduce the model size without re-training the model.\n## Introduction\nLarge pre-trained language models (PLMs) (Devlin et al., 2019;Liu et al., 2019) have achieved great success in a variety of NLP tasks. However, it is difficult to deploy them for real-world applications where computation and memory resources are limited. Reducing the pre-trained model size and speeding up the inference have become a critical issue.\n\nPruning is a common technique for model compression. It identifies and removes redundant or less important neurons from the networks. From the view of the model structure, pruning methods can be categorized into unstructured pruning and structured pruning. In the unstructured pruning, each model parameter is individually removed if it reaches some criteria based on the magnitude or importance score (Han et al., 2015;Zhu and Gupta, 2018;. The unstructured pruning results in sparse matrices and allows for significant model compression, but the inference speed can hardly be improved without specialized devices. While in the structured pruning, rows or columns of the parameters are removed from the weight matrices (McCarley, 2019;Michel et al., 2019;Voita et al., 2019;Lagunas et al., 2021;Hou et al., 2020). Thus, the resulting model speeds up on the common CPU and GPU devices.\n\nPruning methods can also be classified into optimization-free methods (Michel et al., 2019) and the ones that involve optimization (Frankle and Carbin, 2019;Lagunas et al., 2021). The latter usually achieves higher performance, but the former runs faster and is more convenient to use.\n\nPruning PLMs has been of growing interest. Most of the works focus on reducing transformer size while ignoring the vocabulary (Abdaoui et al., 2020). Pruning vocabulary can greatly reduce the model size for multilingual PLMs.\n\nIn this paper, we present TextPruner, a model pruning toolkit for PLMs. It combines both transformer pruning and vocabulary pruning. The purpose of TextPruner is to offer a universal, fast, and easy-to-use tool for model compression. We expect it can be accessible to users with little model training experience. Therefore, we implement the structured optimization-free",
            "reference_string": "[247794014 | Yang et al. | 2022 | Citations: 12]"
        },
        {
            "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
            "venue": "International Joint Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 79,
            "citation_count": 45,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108048327",
                    "name": "Xindi Wang"
                },
                {
                    "authorId": "1904419",
                    "name": "Mahsa Salmani"
                },
                {
                    "authorId": "2282534833",
                    "name": "Parsa Omidi"
                },
                {
                    "authorId": "2283447900",
                    "name": "Xiangyu Ren"
                },
                {
                    "authorId": "2066076226",
                    "name": "Mehdi Rezagholizadeh"
                },
                {
                    "authorId": "50782111",
                    "name": "A. Eshaghi"
                }
            ],
            "abstract": "Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational cost. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.",
            "corpus_id": 267412232,
            "sentences": [
                {
                    "corpus_id": "267412232",
                    "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
                    "text": "Pruning can help optimize the model for deployment and make the model more efficient in terms of computation complexity and memory usage. Accordingly, pruning can be considered as an approach to enable a language model to support longer sequence length, while maintaining the desirable complexity and performance. In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters. LLM-Pruner [Ma et al., 2023a] is a structural task-agnostic pruning approach that selectively removes non-critical connection structures considering both first-order information and an approximated Hessian information gradient information. As an alternative, Sheared LLaMA [Xia et al., 2024] utilizes a two-stage approach for pruning an LLM. In the first stage, it exploits targeted structured pruning to prune a large model to a targeted shape by pruning layers, heads, and intermediate connections. In the second stage, the batches of data are loaded dynamically and the model structure is modified in each training iteration based on losses in various domains. As a result, Sheared LLaMA achieves a compressed model that can outperform the LLMs, with the same size but trained from scratch. \n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models. However, SparseGPT heavily relies on weight updates.",
                    "score": 0.5702139366846237,
                    "section_title": "Model Compression",
                    "char_start_offset": 28041,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 402
                        },
                        {
                            "start": 405,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 880
                        },
                        {
                            "start": 881,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1304
                        },
                        {
                            "start": 1305,
                            "end": 1434
                        },
                        {
                            "start": 1437,
                            "end": 1611
                        },
                        {
                            "start": 1612,
                            "end": 1866
                        },
                        {
                            "start": 1867,
                            "end": 2023
                        },
                        {
                            "start": 2024,
                            "end": 2076
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83740234375
                }
            ],
            "relevance_judgement": 0.83740234375,
            "relevance_judgment_input_expanded": "# Title: Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models\n# Venue: International Joint Conference on Artificial Intelligence\n# Authors: Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, A. Eshaghi\n## Abstract\nRecently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational cost. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.\n## Model Compression\nPruning can help optimize the model for deployment and make the model more efficient in terms of computation complexity and memory usage. Accordingly, pruning can be considered as an approach to enable a language model to support longer sequence length, while maintaining the desirable complexity and performance. In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters. LLM-Pruner [Ma et al., 2023a] is a structural task-agnostic pruning approach that selectively removes non-critical connection structures considering both first-order information and an approximated Hessian information gradient information. As an alternative, Sheared LLaMA [Xia et al., 2024] utilizes a two-stage approach for pruning an LLM. In the first stage, it exploits targeted structured pruning to prune a large model to a targeted shape by pruning layers, heads, and intermediate connections. In the second stage, the batches of data are loaded dynamically and the model structure is modified in each training iteration based on losses in various domains. As a result, Sheared LLaMA achieves a compressed model that can outperform the LLMs, with the same size but trained from scratch. \n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models. However, SparseGPT heavily relies on weight updates.",
            "reference_string": "[267412232 | Wang et al. | 2024 | Citations: 45]"
        },
        {
            "title": "CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information",
            "venue": "International Conference on Computational Linguistics",
            "year": 2024,
            "reference_count": 77,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2199804607",
                    "name": "Yuxin Wang"
                },
                {
                    "authorId": "2272886567",
                    "name": "Ming Ma"
                },
                {
                    "authorId": "2108727290",
                    "name": "Zekun Wang"
                },
                {
                    "authorId": "2218647765",
                    "name": "Jingchang Chen"
                },
                {
                    "authorId": "2322634623",
                    "name": "Huiming Fan"
                },
                {
                    "authorId": "2223458982",
                    "name": "Liping Shan"
                },
                {
                    "authorId": "2287799372",
                    "name": "Qing Yang"
                },
                {
                    "authorId": "2284642281",
                    "name": "Dongliang Xu"
                },
                {
                    "authorId": "2112748107",
                    "name": "Ming Liu"
                },
                {
                    "authorId": "2247852651",
                    "name": "Bing Qin"
                }
            ],
            "abstract": "The colossal parameters and computational overhead of Large Language Models (LLMs) challenge their real-world applications. Network pruning, which targets unstructured or structured sparsity by removing redundant parameters, has recently been explored for LLM acceleration. Existing LLM pruning works focus on unstructured pruning, which typically requires special hardware support for a practical speed-up. In contrast, structured pruning can reduce latency on general devices. However, it remains a challenge to perform structured pruning efficiently and maintain performance, especially at high sparsity ratios. To this end, we introduce an efficient structured pruning framework named CFSP, which leverages both Coarse (interblock) and Fine-grained (intrablock) activation information as an importance criterion to guide pruning. The pruning is highly efficient, as it only requires one forward pass to compute feature activations. Specifically, we first allocate the sparsity budget across blocks based on their importance and then retain important weights within each block. In addition, we introduce a recovery fine-tuning strategy that adaptively allocates training overhead based on coarse-grained importance to further improve performance. Experimental results demonstrate that CFSP outperforms existing methods on diverse models across various sparsity budgets. Our code will be available at https://github.com/wyxscir/CFSP.",
            "corpus_id": 272770793,
            "sentences": [
                {
                    "corpus_id": "272770793",
                    "title": "CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information",
                    "text": "The colossal parameters and computational overhead of Large Language Models (LLMs) challenge their real-world applications. Network pruning, which targets unstructured or structured sparsity by removing redundant parameters, has recently been explored for LLM acceleration. Existing LLM pruning works focus on unstructured pruning, which typically requires special hardware support for a practical speed-up. In contrast, structured pruning can reduce latency on general devices. However, it remains a challenge to perform structured pruning efficiently and maintain performance, especially at high sparsity ratios. To this end, we introduce an efficient structured pruning framework named CFSP, which leverages both Coarse (interblock) and Fine-grained (intrablock) activation information as an importance criterion to guide pruning. The pruning is highly efficient, as it only requires one forward pass to compute feature activations. Specifically, we first allocate the sparsity budget across blocks based on their importance and then retain important weights within each block. In addition, we introduce a recovery fine-tuning strategy that adaptively allocates training overhead based on coarse-grained importance to further improve performance. Experimental results demonstrate that CFSP outperforms existing methods on diverse models across various sparsity budgets. Our code will be available at https://github.com/wyxscir/CFSP.",
                    "score": 0.531964122101348,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83203125
                },
                {
                    "corpus_id": "272770793",
                    "title": "CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information",
                    "text": "Although scaling up Large Language Models (LLMs) brings remarkable performance (Brown et al., 2020;OpenAI, 2023;Gemini Team et al., 2023;Meta, 2024;DeepSeek-AI et al., 2024;Yang et al., 2024a), increasing parameters brings more computations and memory consumption, posing a significant challenge of deploying in practical applications. To address this, various model compression methods for LLMs are proposed (Dettmers et al., 2022;Frantar et al., 2022;Lin et al., 2023;Muralidharan et al., 2024). Existing LLM pruning work (Frantar and Alistarh, 2023;Sun et al., 2023;Xu et al., 2024a;Zhang et al., 2024b) focuses mainly on unstructured or semi-structured sparsity. However, these paradigms require specific hardware to achieve practical acceleration. \n\nIn contrast, structured pruning, which imposes structured sparsity by removing groups of consecutive parameters (Louizos et al., 2017;Wang et al., 2020;Xia et al., 2022), is more hardware-friendly on general devices. However, there are some challenges involved in existing structured pruning methods for LLMs: (1) They typically introduce learnable masks to search (Xia et al., 2023;Dery et al., 2024) or utilize gradients to guide pruning (Ma et al., 2023;Zhang et al., 2023a). Unfortunately, they require significant computational overhead, especially for large-scale (e.g., 70B) models. (2) It is also worth noting that they usually assign a uniform sparsity budget per block, which is suboptimal since LLM blocks have different significance in the representation functionality (Gromov et al., 2024a). Moreover, they usually involve a recovery fine-tuning with Low-Rank Adapter (LoRA) (Hu et al., 2022) to enhance pruned models, which also introduce training overhead and overlook the varying importance of blocks.",
                    "score": 0.5878192833423119,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 666
                        },
                        {
                            "start": 667,
                            "end": 752
                        },
                        {
                            "start": 755,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1559
                        },
                        {
                            "start": 1560,
                            "end": 1772
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 889,
                            "end": 907,
                            "matchedPaperCorpusId": "204009154"
                        },
                        {
                            "start": 907,
                            "end": 924,
                            "matchedPaperCorpusId": "247922354"
                        },
                        {
                            "start": 1195,
                            "end": 1212,
                            "matchedPaperCorpusId": "268253513"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8076171875
                }
            ],
            "relevance_judgement": 0.83203125,
            "relevance_judgment_input_expanded": "# Title: CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information\n# Venue: International Conference on Computational Linguistics\n# Authors: Yuxin Wang, Ming Ma, Zekun Wang, Jingchang Chen, Huiming Fan, Liping Shan, Qing Yang, Dongliang Xu, Ming Liu, Bing Qin\n## Abstract\nThe colossal parameters and computational overhead of Large Language Models (LLMs) challenge their real-world applications. Network pruning, which targets unstructured or structured sparsity by removing redundant parameters, has recently been explored for LLM acceleration. Existing LLM pruning works focus on unstructured pruning, which typically requires special hardware support for a practical speed-up. In contrast, structured pruning can reduce latency on general devices. However, it remains a challenge to perform structured pruning efficiently and maintain performance, especially at high sparsity ratios. To this end, we introduce an efficient structured pruning framework named CFSP, which leverages both Coarse (interblock) and Fine-grained (intrablock) activation information as an importance criterion to guide pruning. The pruning is highly efficient, as it only requires one forward pass to compute feature activations. Specifically, we first allocate the sparsity budget across blocks based on their importance and then retain important weights within each block. In addition, we introduce a recovery fine-tuning strategy that adaptively allocates training overhead based on coarse-grained importance to further improve performance. Experimental results demonstrate that CFSP outperforms existing methods on diverse models across various sparsity budgets. Our code will be available at https://github.com/wyxscir/CFSP.\n## Introduction\nAlthough scaling up Large Language Models (LLMs) brings remarkable performance (Brown et al., 2020;OpenAI, 2023;Gemini Team et al., 2023;Meta, 2024;DeepSeek-AI et al., 2024;Yang et al., 2024a), increasing parameters brings more computations and memory consumption, posing a significant challenge of deploying in practical applications. To address this, various model compression methods for LLMs are proposed (Dettmers et al., 2022;Frantar et al., 2022;Lin et al., 2023;Muralidharan et al., 2024). Existing LLM pruning work (Frantar and Alistarh, 2023;Sun et al., 2023;Xu et al., 2024a;Zhang et al., 2024b) focuses mainly on unstructured or semi-structured sparsity. However, these paradigms require specific hardware to achieve practical acceleration. \n\nIn contrast, structured pruning, which imposes structured sparsity by removing groups of consecutive parameters (Louizos et al., 2017;Wang et al., 2020;Xia et al., 2022), is more hardware-friendly on general devices. However, there are some challenges involved in existing structured pruning methods for LLMs: (1) They typically introduce learnable masks to search (Xia et al., 2023;Dery et al., 2024) or utilize gradients to guide pruning (Ma et al., 2023;Zhang et al., 2023a). Unfortunately, they require significant computational overhead, especially for large-scale (e.g., 70B) models. (2) It is also worth noting that they usually assign a uniform sparsity budget per block, which is suboptimal since LLM blocks have different significance in the representation functionality (Gromov et al., 2024a). Moreover, they usually involve a recovery fine-tuning with Low-Rank Adapter (LoRA) (Hu et al., 2022) to enhance pruned models, which also introduce training overhead and overlook the varying importance of blocks.",
            "reference_string": "[272770793 | Wang et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Mosaic: Composite Projection Pruning for Resource-efficient LLMs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 52,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.06323, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2239202278",
                    "name": "Bailey J. Eccles"
                },
                {
                    "authorId": "2298757500",
                    "name": "Leon Wong"
                },
                {
                    "authorId": "2284990688",
                    "name": "Blesson Varghese"
                }
            ],
            "abstract": "Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models.",
            "corpus_id": 277634221,
            "sentences": [
                {
                    "corpus_id": "277634221",
                    "title": "Mosaic: Composite Projection Pruning for Resource-efficient LLMs",
                    "text": "Pruning removes an individual or a group of parameters from the model using a ranking algorithm [12]. There are two categories of pruning: Unstructured pruning refers to setting parameter values to zero [12]; quality is maintained while model size is unaffected. Structured pruning refers to removing data structures containing parameters such as attention heads [13]; this reduces model size and inference latency but at the cost of model quality. In short, existing pruning methods do not adequately balance runtime performance and LLM quality. \n\nOur work, Mosaic, is positioned to address these shortcomings. Existing pruning methods focus on coarse-grained pruning at the global and layer level of the LLM (further discussed in Section II). They prune every LLM component uniformly. This results in removing parts of the model that are critical to quality. Mosaic introduces novel fine-grained pruning of LLM projections. We leverage non-uniform pruning and apply it to different components of the LLM to selectively retain critical model parts. In addition, Mosaic synergistically combines unstructured and structured pruning in LLMs for the first time to create 'composite projection pruning'. This pruning approach can produce compressed and resource-efficient LLMs that fit in limited memory and provide fast inference while having comparable quality to the foundation LLM. The models produced by Mosaic can be deployed on any hardware platform without requiring specific hardware/software accelerators. \n\nOur research contributions are as follows: \n\n(1) Mosaic, a novel system for compressing foundation LLMs for hardware-limited environments. Mosaic is 7.19\u00d7 faster in producing compressed models than existing approaches. \n\n(2) LLM projection pruning, a new method that maintains arXiv:2504.06323v1 [cs. \n\nLG] 8 Apr 2025 quality at higher compression levels. The method determines a projection outlier distribution to prune projections nonuniformly. Projection pruning is a performance-efficient extension of fine-grained LLM pruning explored in prior work. \n\nMosaic models produced by projection pruning achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models from uniform pruning.",
                    "score": 0.5015991855010252,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 1959,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 101
                        },
                        {
                            "start": 102,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 546
                        },
                        {
                            "start": 549,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 786
                        },
                        {
                            "start": 787,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1381
                        },
                        {
                            "start": 1382,
                            "end": 1511
                        },
                        {
                            "start": 1514,
                            "end": 1556
                        },
                        {
                            "start": 1559,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1732
                        },
                        {
                            "start": 1735,
                            "end": 1809
                        },
                        {
                            "start": 1810,
                            "end": 1814
                        },
                        {
                            "start": 1817,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 1960
                        },
                        {
                            "start": 1961,
                            "end": 2068
                        },
                        {
                            "start": 2071,
                            "end": 2212
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 96,
                            "end": 100,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 203,
                            "end": 207,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 363,
                            "end": 367,
                            "matchedPaperCorpusId": "258823276"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8271484375
                }
            ],
            "relevance_judgement": 0.8271484375,
            "relevance_judgment_input_expanded": "# Title: Mosaic: Composite Projection Pruning for Resource-efficient LLMs\n# Venue: arXiv.org\n# Authors: Bailey J. Eccles, Leon Wong, Blesson Varghese\n## Abstract\nExtensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models.\n## I. INTRODUCTION\nPruning removes an individual or a group of parameters from the model using a ranking algorithm [12]. There are two categories of pruning: Unstructured pruning refers to setting parameter values to zero [12]; quality is maintained while model size is unaffected. Structured pruning refers to removing data structures containing parameters such as attention heads [13]; this reduces model size and inference latency but at the cost of model quality. In short, existing pruning methods do not adequately balance runtime performance and LLM quality. \n\nOur work, Mosaic, is positioned to address these shortcomings. Existing pruning methods focus on coarse-grained pruning at the global and layer level of the LLM (further discussed in Section II). They prune every LLM component uniformly. This results in removing parts of the model that are critical to quality. Mosaic introduces novel fine-grained pruning of LLM projections. We leverage non-uniform pruning and apply it to different components of the LLM to selectively retain critical model parts. In addition, Mosaic synergistically combines unstructured and structured pruning in LLMs for the first time to create 'composite projection pruning'. This pruning approach can produce compressed and resource-efficient LLMs that fit in limited memory and provide fast inference while having comparable quality to the foundation LLM. The models produced by Mosaic can be deployed on any hardware platform without requiring specific hardware/software accelerators. \n\nOur research contributions are as follows: \n\n(1) Mosaic, a novel system for compressing foundation LLMs for hardware-limited environments. Mosaic is 7.19\u00d7 faster in producing compressed models than existing approaches. \n\n(2) LLM projection pruning, a new method that maintains arXiv:2504.06323v1 [cs. \n\nLG] 8 Apr 2025 quality at higher compression levels. The method determines a projection outlier distribution to prune projections nonuniformly. Projection pruning is a performance-efficient extension of fine-grained LLM pruning explored in prior work. \n\nMosaic models produced by projection pruning achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models from uniform pruning.",
            "reference_string": "[277634221 | Eccles et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.02267, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238461878",
                    "name": "Aaron Klein"
                },
                {
                    "authorId": "2238481381",
                    "name": "Jacek Golebiowski"
                },
                {
                    "authorId": "2238532717",
                    "name": "Xingchen Ma"
                },
                {
                    "authorId": "2299943113",
                    "name": "Valerio Perrone"
                },
                {
                    "authorId": "2262457089",
                    "name": "C\u00e9dric Archambeau"
                }
            ],
            "abstract": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the state-of-the-art for natural language understanding task when fine-tuned on labeled data. However, their large size poses challenges in deploying them for inference in real-world applications, due to significant GPU memory requirements and high inference latency. This paper explores neural architecture search (NAS) for structural pruning to find sub-parts of the fine-tuned network that optimally trade-off efficiency, for example in terms of model size or latency, and generalization performance. We also show how we can utilize more recently developed two-stage weight-sharing NAS approaches in this setting to accelerate the search process. Unlike traditional pruning methods with fixed thresholds, we propose to adopt a multi-objective approach that identifies the Pareto optimal set of sub-networks, allowing for a more flexible and automated compression process.",
            "corpus_id": 269588232,
            "sentences": [
                {
                    "corpus_id": "269588232",
                    "title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search",
                    "text": "Pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b) are widely used for natural language understanding (NLU) tasks when large amount of labelled data is available for fine-tuning. However, deploying PLMs for inference can be challenging due to their large parameter count. They demand significant GPU memory and exhibit high inference latency, making them impractical for many real-world applications, for example when used in an end-point for a web service or deployed on an embedded systems. Recent work (Blalock et al., 2020;Kwon et al., 2022;Michel et al., 2019;Sajjad et al., 2022) demonstrated that in many cases only a subset of the pre-trained model's parameters significantly contributes to the downstream task performance. This allows for compressing the model by pruning parts of the network while minimizing performance deterioration. \n\nUnstructured pruning (Blalock et al., 2020) computes a score for each weight in the network, such as the weight's magnitude, and removes weights with scores below a predetermined threshold. This approach often achieves high pruning rates with minimal performance degradation, but it also leads to sparse weight matrices, which are not well-supported by commonly used machine learning frameworks. Structured pruning (Michel et al., 2019;Sajjad et al., 2022) removes larger components of the network, such as layers or  Afterwards, we run a multi-objective search to select the optimal set of sub-networks that balance parameter count and validation error. \n\nheads. Although it typically does not achieve the same pruning rates as unstructured pruning, it only prunes entire columns/rows of the weight matrix, making it compatible with popular deep learning frameworks and hardware. \n\nNeural architecture search (Zoph & Le, 2017;Real et al., 2017;Bergstra et al., 2013) (NAS) finds more resource efficient neural network architectures in a data-driven way by casting it as an optimization problem.",
                    "score": 0.5672338544232967,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 781
                        },
                        {
                            "start": 782,
                            "end": 895
                        },
                        {
                            "start": 898,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1552
                        },
                        {
                            "start": 1555,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1778
                        },
                        {
                            "start": 1781,
                            "end": 1993
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 48,
                            "end": 69,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 595,
                            "end": 615,
                            "matchedPaperCorpusId": "166227946"
                        },
                        {
                            "start": 1313,
                            "end": 1334,
                            "matchedPaperCorpusId": "166227946"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82666015625
                }
            ],
            "relevance_judgement": 0.82666015625,
            "relevance_judgment_input_expanded": "# Title: Structural Pruning of Pre-trained Language Models via Neural Architecture Search\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Aaron Klein, Jacek Golebiowski, Xingchen Ma, Valerio Perrone, C\u00e9dric Archambeau\n## Abstract\nPre-trained language models (PLM), for example BERT or RoBERTa, mark the state-of-the-art for natural language understanding task when fine-tuned on labeled data. However, their large size poses challenges in deploying them for inference in real-world applications, due to significant GPU memory requirements and high inference latency. This paper explores neural architecture search (NAS) for structural pruning to find sub-parts of the fine-tuned network that optimally trade-off efficiency, for example in terms of model size or latency, and generalization performance. We also show how we can utilize more recently developed two-stage weight-sharing NAS approaches in this setting to accelerate the search process. Unlike traditional pruning methods with fixed thresholds, we propose to adopt a multi-objective approach that identifies the Pareto optimal set of sub-networks, allowing for a more flexible and automated compression process.\n## Introduction\nPre-trained language models (PLMs) such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b) are widely used for natural language understanding (NLU) tasks when large amount of labelled data is available for fine-tuning. However, deploying PLMs for inference can be challenging due to their large parameter count. They demand significant GPU memory and exhibit high inference latency, making them impractical for many real-world applications, for example when used in an end-point for a web service or deployed on an embedded systems. Recent work (Blalock et al., 2020;Kwon et al., 2022;Michel et al., 2019;Sajjad et al., 2022) demonstrated that in many cases only a subset of the pre-trained model's parameters significantly contributes to the downstream task performance. This allows for compressing the model by pruning parts of the network while minimizing performance deterioration. \n\nUnstructured pruning (Blalock et al., 2020) computes a score for each weight in the network, such as the weight's magnitude, and removes weights with scores below a predetermined threshold. This approach often achieves high pruning rates with minimal performance degradation, but it also leads to sparse weight matrices, which are not well-supported by commonly used machine learning frameworks. Structured pruning (Michel et al., 2019;Sajjad et al., 2022) removes larger components of the network, such as layers or  Afterwards, we run a multi-objective search to select the optimal set of sub-networks that balance parameter count and validation error. \n\nheads. Although it typically does not achieve the same pruning rates as unstructured pruning, it only prunes entire columns/rows of the weight matrix, making it compatible with popular deep learning frameworks and hardware. \n\nNeural architecture search (Zoph & Le, 2017;Real et al., 2017;Bergstra et al., 2013) (NAS) finds more resource efficient neural network architectures in a data-driven way by casting it as an optimization problem.",
            "reference_string": "[269588232 | Klein et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Task-specific Compression for Multi-task Language Models using Attribution-based Pruning",
            "venue": "Findings",
            "year": 2022,
            "reference_count": 45,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-eacl.43.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.04157, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153401901",
                    "name": "Nakyeong Yang"
                },
                {
                    "authorId": "2164385860",
                    "name": "Yunah Jang"
                },
                {
                    "authorId": "2109339794",
                    "name": "Hwanhee Lee"
                },
                {
                    "authorId": "2164458254",
                    "name": "Seohyeong Jung"
                },
                {
                    "authorId": "1731707",
                    "name": "Kyomin Jung"
                }
            ],
            "abstract": "Multi-task language models show outstanding performance for various natural language understanding tasks with only a single model.However, these language models inevitably utilize an unnecessarily large number of model parameters, even when used only for a specific task.In this paper, we propose a novel training-free compression method for multi-task language models using pruning method.Specifically, we use an attribution method to determine which neurons are essential for performing a specific task.We task-specifically prune unimportant neurons and leave only task-specific parameters.Furthermore, we extend our method to be applicable in both low-resource and unsupervised settings. Since our compression method is training-free, it uses little computing resources and does not update the pre-trained parameters of language models, reducing storage space usage.Experimental results on the six widely-used datasets show that our proposed pruning method significantly outperforms baseline pruning methods.In addition, we demonstrate that our method preserves performance even in an unseen domain setting.",
            "corpus_id": 256827874,
            "sentences": [
                {
                    "corpus_id": "256827874",
                    "title": "Task-specific Compression for Multi-task Language Models using Attribution-based Pruning",
                    "text": "One of the ways to categorize network pruning is to compare structured pruning to unstructured pruning. For structured pruning (Li et al., 2016;Hu et al., 2016;Wen et al., 2016), groups of weight con-nections are removed from a network together, such as entire channels or filters in CNN-based networks and layers or attention heads in transformer-based networks. For unstructured pruning (Han et al., 2015a,b), weight connections are removed from a network individually. However, unstructured pruning methods produce large sparse weight matrices which are computationally inefficient unless equipped with a specifically designed hardware. In this paper, we utilize the structured pruning method to propose a compression method that enables efficient weight matrix multiplication computation.",
                    "score": 0.5068762782207589,
                    "section_title": "Network Pruning",
                    "char_start_offset": 6284,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 104,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 792
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 160,
                            "end": 177,
                            "matchedPaperCorpusId": "2056019"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8232421875
                }
            ],
            "relevance_judgement": 0.8232421875,
            "relevance_judgment_input_expanded": "# Title: Task-specific Compression for Multi-task Language Models using Attribution-based Pruning\n# Venue: Findings\n# Authors: Nakyeong Yang, Yunah Jang, Hwanhee Lee, Seohyeong Jung, Kyomin Jung\n## Abstract\nMulti-task language models show outstanding performance for various natural language understanding tasks with only a single model.However, these language models inevitably utilize an unnecessarily large number of model parameters, even when used only for a specific task.In this paper, we propose a novel training-free compression method for multi-task language models using pruning method.Specifically, we use an attribution method to determine which neurons are essential for performing a specific task.We task-specifically prune unimportant neurons and leave only task-specific parameters.Furthermore, we extend our method to be applicable in both low-resource and unsupervised settings. Since our compression method is training-free, it uses little computing resources and does not update the pre-trained parameters of language models, reducing storage space usage.Experimental results on the six widely-used datasets show that our proposed pruning method significantly outperforms baseline pruning methods.In addition, we demonstrate that our method preserves performance even in an unseen domain setting.\n## Network Pruning\nOne of the ways to categorize network pruning is to compare structured pruning to unstructured pruning. For structured pruning (Li et al., 2016;Hu et al., 2016;Wen et al., 2016), groups of weight con-nections are removed from a network together, such as entire channels or filters in CNN-based networks and layers or attention heads in transformer-based networks. For unstructured pruning (Han et al., 2015a,b), weight connections are removed from a network individually. However, unstructured pruning methods produce large sparse weight matrices which are computationally inefficient unless equipped with a specifically designed hardware. In this paper, we utilize the structured pruning method to propose a compression method that enables efficient weight matrix multiplication computation.",
            "reference_string": "[256827874 | Yang et al. | 2022 | Citations: 8]"
        },
        {
            "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 198,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.17119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258141722",
                    "name": "Muskan Garg"
                },
                {
                    "authorId": "2278330619",
                    "name": "Shaina Raza"
                },
                {
                    "authorId": "3023076",
                    "name": "Shebuti Rayana"
                },
                {
                    "authorId": "2278394763",
                    "name": "Xingyi Liu"
                },
                {
                    "authorId": "2267490593",
                    "name": "Sunghwan Sohn"
                }
            ],
            "abstract": "Despite substantial progress in healthcare applications driven by large language models (LLMs), growing concerns around data privacy, and limited resources; the small language models (SLMs) offer a scalable and clinically viable solution for efficient performance in resource-constrained environments for next-generation healthcare informatics. Our comprehensive survey presents a taxonomic framework to identify and categorize them for healthcare professionals and informaticians. The timeline of healthcare SLM contributions establishes a foundational framework for analyzing models across three dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present a taxonomic framework to identify the architectural foundations for building models from scratch; adapting SLMs to clinical precision through prompting, instruction fine-tuning, and reasoning; and accessibility and sustainability through compression techniques. Our primary objective is to offer a comprehensive survey for healthcare professionals, introducing recent innovations in model optimization and equipping them with curated resources to support future research and development in the field. Aiming to showcase the groundbreaking advancements in SLMs for healthcare, we present a comprehensive compilation of experimental results across widely studied NLP tasks in healthcare to highlight the transformative potential of SLMs in healthcare. The updated repository is available at Github",
            "corpus_id": 278033481,
            "sentences": [
                {
                    "corpus_id": "278033481",
                    "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey",
                    "text": "Pruning is a long-established technique for model compression that reduces the size of a model by removing unnecessary or less important parameters [61,90]. In language models, many weights can be zeroed out or eliminated with little to no effect on overall performance [18]. By pruning these redundant parameters, we can significantly shrink model size and accelerate inference, with only a minor drop in accuracy if done carefully. Pruning thus makes models more storage-friendly, memoryefficient, and computation-efficient. Based on the granularity of removal, pruning is categorized as unstructured pruning and structured pruning. \n\nStructured pruning streamlines LLM by eliminating entire architectural components-such as neurons, filter channels, attention heads, or even layers-instead of removing individual parameters [126,198]. By selectively pruning structurally coherent groups of parameters, this approach yields a smaller yet regularly structured model-maintaining dense connectivity while reducing inference costs, for instance, LLM-Pruner [126]. Unstructured pruning removes individual weights from LLM without considering any specific structure within the model [41]. Unstructured pruning removes less important weights based on a threshold, resulting in a sparse model where the architecture remains unchanged, as long as enough weights stay to maintain performance. Unstructured pruning can deliver higher accuracy for a given compression ratio by allowing more targeted weight removal. However, its irregular sparsity pattern leads to inefficient memory access and limited speed-ups on standard hardware. To fully leverage the compression benefits, specialized libraries or hardware are needed, and even then, acceleration gains are modest unless the model is highly sparse. Post unstructured pruning, significant retraining or fine-tuning is often required to recover lost accuracy that can be resource-intensive. \n\nDiscussion on pruning: Contextual pruning is a promising method for building domain-specific language models [177]. Recent research has focused on one-shot pruning methods and improved criteria to minimize the need for costly retraining [211], validating its performance for healthcare tasks.",
                    "score": 0.759524151864561,
                    "section_title": "Pruning",
                    "char_start_offset": 44224,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 275
                        },
                        {
                            "start": 276,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 634
                        },
                        {
                            "start": 637,
                            "end": 837
                        },
                        {
                            "start": 838,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1624
                        },
                        {
                            "start": 1625,
                            "end": 1794
                        },
                        {
                            "start": 1795,
                            "end": 1934
                        },
                        {
                            "start": 1937,
                            "end": 2052
                        },
                        {
                            "start": 2053,
                            "end": 2229
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 148,
                            "end": 152,
                            "matchedPaperCorpusId": "2238772"
                        },
                        {
                            "start": 152,
                            "end": 155,
                            "matchedPaperCorpusId": "7785881"
                        },
                        {
                            "start": 270,
                            "end": 274,
                            "matchedPaperCorpusId": "276575434"
                        },
                        {
                            "start": 827,
                            "end": 832,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 1055,
                            "end": 1060,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 1179,
                            "end": 1183,
                            "matchedPaperCorpusId": "255372747"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.822265625
                }
            ],
            "relevance_judgement": 0.822265625,
            "relevance_judgment_input_expanded": "# Title: The Rise of Small Language Models in Healthcare: A Comprehensive Survey\n# Venue: arXiv.org\n# Authors: Muskan Garg, Shaina Raza, Shebuti Rayana, Xingyi Liu, Sunghwan Sohn\n## Abstract\nDespite substantial progress in healthcare applications driven by large language models (LLMs), growing concerns around data privacy, and limited resources; the small language models (SLMs) offer a scalable and clinically viable solution for efficient performance in resource-constrained environments for next-generation healthcare informatics. Our comprehensive survey presents a taxonomic framework to identify and categorize them for healthcare professionals and informaticians. The timeline of healthcare SLM contributions establishes a foundational framework for analyzing models across three dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present a taxonomic framework to identify the architectural foundations for building models from scratch; adapting SLMs to clinical precision through prompting, instruction fine-tuning, and reasoning; and accessibility and sustainability through compression techniques. Our primary objective is to offer a comprehensive survey for healthcare professionals, introducing recent innovations in model optimization and equipping them with curated resources to support future research and development in the field. Aiming to showcase the groundbreaking advancements in SLMs for healthcare, we present a comprehensive compilation of experimental results across widely studied NLP tasks in healthcare to highlight the transformative potential of SLMs in healthcare. The updated repository is available at Github\n## Pruning\nPruning is a long-established technique for model compression that reduces the size of a model by removing unnecessary or less important parameters [61,90]. In language models, many weights can be zeroed out or eliminated with little to no effect on overall performance [18]. By pruning these redundant parameters, we can significantly shrink model size and accelerate inference, with only a minor drop in accuracy if done carefully. Pruning thus makes models more storage-friendly, memoryefficient, and computation-efficient. Based on the granularity of removal, pruning is categorized as unstructured pruning and structured pruning. \n\nStructured pruning streamlines LLM by eliminating entire architectural components-such as neurons, filter channels, attention heads, or even layers-instead of removing individual parameters [126,198]. By selectively pruning structurally coherent groups of parameters, this approach yields a smaller yet regularly structured model-maintaining dense connectivity while reducing inference costs, for instance, LLM-Pruner [126]. Unstructured pruning removes individual weights from LLM without considering any specific structure within the model [41]. Unstructured pruning removes less important weights based on a threshold, resulting in a sparse model where the architecture remains unchanged, as long as enough weights stay to maintain performance. Unstructured pruning can deliver higher accuracy for a given compression ratio by allowing more targeted weight removal. However, its irregular sparsity pattern leads to inefficient memory access and limited speed-ups on standard hardware. To fully leverage the compression benefits, specialized libraries or hardware are needed, and even then, acceleration gains are modest unless the model is highly sparse. Post unstructured pruning, significant retraining or fine-tuning is often required to recover lost accuracy that can be resource-intensive. \n\nDiscussion on pruning: Contextual pruning is a promising method for building domain-specific language models [177]. Recent research has focused on one-shot pruning methods and improved criteria to minimize the need for costly retraining [211], validating its performance for healthcare tasks.",
            "reference_string": "[278033481 | Garg et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Sample-aware Adaptive Structured Pruning for Large Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.06184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1511692806",
                    "name": "Jun Kong"
                },
                {
                    "authorId": "2206497323",
                    "name": "Xinge Ma"
                },
                {
                    "authorId": "2143717068",
                    "name": "Jin Wang"
                },
                {
                    "authorId": "2144497782",
                    "name": "Xuejie Zhang"
                }
            ],
            "abstract": "Large language models (LLMs) have achieved outstanding performance in natural language processing, but enormous model sizes and high computational costs limit their practical deployment. Structured pruning can effectively reduce the resource demands for deployment by removing redundant model parameters. However, the randomly selected calibration data and fixed single importance estimation metrics in existing structured pruning methods lead to degraded performance of pruned models. This study introduces AdaPruner, a sample-aware adaptive structured pruning framework for LLMs, aiming to optimize the calibration data and importance estimation metrics in the structured pruning process. Specifically, AdaPruner effectively removes redundant parameters from LLMs by constructing a structured pruning solution space and then employing Bayesian optimization to adaptively search for the optimal calibration data and importance estimation metrics. Experimental results show that the AdaPruner outperforms existing structured pruning methods on a family of LLMs with varying pruning ratios, demonstrating its applicability and robustness. Remarkably, at a 20% pruning ratio, the model pruned with AdaPruner maintains 97% of the performance of the unpruned model.",
            "corpus_id": 276902790,
            "sentences": [
                {
                    "corpus_id": "276902790",
                    "title": "Sample-aware Adaptive Structured Pruning for Large Language Models",
                    "text": "For example, LLM-Pruner (Ma, Fang, and Wang 2023) is the first attempt at structured pruning of LLMs, reducing model computation and memory usage while keeping the overall structure of the LLMs. Specifically, it utilizes a dependency detection algorithm to identify coupled structures in the LLMs, scores the importance of the coupled structures, and selectively removes non-critical structures according to first-order and approximate Hessianbased information. Finally, LLM-Pruner uses Low-Rank-Adaptor (LoRA) for fine-tuning to recover the pruned model weights. Further, LoRAShear (Chen et al. 2023) utilizes the Lora Half-Space Projected Gradient (LH-SPG) technique for incremental structured pruning and knowledge recovery. LoRAShear can be applied to various LLMs through dependency graph analysis and sparsity optimization. Recently, Sheared-LLaMA (Xia et al. 2024) is a directed structured pruning that prunes a large model to a specified target structure. It then trains the pruned model using dynamically loaded data based on the loss reduction ratio in each domain, thus improving data usage efficiency and model performance. However, Sheared-LLaMA uses substantial computational resources for subsequent pre-training to performance recovery. Like LLM-Pruner, the number of layers is constant. Shortened LLaMA (Kim et al. 2024) proposes that a simple deep pruning method can achieve promising performance and improve inference speed in a zero-shot task. \u2022 Unstructured pruning. It performs pruning operations on weights or neurons instead of structured blocks. Although unstructured pruning typically results in higher compression ratios, the need for specific hardware support leads to the failure to achieve proper inference acceleration or storage reduction. SparseGPT (Frantar and Alistarh 2023) is the first approach to perform unstructured pruning on LLMs. It applies the Hessian inverse to pruning and uses a complex weight-updating process that includes synchronized second-order Hessian updates. The  strategy is computationally expensive to execute iteratively between weight pruning and weight updating at each layer.",
                    "score": 0.5742489573271503,
                    "section_title": "Model Quantization Model quantization compresses",
                    "char_start_offset": 25661,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 563
                        },
                        {
                            "start": 564,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1252
                        },
                        {
                            "start": 1253,
                            "end": 1303
                        },
                        {
                            "start": 1304,
                            "end": 1463
                        },
                        {
                            "start": 1464,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 1570
                        },
                        {
                            "start": 1571,
                            "end": 1771
                        },
                        {
                            "start": 1772,
                            "end": 1872
                        },
                        {
                            "start": 1873,
                            "end": 2014
                        },
                        {
                            "start": 2015,
                            "end": 2138
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 24,
                            "end": 49,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 854,
                            "end": 870,
                            "matchedPaperCorpusId": "263830786"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8193359375
                }
            ],
            "relevance_judgement": 0.8193359375,
            "relevance_judgment_input_expanded": "# Title: Sample-aware Adaptive Structured Pruning for Large Language Models\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Jun Kong, Xinge Ma, Jin Wang, Xuejie Zhang\n## Abstract\nLarge language models (LLMs) have achieved outstanding performance in natural language processing, but enormous model sizes and high computational costs limit their practical deployment. Structured pruning can effectively reduce the resource demands for deployment by removing redundant model parameters. However, the randomly selected calibration data and fixed single importance estimation metrics in existing structured pruning methods lead to degraded performance of pruned models. This study introduces AdaPruner, a sample-aware adaptive structured pruning framework for LLMs, aiming to optimize the calibration data and importance estimation metrics in the structured pruning process. Specifically, AdaPruner effectively removes redundant parameters from LLMs by constructing a structured pruning solution space and then employing Bayesian optimization to adaptively search for the optimal calibration data and importance estimation metrics. Experimental results show that the AdaPruner outperforms existing structured pruning methods on a family of LLMs with varying pruning ratios, demonstrating its applicability and robustness. Remarkably, at a 20% pruning ratio, the model pruned with AdaPruner maintains 97% of the performance of the unpruned model.\n## Model Quantization Model quantization compresses\nFor example, LLM-Pruner (Ma, Fang, and Wang 2023) is the first attempt at structured pruning of LLMs, reducing model computation and memory usage while keeping the overall structure of the LLMs. Specifically, it utilizes a dependency detection algorithm to identify coupled structures in the LLMs, scores the importance of the coupled structures, and selectively removes non-critical structures according to first-order and approximate Hessianbased information. Finally, LLM-Pruner uses Low-Rank-Adaptor (LoRA) for fine-tuning to recover the pruned model weights. Further, LoRAShear (Chen et al. 2023) utilizes the Lora Half-Space Projected Gradient (LH-SPG) technique for incremental structured pruning and knowledge recovery. LoRAShear can be applied to various LLMs through dependency graph analysis and sparsity optimization. Recently, Sheared-LLaMA (Xia et al. 2024) is a directed structured pruning that prunes a large model to a specified target structure. It then trains the pruned model using dynamically loaded data based on the loss reduction ratio in each domain, thus improving data usage efficiency and model performance. However, Sheared-LLaMA uses substantial computational resources for subsequent pre-training to performance recovery. Like LLM-Pruner, the number of layers is constant. Shortened LLaMA (Kim et al. 2024) proposes that a simple deep pruning method can achieve promising performance and improve inference speed in a zero-shot task. \u2022 Unstructured pruning. It performs pruning operations on weights or neurons instead of structured blocks. Although unstructured pruning typically results in higher compression ratios, the need for specific hardware support leads to the failure to achieve proper inference acceleration or storage reduction. SparseGPT (Frantar and Alistarh 2023) is the first approach to perform unstructured pruning on LLMs. It applies the Hessian inverse to pruning and uses a complex weight-updating process that includes synchronized second-order Hessian updates. The  strategy is computationally expensive to execute iteratively between weight pruning and weight updating at each layer.",
            "reference_string": "[276902790 | Kong et al. | 2025 | Citations: 0]"
        },
        {
            "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 57,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.17905, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332540527",
                    "name": "Mingkuan Feng"
                },
                {
                    "authorId": "2141911656",
                    "name": "Jinyang Wu"
                },
                {
                    "authorId": "2298428469",
                    "name": "Shuai Zhang"
                },
                {
                    "authorId": "2221575807",
                    "name": "Pengpeng Shao"
                },
                {
                    "authorId": "2300370837",
                    "name": "Ruihan Jin"
                },
                {
                    "authorId": "1718662",
                    "name": "Zhengqi Wen"
                },
                {
                    "authorId": "2298423822",
                    "name": "Jianhua Tao"
                },
                {
                    "authorId": "1471057495",
                    "name": "Feihu Che"
                }
            ],
            "abstract": "Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to reduce model size through pruning techniques. However, existing pruning methods typically follow a prune-then-finetune paradigm. Since the pruned components still contain valuable information, their direct removal often leads to irreversible performance degradation, imposing a substantial computational burden to recover performance during finetuning. In this paper, we propose a novel paradigm that first applies regularization, then prunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a simple and effective Data-driven Regularized Structured Streamlining method for LLMs. By leveraging a small amount of data to regularize the components to be pruned, DReSS explicitly transfers the important information to the remaining parts of the model in advance. Compared to direct pruning, this can reduce the information loss caused by parameter removal, thereby enhancing its language modeling capabilities. Experimental results demonstrate that DReSS significantly outperforms existing pruning methods even under extreme pruning ratios, significantly reducing latency and increasing throughput.",
            "corpus_id": 275993741,
            "sentences": [
                {
                    "corpus_id": "275993741",
                    "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models",
                    "text": "Large language models (LLMs) have achieved significant advancements across a wide range of tasks and domains, Preprint demonstrating their robust capabilities (Zhang et al., 2022;Achiam et al., 2023;Touvron et al., 2023;Wu et al., 2024). However, as the model size increases, the growing number of parameters leads to significant computational and memory requirements, which significantly hinder the practical deployment of LLMs. Consequently, it is urgent to develop methods that can reduce model size while maintaining performance. \n\nTo address these challenges, several methods have been proposed, including pruning (Frantar & Alistarh, 2023;Sun et al., 2023;Ma et al., 2023;An et al., 2024), quantization (Frantar et al., 2022;Xiao et al., 2023), knowledge distillation (Shridhar et al., 2022;Hsieh et al., 2023), and low-rank decomposition (Saha et al., 2023). In this work, we mainly focus on pruning-an efficient and highly generalizable approach that can be seamlessly integrated with other model compression strategies. Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2023) and structured pruning (Ma et al., 2023;An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns. Despite these advancements, existing structured pruning methods still have some limitations. They all follow the paradigm of first selecting channels or layers to prune based on a designed metric, and then performing RFT (Chavan et al., 2024).",
                    "score": 0.6472855000175366,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 533
                        },
                        {
                            "start": 536,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1423
                        },
                        {
                            "start": 1424,
                            "end": 1560
                        },
                        {
                            "start": 1561,
                            "end": 1653
                        },
                        {
                            "start": 1654,
                            "end": 1804
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 199,
                            "end": 220,
                            "matchedPaperCorpusId": "257219404"
                        },
                        {
                            "start": 619,
                            "end": 645,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 662,
                            "end": 678,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 845,
                            "end": 864,
                            "matchedPaperCorpusId": "262233736"
                        },
                        {
                            "start": 1123,
                            "end": 1149,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1190,
                            "end": 1207,
                            "matchedPaperCorpusId": "258823276"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8154296875
                }
            ],
            "relevance_judgement": 0.8154296875,
            "relevance_judgment_input_expanded": "# Title: DReSS: Data-driven Regularized Structured Streamlining for Large Language Models\n# Venue: arXiv.org\n# Authors: Mingkuan Feng, Jinyang Wu, Shuai Zhang, Pengpeng Shao, Ruihan Jin, Zhengqi Wen, Jianhua Tao, Feihu Che\n## Abstract\nLarge language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to reduce model size through pruning techniques. However, existing pruning methods typically follow a prune-then-finetune paradigm. Since the pruned components still contain valuable information, their direct removal often leads to irreversible performance degradation, imposing a substantial computational burden to recover performance during finetuning. In this paper, we propose a novel paradigm that first applies regularization, then prunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a simple and effective Data-driven Regularized Structured Streamlining method for LLMs. By leveraging a small amount of data to regularize the components to be pruned, DReSS explicitly transfers the important information to the remaining parts of the model in advance. Compared to direct pruning, this can reduce the information loss caused by parameter removal, thereby enhancing its language modeling capabilities. Experimental results demonstrate that DReSS significantly outperforms existing pruning methods even under extreme pruning ratios, significantly reducing latency and increasing throughput.\n## Introduction\nLarge language models (LLMs) have achieved significant advancements across a wide range of tasks and domains, Preprint demonstrating their robust capabilities (Zhang et al., 2022;Achiam et al., 2023;Touvron et al., 2023;Wu et al., 2024). However, as the model size increases, the growing number of parameters leads to significant computational and memory requirements, which significantly hinder the practical deployment of LLMs. Consequently, it is urgent to develop methods that can reduce model size while maintaining performance. \n\nTo address these challenges, several methods have been proposed, including pruning (Frantar & Alistarh, 2023;Sun et al., 2023;Ma et al., 2023;An et al., 2024), quantization (Frantar et al., 2022;Xiao et al., 2023), knowledge distillation (Shridhar et al., 2022;Hsieh et al., 2023), and low-rank decomposition (Saha et al., 2023). In this work, we mainly focus on pruning-an efficient and highly generalizable approach that can be seamlessly integrated with other model compression strategies. Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2023) and structured pruning (Ma et al., 2023;An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns. Despite these advancements, existing structured pruning methods still have some limitations. They all follow the paradigm of first selecting channels or layers to prune based on a designed metric, and then performing RFT (Chavan et al., 2024).",
            "reference_string": "[275993741 | Feng et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 26,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17071, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003814068",
                    "name": "Ashhadul Islam"
                },
                {
                    "authorId": "102804035",
                    "name": "S. Belhaouari"
                },
                {
                    "authorId": "2256848295",
                    "name": "Amine Bermak"
                }
            ],
            "abstract": "The exponential growth of large language models (LLMs) like ChatGPT has revolutionized artificial intelligence, offering unprecedented capabilities in natural language processing. However, the extensive computational resources required for training these models have significant environmental implications, including high carbon emissions, energy consumption, and water usage. This research presents a novel approach to LLM pruning, focusing on the systematic evaluation of individual weight importance throughout the training process. By monitoring parameter evolution over time, we propose a method that effectively reduces model size without compromising performance. Extensive experiments with both a scaled-down LLM and a large multimodal model reveal that moderate pruning enhances efficiency and reduces loss, while excessive pruning drastically deteriorates model performance. These findings highlight the critical need for optimized AI models to ensure sustainable development, balancing technological advancement with environmental responsibility.",
            "corpus_id": 276576138,
            "sentences": [
                {
                    "corpus_id": "276576138",
                    "title": "Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability",
                    "text": "Methods like SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023) use sophisticated weight updates and pruning without retraining, respectively, while PST (Li et al., 2022) combines unstructured pruning with efficient fine-tuning. Structured pruning methods (Chen et al., 2021b(Chen et al., , 2023) ) remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency. Techniques such as LLM-Pruner (Ma et al., 2023) and LoRAPrune (Zhang et al., 2023) focus on efficient deployment and inference acceleration, with Sheared-LLaMA (Xia et al., 2023) aiming to prune models to a target architecture and train them dynamically. Furthermore, the compression of language models has garnered significant attention, leading to various methods like network pruning, knowledge distillation, and quantization (Bai et al., 2020;Brown et al., 2020;Devlin, 2018). Pruning, especially structural pruning, remains a crucial focus due to its hardware-friendly nature, with methods varying from l1-dependent pruning (Zafrir et al., 2021) to more advanced techniques like the optimal brain surgeon (LeCun et al., 1989). Efficient compression and low-resource strategies are increasingly essential, with advancements in layer-wise optimal brain surgeon and data-free pruning approaches aiming to optimize the balance between compression efficiency and training data availability (Kurti\u0107 et al., 2024;Srinivas and Babu, 2015).",
                    "score": 0.5790977005991614,
                    "section_title": "Related Work",
                    "char_start_offset": 5555,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 901
                        },
                        {
                            "start": 902,
                            "end": 1152
                        },
                        {
                            "start": 1153,
                            "end": 1457
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 51,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 273,
                            "end": 292,
                            "matchedPaperCorpusId": "235899080"
                        },
                        {
                            "start": 451,
                            "end": 468,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 868,
                            "end": 887,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1131,
                            "end": 1151,
                            "matchedPaperCorpusId": "7785881"
                        },
                        {
                            "start": 1411,
                            "end": 1432,
                            "matchedPaperCorpusId": "256662263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8125
                }
            ],
            "relevance_judgement": 0.8125,
            "relevance_judgment_input_expanded": "# Title: Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability\n# Venue: arXiv.org\n# Authors: Ashhadul Islam, S. Belhaouari, Amine Bermak\n## Abstract\nThe exponential growth of large language models (LLMs) like ChatGPT has revolutionized artificial intelligence, offering unprecedented capabilities in natural language processing. However, the extensive computational resources required for training these models have significant environmental implications, including high carbon emissions, energy consumption, and water usage. This research presents a novel approach to LLM pruning, focusing on the systematic evaluation of individual weight importance throughout the training process. By monitoring parameter evolution over time, we propose a method that effectively reduces model size without compromising performance. Extensive experiments with both a scaled-down LLM and a large multimodal model reveal that moderate pruning enhances efficiency and reduces loss, while excessive pruning drastically deteriorates model performance. These findings highlight the critical need for optimized AI models to ensure sustainable development, balancing technological advancement with environmental responsibility.\n## Related Work\nMethods like SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023) use sophisticated weight updates and pruning without retraining, respectively, while PST (Li et al., 2022) combines unstructured pruning with efficient fine-tuning. Structured pruning methods (Chen et al., 2021b(Chen et al., , 2023) ) remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency. Techniques such as LLM-Pruner (Ma et al., 2023) and LoRAPrune (Zhang et al., 2023) focus on efficient deployment and inference acceleration, with Sheared-LLaMA (Xia et al., 2023) aiming to prune models to a target architecture and train them dynamically. Furthermore, the compression of language models has garnered significant attention, leading to various methods like network pruning, knowledge distillation, and quantization (Bai et al., 2020;Brown et al., 2020;Devlin, 2018). Pruning, especially structural pruning, remains a crucial focus due to its hardware-friendly nature, with methods varying from l1-dependent pruning (Zafrir et al., 2021) to more advanced techniques like the optimal brain surgeon (LeCun et al., 1989). Efficient compression and low-resource strategies are increasingly essential, with advancements in layer-wise optimal brain surgeon and data-free pruning approaches aiming to optimize the balance between compression efficiency and training data availability (Kurti\u0107 et al., 2024;Srinivas and Babu, 2015).",
            "reference_string": "[276576138 | Islam et al. | 2025 | Citations: 0]"
        },
        {
            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 150,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.15347, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108418112",
                    "name": "Seungcheol Park"
                },
                {
                    "authorId": "2281832669",
                    "name": "Jaehyeon Choi"
                },
                {
                    "authorId": "2281792832",
                    "name": "Sojin Lee"
                },
                {
                    "authorId": "2281746333",
                    "name": "U. Kang"
                }
            ],
            "abstract": "How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.",
            "corpus_id": 267312283,
            "sentences": [
                {
                    "corpus_id": "267312283",
                    "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
                    "text": "Unstructured pruning algorithms [76,122,159,162] have demonstrated high accuracy in compressing language models achieving up to 97% compression rate with a small accuracy degradation. However, their drawback lies in the acceleration difficulty due to the resulting sparse pruning patterns. Therefore, researchers have focused on extending the findings in unstructured pruning algorithms to structured pruning algorithms to achieve acceleration. For example, BMP [82] extends the findings of MvP [122] considering that the magnitude of gradient of an objective function with respect to a weight is more important than the magnitude of the weight. Similarly, ZipLM [77] achieves a good performance on structured pruning of BERT by developing the findings in oBERT [76] that the optimal error compensation strategy of OBS [52] is useful for pruning BERT. [129], *: includes token embeddings, \u2720: a compressed model \u2020: geometric mean of speedups in individual affine transformations When extending unstructured pruning algorithms to structured pruning algorithms, it is crucial to determine the granularity for pruning, considering the characteristics of the model. Michel et al. [107] experimentally demonstrate that MHA sublayers are robust to the pruning of attention heads by showing that approximately 40% of attention heads are prunable with negligible accuracy degradation. Similarly, BMP [82] discovers the pattern that parameters in certain attention heads in MHA sublayers are entirely removed when we perform block-wise pruning with 16x16 weight blocks. \n\nFor FFN sublayers, the weight blocks within specific neurons in intermediate layers are entirely removed, while weight blocks in other neurons are preserved. Based on these findings, subsequent structured pruning algorithms [56,77,81,97,100,105,110,112,155,159] employ attention heads and intermediate neurons as the pruning granularities for MHA and FFN sublayers, respectively.",
                    "score": 0.5391201424548863,
                    "section_title": "Pruning Granularity: Unstructured vs. Structured",
                    "char_start_offset": 20978,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 289
                        },
                        {
                            "start": 290,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 1160
                        },
                        {
                            "start": 1161,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1559
                        },
                        {
                            "start": 1562,
                            "end": 1719
                        },
                        {
                            "start": 1720,
                            "end": 1941
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 32,
                            "end": 36,
                            "matchedPaperCorpusId": "247446572"
                        },
                        {
                            "start": 40,
                            "end": 44,
                            "matchedPaperCorpusId": "245131431"
                        },
                        {
                            "start": 462,
                            "end": 466,
                            "matchedPaperCorpusId": "237485472"
                        },
                        {
                            "start": 762,
                            "end": 766,
                            "matchedPaperCorpusId": "247446572"
                        },
                        {
                            "start": 1391,
                            "end": 1395,
                            "matchedPaperCorpusId": "237485472"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.806640625
                }
            ],
            "relevance_judgement": 0.806640625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Compression Algorithms for Language Models\n# Venue: arXiv.org\n# Authors: Seungcheol Park, Jaehyeon Choi, Sojin Lee, U. Kang\n## Abstract\nHow can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.\n## Pruning Granularity: Unstructured vs. Structured\nUnstructured pruning algorithms [76,122,159,162] have demonstrated high accuracy in compressing language models achieving up to 97% compression rate with a small accuracy degradation. However, their drawback lies in the acceleration difficulty due to the resulting sparse pruning patterns. Therefore, researchers have focused on extending the findings in unstructured pruning algorithms to structured pruning algorithms to achieve acceleration. For example, BMP [82] extends the findings of MvP [122] considering that the magnitude of gradient of an objective function with respect to a weight is more important than the magnitude of the weight. Similarly, ZipLM [77] achieves a good performance on structured pruning of BERT by developing the findings in oBERT [76] that the optimal error compensation strategy of OBS [52] is useful for pruning BERT. [129], *: includes token embeddings, \u2720: a compressed model \u2020: geometric mean of speedups in individual affine transformations When extending unstructured pruning algorithms to structured pruning algorithms, it is crucial to determine the granularity for pruning, considering the characteristics of the model. Michel et al. [107] experimentally demonstrate that MHA sublayers are robust to the pruning of attention heads by showing that approximately 40% of attention heads are prunable with negligible accuracy degradation. Similarly, BMP [82] discovers the pattern that parameters in certain attention heads in MHA sublayers are entirely removed when we perform block-wise pruning with 16x16 weight blocks. \n\nFor FFN sublayers, the weight blocks within specific neurons in intermediate layers are entirely removed, while weight blocks in other neurons are preserved. Based on these findings, subsequent structured pruning algorithms [56,77,81,97,100,105,110,112,155,159] employ attention heads and intermediate neurons as the pruning granularities for MHA and FFN sublayers, respectively.",
            "reference_string": "[267312283 | Park et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Lillama: Large Language Models Compression via Low-Rank Feature Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.16719, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336865596",
                    "name": "Yaya Sy"
                },
                {
                    "authorId": "2257152061",
                    "name": "Christophe Cerisara"
                },
                {
                    "authorId": "1696945",
                    "name": "I. Illina"
                }
            ],
            "abstract": "Current LLM structured pruning methods typically involve two steps: (1) compression with calibration data and (2) costly continued pretraining on billions of tokens to recover lost performance. This second step is necessary as the first significantly impacts model accuracy. Prior research suggests pretrained Transformer weights aren't inherently low-rank, unlike their activations, which may explain this drop. Based on this observation, we propose Lillama, a compression method that locally distills activations with low-rank weights. Using SVD for initialization and a joint loss combining teacher and student activations, we accelerate convergence and reduce memory use with local gradient updates. Lillama compresses Mixtral-8x7B within minutes on a single A100 GPU, removing 10 billion parameters while retaining over 95% of its original performance. Phi-2 3B can be compressed by 40% with just 13 million calibration tokens, resulting in a small model that competes with recent models of similar size. The method generalizes well to non-transformer architectures, compressing Mamba-3B by 20% while maintaining 99% performance.",
            "corpus_id": 274981759,
            "sentences": [
                {
                    "corpus_id": "274981759",
                    "title": "Lillama: Large Language Models Compression via Low-Rank Feature Distillation",
                    "text": "We discuss next previous compression methods, highlight their limitations, and explain how our approach addresses them. Table 1 positions our approach to recent compression methods. \n\nPruning methods remove unimportant weights in the pre-trained model (LeCun et al., 1989;Han et al., 2015). Structured Pruning removes entire groups of parameters, which results in a smaller and faster model (Xia et al., 2024;Ma et al., 2023). Ma et al. (2023) propose a new gradient-based criterion to eliminate substructures in LLMs, while Xia et al. (2024) use a joint loss combining a pruning mask loss with the language modeling loss. However, optimizing these criteria can be computationally intensive. For example, the pruning step of Sheared-LLaMA (Xia et al., 2023) is 5x expensive compared to standard LM training, according to the authors. In contrast, thanks to the local gradient updates, our approach is computationally efficient, allowing us to compress a 47B model within minutes on a single A100 GPU. Regarding unstructured pruning, these methods do not provide any gains in terms of memory or speedup, at least with current algorithmic implementations. Semi-structured pruning (e.g., 2:4 and 4:8) (Sun et al., 2024;Frantar and Alistarh, 2023;Liu et al., 2024) does not lead to memory gain but can speed up processing on kernels optimized for such matrix structures. On the other hand, our method, which directly shrinks matrices, saves memory across all hardware and leads to speed up, as fewer computations are performed. \n\nLow-Rank Decomposition compresses a model by approximating its pre-trained matrices with lowerdimension ones. Based on the observation by Li et al. (2018) that Neural Networks have lower intrinsic dimensions, Aghajanyan et al. (2020) show that Transformer language models also require lower intrinsic dimensions depending on the language task.",
                    "score": 0.5857756351247755,
                    "section_title": "Related Works",
                    "char_start_offset": 2705,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 181
                        },
                        {
                            "start": 184,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1153
                        },
                        {
                            "start": 1154,
                            "end": 1366
                        },
                        {
                            "start": 1367,
                            "end": 1523
                        },
                        {
                            "start": 1526,
                            "end": 1635
                        },
                        {
                            "start": 1636,
                            "end": 1869
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 252,
                            "end": 272,
                            "matchedPaperCorpusId": "7785881"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80615234375
                }
            ],
            "relevance_judgement": 0.80615234375,
            "relevance_judgment_input_expanded": "# Title: Lillama: Large Language Models Compression via Low-Rank Feature Distillation\n# Venue: arXiv.org\n# Authors: Yaya Sy, Christophe Cerisara, I. Illina\n## Abstract\nCurrent LLM structured pruning methods typically involve two steps: (1) compression with calibration data and (2) costly continued pretraining on billions of tokens to recover lost performance. This second step is necessary as the first significantly impacts model accuracy. Prior research suggests pretrained Transformer weights aren't inherently low-rank, unlike their activations, which may explain this drop. Based on this observation, we propose Lillama, a compression method that locally distills activations with low-rank weights. Using SVD for initialization and a joint loss combining teacher and student activations, we accelerate convergence and reduce memory use with local gradient updates. Lillama compresses Mixtral-8x7B within minutes on a single A100 GPU, removing 10 billion parameters while retaining over 95% of its original performance. Phi-2 3B can be compressed by 40% with just 13 million calibration tokens, resulting in a small model that competes with recent models of similar size. The method generalizes well to non-transformer architectures, compressing Mamba-3B by 20% while maintaining 99% performance.\n## Related Works\nWe discuss next previous compression methods, highlight their limitations, and explain how our approach addresses them. Table 1 positions our approach to recent compression methods. \n\nPruning methods remove unimportant weights in the pre-trained model (LeCun et al., 1989;Han et al., 2015). Structured Pruning removes entire groups of parameters, which results in a smaller and faster model (Xia et al., 2024;Ma et al., 2023). Ma et al. (2023) propose a new gradient-based criterion to eliminate substructures in LLMs, while Xia et al. (2024) use a joint loss combining a pruning mask loss with the language modeling loss. However, optimizing these criteria can be computationally intensive. For example, the pruning step of Sheared-LLaMA (Xia et al., 2023) is 5x expensive compared to standard LM training, according to the authors. In contrast, thanks to the local gradient updates, our approach is computationally efficient, allowing us to compress a 47B model within minutes on a single A100 GPU. Regarding unstructured pruning, these methods do not provide any gains in terms of memory or speedup, at least with current algorithmic implementations. Semi-structured pruning (e.g., 2:4 and 4:8) (Sun et al., 2024;Frantar and Alistarh, 2023;Liu et al., 2024) does not lead to memory gain but can speed up processing on kernels optimized for such matrix structures. On the other hand, our method, which directly shrinks matrices, saves memory across all hardware and leads to speed up, as fewer computations are performed. \n\nLow-Rank Decomposition compresses a model by approximating its pre-trained matrices with lowerdimension ones. Based on the observation by Li et al. (2018) that Neural Networks have lower intrinsic dimensions, Aghajanyan et al. (2020) show that Transformer language models also require lower intrinsic dimensions depending on the language task.",
            "reference_string": "[274981759 | Sy et al. | 2024 | Citations: 0]"
        },
        {
            "title": "On Accelerating Edge AI: Optimizing Resource-Constrained Environments",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 133,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.15014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342530769",
                    "name": "Jacob Sander"
                },
                {
                    "authorId": "2342484279",
                    "name": "Achraf Cohen"
                },
                {
                    "authorId": "69387233",
                    "name": "Venkateswara Dasari"
                },
                {
                    "authorId": "1712010",
                    "name": "K. Venable"
                },
                {
                    "authorId": "3116427",
                    "name": "B. Jalaeian"
                }
            ],
            "abstract": "Resource-constrained edge deployments demand AI solutions that balance high performance with stringent compute, memory, and energy limitations. In this survey, we present a comprehensive overview of the primary strategies for accelerating deep learning models under such constraints. First, we examine model compression techniques-pruning, quantization, tensor decomposition, and knowledge distillation-that streamline large models into smaller, faster, and more efficient variants. Next, we explore Neural Architecture Search (NAS), a class of automated methods that discover architectures inherently optimized for particular tasks and hardware budgets. We then discuss compiler and deployment frameworks, such as TVM, TensorRT, and OpenVINO, which provide hardware-tailored optimizations at inference time. By integrating these three pillars into unified pipelines, practitioners can achieve multi-objective goals, including latency reduction, memory savings, and energy efficiency-all while maintaining competitive accuracy. We also highlight emerging frontiers in hierarchical NAS, neurosymbolic approaches, and advanced distillation tailored to large language models, underscoring open challenges like pre-training pruning for massive networks. Our survey offers practical insights, identifies current research gaps, and outlines promising directions for building scalable, platform-independent frameworks to accelerate deep learning models at the edge.",
            "corpus_id": 275920740,
            "sentences": [
                {
                    "corpus_id": "275920740",
                    "title": "On Accelerating Edge AI: Optimizing Resource-Constrained Environments",
                    "text": "While structured pruning offers consistent inference acceleration across hardware, unstructured pruning often achieves higher compression ratios but requires specialized hardware or compiler optimizations to realize speedups. \n\nStructured pruning maximizes speed improvements by carefully selecting architectural elements to prune in a way that minimizes performance degradation. Semi-structured pruning is a catch-all category for many methods that blend elements of structured and unstructured pruning Xu et al. [2024a], Ma et al. [2020], Meng et al. [2020]. By sequentially performing coarse pruning and then fine-grained pruning of the remaining structures, authors achieve greater compression levels while retaining performance. \n\nAs we mentioned, pruning seeks to decrease neural networks' size and computational expenses by eliminating weights or structural elements that minimally affect overall performance. Following this, we will introduce various pruning strategies-which differ mainly by the timing of their application-and emphasize significant research gaps. \n\nPre-Training Pruning: One forward-looking method is to prune networks based on randomly initialized weights Wang et al. [2020a]. Though appealing for saving training time (because pruned weights require no subsequent computation), this approach risks issues like layer collapse Tanaka et al. [2020]. It has been applied to convolutional neural nets (CNNs) Lee et al. [2019Lee et al. [ , 2020]], but scaling it up to large models remains challenging, given the expense of training even pruned networks. \n\nPruning During Training: Another strategy embeds pruning into the training loop, iteratively updating which weights remain active. For instance, the RigL algorithm Evci et al. [2021] periodically removes and regrows weights to maintain model capacity. Structured sparsity learning (SSL) Wen et al. [2016], network slimming Liu et al. [2017], and differentiable methods like Differential Sparsity Allocation (DSA) Ning et al. [2020] similarly integrate pruning decisions with gradient-based updates. In neural architecture search, a related concept called Progressive Shrinking alternates between pruning and training to explore potential architectures Wang et al. [2020b].",
                    "score": 0.5169669868563971,
                    "section_title": "Pruning",
                    "char_start_offset": 6723,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 225
                        },
                        {
                            "start": 228,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 733
                        },
                        {
                            "start": 736,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1073
                        },
                        {
                            "start": 1076,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1577
                        },
                        {
                            "start": 1580,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 2078
                        },
                        {
                            "start": 2079,
                            "end": 2252
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80126953125
                }
            ],
            "relevance_judgement": 0.80126953125,
            "relevance_judgment_input_expanded": "# Title: On Accelerating Edge AI: Optimizing Resource-Constrained Environments\n# Venue: arXiv.org\n# Authors: Jacob Sander, Achraf Cohen, Venkateswara Dasari, K. Venable, B. Jalaeian\n## Abstract\nResource-constrained edge deployments demand AI solutions that balance high performance with stringent compute, memory, and energy limitations. In this survey, we present a comprehensive overview of the primary strategies for accelerating deep learning models under such constraints. First, we examine model compression techniques-pruning, quantization, tensor decomposition, and knowledge distillation-that streamline large models into smaller, faster, and more efficient variants. Next, we explore Neural Architecture Search (NAS), a class of automated methods that discover architectures inherently optimized for particular tasks and hardware budgets. We then discuss compiler and deployment frameworks, such as TVM, TensorRT, and OpenVINO, which provide hardware-tailored optimizations at inference time. By integrating these three pillars into unified pipelines, practitioners can achieve multi-objective goals, including latency reduction, memory savings, and energy efficiency-all while maintaining competitive accuracy. We also highlight emerging frontiers in hierarchical NAS, neurosymbolic approaches, and advanced distillation tailored to large language models, underscoring open challenges like pre-training pruning for massive networks. Our survey offers practical insights, identifies current research gaps, and outlines promising directions for building scalable, platform-independent frameworks to accelerate deep learning models at the edge.\n## Pruning\nWhile structured pruning offers consistent inference acceleration across hardware, unstructured pruning often achieves higher compression ratios but requires specialized hardware or compiler optimizations to realize speedups. \n\nStructured pruning maximizes speed improvements by carefully selecting architectural elements to prune in a way that minimizes performance degradation. Semi-structured pruning is a catch-all category for many methods that blend elements of structured and unstructured pruning Xu et al. [2024a], Ma et al. [2020], Meng et al. [2020]. By sequentially performing coarse pruning and then fine-grained pruning of the remaining structures, authors achieve greater compression levels while retaining performance. \n\nAs we mentioned, pruning seeks to decrease neural networks' size and computational expenses by eliminating weights or structural elements that minimally affect overall performance. Following this, we will introduce various pruning strategies-which differ mainly by the timing of their application-and emphasize significant research gaps. \n\nPre-Training Pruning: One forward-looking method is to prune networks based on randomly initialized weights Wang et al. [2020a]. Though appealing for saving training time (because pruned weights require no subsequent computation), this approach risks issues like layer collapse Tanaka et al. [2020]. It has been applied to convolutional neural nets (CNNs) Lee et al. [2019Lee et al. [ , 2020]], but scaling it up to large models remains challenging, given the expense of training even pruned networks. \n\nPruning During Training: Another strategy embeds pruning into the training loop, iteratively updating which weights remain active. For instance, the RigL algorithm Evci et al. [2021] periodically removes and regrows weights to maintain model capacity. Structured sparsity learning (SSL) Wen et al. [2016], network slimming Liu et al. [2017], and differentiable methods like Differential Sparsity Allocation (DSA) Ning et al. [2020] similarly integrate pruning decisions with gradient-based updates. In neural architecture search, a related concept called Progressive Shrinking alternates between pruning and training to explore potential architectures Wang et al. [2020b].",
            "reference_string": "[275920740 | Sander et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 50,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.21174, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2331613426",
                    "name": "Leandro Giusti Mugnaini"
                },
                {
                    "authorId": "2303405382",
                    "name": "B. Yamamoto"
                },
                {
                    "authorId": "2358264111",
                    "name": "Lucas Lauton de Alcantara"
                },
                {
                    "authorId": "2358264489",
                    "name": "Victor Zacarias"
                },
                {
                    "authorId": "2358263602",
                    "name": "Edson Bollis"
                },
                {
                    "authorId": "2344249909",
                    "name": "Lucas F. A. O. Pellicer"
                },
                {
                    "authorId": "2303454322",
                    "name": "A. H. R. Costa"
                },
                {
                    "authorId": "2303404119",
                    "name": "Artur Jordao"
                }
            ],
            "abstract": "Deep learning drives a new wave in computing systems and triggers the automation of increasingly complex problems. In particular, Large Language Models (LLMs) have significantly advanced cognitive tasks, often matching or even surpassing human-level performance. However, their extensive parameters result in high computational costs and slow inference, posing challenges for deployment in resource-limited settings. Among the strategies to overcome the aforementioned challenges, pruning emerges as a successful mechanism since it reduces model size while maintaining predictive ability. In this paper, we introduce AMP: Attention Heads and MLP Pruning, a novel structured pruning method that efficiently compresses LLMs by removing less critical structures within Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By projecting the input data onto weights, AMP assesses structural importance and overcomes the limitations of existing techniques, which often fall short in flexibility or efficiency. In particular, AMP surpasses the current state-of-the-art on commonsense reasoning tasks by up to 1.49 percentage points, achieving a 30% pruning ratio with minimal impact on zero-shot task performance. Moreover, AMP also improves inference speeds, making it well-suited for deployment in resource-constrained environments. We confirm the flexibility of AMP on different families of LLMs, including LLaMA and Phi.",
            "corpus_id": 278208127,
            "sentences": [
                {
                    "corpus_id": "278208127",
                    "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning",
                    "text": "Within the evolving landscape of Artificial Intelligence, Large Language Models (LLMs) stand out as a pivotal force, propelling Natural Language Processing towards unprecedented stages -often matching or even surpassing human-level performance in many language benchmarks [1]- [3]. However, this performance often comes at the cost of larger model sizes, with models such as DeepSeek-V3 [4] reaching the mark of 671 billion parameters. \n\nThis substantial size introduces significant challenges for the deployment of LLMs in low-resource and time-critical applications due to high computational costs and slow inference [5]. To overcome these challenges, some techniques such as pruning [6] and quantization [7], [8] aim to reduce the model size, thereby lowering computational overhead. \n\nRecent studies confirm pruning as a promising solution to compress models as it maintains predictive ability and is often hardware-agnostic [9]- [13]. Within the field of LLMs, pruning \u00a7 Equal contribution. \n\ntechniques fall into three main categories: structured, semistructured and unstructured pruning. \n\nStructured pruning removes entire components -such as attention heads or layers -while preserving the overall network structure without introducing sparsity (i.e., without zeroing out a significant portion of the model's parameters) [6]. However, the removal of larger and potentially more critical components may result in performance degradation, typically requiring Parameter-Efficient Fine-Tuning (PEFT) techniques for performance recovery [14]. Due to the removal of complete components, structured pruning usually achieves inference acceleration and memory reduction without the need for specialized hardware or software [15]. \n\nSemi-structured (a.k.a. structured N : M ) pruning promotes model sparsity by removing groups of consecutive parameters following a pruning mask [16]. Specifically, structured N : M sparsity requires that at least N out of every M consecutive weights be non-zero [17], [18]. While this is a promising technique, it requires specialized hardware to achieve practical speedup, making it less suitable for deployment on consumergrade GPUs [17].",
                    "score": 0.6215707030653611,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 435
                        },
                        {
                            "start": 438,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 786
                        },
                        {
                            "start": 789,
                            "end": 939
                        },
                        {
                            "start": 940,
                            "end": 995
                        },
                        {
                            "start": 998,
                            "end": 1094
                        },
                        {
                            "start": 1097,
                            "end": 1334
                        },
                        {
                            "start": 1335,
                            "end": 1546
                        },
                        {
                            "start": 1547,
                            "end": 1729
                        },
                        {
                            "start": 1732,
                            "end": 1755
                        },
                        {
                            "start": 1756,
                            "end": 1882
                        },
                        {
                            "start": 1883,
                            "end": 2006
                        },
                        {
                            "start": 2007,
                            "end": 2173
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 277,
                            "end": 280,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 619,
                            "end": 622,
                            "matchedPaperCorpusId": "257495837"
                        },
                        {
                            "start": 707,
                            "end": 710,
                            "matchedPaperCorpusId": "259298689"
                        },
                        {
                            "start": 929,
                            "end": 932,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 934,
                            "end": 938,
                            "matchedPaperCorpusId": "266573164"
                        },
                        {
                            "start": 1541,
                            "end": 1545,
                            "matchedPaperCorpusId": "268553763"
                        },
                        {
                            "start": 1724,
                            "end": 1728,
                            "matchedPaperCorpusId": "267413136"
                        },
                        {
                            "start": 1877,
                            "end": 1881,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1995,
                            "end": 1999,
                            "matchedPaperCorpusId": "231847094"
                        },
                        {
                            "start": 2001,
                            "end": 2005,
                            "matchedPaperCorpusId": "233296249"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7861328125
                }
            ],
            "relevance_judgement": 0.7861328125,
            "relevance_judgment_input_expanded": "# Title: Efficient LLMs with AMP: Attention Heads and MLP Pruning\n# Venue: arXiv.org\n# Authors: Leandro Giusti Mugnaini, B. Yamamoto, Lucas Lauton de Alcantara, Victor Zacarias, Edson Bollis, Lucas F. A. O. Pellicer, A. H. R. Costa, Artur Jordao\n## Abstract\nDeep learning drives a new wave in computing systems and triggers the automation of increasingly complex problems. In particular, Large Language Models (LLMs) have significantly advanced cognitive tasks, often matching or even surpassing human-level performance. However, their extensive parameters result in high computational costs and slow inference, posing challenges for deployment in resource-limited settings. Among the strategies to overcome the aforementioned challenges, pruning emerges as a successful mechanism since it reduces model size while maintaining predictive ability. In this paper, we introduce AMP: Attention Heads and MLP Pruning, a novel structured pruning method that efficiently compresses LLMs by removing less critical structures within Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By projecting the input data onto weights, AMP assesses structural importance and overcomes the limitations of existing techniques, which often fall short in flexibility or efficiency. In particular, AMP surpasses the current state-of-the-art on commonsense reasoning tasks by up to 1.49 percentage points, achieving a 30% pruning ratio with minimal impact on zero-shot task performance. Moreover, AMP also improves inference speeds, making it well-suited for deployment in resource-constrained environments. We confirm the flexibility of AMP on different families of LLMs, including LLaMA and Phi.\n## I. INTRODUCTION\nWithin the evolving landscape of Artificial Intelligence, Large Language Models (LLMs) stand out as a pivotal force, propelling Natural Language Processing towards unprecedented stages -often matching or even surpassing human-level performance in many language benchmarks [1]- [3]. However, this performance often comes at the cost of larger model sizes, with models such as DeepSeek-V3 [4] reaching the mark of 671 billion parameters. \n\nThis substantial size introduces significant challenges for the deployment of LLMs in low-resource and time-critical applications due to high computational costs and slow inference [5]. To overcome these challenges, some techniques such as pruning [6] and quantization [7], [8] aim to reduce the model size, thereby lowering computational overhead. \n\nRecent studies confirm pruning as a promising solution to compress models as it maintains predictive ability and is often hardware-agnostic [9]- [13]. Within the field of LLMs, pruning \u00a7 Equal contribution. \n\ntechniques fall into three main categories: structured, semistructured and unstructured pruning. \n\nStructured pruning removes entire components -such as attention heads or layers -while preserving the overall network structure without introducing sparsity (i.e., without zeroing out a significant portion of the model's parameters) [6]. However, the removal of larger and potentially more critical components may result in performance degradation, typically requiring Parameter-Efficient Fine-Tuning (PEFT) techniques for performance recovery [14]. Due to the removal of complete components, structured pruning usually achieves inference acceleration and memory reduction without the need for specialized hardware or software [15]. \n\nSemi-structured (a.k.a. structured N : M ) pruning promotes model sparsity by removing groups of consecutive parameters following a pruning mask [16]. Specifically, structured N : M sparsity requires that at least N out of every M consecutive weights be non-zero [17], [18]. While this is a promising technique, it requires specialized hardware to achieve practical speedup, making it less suitable for deployment on consumergrade GPUs [17].",
            "reference_string": "[278208127 | Mugnaini et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Thanos: A Block-wise Pruning Algorithm for Efficient Large Language Model Compression",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 53,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.05346, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268766339",
                    "name": "Ivan Ilin"
                },
                {
                    "authorId": "2268766087",
                    "name": "Peter Richt\u00e1rik"
                }
            ],
            "abstract": "This paper presents Thanos, a novel weight-pruning algorithm designed to reduce the memory footprint and enhance the computational efficiency of large language models (LLMs) by removing redundant weights while maintaining accuracy. Thanos introduces a block-wise pruning strategy with adaptive masks that dynamically adjust to weight importance, enabling flexible sparsity patterns and structured formats, such as $n:m$ sparsity, optimized for hardware acceleration. Experimental evaluations demonstrate that Thanos achieves state-of-the-art performance in structured pruning and outperforms existing methods in unstructured pruning. By providing an efficient and adaptable approach to model compression, Thanos offers a practical solution for deploying large models in resource-constrained environments.",
            "corpus_id": 277626866,
            "sentences": [],
            "relevance_judgement": 0.7861328125,
            "relevance_judgment_input_expanded": "# Title: Thanos: A Block-wise Pruning Algorithm for Efficient Large Language Model Compression\n# Venue: arXiv.org\n# Authors: Ivan Ilin, Peter Richt\u00e1rik\n## Abstract\nThis paper presents Thanos, a novel weight-pruning algorithm designed to reduce the memory footprint and enhance the computational efficiency of large language models (LLMs) by removing redundant weights while maintaining accuracy. Thanos introduces a block-wise pruning strategy with adaptive masks that dynamically adjust to weight importance, enabling flexible sparsity patterns and structured formats, such as $n:m$ sparsity, optimized for hardware acceleration. Experimental evaluations demonstrate that Thanos achieves state-of-the-art performance in structured pruning and outperforms existing methods in unstructured pruning. By providing an efficient and adaptable approach to model compression, Thanos offers a practical solution for deploying large models in resource-constrained environments.\n",
            "reference_string": "[277626866 | Ilin et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.00985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34920835",
                    "name": "Ayan Sengupta"
                },
                {
                    "authorId": "2345922770",
                    "name": "Yash Goel"
                },
                {
                    "authorId": "2249914540",
                    "name": "Tanmoy Chakraborty"
                }
            ],
            "abstract": "We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.",
            "corpus_id": 278310893,
            "sentences": [
                {
                    "corpus_id": "278310893",
                    "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
                    "text": "A branch of research has focused on compressing LLMs to reduce computational and hardware requirements using various pruning techniques. Unstructured pruning (Sun et al., 2023) removes individual weights, producing sparse matrices that maintain performance but are less hardwareefficient. Semi-structured pruning (Frantar & Alistarh, 2023), such as the 2:4 sparsity pattern (Pool et al., 2021), introduces a hardware-friendly structured sparsity that accelerates computation. Structured pruning (Ashkboos et al., 2024;Yuan et al., 2023;Sengupta et al., 2025) takes a broader approach by removing entire components, such as Transformer layers (depth pruning) (Fan et al., 2019) or reducing embedding dimensions and attention heads (width pruning) (Zhu et al., 2021). After pruning, post-training is crucial to mitigate performance degradation. This involves fine-tuning or continual pre-training on datasets tailored to enhance performance recovery while maintaining efficiency. The P2 law (Chen et al., 2024b), highlighted in Equation 11, provides a predictive framework for the posttraining loss, considering factors such as pruning rate, model size, pre-pruning loss, and the number of training tokens. This law enables to balance computational costs with performance recovery by identifying optimal post-training data sizes. While higher pruning rates inevitably lead to larger initial losses, effective post-training strategies significantly minimize this impact, allowing smaller models to perform on par with their larger counterparts in many scenarios. \n\nwhere L 0 is the uncompressed model loss, \u03c1 is the pruning rate, N 0 is the model size before pruning, D is the number of post-training tokens, and N C , D C , E, \u03b1, \u03b2, \u03b3 are fitting constants.",
                    "score": 0.570205845990089,
                    "section_title": "Model Downscaling",
                    "char_start_offset": 16924,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 288
                        },
                        {
                            "start": 289,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 842
                        },
                        {
                            "start": 843,
                            "end": 977
                        },
                        {
                            "start": 978,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1327
                        },
                        {
                            "start": 1328,
                            "end": 1559
                        },
                        {
                            "start": 1562,
                            "end": 1755
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78271484375
                }
            ],
            "relevance_judgement": 0.78271484375,
            "relevance_judgment_input_expanded": "# Title: Position: Enough of Scaling LLMs! Lets Focus on Downscaling\n# Venue: arXiv.org\n# Authors: Ayan Sengupta, Yash Goel, Tanmoy Chakraborty\n## Abstract\nWe challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.\n## Model Downscaling\nA branch of research has focused on compressing LLMs to reduce computational and hardware requirements using various pruning techniques. Unstructured pruning (Sun et al., 2023) removes individual weights, producing sparse matrices that maintain performance but are less hardwareefficient. Semi-structured pruning (Frantar & Alistarh, 2023), such as the 2:4 sparsity pattern (Pool et al., 2021), introduces a hardware-friendly structured sparsity that accelerates computation. Structured pruning (Ashkboos et al., 2024;Yuan et al., 2023;Sengupta et al., 2025) takes a broader approach by removing entire components, such as Transformer layers (depth pruning) (Fan et al., 2019) or reducing embedding dimensions and attention heads (width pruning) (Zhu et al., 2021). After pruning, post-training is crucial to mitigate performance degradation. This involves fine-tuning or continual pre-training on datasets tailored to enhance performance recovery while maintaining efficiency. The P2 law (Chen et al., 2024b), highlighted in Equation 11, provides a predictive framework for the posttraining loss, considering factors such as pruning rate, model size, pre-pruning loss, and the number of training tokens. This law enables to balance computational costs with performance recovery by identifying optimal post-training data sizes. While higher pruning rates inevitably lead to larger initial losses, effective post-training strategies significantly minimize this impact, allowing smaller models to perform on par with their larger counterparts in many scenarios. \n\nwhere L 0 is the uncompressed model loss, \u03c1 is the pruning rate, N 0 is the model size before pruning, D is the number of post-training tokens, and N C , D C , E, \u03b1, \u03b2, \u03b3 are fitting constants.",
            "reference_string": "[278310893 | Sengupta et al. | 2025 | Citations: 0]"
        },
        {
            "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.07780, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2331680259",
                    "name": "Shengkun Tang"
                },
                {
                    "authorId": "2326834165",
                    "name": "Oliver Sieberling"
                },
                {
                    "authorId": "40992614",
                    "name": "Eldar Kurtic"
                },
                {
                    "authorId": "2344981603",
                    "name": "Zhiqiang Shen"
                },
                {
                    "authorId": "3311387",
                    "name": "Dan Alistarh"
                }
            ],
            "abstract": "Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose DarwinLM, a method for training-aware structured pruning. DarwinLM builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, DarwinLM surpasses ShearedLlama while requiring 5x less training data during post-compression training. Code is at: https://github.com/IST-DASLab/DarwinLM",
            "corpus_id": 276259010,
            "sentences": [
                {
                    "corpus_id": "276259010",
                    "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
                    "text": "The high accuracy of Transformer-based models on a wide range of tasks comes with massive computational requirements, which hinders deployability. A long line of research has been conducted to reduce the computational cost of large language models via methods such as quantization (Frantar et al., 2022;Dettmers et al., 2023), pruning (Xia et al., 2024;Frantar & Alistarh, 2023) and distillation (Hsieh et al., 2023). In this work, we explore structured pruning on large language models (LLMs) by removing whole rows or columns in a weight matrix, resulting in regular but smaller matrices. Unlike unstructured pruning (Frantar & Alistarh, 2023), the model produced by structured pruning can be accelerated on mainstream hardware without any specific design for computation. \n\nWhile conventional pruning methods generally prune each layer or block uniformly, non-uniform compression methods, e.g. (Yin et al., 2023;Sieberling et al., 2024), showed that different LLM layers can have significantly different \"sensitivities\" to pruning, which can be leveraged to obtain higher compression while preserving accuracy. To address this, smaller-scale pruning methods such as ZipLM (Kurti\u0107 et al., 2024) propose to utilize a dynamic-programmingbased search (Frantar & Alistarh, 2022) to obtain a sparse model with runtime guarantees. However, there are several challenges for methods such as ZipLM on large-scale models such as Llama-2-7B (Touvron et al., 2023): for instance, ZipLM only considers the local layer-wise error during the search, which is not consistent with performance on in-context learning (ICL) or downstream tasks. \n\nOverview. In this paper, we propose a new structured pruning method based on evolutionary search called DarwinLM, which addresses these issues. Specifically, the search starts from a \"parent\" model, generated by pruning the original model using second-order information.",
                    "score": 0.5399151981622122,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 774
                        },
                        {
                            "start": 777,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1627
                        },
                        {
                            "start": 1630,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1773
                        },
                        {
                            "start": 1774,
                            "end": 1900
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 915,
                            "end": 939,
                            "matchedPaperCorpusId": "273482154"
                        },
                        {
                            "start": 1175,
                            "end": 1196,
                            "matchedPaperCorpusId": "247446572"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.779296875
                }
            ],
            "relevance_judgement": 0.779296875,
            "relevance_judgment_input_expanded": "# Title: DarwinLM: Evolutionary Structured Pruning of Large Language Models\n# Venue: arXiv.org\n# Authors: Shengkun Tang, Oliver Sieberling, Eldar Kurtic, Zhiqiang Shen, Dan Alistarh\n## Abstract\nLarge Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose DarwinLM, a method for training-aware structured pruning. DarwinLM builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, DarwinLM surpasses ShearedLlama while requiring 5x less training data during post-compression training. Code is at: https://github.com/IST-DASLab/DarwinLM\n## Introduction\nThe high accuracy of Transformer-based models on a wide range of tasks comes with massive computational requirements, which hinders deployability. A long line of research has been conducted to reduce the computational cost of large language models via methods such as quantization (Frantar et al., 2022;Dettmers et al., 2023), pruning (Xia et al., 2024;Frantar & Alistarh, 2023) and distillation (Hsieh et al., 2023). In this work, we explore structured pruning on large language models (LLMs) by removing whole rows or columns in a weight matrix, resulting in regular but smaller matrices. Unlike unstructured pruning (Frantar & Alistarh, 2023), the model produced by structured pruning can be accelerated on mainstream hardware without any specific design for computation. \n\nWhile conventional pruning methods generally prune each layer or block uniformly, non-uniform compression methods, e.g. (Yin et al., 2023;Sieberling et al., 2024), showed that different LLM layers can have significantly different \"sensitivities\" to pruning, which can be leveraged to obtain higher compression while preserving accuracy. To address this, smaller-scale pruning methods such as ZipLM (Kurti\u0107 et al., 2024) propose to utilize a dynamic-programmingbased search (Frantar & Alistarh, 2022) to obtain a sparse model with runtime guarantees. However, there are several challenges for methods such as ZipLM on large-scale models such as Llama-2-7B (Touvron et al., 2023): for instance, ZipLM only considers the local layer-wise error during the search, which is not consistent with performance on in-context learning (ICL) or downstream tasks. \n\nOverview. In this paper, we propose a new structured pruning method based on evolutionary search called DarwinLM, which addresses these issues. Specifically, the search starts from a \"parent\" model, generated by pruning the original model using second-order information.",
            "reference_string": "[276259010 | Tang et al. | 2025 | Citations: 3]"
        },
        {
            "title": "CULL-MT: Compression Using Language and Layer pruning for Machine Translation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.06506, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279336018",
                    "name": "Pedram Rostami"
                },
                {
                    "authorId": "1702695",
                    "name": "M. Dousti"
                }
            ],
            "abstract": "Multilingual machine translation models often outperform traditional bilingual models by leveraging translation knowledge transfer. Recent advancements have led to these models supporting hundreds of languages and achieving state-of-the-art results across various translation directions. However, as these models grow larger, their inference operations become increasingly costly. In many use cases, there is no need to support such a wide range of language pairs, as translation is typically needed in only a few selected directions. In this paper, we present CULL-MT, a compression method for machine translation models based on structural layer pruning and selected language directions. Our approach identifies and prunes unimportant layers using a greedy strategy, then mitigates the impact by applying knowledge distillation from the original model along with parameter-efficient fine-tuning. We apply CULL-MT to the NLLB-3.3B and LLaMA3.1-8B-Instruct models. In a multi-way translation scenario (Persian, French, and German to English), we find the NLLB-3.3B model to be robust, allowing 25% of layers to be pruned with only a 0.9 spBLEU drop. However, LLaMA3.1-8B-Instruct is more sensitive, with a 2.0 spBLEU drop after pruning 5 layers.",
            "corpus_id": 273963228,
            "sentences": [
                {
                    "corpus_id": "273963228",
                    "title": "CULL-MT: Compression Using Language and Layer pruning for Machine Translation",
                    "text": "Pruning (LeCun et al., 1989) is a key approach for compressing neural networks, aiming to identify and remove less important weights using various methods. While unstructured pruning allows for the sparsification of specific weights, it often necessitates specialized hardware for speed improve- ments (Cheng et al., 2024). In contrast, structural pruning (Molchanov et al., 2017) focuses on removing entire filters, blocks, or layers, providing efficiency that is not dependent on specific devices (Cheng et al., 2024). Voita et al. (2019) indicated that the top layers of pre-trained transformer (Vaswani et al., 2017) models are often specialized for the original training objectives. Therefore, Sajjad et al. (2023) showed that the higher layers are not essential for finetuning on downstream tasks and they can be pruned. Peer et al. (2022) introduce a Greedy Layer Pruning (GLP) method that iteratively selects layers to remove, optimizing performance on GLUE tasks with encoder-only models. While their focus is on GLUE tasks, we concentrate on translation tasks involving encoder-decoder and decoder-only architectures. \n\nRecent works concentrate on pruning LLMs. Ma et al. (2023) developed LLM-Pruner, which uses structural pruning to remove non-critical coupled structures based on gradient information, effectively compressing large language models while maintaining their core functionality. This method allows for performance recovery with tuning techniques like LoRA and demonstrates satisfactory zero-shot task capabilities across various models. Furthermore, Gromov et al. (2024) highlighted that deeper layers often exhibit highly similar representations. This similarity allows for significant model compression, particularly by pruning deeper layers starting from the penultimate layer, which can be achieved without major performance loss when combined with light fine-tuning.",
                    "score": 0.5162307891948567,
                    "section_title": "Structural Pruning",
                    "char_start_offset": 6917,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1127
                        },
                        {
                            "start": 1130,
                            "end": 1171
                        },
                        {
                            "start": 1172,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1896
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 8,
                            "end": 27,
                            "matchedPaperCorpusId": "7785881"
                        },
                        {
                            "start": 302,
                            "end": 322,
                            "matchedPaperCorpusId": "260887757"
                        },
                        {
                            "start": 356,
                            "end": 380,
                            "matchedPaperCorpusId": "17240902"
                        },
                        {
                            "start": 499,
                            "end": 519,
                            "matchedPaperCorpusId": "260887757"
                        },
                        {
                            "start": 521,
                            "end": 540,
                            "matchedPaperCorpusId": "202541078"
                        },
                        {
                            "start": 598,
                            "end": 620,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 699,
                            "end": 719,
                            "matchedPaperCorpusId": "251005814"
                        },
                        {
                            "start": 827,
                            "end": 845,
                            "matchedPaperCorpusId": "247771234"
                        },
                        {
                            "start": 1172,
                            "end": 1188,
                            "matchedPaperCorpusId": "258823276"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77880859375
                }
            ],
            "relevance_judgement": 0.77880859375,
            "relevance_judgment_input_expanded": "# Title: CULL-MT: Compression Using Language and Layer pruning for Machine Translation\n# Venue: arXiv.org\n# Authors: Pedram Rostami, M. Dousti\n## Abstract\nMultilingual machine translation models often outperform traditional bilingual models by leveraging translation knowledge transfer. Recent advancements have led to these models supporting hundreds of languages and achieving state-of-the-art results across various translation directions. However, as these models grow larger, their inference operations become increasingly costly. In many use cases, there is no need to support such a wide range of language pairs, as translation is typically needed in only a few selected directions. In this paper, we present CULL-MT, a compression method for machine translation models based on structural layer pruning and selected language directions. Our approach identifies and prunes unimportant layers using a greedy strategy, then mitigates the impact by applying knowledge distillation from the original model along with parameter-efficient fine-tuning. We apply CULL-MT to the NLLB-3.3B and LLaMA3.1-8B-Instruct models. In a multi-way translation scenario (Persian, French, and German to English), we find the NLLB-3.3B model to be robust, allowing 25% of layers to be pruned with only a 0.9 spBLEU drop. However, LLaMA3.1-8B-Instruct is more sensitive, with a 2.0 spBLEU drop after pruning 5 layers.\n## Structural Pruning\nPruning (LeCun et al., 1989) is a key approach for compressing neural networks, aiming to identify and remove less important weights using various methods. While unstructured pruning allows for the sparsification of specific weights, it often necessitates specialized hardware for speed improve- ments (Cheng et al., 2024). In contrast, structural pruning (Molchanov et al., 2017) focuses on removing entire filters, blocks, or layers, providing efficiency that is not dependent on specific devices (Cheng et al., 2024). Voita et al. (2019) indicated that the top layers of pre-trained transformer (Vaswani et al., 2017) models are often specialized for the original training objectives. Therefore, Sajjad et al. (2023) showed that the higher layers are not essential for finetuning on downstream tasks and they can be pruned. Peer et al. (2022) introduce a Greedy Layer Pruning (GLP) method that iteratively selects layers to remove, optimizing performance on GLUE tasks with encoder-only models. While their focus is on GLUE tasks, we concentrate on translation tasks involving encoder-decoder and decoder-only architectures. \n\nRecent works concentrate on pruning LLMs. Ma et al. (2023) developed LLM-Pruner, which uses structural pruning to remove non-critical coupled structures based on gradient information, effectively compressing large language models while maintaining their core functionality. This method allows for performance recovery with tuning techniques like LoRA and demonstrates satisfactory zero-shot task capabilities across various models. Furthermore, Gromov et al. (2024) highlighted that deeper layers often exhibit highly similar representations. This similarity allows for significant model compression, particularly by pruning deeper layers starting from the penultimate layer, which can be achieved without major performance loss when combined with light fine-tuning.",
            "reference_string": "[273963228 | Rostami et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Structured Pruning for Efficient Generative Pre-trained Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 44,
            "citation_count": 23,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-acl.692.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.692?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.692, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144259094",
                    "name": "Chaofan Tao"
                },
                {
                    "authorId": "48557122",
                    "name": "Lu Hou"
                },
                {
                    "authorId": "9583912",
                    "name": "Haoli Bai"
                },
                {
                    "authorId": "2141318792",
                    "name": "Jiansheng Wei"
                },
                {
                    "authorId": "2110310493",
                    "name": "Xin Jiang"
                },
                {
                    "authorId": "30738758",
                    "name": "Qun Liu"
                },
                {
                    "authorId": "47571885",
                    "name": "Ping Luo"
                },
                {
                    "authorId": "1873081",
                    "name": "Ngai Wong"
                }
            ],
            "abstract": "The increasing sizes of large generative Pre-trained Language Models (PLMs) hinder their deployment in real-world applications. To obtain ef\ufb01cient PLMs, previous studies mostly focus on pruning the attention heads and feed-forward networks (FFNs) of the Transformer. Nevertheless, we \ufb01nd that in generative PLMs, the hidden dimension shared by many other modules (e.g., embedding layer and layer normalization) contains persistent outliers regardless of the network input. In this study, we propose S IMPLE , a new structured pruning framework for generative PLMs that comprehensively investigates all the above compress-ible components. To identify redundant network structures, we assign learnable masks over compressible components followed by sparse training. Various sizes of PLMs can be \ufb02exibly extracted via different thresholds, and are then task-speci\ufb01cally \ufb01ne-tuned for further improvement. Extensive experiments on language modeling, summarization and machine translation validate the effectiveness of the proposed method. For example, the pruned BART brings 1.51x/6.96x inference speedup on GPU/CPU with 67% size reduction, and can be further combined with quantization for more than 25 \u00d7 compression.",
            "corpus_id": 259858812,
            "sentences": [
                {
                    "corpus_id": "259858812",
                    "title": "Structured Pruning for Efficient Generative Pre-trained Language Models",
                    "text": "Large-scale generative pretrained language models (PLMs) (Radford and Narasimhan, 2018;Brown et al., 2020;Lewis et al., 2020;Raffel et al., 2020) show remarkable performance on various tasks. However, their increasing sizes also lead to expensive memory and computation, hindering their deployment in real applications. \n\nRecent attempts (Tao et al., 2022;Frantar et al., 2022;Dettmers et al., 2022;Xiao et al., 2022;Wang et al., 2021) propose to compress generative PLMs models by quantization. However, hardware-dependent low-bit kernels need to be specially developed for real inference speedup. Com-pared to quantization, structured pruning methods prune parts of the model structures without requiring designing extra operators to achieve inference speedup and run-time memory saving. Recently, Anonymous (2023) show that the feed-forward networks (FFNs) of GPT models can be pruned to smaller widths, and Li et al. (2022) compress the BART models by combining layer pruning and model quantization for a higher compression rate. However, these models consider only limited components for pruning, i.e., the FFNs or Transformer layers, which can be restrictive for various deployment requirements. \n\nIn this work, we propose a new structured pruning framework named SIMPLE (Sparsity-Induced Mask learning for Pruning generative pre-trained Language modEls), which offers a wider range of compressible components. Aside from the attention heads and the width of FFNs commonly used for structured pruning of discriminative PLMs, we also propose to prune the hidden dimension, to further push the trade-off between performance and model size. It is motivated by the observation that persistent outliers exist in the hidden dimension of both decoder-only and encoder-decoder generative PLMs. The observation implies that the hidden dimension may be slimmed sharply with a slight performance drop.",
                    "score": 0.4956540846134376,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 319
                        },
                        {
                            "start": 322,
                            "end": 495
                        },
                        {
                            "start": 496,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 789
                        },
                        {
                            "start": 790,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1201
                        },
                        {
                            "start": 1204,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1643
                        },
                        {
                            "start": 1644,
                            "end": 1791
                        },
                        {
                            "start": 1792,
                            "end": 1896
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 87,
                            "end": 106,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 106,
                            "end": 125,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 125,
                            "end": 145,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 377,
                            "end": 399,
                            "matchedPaperCorpusId": "251564521"
                        },
                        {
                            "start": 800,
                            "end": 816,
                            "matchedPaperCorpusId": "256662734"
                        },
                        {
                            "start": 911,
                            "end": 927,
                            "matchedPaperCorpusId": "247596810"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77880859375
                }
            ],
            "relevance_judgement": 0.77880859375,
            "relevance_judgment_input_expanded": "# Title: Structured Pruning for Efficient Generative Pre-trained Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Chaofan Tao, Lu Hou, Haoli Bai, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong\n## Abstract\nThe increasing sizes of large generative Pre-trained Language Models (PLMs) hinder their deployment in real-world applications. To obtain ef\ufb01cient PLMs, previous studies mostly focus on pruning the attention heads and feed-forward networks (FFNs) of the Transformer. Nevertheless, we \ufb01nd that in generative PLMs, the hidden dimension shared by many other modules (e.g., embedding layer and layer normalization) contains persistent outliers regardless of the network input. In this study, we propose S IMPLE , a new structured pruning framework for generative PLMs that comprehensively investigates all the above compress-ible components. To identify redundant network structures, we assign learnable masks over compressible components followed by sparse training. Various sizes of PLMs can be \ufb02exibly extracted via different thresholds, and are then task-speci\ufb01cally \ufb01ne-tuned for further improvement. Extensive experiments on language modeling, summarization and machine translation validate the effectiveness of the proposed method. For example, the pruned BART brings 1.51x/6.96x inference speedup on GPU/CPU with 67% size reduction, and can be further combined with quantization for more than 25 \u00d7 compression.\n## Introduction\nLarge-scale generative pretrained language models (PLMs) (Radford and Narasimhan, 2018;Brown et al., 2020;Lewis et al., 2020;Raffel et al., 2020) show remarkable performance on various tasks. However, their increasing sizes also lead to expensive memory and computation, hindering their deployment in real applications. \n\nRecent attempts (Tao et al., 2022;Frantar et al., 2022;Dettmers et al., 2022;Xiao et al., 2022;Wang et al., 2021) propose to compress generative PLMs models by quantization. However, hardware-dependent low-bit kernels need to be specially developed for real inference speedup. Com-pared to quantization, structured pruning methods prune parts of the model structures without requiring designing extra operators to achieve inference speedup and run-time memory saving. Recently, Anonymous (2023) show that the feed-forward networks (FFNs) of GPT models can be pruned to smaller widths, and Li et al. (2022) compress the BART models by combining layer pruning and model quantization for a higher compression rate. However, these models consider only limited components for pruning, i.e., the FFNs or Transformer layers, which can be restrictive for various deployment requirements. \n\nIn this work, we propose a new structured pruning framework named SIMPLE (Sparsity-Induced Mask learning for Pruning generative pre-trained Language modEls), which offers a wider range of compressible components. Aside from the attention heads and the width of FFNs commonly used for structured pruning of discriminative PLMs, we also propose to prune the hidden dimension, to further push the trade-off between performance and model size. It is motivated by the observation that persistent outliers exist in the hidden dimension of both decoder-only and encoder-decoder generative PLMs. The observation implies that the hidden dimension may be slimmed sharply with a slight performance drop.",
            "reference_string": "[259858812 | Tao et al. | 2023 | Citations: 23]"
        },
        {
            "title": "Tailored Channel Pruning: Achieve Targeted Model Complexity Through Adaptive Sparsity Regularization",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 67,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2025.3529465",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3529465?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3529465, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2341653556",
                    "name": "Suwoong Lee"
                },
                {
                    "authorId": "2279335566",
                    "name": "Yunho Jeon"
                },
                {
                    "authorId": "2341658436",
                    "name": "Seungjae Lee"
                },
                {
                    "authorId": "2279401927",
                    "name": "Junmo Kim"
                }
            ],
            "abstract": "In deep learning, the size and complexity of neural networks have been rapidly increased to achieve higher performance. However, this poses a challenge when utilized in resource-limited environments, such as mobile devices, particularly when trying to preserve the network\u2019s performance. To address this problem, structured pruning has been widely studied as it effectively reduces the network with little impact on performance. To enhance a model\u2019s performance with limited resources, it is crucial to 1) utilize all available resources and 2) maximize performance within these limitations. However, existing pruning methods often require iterations of training and pruning or many experiments to find hyperparameters that satisfy a given budget or forcibly truncate parameters with a given budget, resulting in performance loss. To solve this problem, we propose a novel channel pruning method called Tailored Channel Pruning. Given a target budget (e.g., FLOPs and parameters), our method outputs a tailored network that automatically takes the budget into account during training and satisfies the target budget. During the integrated training and pruning process, our method adaptively controls sparsity regularization and selects important weights that can help maximize the accuracy within the target budget. Through various experiments on the CIFAR-10 and ImageNet datasets, we demonstrate the effectiveness of the proposed method and achieve state-of-the-art accuracy after pruning.",
            "corpus_id": 275550746,
            "sentences": [
                {
                    "corpus_id": "275550746",
                    "title": "Tailored Channel Pruning: Achieve Targeted Model Complexity Through Adaptive Sparsity Regularization",
                    "text": "Unlike unstructured pruning, which removes connections with individual weights [8], [9], [10], structured pruning removes whole structured components, such as channels or layers from a neural network. Although structured pruning generally has a lower compression rate than unstructured pruning, it provides practical performance improvements on modern GPUs where computations are performed at the tensor level due to computational efficiency. For structured pruning, some studies [18], [19], [21], [27], [28], [29] have focused on defining suitable importance measures for selecting parameters to remove from a pre-trained model. These methods have a weakness due to the way they rank the importance of groups of parameters: even if they are perfectly ranked, pruning can degrade performance if lower-ranked groups contain meaningful activations.",
                    "score": 0.5165431484690157,
                    "section_title": "II. RELATED WORKS A. STRUCTURED PRUNING",
                    "char_start_offset": 6469,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 442
                        },
                        {
                            "start": 443,
                            "end": 629
                        },
                        {
                            "start": 630,
                            "end": 846
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 84,
                            "end": 87,
                            "matchedPaperCorpusId": "17240902"
                        },
                        {
                            "start": 89,
                            "end": 93,
                            "matchedPaperCorpusId": "53388625"
                        },
                        {
                            "start": 480,
                            "end": 484,
                            "matchedPaperCorpusId": "14089312"
                        },
                        {
                            "start": 486,
                            "end": 490,
                            "matchedPaperCorpusId": "51608028"
                        },
                        {
                            "start": 492,
                            "end": 496,
                            "matchedPaperCorpusId": "4142619"
                        },
                        {
                            "start": 498,
                            "end": 502,
                            "matchedPaperCorpusId": "258549779"
                        },
                        {
                            "start": 504,
                            "end": 508,
                            "matchedPaperCorpusId": "174800733"
                        },
                        {
                            "start": 510,
                            "end": 514,
                            "matchedPaperCorpusId": "233394250"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77783203125
                }
            ],
            "relevance_judgement": 0.77783203125,
            "relevance_judgment_input_expanded": "# Title: Tailored Channel Pruning: Achieve Targeted Model Complexity Through Adaptive Sparsity Regularization\n# Venue: IEEE Access\n# Authors: Suwoong Lee, Yunho Jeon, Seungjae Lee, Junmo Kim\n## Abstract\nIn deep learning, the size and complexity of neural networks have been rapidly increased to achieve higher performance. However, this poses a challenge when utilized in resource-limited environments, such as mobile devices, particularly when trying to preserve the network\u2019s performance. To address this problem, structured pruning has been widely studied as it effectively reduces the network with little impact on performance. To enhance a model\u2019s performance with limited resources, it is crucial to 1) utilize all available resources and 2) maximize performance within these limitations. However, existing pruning methods often require iterations of training and pruning or many experiments to find hyperparameters that satisfy a given budget or forcibly truncate parameters with a given budget, resulting in performance loss. To solve this problem, we propose a novel channel pruning method called Tailored Channel Pruning. Given a target budget (e.g., FLOPs and parameters), our method outputs a tailored network that automatically takes the budget into account during training and satisfies the target budget. During the integrated training and pruning process, our method adaptively controls sparsity regularization and selects important weights that can help maximize the accuracy within the target budget. Through various experiments on the CIFAR-10 and ImageNet datasets, we demonstrate the effectiveness of the proposed method and achieve state-of-the-art accuracy after pruning.\n## II. RELATED WORKS A. STRUCTURED PRUNING\nUnlike unstructured pruning, which removes connections with individual weights [8], [9], [10], structured pruning removes whole structured components, such as channels or layers from a neural network. Although structured pruning generally has a lower compression rate than unstructured pruning, it provides practical performance improvements on modern GPUs where computations are performed at the tensor level due to computational efficiency. For structured pruning, some studies [18], [19], [21], [27], [28], [29] have focused on defining suitable importance measures for selecting parameters to remove from a pre-trained model. These methods have a weakness due to the way they rank the importance of groups of parameters: even if they are perfectly ranked, pruning can degrade performance if lower-ranked groups contain meaningful activations.",
            "reference_string": "[275550746 | Lee et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
            "venue": "International Conference on Computational Linguistics",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301331844",
                    "name": "Guanchen Li"
                },
                {
                    "authorId": "2270847262",
                    "name": "Xiandong Zhao"
                },
                {
                    "authorId": "2316517251",
                    "name": "Lian Liu"
                },
                {
                    "authorId": "2307589652",
                    "name": "Zeping Li"
                },
                {
                    "authorId": "2279335698",
                    "name": "Dong Li"
                },
                {
                    "authorId": "2279539118",
                    "name": "Lu Tian"
                },
                {
                    "authorId": "2316522396",
                    "name": "Jie He"
                },
                {
                    "authorId": "2316484957",
                    "name": "Ashish Sirasao"
                },
                {
                    "authorId": "2271751612",
                    "name": "E. Barsoum"
                }
            ],
            "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.",
            "corpus_id": 271909421,
            "sentences": [
                {
                    "corpus_id": "271909421",
                    "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
                    "text": "To illustrate the enhanced efficiency of pruned models, we present the inference speed of dense and sparse models on AMD CPU. We use the DeepSparse library [12] and apply 50% unstructured pruning on OPTs and LLaMAs in this experiment. Table 7 indicates that the pruned models can achieve 1.19x \u223c 1.87x speedup compared to their dense counterparts. This significant boost in inference speed underscores the critical importance of model pruning in practical applications. Pruning for Language Model Compression. The surging complexity of Transformer-based language models, which now feature hundreds of billions of parameters, has accentuated the urgent need for effective and efficient model pruning methods [6,8,9]. These pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,14]. Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]. Techniques such as SparseGPT [3] extend the OBS [9] methodology to column-wise prune weights, allowing the modification of values in the unpruned columns to compensate for the pruning errors. Aaquib et al. [28] enhance SparseGPT by incorporating minimal iterative task fine-tuning during the pruning process, demonstrating performance improvements at high sparsity levels. Wanda [27] introduces a simple yet effective no-retraining-needed pruning strategy that prunes weights based on their magnitudes and corresponding activations. OWL [39] considers the inhomogeneous distribution of interlayer outliers and further extends Wanda to a non-uniform sparsity distribution, achieving better performance. DS\u2205T [42] and SPP [16], as fine-tuning methods designed for sparse models, can improve the performance of pruned PLMs within limited complexity. \n\nWeight Distribution Optimization. Various techniques have been employed to understand and optimize weight distributions in the quest for more efficient neural networks.",
                    "score": 0.5955480025336286,
                    "section_title": "Efficiency Analysis",
                    "char_start_offset": 19063,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1148
                        },
                        {
                            "start": 1149,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1521
                        },
                        {
                            "start": 1522,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1850
                        },
                        {
                            "start": 1851,
                            "end": 1995
                        },
                        {
                            "start": 1998,
                            "end": 2031
                        },
                        {
                            "start": 2032,
                            "end": 2166
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 707,
                            "end": 710,
                            "matchedPaperCorpusId": "2134321"
                        },
                        {
                            "start": 710,
                            "end": 712,
                            "matchedPaperCorpusId": "2238772"
                        },
                        {
                            "start": 712,
                            "end": 714,
                            "matchedPaperCorpusId": "61815367"
                        },
                        {
                            "start": 951,
                            "end": 954,
                            "matchedPaperCorpusId": "260815690"
                        },
                        {
                            "start": 1178,
                            "end": 1181,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1197,
                            "end": 1200,
                            "matchedPaperCorpusId": "61815367"
                        },
                        {
                            "start": 1355,
                            "end": 1359,
                            "matchedPaperCorpusId": "259950394"
                        },
                        {
                            "start": 1686,
                            "end": 1690,
                            "matchedPaperCorpusId": "263829692"
                        },
                        {
                            "start": 1856,
                            "end": 1860,
                            "matchedPaperCorpusId": "264128029"
                        },
                        {
                            "start": 1869,
                            "end": 1873,
                            "matchedPaperCorpusId": "270063400"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77734375
                }
            ],
            "relevance_judgement": 0.77734375,
            "relevance_judgment_input_expanded": "# Title: Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism\n# Venue: International Conference on Computational Linguistics\n# Authors: Guanchen Li, Xiandong Zhao, Lian Liu, Zeping Li, Dong Li, Lu Tian, Jie He, Ashish Sirasao, E. Barsoum\n## Abstract\nPre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.\n## Efficiency Analysis\nTo illustrate the enhanced efficiency of pruned models, we present the inference speed of dense and sparse models on AMD CPU. We use the DeepSparse library [12] and apply 50% unstructured pruning on OPTs and LLaMAs in this experiment. Table 7 indicates that the pruned models can achieve 1.19x \u223c 1.87x speedup compared to their dense counterparts. This significant boost in inference speed underscores the critical importance of model pruning in practical applications. Pruning for Language Model Compression. The surging complexity of Transformer-based language models, which now feature hundreds of billions of parameters, has accentuated the urgent need for effective and efficient model pruning methods [6,8,9]. These pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,14]. Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]. Techniques such as SparseGPT [3] extend the OBS [9] methodology to column-wise prune weights, allowing the modification of values in the unpruned columns to compensate for the pruning errors. Aaquib et al. [28] enhance SparseGPT by incorporating minimal iterative task fine-tuning during the pruning process, demonstrating performance improvements at high sparsity levels. Wanda [27] introduces a simple yet effective no-retraining-needed pruning strategy that prunes weights based on their magnitudes and corresponding activations. OWL [39] considers the inhomogeneous distribution of interlayer outliers and further extends Wanda to a non-uniform sparsity distribution, achieving better performance. DS\u2205T [42] and SPP [16], as fine-tuning methods designed for sparse models, can improve the performance of pruned PLMs within limited complexity. \n\nWeight Distribution Optimization. Various techniques have been employed to understand and optimize weight distributions in the quest for more efficient neural networks.",
            "reference_string": "[271909421 | Li et al. | 2024 | Citations: 1]"
        },
        {
            "title": "What Matters In The Structured Pruning of Generative Language Models?",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 56,
            "citation_count": 34,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.03773",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.03773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1413038175",
                    "name": "Michael Santacroce"
                },
                {
                    "authorId": "2051054583",
                    "name": "Zixin Wen"
                },
                {
                    "authorId": "1752875",
                    "name": "Yelong Shen"
                },
                {
                    "authorId": "152244300",
                    "name": "Yuan-Fang Li"
                }
            ],
            "abstract": "Auto-regressive large language models such as GPT-3 require enormous computational resources to use. Traditionally, structured pruning methods are employed to reduce resource usage. However, their application to and efficacy for generative language models is heavily under-explored. In this paper we conduct an comprehensive evaluation of common structured pruning methods, including magnitude, random, and movement pruning on the feed-forward layers in GPT-type models. Unexpectedly, random pruning results in performance that is comparable to the best established methods, across multiple natural language generation tasks. To understand these results, we provide a framework for measuring neuron-level redundancy of models pruned by different methods, and discover that established structured pruning methods do not take into account the distinctiveness of neurons, leaving behind excess redundancies. In view of this, we introduce Globally Unique Movement (GUM) to improve the uniqueness of neurons in pruned models. We then discuss the effects of our techniques on different redundancy metrics to explain the improved performance.",
            "corpus_id": 256662734,
            "sentences": [
                {
                    "corpus_id": "256662734",
                    "title": "What Matters In The Structured Pruning of Generative Language Models?",
                    "text": "Large language models (LLMs), such as the state-of-the-art GPT-3 model (Brown et al., 2020) with up to 175 billion parameters, have achieved remarkable performance in natural language processing (NLP) tasks. However, training and deploying such massive models also poses significant challenges in terms of computational cost, energy consumption, and environmental impact. Therefore, it is crucial to develop effective methods to reduce the size of LLMs without compromising their quality. \n\nNeural network pruning is a long-standing model compression method (Janowsky, 1989;Mozer & Smolensky, 1988;Frankle & Carbin, 2018;Karnin, 1990;Blalock et al., 2020). It can be broadly classified into two types: unstructured and structured. Unstructured pruning removes individual weights from the network based on some criteria, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2020b;Xu et al., 2021;Zafrir et al., 2021;Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast services based on LLMs. \n\nExisting work on structured pruning for LMs focuses on BERT-like networks (Devlin et al., 2018) that consist of an encoder-decoder or an encoder-only architecture (Li et al., 2020;Xia et al., 2022;Zhang et al., 2022;Yao et al., 2021). These models are mainly used for natural language understanding (NLU) tasks, such as question answering, sentiment analysis, or natural language inference. Among the various methods, Block Movement Pruning (Lagunas et al., 2021) is a recent and popular technique that removes weight blocks based on movement.",
                    "score": 0.6010975172748023,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 488
                        },
                        {
                            "start": 491,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1085
                        },
                        {
                            "start": 1086,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1478
                        },
                        {
                            "start": 1481,
                            "end": 1715
                        },
                        {
                            "start": 1716,
                            "end": 1871
                        },
                        {
                            "start": 1872,
                            "end": 2024
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 558,
                            "end": 574,
                            "matchedPaperCorpusId": "31375995"
                        },
                        {
                            "start": 574,
                            "end": 598,
                            "matchedPaperCorpusId": "17651092"
                        },
                        {
                            "start": 621,
                            "end": 634,
                            "matchedPaperCorpusId": "1101832"
                        },
                        {
                            "start": 1162,
                            "end": 1182,
                            "matchedPaperCorpusId": "204009154"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77490234375
                }
            ],
            "relevance_judgement": 0.77490234375,
            "relevance_judgment_input_expanded": "# Title: What Matters In The Structured Pruning of Generative Language Models?\n# Venue: arXiv.org\n# Authors: Michael Santacroce, Zixin Wen, Yelong Shen, Yuan-Fang Li\n## Abstract\nAuto-regressive large language models such as GPT-3 require enormous computational resources to use. Traditionally, structured pruning methods are employed to reduce resource usage. However, their application to and efficacy for generative language models is heavily under-explored. In this paper we conduct an comprehensive evaluation of common structured pruning methods, including magnitude, random, and movement pruning on the feed-forward layers in GPT-type models. Unexpectedly, random pruning results in performance that is comparable to the best established methods, across multiple natural language generation tasks. To understand these results, we provide a framework for measuring neuron-level redundancy of models pruned by different methods, and discover that established structured pruning methods do not take into account the distinctiveness of neurons, leaving behind excess redundancies. In view of this, we introduce Globally Unique Movement (GUM) to improve the uniqueness of neurons in pruned models. We then discuss the effects of our techniques on different redundancy metrics to explain the improved performance.\n## INTRODUCTION\nLarge language models (LLMs), such as the state-of-the-art GPT-3 model (Brown et al., 2020) with up to 175 billion parameters, have achieved remarkable performance in natural language processing (NLP) tasks. However, training and deploying such massive models also poses significant challenges in terms of computational cost, energy consumption, and environmental impact. Therefore, it is crucial to develop effective methods to reduce the size of LLMs without compromising their quality. \n\nNeural network pruning is a long-standing model compression method (Janowsky, 1989;Mozer & Smolensky, 1988;Frankle & Carbin, 2018;Karnin, 1990;Blalock et al., 2020). It can be broadly classified into two types: unstructured and structured. Unstructured pruning removes individual weights from the network based on some criteria, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2020b;Xu et al., 2021;Zafrir et al., 2021;Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast services based on LLMs. \n\nExisting work on structured pruning for LMs focuses on BERT-like networks (Devlin et al., 2018) that consist of an encoder-decoder or an encoder-only architecture (Li et al., 2020;Xia et al., 2022;Zhang et al., 2022;Yao et al., 2021). These models are mainly used for natural language understanding (NLU) tasks, such as question answering, sentiment analysis, or natural language inference. Among the various methods, Block Movement Pruning (Lagunas et al., 2021) is a recent and popular technique that removes weight blocks based on movement.",
            "reference_string": "[256662734 | Santacroce et al. | 2023 | Citations: 34]"
        },
        {
            "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.11681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2230189096",
                    "name": "Hongrong Cheng"
                },
                {
                    "authorId": "2311635513",
                    "name": "Miao Zhang"
                },
                {
                    "authorId": "2262197686",
                    "name": "J. Q. Shi"
                }
            ],
            "abstract": "As Large Language Models (LLMs) grow dramatically in size, there is an increasing trend in compressing and speeding up these models. Previous studies have highlighted the usefulness of gradients for importance scoring in neural network compressing, especially in pruning medium-size networks. However, the substantial memory requirements involved in calculating gradients with backpropagation impede the utilization of gradients in guiding LLM pruning. As a result, most pruning strategies for LLMs rely on gradient-free criteria, such as weight magnitudes or a mix of magnitudes and activations. In this paper, we devise a hybrid pruning criterion, which appropriately integrates magnitude, activation, and gradient to capitalize on feature map sensitivity for pruning LLMs. To overcome memory requirement barriers, we estimate gradients using only forward passes. Based on this, we propose a Memory-effIcieNt structured prunIng procedure for LLMs (MINI-LLM) to remove no-critical channels and multi-attention heads. Experimental results demonstrate the superior performance of MINI-LLM over existing gradient-free methods on three LLMs: LLaMA, BLOOM, and OPT across various downstream tasks (classification, multiple-choice, and generation), while MINI-LLM maintains a GPU memory footprint akin to gradient-free methods.",
            "corpus_id": 271217883,
            "sentences": [
                {
                    "corpus_id": "271217883",
                    "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models",
                    "text": "Structured/Unstructured/Semi-structured LLM pruning.\n\nThe pruning methods for LLMs can still be generally cat-egorized as unstructured ([Sun et al., 2024;Frantar et al., 2022]), semi-structured ( [Frantar and Alistarh, 2023]), and structured ( [Ma et al., 2023;Wang et al., 2020b]) pruning methods, similar to the categorization for pruning small and mid-size neural networks.Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models.However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware.In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]).Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup.In this paper, we focus on structured pruning for LLMs.\n\nPruning criteria for LLMs.Neural network pruning methods search for an optimal subnetwork by removing unimportant weights.As one of the most popular criterion factors, gradients have already been demonstrated effective in constructing scoring functions for pruning small or medium-size networks [Liu et al., 2021;Fu et al., 2022;Wang et al., 2020a;Yu et al., 2022;Molchanov et al., 2019;Kwon et al., 2022].However, calculating gradients using backpropagation is highly resource-intensive for GPU memory, making it challenging to implement for LLMs, where meeting such high memory demands is difficult.",
                    "score": 0.49564616656112404,
                    "section_title": "Related Work",
                    "char_start_offset": 5188,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 52
                        },
                        {
                            "start": 54,
                            "end": 376
                        },
                        {
                            "start": 376,
                            "end": 552
                        },
                        {
                            "start": 552,
                            "end": 711
                        },
                        {
                            "start": 711,
                            "end": 1001
                        },
                        {
                            "start": 1001,
                            "end": 1146
                        },
                        {
                            "start": 1146,
                            "end": 1201
                        },
                        {
                            "start": 1203,
                            "end": 1229
                        },
                        {
                            "start": 1229,
                            "end": 1325
                        },
                        {
                            "start": 1325,
                            "end": 1609
                        },
                        {
                            "start": 1609,
                            "end": 1804
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 175,
                            "matchedPaperCorpusId": "249282255"
                        },
                        {
                            "start": 196,
                            "end": 224,
                            "matchedPaperCorpusId": "249282255"
                        },
                        {
                            "start": 244,
                            "end": 261,
                            "matchedPaperCorpusId": "17240902"
                        },
                        {
                            "start": 261,
                            "end": 280,
                            "matchedPaperCorpusId": "253708271"
                        },
                        {
                            "start": 952,
                            "end": 972,
                            "matchedPaperCorpusId": "247318543"
                        },
                        {
                            "start": 972,
                            "end": 999,
                            "matchedPaperCorpusId": "249282255"
                        },
                        {
                            "start": 1056,
                            "end": 1084,
                            "matchedPaperCorpusId": "249282255"
                        },
                        {
                            "start": 1498,
                            "end": 1516,
                            "matchedPaperCorpusId": "235825363"
                        },
                        {
                            "start": 1516,
                            "end": 1532,
                            "matchedPaperCorpusId": "249282255"
                        },
                        {
                            "start": 1532,
                            "end": 1551,
                            "matchedPaperCorpusId": "5358338"
                        },
                        {
                            "start": 1551,
                            "end": 1567,
                            "matchedPaperCorpusId": "247318543"
                        },
                        {
                            "start": 1567,
                            "end": 1590,
                            "matchedPaperCorpusId": "17240902"
                        },
                        {
                            "start": 1590,
                            "end": 1608,
                            "matchedPaperCorpusId": "247446572"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7734375
                }
            ],
            "relevance_judgement": 0.7734375,
            "relevance_judgment_input_expanded": "# Title: MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models\n# Venue: arXiv.org\n# Authors: Hongrong Cheng, Miao Zhang, J. Q. Shi\n## Abstract\nAs Large Language Models (LLMs) grow dramatically in size, there is an increasing trend in compressing and speeding up these models. Previous studies have highlighted the usefulness of gradients for importance scoring in neural network compressing, especially in pruning medium-size networks. However, the substantial memory requirements involved in calculating gradients with backpropagation impede the utilization of gradients in guiding LLM pruning. As a result, most pruning strategies for LLMs rely on gradient-free criteria, such as weight magnitudes or a mix of magnitudes and activations. In this paper, we devise a hybrid pruning criterion, which appropriately integrates magnitude, activation, and gradient to capitalize on feature map sensitivity for pruning LLMs. To overcome memory requirement barriers, we estimate gradients using only forward passes. Based on this, we propose a Memory-effIcieNt structured prunIng procedure for LLMs (MINI-LLM) to remove no-critical channels and multi-attention heads. Experimental results demonstrate the superior performance of MINI-LLM over existing gradient-free methods on three LLMs: LLaMA, BLOOM, and OPT across various downstream tasks (classification, multiple-choice, and generation), while MINI-LLM maintains a GPU memory footprint akin to gradient-free methods.\n## Related Work\nStructured/Unstructured/Semi-structured LLM pruning.\n\nThe pruning methods for LLMs can still be generally cat-egorized as unstructured ([Sun et al., 2024;Frantar et al., 2022]), semi-structured ( [Frantar and Alistarh, 2023]), and structured ( [Ma et al., 2023;Wang et al., 2020b]) pruning methods, similar to the categorization for pruning small and mid-size neural networks.Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models.However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware.In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]).Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup.In this paper, we focus on structured pruning for LLMs.\n\nPruning criteria for LLMs.Neural network pruning methods search for an optimal subnetwork by removing unimportant weights.As one of the most popular criterion factors, gradients have already been demonstrated effective in constructing scoring functions for pruning small or medium-size networks [Liu et al., 2021;Fu et al., 2022;Wang et al., 2020a;Yu et al., 2022;Molchanov et al., 2019;Kwon et al., 2022].However, calculating gradients using backpropagation is highly resource-intensive for GPU memory, making it challenging to implement for LLMs, where meeting such high memory demands is difficult.",
            "reference_string": "[271217883 | Cheng et al. | 2024 | Citations: 3]"
        },
        {
            "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.11057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321454694",
                    "name": "Bo Lv"
                },
                {
                    "authorId": "2321495086",
                    "name": "Quan Zhou"
                },
                {
                    "authorId": "2216237674",
                    "name": "Xuanang Ding"
                },
                {
                    "authorId": "2321964263",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "2321489070",
                    "name": "Zeming Ma"
                }
            ],
            "abstract": "The bottleneck associated with the key-value(KV) cache presents a significant challenge during the inference processes of large language models. While depth pruning accelerates inference, it requires extensive recovery training, which can take up to two weeks. On the other hand, width pruning retains much of the performance but offers slight speed gains. To tackle these challenges, we propose KVPruner to improve model efficiency while maintaining performance. Our method uses global perplexity-based analysis to determine the importance ratio for each block and provides multiple strategies to prune non-essential KV channels within blocks. Compared to the original model, KVPruner reduces runtime memory usage by 50% and boosts throughput by over 35%. Additionally, our method requires only two hours of LoRA fine-tuning on small datasets to recover most of the performance.",
            "corpus_id": 272693912,
            "sentences": [
                {
                    "corpus_id": "272693912",
                    "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models",
                    "text": "Large language models (LLMs) refer to natural language processing (NLP) models with a massive number of parameters [1]- [5], commonly based on the Transformer architecture [5]. These models have also found widespread applications in fields such as speech processing [6]- [8] and computer vision [9]- [11]. In recent years, LLMs have demonstrated remarkable capabilities in handling complex tasks in applications like dialogue systems [12], [13] and knowledge-based question answering [14]- [16], significantly accelerating the development of downstream applications. However, as model sizes continue to grow, the challenges related to inference efficiency have become more pronounced. \n\nCurrently, optimization methods for large models include pruning (structured pruning [17]- [19] and unstructured pruning [20], [21]), quantization [22]- [24], and distillation [25], [26]. This work focuses on structured pruning, making it more deployment-friendly and hardware-friendly. In modern structured pruning algorithms for LLMs, LLM-Pruner [17] achieves model size reduction by removing inter-group dependencies in the network. Sheared-LLaMA [19] not only removes structures within groups but also prunes less important blocks to achieve compression. \n\nThese methods can employ LoRA [27] to recover performance, but the gains in runtime memory efficiency and inference performance are still minimal. Shortened-LLM [18] aggressively removes entire blocks to speed up inference, but The top left compares perplexity(PPL) across different strategies under the same pruning ratio and fine-tuning steps, where our method demonstrates superior performance. The bottom left shows the keyvalue (KV) cache usage, where our approach achieves more significant KV memory pruning at both strategy-level and model parameter-level pruning ratios. Right side: Under the same model parameter settings, KVPruner achieves faster inference speeds compared to Shortened-LLM [18] and LLM-Pruner [17] pruning method. \n\nit shows that directly removing blocks requires retraining with CPT [18], which can take several weeks to restore the pruned model.",
                    "score": 0.5581889037805969,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 684
                        },
                        {
                            "start": 687,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1122
                        },
                        {
                            "start": 1123,
                            "end": 1245
                        },
                        {
                            "start": 1248,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1645
                        },
                        {
                            "start": 1646,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 1988
                        },
                        {
                            "start": 1991,
                            "end": 2122
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 115,
                            "end": 118,
                            "matchedPaperCorpusId": "258546397"
                        },
                        {
                            "start": 120,
                            "end": 123,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 172,
                            "end": 175,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 271,
                            "end": 274,
                            "matchedPaperCorpusId": "239024736"
                        },
                        {
                            "start": 300,
                            "end": 304,
                            "matchedPaperCorpusId": "233444273"
                        },
                        {
                            "start": 434,
                            "end": 438,
                            "matchedPaperCorpusId": "259129398"
                        },
                        {
                            "start": 440,
                            "end": 444,
                            "matchedPaperCorpusId": "1820614"
                        },
                        {
                            "start": 484,
                            "end": 488,
                            "matchedPaperCorpusId": "261048772"
                        },
                        {
                            "start": 490,
                            "end": 494,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 772,
                            "end": 776,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 814,
                            "end": 818,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1035,
                            "end": 1039,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 1968,
                            "end": 1972,
                            "matchedPaperCorpusId": "258823276"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76953125
                }
            ],
            "relevance_judgement": 0.76953125,
            "relevance_judgment_input_expanded": "# Title: KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models\n# Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing\n# Authors: Bo Lv, Quan Zhou, Xuanang Ding, Yan Wang, Zeming Ma\n## Abstract\nThe bottleneck associated with the key-value(KV) cache presents a significant challenge during the inference processes of large language models. While depth pruning accelerates inference, it requires extensive recovery training, which can take up to two weeks. On the other hand, width pruning retains much of the performance but offers slight speed gains. To tackle these challenges, we propose KVPruner to improve model efficiency while maintaining performance. Our method uses global perplexity-based analysis to determine the importance ratio for each block and provides multiple strategies to prune non-essential KV channels within blocks. Compared to the original model, KVPruner reduces runtime memory usage by 50% and boosts throughput by over 35%. Additionally, our method requires only two hours of LoRA fine-tuning on small datasets to recover most of the performance.\n## I. INTRODUCTION\nLarge language models (LLMs) refer to natural language processing (NLP) models with a massive number of parameters [1]- [5], commonly based on the Transformer architecture [5]. These models have also found widespread applications in fields such as speech processing [6]- [8] and computer vision [9]- [11]. In recent years, LLMs have demonstrated remarkable capabilities in handling complex tasks in applications like dialogue systems [12], [13] and knowledge-based question answering [14]- [16], significantly accelerating the development of downstream applications. However, as model sizes continue to grow, the challenges related to inference efficiency have become more pronounced. \n\nCurrently, optimization methods for large models include pruning (structured pruning [17]- [19] and unstructured pruning [20], [21]), quantization [22]- [24], and distillation [25], [26]. This work focuses on structured pruning, making it more deployment-friendly and hardware-friendly. In modern structured pruning algorithms for LLMs, LLM-Pruner [17] achieves model size reduction by removing inter-group dependencies in the network. Sheared-LLaMA [19] not only removes structures within groups but also prunes less important blocks to achieve compression. \n\nThese methods can employ LoRA [27] to recover performance, but the gains in runtime memory efficiency and inference performance are still minimal. Shortened-LLM [18] aggressively removes entire blocks to speed up inference, but The top left compares perplexity(PPL) across different strategies under the same pruning ratio and fine-tuning steps, where our method demonstrates superior performance. The bottom left shows the keyvalue (KV) cache usage, where our approach achieves more significant KV memory pruning at both strategy-level and model parameter-level pruning ratios. Right side: Under the same model parameter settings, KVPruner achieves faster inference speeds compared to Shortened-LLM [18] and LLM-Pruner [17] pruning method. \n\nit shows that directly removing blocks requires retraining with CPT [18], which can take several weeks to restore the pruned model.",
            "reference_string": "[272693912 | Lv et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.01943, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2300138950",
                    "name": "Zhiyu Guo"
                },
                {
                    "authorId": "2300756",
                    "name": "Hidetaka Kamigaito"
                },
                {
                    "authorId": "2299941873",
                    "name": "Taro Wanatnabe"
                }
            ],
            "abstract": "The rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent GLU-based LLMs pruning, which incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on LLaMA2, Mistral, and Gemma model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.",
            "corpus_id": 270621063,
            "sentences": [
                {
                    "corpus_id": "270621063",
                    "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
                    "text": "Pruning LLM. Neural network pruning in LLM can be broadly categorized into two groups: structured pruning (Ma et al., 2023;Zhang et al., 2023) and unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2024). Ma et al. (2023) proposes a dependency detection algorithm to detect and prune non-critical grouped structures followed by LoRA fine-tuning. Although structured pruning can usually achieve better hardware efficiency, the accuracy drops a lot even at a low compression rate. Unstructured pruning can yield a higher compression rate and achieve acceleration on Nvidia's GPUs by employing a hardware-friendly N:M sparsity pattern. SparseGPT (Frantar & Alistarh, 2023) leverages the Hessian inverse for pruning and reduces reconstruction error of dense and sparse weights by subsequent weight updates. Wanda (Sun et al., 2024) employs an efficient method that augments input activations into weight magnitudes, and matches the performance of SparseGPT at medium sparsity. Our work incorporates dependency information into unstructured pruning, achieving a novel pruning paradigm. \n\nInherent Sparsity of Transformer MLP. Interestingly, sparsity within the MLP activations of trained Transformer-based models occurs innately even without applying explicit regularizations or constraints (Zhang et al., 2022;Li et al., 2023;Dong et al., 2023). Such a phenomenon is prevalent in learned Transformers, including other zero-saturating functions. Outlier-dependent LLM Compression. Outlier features, defined as features with magnitudes substantially larger than others, are a notable characteristic of LLMs (Dettmers et al., 2022). Despite making up only a small fraction of all feature dimensions, these outliers play a critical role in attention and predictive performance. Such observation has motivated the development of LLM-specific quantization methods (Dettmers et al., 2022;Xiao et al., 2023;Lin et al., 2024;Ashkboos et al., 2023) to handle outliers more effectively.",
                    "score": 0.5072122305984246,
                    "section_title": "Related Work",
                    "char_start_offset": 22888,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 12
                        },
                        {
                            "start": 13,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 486
                        },
                        {
                            "start": 487,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 810
                        },
                        {
                            "start": 811,
                            "end": 980
                        },
                        {
                            "start": 981,
                            "end": 1088
                        },
                        {
                            "start": 1091,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1349
                        },
                        {
                            "start": 1350,
                            "end": 1448
                        },
                        {
                            "start": 1449,
                            "end": 1483
                        },
                        {
                            "start": 1484,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1777
                        },
                        {
                            "start": 1778,
                            "end": 1979
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 106,
                            "end": 123,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 194,
                            "end": 211,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 213,
                            "end": 229,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 817,
                            "end": 835,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 1314,
                            "end": 1330,
                            "matchedPaperCorpusId": "259138847"
                        },
                        {
                            "start": 1330,
                            "end": 1348,
                            "matchedPaperCorpusId": "268955234"
                        },
                        {
                            "start": 1609,
                            "end": 1632,
                            "matchedPaperCorpusId": "258509304"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76953125
                }
            ],
            "relevance_judgement": 0.76953125,
            "relevance_judgment_input_expanded": "# Title: Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Zhiyu Guo, Hidetaka Kamigaito, Taro Wanatnabe\n## Abstract\nThe rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent GLU-based LLMs pruning, which incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on LLaMA2, Mistral, and Gemma model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.\n## Related Work\nPruning LLM. Neural network pruning in LLM can be broadly categorized into two groups: structured pruning (Ma et al., 2023;Zhang et al., 2023) and unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2024). Ma et al. (2023) proposes a dependency detection algorithm to detect and prune non-critical grouped structures followed by LoRA fine-tuning. Although structured pruning can usually achieve better hardware efficiency, the accuracy drops a lot even at a low compression rate. Unstructured pruning can yield a higher compression rate and achieve acceleration on Nvidia's GPUs by employing a hardware-friendly N:M sparsity pattern. SparseGPT (Frantar & Alistarh, 2023) leverages the Hessian inverse for pruning and reduces reconstruction error of dense and sparse weights by subsequent weight updates. Wanda (Sun et al., 2024) employs an efficient method that augments input activations into weight magnitudes, and matches the performance of SparseGPT at medium sparsity. Our work incorporates dependency information into unstructured pruning, achieving a novel pruning paradigm. \n\nInherent Sparsity of Transformer MLP. Interestingly, sparsity within the MLP activations of trained Transformer-based models occurs innately even without applying explicit regularizations or constraints (Zhang et al., 2022;Li et al., 2023;Dong et al., 2023). Such a phenomenon is prevalent in learned Transformers, including other zero-saturating functions. Outlier-dependent LLM Compression. Outlier features, defined as features with magnitudes substantially larger than others, are a notable characteristic of LLMs (Dettmers et al., 2022). Despite making up only a small fraction of all feature dimensions, these outliers play a critical role in attention and predictive performance. Such observation has motivated the development of LLM-specific quantization methods (Dettmers et al., 2022;Xiao et al., 2023;Lin et al., 2024;Ashkboos et al., 2023) to handle outliers more effectively.",
            "reference_string": "[270621063 | Guo et al. | 2024 | Citations: 1]"
        },
        {
            "title": "A Convex-optimization-based Layer-wise Post-training Pruner for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.03728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2114873379",
                    "name": "Pengxiang Zhao"
                },
                {
                    "authorId": "2283405371",
                    "name": "Hanyu Hu"
                },
                {
                    "authorId": "2293474362",
                    "name": "Ping Li"
                },
                {
                    "authorId": "2293396844",
                    "name": "Yi Zheng"
                },
                {
                    "authorId": "2293231668",
                    "name": "Zhefeng Wang"
                },
                {
                    "authorId": "2283433779",
                    "name": "Xiaoming Yuan"
                }
            ],
            "abstract": "Pruning is a critical strategy for compressing trained large language models (LLMs), aiming at substantial memory conservation and computational acceleration without compromising performance. However, existing pruning methods often necessitate inefficient retraining for billion-scale LLMs or rely on heuristic methods such as the optimal brain surgeon framework, which degrade performance. In this paper, we introduce FISTAPruner, the first post-training pruner based on convex optimization models and algorithms. Specifically, we propose a convex optimization model incorporating $\\ell_1$ norm to induce sparsity and utilize the FISTA solver for optimization. FISTAPruner incorporates an intra-layer cumulative error correction mechanism and supports parallel pruning. We comprehensively evaluate FISTAPruner on models such as OPT, LLaMA, LLaMA-2, and LLaMA-3 with 125M to 70B parameters under unstructured and 2:4 semi-structured sparsity, demonstrating superior performance over existing state-of-the-art methods across various language benchmarks.",
            "corpus_id": 271744772,
            "sentences": [
                {
                    "corpus_id": "271744772",
                    "title": "A Convex-optimization-based Layer-wise Post-training Pruner for Large Language Models",
                    "text": "These methods prune LLMs in a single step, simplifying implementation and reducing both time and computational demands. Consequently, various one-shot pruning algorithms have been developed under different sparsity frameworks. For structured pruning, SliceGPT (Ashkboos et al., 2024) and Eigenpruning (Vergara-Browne et al., 2024) utilize singular value decompositions to prune singular values of weight matrices and reduce model dimensions. ZipLM (Kurti\u0107 et al., 2024) adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Our proposed FISTAPruner focuses on unstructured and semi-structured pruning, and thus is orthogonal to these structured pruning methods, enabling further model compression. For unstructured and semi-structured pruning, SparseGPT (Frantar and Alistarh, 2023) and ISC (Shao et al., 2024) leverage the OBS framework to calculate saliency for each entry using the inverse Hessian of the loss metric, based on which pruning masks are generated and weights updated. Wanda (Sun et al., 2023) implements a heuristic approach, removing weights based on the product of their magnitudes and activations without compensation. (Bo\u017ea, 2024) employs ADMM to optimize weight updates under iteratively refined pruning masks chosen through heuristic methods. These strategies adopt a layer-wise pruning strategy, where errors between the pruned output and the original output of each operator accumulates. Moreover, due to their heuristic nature, the performances of the pruned models are unstable and compromised. In contrast, our proposed FISTAPruner introduces a novel layer-wise one-shot pruning approach by formulating it as a convex optimization problem with intra-layer error corrections. It employs the FISTA solver to efficiently compute optimal pruned weights, providing theoretical guarantees for performance stability and effectiveness.",
                    "score": 0.5160946817326528,
                    "section_title": "Background and Related Work",
                    "char_start_offset": 6720,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 1037
                        },
                        {
                            "start": 1038,
                            "end": 1191
                        },
                        {
                            "start": 1192,
                            "end": 1318
                        },
                        {
                            "start": 1319,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1755
                        },
                        {
                            "start": 1756,
                            "end": 1908
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 448,
                            "end": 469,
                            "matchedPaperCorpusId": "256662263"
                        },
                        {
                            "start": 807,
                            "end": 835,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 844,
                            "end": 863,
                            "matchedPaperCorpusId": "264146174"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76611328125
                }
            ],
            "relevance_judgement": 0.76611328125,
            "relevance_judgment_input_expanded": "# Title: A Convex-optimization-based Layer-wise Post-training Pruner for Large Language Models\n# Venue: arXiv.org\n# Authors: Pengxiang Zhao, Hanyu Hu, Ping Li, Yi Zheng, Zhefeng Wang, Xiaoming Yuan\n## Abstract\nPruning is a critical strategy for compressing trained large language models (LLMs), aiming at substantial memory conservation and computational acceleration without compromising performance. However, existing pruning methods often necessitate inefficient retraining for billion-scale LLMs or rely on heuristic methods such as the optimal brain surgeon framework, which degrade performance. In this paper, we introduce FISTAPruner, the first post-training pruner based on convex optimization models and algorithms. Specifically, we propose a convex optimization model incorporating $\\ell_1$ norm to induce sparsity and utilize the FISTA solver for optimization. FISTAPruner incorporates an intra-layer cumulative error correction mechanism and supports parallel pruning. We comprehensively evaluate FISTAPruner on models such as OPT, LLaMA, LLaMA-2, and LLaMA-3 with 125M to 70B parameters under unstructured and 2:4 semi-structured sparsity, demonstrating superior performance over existing state-of-the-art methods across various language benchmarks.\n## Background and Related Work\nThese methods prune LLMs in a single step, simplifying implementation and reducing both time and computational demands. Consequently, various one-shot pruning algorithms have been developed under different sparsity frameworks. For structured pruning, SliceGPT (Ashkboos et al., 2024) and Eigenpruning (Vergara-Browne et al., 2024) utilize singular value decompositions to prune singular values of weight matrices and reduce model dimensions. ZipLM (Kurti\u0107 et al., 2024) adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Our proposed FISTAPruner focuses on unstructured and semi-structured pruning, and thus is orthogonal to these structured pruning methods, enabling further model compression. For unstructured and semi-structured pruning, SparseGPT (Frantar and Alistarh, 2023) and ISC (Shao et al., 2024) leverage the OBS framework to calculate saliency for each entry using the inverse Hessian of the loss metric, based on which pruning masks are generated and weights updated. Wanda (Sun et al., 2023) implements a heuristic approach, removing weights based on the product of their magnitudes and activations without compensation. (Bo\u017ea, 2024) employs ADMM to optimize weight updates under iteratively refined pruning masks chosen through heuristic methods. These strategies adopt a layer-wise pruning strategy, where errors between the pruned output and the original output of each operator accumulates. Moreover, due to their heuristic nature, the performances of the pruned models are unstable and compromised. In contrast, our proposed FISTAPruner introduces a novel layer-wise one-shot pruning approach by formulating it as a convex optimization problem with intra-layer error corrections. It employs the FISTA solver to efficiently compute optimal pruned weights, providing theoretical guarantees for performance stability and effectiveness.",
            "reference_string": "[271744772 | Zhao et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11700, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2201687496",
                    "name": "Shuzhou Yuan"
                },
                {
                    "authorId": "2197254657",
                    "name": "Ercong Nie"
                },
                {
                    "authorId": "2188764477",
                    "name": "Bolei Ma"
                },
                {
                    "authorId": "2281825175",
                    "name": "Michael Farber"
                }
            ],
            "abstract": "Large Language Models (LLMs) possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as model pruning or distillation offer ways for reducing model size, they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in LLMs. Surprisingly, we observe that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks. Remarkably, in certain cases, models with a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of LLMs while preserving their performance, thereby opening avenues for significantly more efficient use of LLMs.",
            "corpus_id": 267751193,
            "sentences": [
                {
                    "corpus_id": "267751193",
                    "title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers",
                    "text": "Pruning, a prevalent technique for model compression method, seeks to discard non-essential parameters within language models, thereby reducing computational and storage costs while maintaining performance [13], [31], [35]. Pruning methods can be broadly categorized into unstructured pruning, which removes individual weights, and structured pruning, which removes higher-granularity structures such as neurons, attention heads, or entire layers. Layer-wise pruning, a specific form of structured pruning, has been explored in several studies. Fan et al. (2019) [5] introduced LayerDrop, a structured dropout method that randomly drops layers during training to improve robustness to layer reduction at inference time. Additionally, several studies have investigated the disparities in representations across different layers, highlighting the significance of updating the last few layers for language models [12], [14], [18]. Building on this foundation, Peer et al. (2022) [22] and Sajjad et al. (2023) [24] aimed to identify and remove an optimal subset of layers directly from the pretrained models for use in downstream tasks. These works focus on pruning layers to reduce model size while preserving taskspecific performance within the standard fine-tuning paradigm. However, they do not explore the interaction between layer pruning and prompt-based fine-tuning, which is the focus of our work. \n\nRecent studies have explored the application of pruning techniques to large language models, such as Sheared LLMs [34] and SparseGPT [6], which aims to sparsify or prune LLMs for efficient inference. These methods often involve complex optimization strategies or require additional training steps [1], [4], [11]. In contrast, our approach is a simple top-layer pruning strategy that directly removes layers from LLMs and evaluates their performance in prompt-based fine-tuning scenarios. Unlike prior work, we quantitatively investigate the impact of layer pruning on LLMs in few-shot learning settings, demonstrating that even drastic reductions in layers can maintain or improve performance.",
                    "score": 0.5003881413429174,
                    "section_title": "A. Model Pruning",
                    "char_start_offset": 3976,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 223
                        },
                        {
                            "start": 224,
                            "end": 447
                        },
                        {
                            "start": 448,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 719
                        },
                        {
                            "start": 720,
                            "end": 927
                        },
                        {
                            "start": 928,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1402
                        },
                        {
                            "start": 1405,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1717
                        },
                        {
                            "start": 1718,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2098
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 206,
                            "end": 210,
                            "matchedPaperCorpusId": "7785881"
                        },
                        {
                            "start": 212,
                            "end": 216,
                            "matchedPaperCorpusId": "258865530"
                        },
                        {
                            "start": 218,
                            "end": 222,
                            "matchedPaperCorpusId": "247922354"
                        },
                        {
                            "start": 545,
                            "end": 562,
                            "matchedPaperCorpusId": "202750230"
                        },
                        {
                            "start": 563,
                            "end": 566,
                            "matchedPaperCorpusId": "202750230"
                        },
                        {
                            "start": 910,
                            "end": 914,
                            "matchedPaperCorpusId": "201645145"
                        },
                        {
                            "start": 922,
                            "end": 926,
                            "matchedPaperCorpusId": "84841767"
                        },
                        {
                            "start": 976,
                            "end": 980,
                            "matchedPaperCorpusId": "247771234"
                        },
                        {
                            "start": 985,
                            "end": 1005,
                            "matchedPaperCorpusId": "251005814"
                        },
                        {
                            "start": 1006,
                            "end": 1010,
                            "matchedPaperCorpusId": "251005814"
                        },
                        {
                            "start": 1519,
                            "end": 1523,
                            "matchedPaperCorpusId": "263830786"
                        },
                        {
                            "start": 1538,
                            "end": 1541,
                            "matchedPaperCorpusId": "255372747"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7646484375
                }
            ],
            "relevance_judgement": 0.7646484375,
            "relevance_judgment_input_expanded": "# Title: Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers\n# Venue: arXiv.org\n# Authors: Shuzhou Yuan, Ercong Nie, Bolei Ma, Michael Farber\n## Abstract\nLarge Language Models (LLMs) possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as model pruning or distillation offer ways for reducing model size, they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in LLMs. Surprisingly, we observe that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks. Remarkably, in certain cases, models with a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of LLMs while preserving their performance, thereby opening avenues for significantly more efficient use of LLMs.\n## A. Model Pruning\nPruning, a prevalent technique for model compression method, seeks to discard non-essential parameters within language models, thereby reducing computational and storage costs while maintaining performance [13], [31], [35]. Pruning methods can be broadly categorized into unstructured pruning, which removes individual weights, and structured pruning, which removes higher-granularity structures such as neurons, attention heads, or entire layers. Layer-wise pruning, a specific form of structured pruning, has been explored in several studies. Fan et al. (2019) [5] introduced LayerDrop, a structured dropout method that randomly drops layers during training to improve robustness to layer reduction at inference time. Additionally, several studies have investigated the disparities in representations across different layers, highlighting the significance of updating the last few layers for language models [12], [14], [18]. Building on this foundation, Peer et al. (2022) [22] and Sajjad et al. (2023) [24] aimed to identify and remove an optimal subset of layers directly from the pretrained models for use in downstream tasks. These works focus on pruning layers to reduce model size while preserving taskspecific performance within the standard fine-tuning paradigm. However, they do not explore the interaction between layer pruning and prompt-based fine-tuning, which is the focus of our work. \n\nRecent studies have explored the application of pruning techniques to large language models, such as Sheared LLMs [34] and SparseGPT [6], which aims to sparsify or prune LLMs for efficient inference. These methods often involve complex optimization strategies or require additional training steps [1], [4], [11]. In contrast, our approach is a simple top-layer pruning strategy that directly removes layers from LLMs and evaluates their performance in prompt-based fine-tuning scenarios. Unlike prior work, we quantitatively investigate the impact of layer pruning on LLMs in few-shot learning settings, demonstrating that even drastic reductions in layers can maintain or improve performance.",
            "reference_string": "[267751193 | Yuan et al. | 2024 | Citations: 3]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "278033481",
            "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey",
            "text": "Pruning is a long-established technique for model compression that reduces the size of a model by removing unnecessary or less important parameters [61,90]. In language models, many weights can be zeroed out or eliminated with little to no effect on overall performance [18]. By pruning these redundant parameters, we can significantly shrink model size and accelerate inference, with only a minor drop in accuracy if done carefully. Pruning thus makes models more storage-friendly, memoryefficient, and computation-efficient. Based on the granularity of removal, pruning is categorized as unstructured pruning and structured pruning. \n\nStructured pruning streamlines LLM by eliminating entire architectural components-such as neurons, filter channels, attention heads, or even layers-instead of removing individual parameters [126,198]. By selectively pruning structurally coherent groups of parameters, this approach yields a smaller yet regularly structured model-maintaining dense connectivity while reducing inference costs, for instance, LLM-Pruner [126]. Unstructured pruning removes individual weights from LLM without considering any specific structure within the model [41]. Unstructured pruning removes less important weights based on a threshold, resulting in a sparse model where the architecture remains unchanged, as long as enough weights stay to maintain performance. Unstructured pruning can deliver higher accuracy for a given compression ratio by allowing more targeted weight removal. However, its irregular sparsity pattern leads to inefficient memory access and limited speed-ups on standard hardware. To fully leverage the compression benefits, specialized libraries or hardware are needed, and even then, acceleration gains are modest unless the model is highly sparse. Post unstructured pruning, significant retraining or fine-tuning is often required to recover lost accuracy that can be resource-intensive. \n\nDiscussion on pruning: Contextual pruning is a promising method for building domain-specific language models [177]. Recent research has focused on one-shot pruning methods and improved criteria to minimize the need for costly retraining [211], validating its performance for healthcare tasks.",
            "score": 0.759524151864561,
            "section_title": "Pruning",
            "char_start_offset": 44224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1934
                },
                {
                    "start": 1937,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "276575434"
                },
                {
                    "start": 827,
                    "end": 832,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1055,
                    "end": 1060,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "Large Language Models (LLMs) (Brown et al. 2020;Touvron et al. 2023;Zhang et al. 2022;Scao et al. 2022) have recently achieved outstanding performance across various language benchmarks in NLP (Bommarito and Katz 2022;Bubeck et al. 2023;Wei et al. 2022), spurring a large number of open-source applications (Taori et al. 2023;Anand et al. 2023;Richards 2023). These remarkable capabilities typically come with a huge-scale model size with high inference costs. This makes it harder for more people to benefit from LLMs. Due to the computational resource constraints, most of the model compression methods in the pre-LLM era are no longer feasible for LLMs. Model compression methods for LLMs to date focus on model quantization (Dettmers et al. 2022;Xiao et al. 2023;Frantar et al. 2023;Dettmers et al. 2023) and unstructured pruning (Sun et al. 2023;Frantar and Alistarh 2023). \n\nStructured pruning (He and Xiao 2023), which prunes entire rows or columns of weights, offers a promising solution to the deployment challenges of LLMs. Unlike unstructured pruning, structured pruning reduces both parameters and inference time without relying on specific hardware, making it more widely applicable (Anwar, Hwang, and Sung 2017). For effective structured pruning, it's crucial to have a metric that captures the collective significance of an entire row or column. However, current unstructured pruning techniques for LLMs, as seen in methods like (Sun et al. 2023;Frantar and Alistarh 2023), primarily focus on the importance of individual elements of each row in isolation. This absence of structured metrics that evaluate entire rows or columns makes them less suitable for structured pruning. The recent LLM-Pruner (Ma, Fang, and Wang 2023) attempted structured pruning for LLMs, but its dependence on LoRA finetuning (Hu et al. 2021) creates a tough trade-off between high computation and effective pruning, limiting its use in larger models.",
            "score": 0.7265949506355132,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 68,
                    "matchedPaperCorpusId": "221082307"
                },
                {
                    "start": 237,
                    "end": 253,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 307,
                    "end": 326,
                    "matchedPaperCorpusId": "221082307"
                },
                {
                    "start": 750,
                    "end": 767,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1196,
                    "end": 1225,
                    "matchedPaperCorpusId": "7333079"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76513671875
        },
        {
            "corpus_id": "278338989",
            "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods",
            "text": "The rapid advancement of large language models (LLMs) has revolutionized natural language processing (Zhang et al., 2022;OpenAI, 2023;Touvron et al., 2023a,b;Meta AI, 2023;Gemini Team et al., 2023;Grattafiori et al., 2024;Apple Inc, 2024;DeepSeek-AI, 2025;Qwen Team, 2025), yet their enormous size poses significant challenges for practical deployment. Pruning techniques have emerged as a cornerstone for LLM compression (Frantar and Alistarh, 2023;Sun et al., 2023;Ma et al., 2023;Shen et al., 2024;Fang et al., 2024), offering a promising solution to reduce model size and computational requirements while preserving performance. While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware Middle: We propose a penalty method to decide which parts of the weight matrices should be pruned. Right: We develop an alternating minimization algorithm to efficiently update the remaining weights to restore model performance. \n\nor sparse computations. These advantages make structured pruning a robust and hardware-agnostic strategy for LLM compression.",
            "score": 0.6959867323037338,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 422,
                    "end": 450,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 660,
                    "end": 688,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 960,
                    "end": 981,
                    "matchedPaperCorpusId": "240288854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "268032346",
            "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
            "text": "Compression of Large Language Models. Numerous technologies aim to mitigate the memory and computation demands of Large Language Models (LLMs). These techniques can be broadly categorized into two primary types: quantization (Frantar et al., 2022;Lin et al., 2023;Shao et al., 2023) and pruning (Sun et al., 2023;Frantar & Alistarh, 2023;Ma et al., 2023). Quantization converts full-precision values to low-bit representations, while pruning selectively eliminates insignificant weights. These two compression strategies are distinct but can be synergistically combined to enhance the compression ratio (Frantar et al., 2022;Kim et al., 2023). In this paper, we focus on impelling the performance of LLM pruning. \n\nPruning of Large Language Models. Pruning methods for neural networks can be broadly classified into structured pruning (Ma et al., 2023;Huang et al., 2020) and unstructured pruning (Frantar et al., 2022;Sun et al., 2023;Zhang et al., 2023;2022b). Conventional techniques such as those in (Huang et al., 2020;Zhang et al., 2023) are ill-suited for LLMs due to their reliance on extensive retraining, a challenge amplified within the context of LLMs. In contrast, LLM-specific pruning methods emphasize data and time efficiency. Regarding structured pruning, LLMpruner (Ma et al., 2023) delves into the structured pruning of LLMs and employs LoRA to recuperate the performance of pruned models. In the realm of unstructured pruning, SparseGPT (Frantar & Alistarh, 2023) introduces an efficient technique for estimating the Hessian matrix, thereby adapting the traditional OBS approach (Hassibi et al., 1993) to large-scale models. Furthermore, Wanda (Sun et al., 2023) adopts a straightforward strategy, eliminating weights based on the product of weight and activation values.",
            "score": 0.6757091374206157,
            "section_title": "RELATED WORK",
            "char_start_offset": 5765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1791
                }
            ],
            "ref_mentions": [
                {
                    "start": 936,
                    "end": 955,
                    "matchedPaperCorpusId": "233297054"
                },
                {
                    "start": 1024,
                    "end": 1043,
                    "matchedPaperCorpusId": "233297054"
                },
                {
                    "start": 1599,
                    "end": 1621,
                    "matchedPaperCorpusId": "61815367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "266176746",
            "title": "Data Pruning for Efficient Model Pruning in Neural Machine Translation",
            "text": "The goal of model pruning methods is to reduce the memory footprint and increase the efficiency of neural networks through sparsity induction. The two primary approaches for pruning language models are (i) structured and (ii) unstructured. Structured pruning aims to remove network blocks, whereas unstructured pruning removes the least important weights wherever they occur in the network.",
            "score": 0.6623376810784845,
            "section_title": "Model Pruning",
            "char_start_offset": 7102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 390
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.411376953125
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "Unstructured pruning is a NLP approach used to minimise the size and complexity of neural networks used for language modelling, text classification, machine translation, and other NLP applications.This pruning technique involves selectively removing individual weights or parameters from the network, typically based on their magnitudes or importance scores.By eliminating redundant or less significant parameters, unstructured pruning aims to achieve model compression, improved efficiency, and reduced computational requirements without significantly sacrificing performance.The process of unstructured pruning involves two main steps: identification and removal.In the identification step, each weight or parameter in the network is evaluated based on a predefined criterion, such as magnitude or sensitivity analysis.Magnitude-based pruning, for example, ranks the weights according to their absolute values, allowing the removal of those with the smallest magnitudes.Depending on the pruning approach, this rating can be done globally or layer-by-layer.Once the weights or parameters are ranked, the removal step involves discarding a certain percentage of the least important ones.This removal can be performed by setting the corresponding weights to zero or by completely eliminating the associated connections.In some cases, a threshold is applied to determine the cutoff point for pruning, allowing finer control over the sparsity level of the pruned model.\n\nOne of the primary advantages of unstructured pruning is its flexibility in targeting specific weights or parameters, making it suitable for reducing the model size while preserving important network structures.However, this flexibility comes at the cost of irregular sparsity patterns, as individual weights are pruned independently.Consequently, unstructured pruning can lead to inefficient memory access and inefficient deployment on hardware accelerators optimized for dense matrix operations.To address these issues, additional techniques such as structured pruning and weight sharing can be applied in conjunction with unstructured pruning [7], [30], [50], [51], [52], [53].\n\nIn the context of NLP, unstructured pruning can be applied to various components of neural network models, including word embeddings, recurrent connections, and fully connected layers.",
            "score": 0.660725695544082,
            "section_title": "B. UNSTRUCTURED PRUNING",
            "char_start_offset": 45148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1187
                },
                {
                    "start": 1187,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1466
                },
                {
                    "start": 1468,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1802
                },
                {
                    "start": 1802,
                    "end": 1965
                },
                {
                    "start": 1965,
                    "end": 2148
                },
                {
                    "start": 2150,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 2131,
                    "end": 2135,
                    "matchedPaperCorpusId": "222297215"
                },
                {
                    "start": 2143,
                    "end": 2147,
                    "matchedPaperCorpusId": "4142619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71923828125
        },
        {
            "corpus_id": "263830786",
            "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
            "text": "Pruning. Structured pruning has been extensively studied as a model compression technique in computer vision and natural language processing, where task-specific models like classification ones are often overparameterized and can be pruned significantly with minimal impact on performance (Han et al., 2016;Wen et al., 2016;Liu et al., 2017;Luo et al., 2017;Cai et al., 2019;Deng et al., 2020;Hou et al., 2020;Wang et al., 2020;Lagunas et al., 2021;Xia et al., 2022;Kurtic et al., 2023). Unstructured pruning (Frankle & Carbin, 2018;Li et al., 2020;Chen et al., 2020;Sanh et al., 2020) prunes individual neurons instead of structured blocks. Though unstructured pruning usually achieve higher compression rates, they are not practical for model speedup. \n\nIn the era of LLMs, the prevalent NLP pipeline has shifted from task-specific models to generalpurpose LMs, which leaves little room for redundancy. Both unstructured pruning, semi-structured pruning (Frantar & Alistarh, 2023;Sun et al., 2023), and structured pruning (Ma et al., 2023) lead to significant performance drops on LLM even at a modest sparsity. Noticeably, all previous works fix the original models or tune them minimally. We see pruning as an initialization and consider it necessary to expend substantial compute to continually pre-training the model to recover performance. \n\nEfficient pre-training approaches. As orthogonal to our pruning approach, There is an extensive body of work on improving efficiency of training LLMs. For example, quantization reduces the numeric precision of model weights and activations and speeds up training and inference (Dettmers et al., 2022;2023;Xiao et al., 2023).",
            "score": 0.6591387618044502,
            "section_title": "RELATED WORK",
            "char_start_offset": 18874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1673
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 307,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 307,
                    "end": 324,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 324,
                    "end": 341,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 341,
                    "end": 358,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 358,
                    "end": 375,
                    "matchedPaperCorpusId": "201666112"
                },
                {
                    "start": 375,
                    "end": 393,
                    "matchedPaperCorpusId": "215799572"
                },
                {
                    "start": 393,
                    "end": 410,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 410,
                    "end": 428,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 449,
                    "end": 466,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 509,
                    "end": 533,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 533,
                    "end": 549,
                    "matchedPaperCorpusId": "263868979"
                },
                {
                    "start": 549,
                    "end": 567,
                    "matchedPaperCorpusId": "220768628"
                },
                {
                    "start": 567,
                    "end": 585,
                    "matchedPaperCorpusId": "218665313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61181640625
        },
        {
            "corpus_id": "268364208",
            "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression",
            "text": "Large Language Model Compression: LLMs in general contain billion-scale parameters. Applying conventional model compression methods for LLMs is unfeasible as they necessitate resourceintensive retraining. Given that, post-training methods that avoid retraining in the compression process have been proposed. In general, these methods can be grouped into four categories: unstructured pruning, structured pruning, quantization, and low-rank approximation. Specifically, unstructured pruning methods (Frantar and Alistarh, 2023) set the individual weights of an LLM to zero without changing its shape. However, irregular sparsification of unstructured pruning is difficult to achieve the desired speedup or memory saving. Unlike unstructured pruning, structured pruning methods (Ma et al., 2023;Ashkboos et al., 2024;Zhong et al., 2024) remove entire channels or other structured components from LLMs, making them easier to implement on hardware. One notable contribution is LLM-Pruner (Ma et al., 2023), which groups weight matrices based on their dependency and assigns the pruning ratio to each group based on the estimated importance. Quantization methods (Lin et al., 2024) compress models by reducing the precision of weight matrices of the LLM. However, similar to unstructured pruning, quantization is also difficult to achieve the desired inference speedup due to the lack of hardware support and efficient kernels for low-precision computation (Lin et al., 2024). Recent studies including PB-LLM (Yuan et al., 2024) and BiLLM (Huang et al., 2024) push the frontier to 1-bit quantization. Nevertheless, these approaches often lead to severe accuracy degradation. \n\nSVD for Language Model Compression: Singular Value Decomposition (SVD) is a widely used low-rank approximation technique to reduce matrix size by approximating a matrix with two smaller low-ranking matrices (Golub et al., 1987). Given that, SVD is commonly used for model compression. \n\nFor instance, DRONE (Chen et al., 2021) achieves optimal SVD compression for small language models such as BERT.",
            "score": 0.6588999979346214,
            "section_title": "RELATED WORK",
            "char_start_offset": 4739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 498,
                    "end": 526,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 793,
                    "end": 815,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 1504,
                    "end": 1523,
                    "matchedPaperCorpusId": "263333921"
                },
                {
                    "start": 1534,
                    "end": 1554,
                    "matchedPaperCorpusId": "267523201"
                },
                {
                    "start": 1879,
                    "end": 1899,
                    "matchedPaperCorpusId": "121324775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8857421875
        },
        {
            "corpus_id": "275993741",
            "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models",
            "text": "Large language models (LLMs) have achieved significant advancements across a wide range of tasks and domains, Preprint demonstrating their robust capabilities (Zhang et al., 2022;Achiam et al., 2023;Touvron et al., 2023;Wu et al., 2024). However, as the model size increases, the growing number of parameters leads to significant computational and memory requirements, which significantly hinder the practical deployment of LLMs. Consequently, it is urgent to develop methods that can reduce model size while maintaining performance. \n\nTo address these challenges, several methods have been proposed, including pruning (Frantar & Alistarh, 2023;Sun et al., 2023;Ma et al., 2023;An et al., 2024), quantization (Frantar et al., 2022;Xiao et al., 2023), knowledge distillation (Shridhar et al., 2022;Hsieh et al., 2023), and low-rank decomposition (Saha et al., 2023). In this work, we mainly focus on pruning-an efficient and highly generalizable approach that can be seamlessly integrated with other model compression strategies. Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2023) and structured pruning (Ma et al., 2023;An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns. Despite these advancements, existing structured pruning methods still have some limitations. They all follow the paradigm of first selecting channels or layers to prune based on a designed metric, and then performing RFT (Chavan et al., 2024).",
            "score": 0.6472855000175366,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1804
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 220,
                    "matchedPaperCorpusId": "257219404"
                },
                {
                    "start": 619,
                    "end": 645,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 662,
                    "end": 678,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 845,
                    "end": 864,
                    "matchedPaperCorpusId": "262233736"
                },
                {
                    "start": 1123,
                    "end": 1149,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1190,
                    "end": 1207,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "267279372",
            "title": "Deep Learning-Based Lightweight Target Detection",
            "text": "Unstructured pruning and structured pruning are two different approaches to neural network pruning. Unstructured pruning refers to pruning individual weights; structured pruning refers to pruning filters/channels/layers. One problem with unstructured pruning methods (direct pruning of weights) is that the resulting weight matrix is sparse and cannot be compressed and accelerated without specialized hardware/libraries. Instead, structured pruning methods prune at the channel or layer level. No specialized hardware/libraries are required to run them since the original convolutional structure is still present [6]. Among structured pruning methods, channel pruning is the most commonly used because it prunes at the finest-grained level while still being compatible with traditional deep learning frameworks [7].",
            "score": 0.6469560689664096,
            "section_title": "Network Pruning: unstructured pruning and structured pruning",
            "char_start_offset": 6222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 816
                }
            ],
            "ref_mentions": [
                {
                    "start": 614,
                    "end": 617,
                    "matchedPaperCorpusId": "195657904"
                },
                {
                    "start": 812,
                    "end": 815,
                    "matchedPaperCorpusId": "575794"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499267578125
        },
        {
            "corpus_id": "277509965",
            "title": "When Reasoning Meets Compression: Benchmarking Compressed Large Reasoning Models on Complex Reasoning Tasks",
            "text": "There are unstructured and structured pruning. For unstructured pruning, individual weights are targeted, which leads to irregular sparsity structure. In contrast, structured pruning involves removing entire network components such as channels or layers (Zhang et al., 2024a). Unstructured pruning usually has better compression performance than structured pruning, while it is easier to achieve inference speedup via structured methods (Zhu et al., 2024). Recent unstructured pruning includes one-shot pruning (Frantar & Alistarh, 2023;Sun et al., 2023), global pruning that makes pruning decisions based on all layers (Bai et al., 2024), and domain-specific pruning (Zhang et al., 2024a). Structured pruning includes gradient-based (Xia et al., 2024;Ma et al., 2023) and non-gradient-based (Ashkboos et al., 2024) methods.",
            "score": 0.6390081763903352,
            "section_title": "Pruning",
            "char_start_offset": 6913,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 824
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 275,
                    "matchedPaperCorpusId": "269741380"
                },
                {
                    "start": 437,
                    "end": 455,
                    "matchedPaperCorpusId": "260900101"
                },
                {
                    "start": 511,
                    "end": 537,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 620,
                    "end": 638,
                    "matchedPaperCorpusId": "268041812"
                },
                {
                    "start": 668,
                    "end": 689,
                    "matchedPaperCorpusId": "269741380"
                },
                {
                    "start": 734,
                    "end": 752,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 752,
                    "end": 768,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75048828125
        },
        {
            "corpus_id": "273228757",
            "title": "Chip-Tuning: Classify Before Language Models Say",
            "text": "Network Pruning. With the growth in the size of language models, the pruning technique has been proposed to eliminate unnecessary weights or structures in language models, thus accelerating language models. The pruning methods can be generally categorized into two types: unstructured pruning and structured pruning. \n\nUnstructured pruning methods focus on the level of individual weights, which try to speed up models by increasing the sparsity level of model weights. SparseGPT (Frantar and Alistarh, 2023) reduces the pruning problem to layer-wise sparse regression and incrementally prunes each column in the weight matrix with a sequence of Hessian inverses. Wanda (Sun et al., 2023) enhances the magnitude pruning approach with input activation norms, effectively reducing the complexity of pruning algorithms. RIA (Zhang et al., 2024a) notices that previous methods tend to prune away entire channels of network weights, and mitigates the issue by jointly considering input and output channels. \n\nStructured pruning methods operate at the level of network structures instead, which compress language models by removing redundant model components. LLMPruner (Ma et al., 2023) employs gradient information as a reference to remove noncritical structures. SliceGPT (Ashkboos et al., 2024) removes rows or columns corresponding to small principal components in the weight matrix to achieve smaller weight matrices. LaCo (Yang et al., 2024) proposes the layer collapse algorithm, which merges adjacent layers while ensuring the representation similarity on few-shot calibration examples. ShortGPT (Men et al., 2024) finds that deep layers of language models are not as effective as expected, and proposes the block importance metric to identify and remove redundant layers. BlockPruner (Zhong et al., 2024) decomposes each Transformer layer into two minimal residual blocks and performs fine-grained block pruning to avoid significant performance loss. \n\nProbing Language Models. The impressive capability of language models raises the hypothesis that language models have gone beyond mere memorization of surface correlations. learn the principles behind the training data and develop internal representations for features (Belinkov, 2022).",
            "score": 0.6359020842914604,
            "section_title": "Related Work",
            "char_start_offset": 4771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1954
                },
                {
                    "start": 1957,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 508,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72998046875
        },
        {
            "corpus_id": "278208127",
            "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning",
            "text": "Within the evolving landscape of Artificial Intelligence, Large Language Models (LLMs) stand out as a pivotal force, propelling Natural Language Processing towards unprecedented stages -often matching or even surpassing human-level performance in many language benchmarks [1]- [3]. However, this performance often comes at the cost of larger model sizes, with models such as DeepSeek-V3 [4] reaching the mark of 671 billion parameters. \n\nThis substantial size introduces significant challenges for the deployment of LLMs in low-resource and time-critical applications due to high computational costs and slow inference [5]. To overcome these challenges, some techniques such as pruning [6] and quantization [7], [8] aim to reduce the model size, thereby lowering computational overhead. \n\nRecent studies confirm pruning as a promising solution to compress models as it maintains predictive ability and is often hardware-agnostic [9]- [13]. Within the field of LLMs, pruning \u00a7 Equal contribution. \n\ntechniques fall into three main categories: structured, semistructured and unstructured pruning. \n\nStructured pruning removes entire components -such as attention heads or layers -while preserving the overall network structure without introducing sparsity (i.e., without zeroing out a significant portion of the model's parameters) [6]. However, the removal of larger and potentially more critical components may result in performance degradation, typically requiring Parameter-Efficient Fine-Tuning (PEFT) techniques for performance recovery [14]. Due to the removal of complete components, structured pruning usually achieves inference acceleration and memory reduction without the need for specialized hardware or software [15]. \n\nSemi-structured (a.k.a. structured N : M ) pruning promotes model sparsity by removing groups of consecutive parameters following a pruning mask [16]. Specifically, structured N : M sparsity requires that at least N out of every M consecutive weights be non-zero [17], [18]. While this is a promising technique, it requires specialized hardware to achieve practical speedup, making it less suitable for deployment on consumergrade GPUs [17].",
            "score": 0.6215707030653611,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1729
                },
                {
                    "start": 1732,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2173
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 619,
                    "end": 622,
                    "matchedPaperCorpusId": "257495837"
                },
                {
                    "start": 707,
                    "end": 710,
                    "matchedPaperCorpusId": "259298689"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 934,
                    "end": 938,
                    "matchedPaperCorpusId": "266573164"
                },
                {
                    "start": 1541,
                    "end": 1545,
                    "matchedPaperCorpusId": "268553763"
                },
                {
                    "start": 1724,
                    "end": 1728,
                    "matchedPaperCorpusId": "267413136"
                },
                {
                    "start": 1877,
                    "end": 1881,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1995,
                    "end": 1999,
                    "matchedPaperCorpusId": "231847094"
                },
                {
                    "start": 2001,
                    "end": 2005,
                    "matchedPaperCorpusId": "233296249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "Their objective was to reduce model size by removing redundant parameters while preserving speech recognition performance.The authors propose a structured pruning method to lessen the size and complexity of self-supervised pre-trained models while maintaining their performance on speech-related tasks.The key contributions and findings of the paper include the following:\n\n1) Pruning Strategy: The authors present a specific pruning strategy tailored for self-supervised pre-trained models in the speech domain.This strategy identifies and removes less important structures or components, such as layers or neurons, based on their significance to the model's overall performance.2) Performance Analysis: The paper evaluates the impact of structured pruning on speech recognition and understanding tasks.It assesses the model's accuracy and efficiency before and after pruning, demonstrating the potential benefits of structured pruning in reducing model size and computational requirements while preserving task performance.3) Comparison with Baselines: The authors compare their proposed structured pruning approach with other baseline methods commonly used for model compressions, such as unstructured pruning or weight quantization.The comparison highlights the advantages and effectiveness of structured pruning specifically for selfsupervised pre-trained models in the speech domain.The results show significant compression and improved accuracy compared to the original model, as validated by experiments on LibriSpeech and SLURP datasets.\n\nMa et al. [29] proposes Large Language Model-Pruner (LLM-Pruner), a task-agnostic compression method, aiming to maintain the multi-task solving and language generation abilities of the original LLM while minimizing reliance on the extensive training dataset.LLM-Pruner adopts structural pruning, removing non-critical coupled structures based on gradient information to preserve most of the LLM's functionality.The pruned models can be efficiently recovered through tuning techniques in a short time and with minimal data.Experimental results on three LLMs, Large Language Model Meta AI (LLaMA), Vicuna, and General Language Model (ChatGLM), demonstrate that the compressed models perform well in zero-shot classification and generation tasks.",
            "score": 0.6215245104473811,
            "section_title": "IV. METHODS",
            "char_start_offset": 18689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 372
                },
                {
                    "start": 374,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 804
                },
                {
                    "start": 804,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1236
                },
                {
                    "start": 1236,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1546
                },
                {
                    "start": 1548,
                    "end": 1806
                },
                {
                    "start": 1806,
                    "end": 1959
                },
                {
                    "start": 1959,
                    "end": 2070
                },
                {
                    "start": 2070,
                    "end": 2291
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "246276158",
            "title": "Iterative Activation-based Structured Pruning",
            "text": "Like other compression techniques, pruning has a performance-accuracy tradeoff, i.e, as we compress the model more aggressively, we achieve better hardware performance (higher reductions in computation and memory requirements) but suffer higher accuracy losses. In this paper, we measure performance across two dimensions. First, memory footprint, i.e., memory space used to store parameters, and second, inference latency, i.e., time to run one inference. Pruning can be broadly categorized into two classes, namely, unstructured pruning and structured pruning. The two techniques take different approaches on memory footprint reduction, and impact inference latency and accuracy differently. \n\nUnstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al. 2018), e.g, (Hill et al. 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al. 2018). \n\ncan choose either structured or unstructured pruning techniques.",
            "score": 0.619585882014117,
            "section_title": "Unstructured vs. Structured Pruning",
            "char_start_offset": 915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1969
                },
                {
                    "start": 1972,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 1250,
                    "end": 1265,
                    "matchedPaperCorpusId": "52048008"
                },
                {
                    "start": 1273,
                    "end": 1291,
                    "matchedPaperCorpusId": "13876417"
                },
                {
                    "start": 1952,
                    "end": 1968,
                    "matchedPaperCorpusId": "52048008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89111328125
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "Pruning techniques provide valuable solutions for improving efficiency and performance in NLP tasks.The choice of pruning technique depends on specific requirements and trade-offs.Structured pruning techniques strike a balance between efficiency and preservation of network structure, while unstructured pruning techniques enable fine-grained reduction but may require additional strategies for performance preservation.Magnitude-based pruning offers a simple approach with efficient results.By effectively applying pruning techniques, NLP models can achieve significant reduction in size and computational complexity without compromising performance, thus paving the way for more efficient NLP applications in resource-constrained environments.\n\nWe have summarized some of the recent works on pruning of Natural Language Processing-based deep learning networks.This section provides more details about the possible comparison of these pruning methods.In order to provide the best pruning method is not simple.There is no clear criterion to describe the best approach.However, their applicability is application-dependent.\n\n\u2022 Structural ways of pruning like filter, channel, and weight pruning are more convenient for applications that require a compact version of the pre-trained model.\n\n\u2022 The applications that require stable accuracy and reduced model size, structural pruning provides reasonable mitigation in network size with minimal accuracy degradation.\n\n\u2022 Structural pruning is more hardware-friendly than other pruning techniques.Organized network sparsity is easy to exploit in memory and processing implementation.\n\n\u2022 Unstructured pruning is reasonable where applications require acceleration and no compromise on performance.Although all the above-mentioned techniques are orthogonal to each other, however, various pruning methods can be combined to maximize the punning advantage with minimum accuracy loss.In the case of Convolution Neural Networks (CNNs) that combine the two dimension convolution for feature extraction and fully connected layer for classification, filter or layer pruning and weight pruning can be applied.",
            "score": 0.6195666752704654,
            "section_title": "VI. CONCLUSION",
            "char_start_offset": 72225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 100,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 492
                },
                {
                    "start": 492,
                    "end": 745
                },
                {
                    "start": 747,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1068
                },
                {
                    "start": 1068,
                    "end": 1122
                },
                {
                    "start": 1124,
                    "end": 1287
                },
                {
                    "start": 1289,
                    "end": 1461
                },
                {
                    "start": 1463,
                    "end": 1540
                },
                {
                    "start": 1540,
                    "end": 1626
                },
                {
                    "start": 1628,
                    "end": 1738
                },
                {
                    "start": 1738,
                    "end": 1922
                },
                {
                    "start": 1922,
                    "end": 2142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "266573164",
            "title": "The LLM Surgeon",
            "text": "We also evaluate our method on downstream tasks as perplexity metrics do not necessarily correlate with downstream performance. Further, we also repeat this experiment using the C4 dataset as reference data for compression, as this is used in prior work (Frantar & Alistarh, 2023) and as this can be regarded a more general reference dataset. In tables 5 and 6 we report 0-shot test performance of structured pruning for LLM surgeon and K-OBD baseline. LLM Surgeon uses data to find a compressed model that has the least negative impact on final test performance. In this section, we explore the extent to which the method can use data to compress specifically to the task at hand. We do so by comparing test performance and equivalences between resulting pruning masks for different language modeling languages: English (EN/wikitext-2), French (FR) and Italian (IT) and the German (DE). We consider 50% unstructured compression using LLM Surgeon with correlated weight updates. For each compressed model, we compare performance on all languages and compare the equivalences between resulting pruning masks (details in appendix B.3), and report results in table 10. Like other methods that use data for compression (Hassibi & Stork, 1992;Frantar & Alistarh, 2023;Wang et al., 2019), we expect to see some correlation between the data used for training and data with good test performance, which is reflected in both test performance and masks. It is important to note that the final performance after compression will depend on the quality of the used dataset for compression. Further, the experiment demonstrates that the method can be used for task-specific compression tailored towards the data used for compression and generalises to high test performance on the associated test data.",
            "score": 0.6142235056050911,
            "section_title": "D DOWNSTREAM TASK PERFORMANCE",
            "char_start_offset": 33920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1788
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 280,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1215,
                    "end": 1238,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 1238,
                    "end": 1263,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1263,
                    "end": 1281,
                    "matchedPaperCorpusId": "155089879"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.194091796875
        },
        {
            "corpus_id": "277452419",
            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
            "text": "For instance, in compression, while structured pruning accelerates inference, it incurs severe accuracy loss (above 50% sparsity triggers catastrophic failure) outperformed by unstructured and quantization methods. Quantization exhibits a \"safe compression zone\" (above 2-bit precision), beyond which performance degrades nonlinearly-yet larger models retain superior robustness under low-bit settings compared to smaller counterparts. With regard to Decoding Methods, deterministic decoding approaches exhibited superior and more stable performance compared to stochastic ones. \n\nAs the parameters of large-scale language models (LLMs) continue to expand, the marginal benefits of increasing model size diminish, while resource consumption escalates. We call for urgent attention to this phenomenon. Future work should focus on refining model health diagnostic methods, exploring new architectures, and developing a systematic robustness evaluation framework to address the challenges posed by large-scale language models in dynamic environments. model performance is paramount. Specific experimental data on unstructured pruning is referenced in Table A2. Additionally, we conducted ablation experiments on the Calibration dataset, and the alignment results on the PTB dataset can be found in Table A3.",
            "score": 0.6127208325447518,
            "section_title": "Conclusion",
            "char_start_offset": 62664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6318359375
        },
        {
            "corpus_id": "260887757",
            "title": "A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations",
            "text": "Unstructured pruning methods ( [13,17]) remove weights anywhere and can achieve high prune ratios with little impact on accuracy. In contrast, structured pruning ( [20,62,67]) conducts pruning on entire filters (or channels, neurons, layers, etc.), resulting in really compressed network and accelerated inference. However, the accuracy is often lower than that of unstructured pruning under the same prune ratio, weight-level scoring, pipeline, and learning schemes. The possible reason is that unstructured pruning only focuses on the importance of individual weights, while structured pruning forces structural coupling, which demands simultaneous pruning across multiple layers and expects all removed weights to be consistently unimportant. However, achieving consistency in identifying unimportant weights under the structural coupling constraints is challenging. Amersfoort et al. [173] argue that SNIP-structured and GraSP-structured methods incur more noise than their vanilla unstructured counterparts. We compare unstructured and structured pruning methods on VGG-16 [1] and report the best results in Table 5 from three random runs. Additionally, we compare unstructured, semi-structured, and structured pruning methods on OPTs [175] with data sourced from [164] (Table 6). As shown in Table 5 and Table 6, at the same prune ratio, unstructured pruning generally outperforms semi-structured (if any), which performs better than structured pruning.",
            "score": 0.6126120804874728,
            "section_title": "Unstructured vs. Structured Pruning",
            "char_start_offset": 47403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1459
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "235825363"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63232421875
        },
        {
            "corpus_id": "273375608",
            "title": "A Review on Edge Large Language Models: Design, Execution, and Applications",
            "text": "Pruning is a key technique for optimizing large language models (LLMs) by reducing the number of parameters, leading to smaller model sizes and faster inference. However, pruning in LLMs is challenging due to the complexity of their architecture and the varying significance of components such as attention heads. Conventional pruning methods, effective in CNNs, face limitations when applied to LLMs [104]. As shown in Fig. 6 (b), specialized pruning techniques for LLMs are generally categorized into structured and unstructured pruning, each with distinct trade-offs.",
            "score": 0.6119638281040649,
            "section_title": "Pruning",
            "char_start_offset": 20640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 570
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 406,
                    "matchedPaperCorpusId": "221761597"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408935546875
        },
        {
            "corpus_id": "248693207",
            "title": "DNA data storage, sequencing data-carrying DNA",
            "text": "We compare the techniques we have tested with each other in Figure 7 based on the accuracy and the model compression. \n\nFigure 7: Performance of our pruned models after pruning using all the strategies we experimented with. Our proposed strategy, iterative global unstructured pruning, is illustrated in red. \n\nTable 4 and Figure 7 indicate that when using structured pruning, we are unable to prune to the same amount as when using unstructured pruning, and that models reduced to the same degree of sparsity when pruned in a structured way have considerably worse accuracy than when pruned unstructured. This is likely due to our structured pruning technique being applied separately to each layer, i.e., we prune all layers evenly rather than globally ranking the weights across all layers to be pruned as we did in case of unstructured pruning. Because the layers in our model have various sizes, implementing global structured pruning is difficult. For example, ordering the columns of weight matrices according to their L1 norm makes no sense when some columns have more elements than others. Future research might be based on further investigation of how to accomplish this successfully or exploring different trimming techniques. \n\nOur results generally suggest that iterative pruning outperforms one-shot pruning for our model, with up to 15.2% higher accuracy at a compression ratio of 99.6%. This is most likely because pruning too much at once means the model can no longer recover, even after retraining. Iterative pruning, on the other hand, allows the model to be trained while being pruned, allowing the remaining weights in the model to adapt to compensate for those that were lost during pruning. \n\nModel weights may not necessarily be eliminated in equal proportions from each layer when using global pruning, e.g., deeper network levels may be pruned more extensively, allowing for more connections from deeper layers to be eliminated. Our analysis summarised in Figure 8 shows, however, that the initial LSTM layer is obviously pruned more than other layers with comparable sparsity at lower compression ratios. On the other hand, at extremely high compression ratios, they all have similar sparsity.",
            "score": 0.6072579730949754,
            "section_title": "Accuracy vs Compression Comparison",
            "char_start_offset": 19247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 120,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2221
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3037109375
        },
        {
            "corpus_id": "246276168",
            "title": "Adaptive Activation-based Structured Pruning",
            "text": "Unstructured vs. Structured Pruning. Unstructured pruning (LeCun et al. (1990); Han et al. (2015); Molchanov et al. (2017)) is a fine-grained approach that prunes individual unimportant elements in weight tensors. It has less impact to model accuracy, compared to structured pruning, but unstructured pruned models are hard to accelerate on commodity hardware. Structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors according to some rulebased heuristics, such as L1-norm (Li et al. (2016)), average percentage of zero (Molchanov et al. (2016)), and other information considering the relationship between neighboring layers (Theis et al. (2018); Lee et al. (2018)). Compared to unstructured pruning, it is more difficult to prune a model without causing accuracy loss using structured pruning, because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. (2016)). However, structured pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al. (2018b)). \n\nOne Shot vs. Iterative Pruning. One-shot pruning prunes a pre-trained model and then retrains it once, whereas iterative pruning prunes and retrains the model in multiple rounds. Both techniques can choose either structured or unstructured pruning techniques. Recently, works based on the Lottery Ticket Hypothesis (LTH) have achieved great successes in creating smaller and more accurate models through iterative pruning with rewinding (Frankle & Carbin (2018)). LTH posits that a dense randomly initialized network has a sub-network, termed as a winning ticket, which can achieve an accuracy comparable to the original network.",
            "score": 0.6058124547003486,
            "section_title": "BACKGROUND AND RELATED WORKS",
            "char_start_offset": 4600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1771
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 78,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 99,
                    "end": 122,
                    "matchedPaperCorpusId": "18201582"
                },
                {
                    "start": 1119,
                    "end": 1137,
                    "matchedPaperCorpusId": "52048008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6953125
        },
        {
            "corpus_id": "248377019",
            "title": "FedDUAP: Federated Learning with Dynamic Update and Adaptive Pruning Using Shared Data on the Server",
            "text": "With unstructured pruning, some parameters are set to 0 without changing the model structure. While it can achieve accuracy comparable to the original model [Zhang et al., 2021] and helps reduce communication costs in FL [Jiang et al., 2019], unstructured pruning has limited advantages on general-purpose hardware in terms of computational cost [Lin et al., 2020a]. In contrast, structured pruning directly changes the structure of the model by removing some neurons (filters in the convolution layers). While it can significantly reduce both computational cost and communication cost [Lin et al., 2020a], it is complicated to determine the number of filters to preserve in the structured pruning method, and the existing methods are not well adapted to FL. Gradient compression or sparsification can be exploited in FL to reduce communication cost [Kone\u010dn\u1ef3 et al., 2016], which has been extensively researched in literature and won't be further explored in this paper.",
            "score": 0.6044871282692184,
            "section_title": "Related Work",
            "char_start_offset": 8136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 970
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 177,
                    "matchedPaperCorpusId": "244958525"
                },
                {
                    "start": 221,
                    "end": 241,
                    "matchedPaperCorpusId": "53820846"
                },
                {
                    "start": 346,
                    "end": 365,
                    "matchedPaperCorpusId": "235078788"
                },
                {
                    "start": 586,
                    "end": 605,
                    "matchedPaperCorpusId": "235078788"
                },
                {
                    "start": 850,
                    "end": 872,
                    "matchedPaperCorpusId": "53820846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418701171875
        },
        {
            "corpus_id": "264146875",
            "title": "NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models",
            "text": "In recent years, pre-trained language models (LMs) have demonstrated their effectiveness in various downstream tasks, such as natural language understanding (NLU) and natural language generation (NLG). Especially, there have been three main types of research, e.g.,encoder-only LMs (Devlin et al., 2019;He et al., 2023), decoder-only LMs (Touvron et al., 2023;OpenAI, 2023), and encoder-decoder LMs (Lewis et al., 2020;Raffel et al., 2020;Chung et al., 2022b;Tay et al., 2023), which aim for their specific expertise. Among these various types of LMs, we will focus on the widely studied and utilized encoder-decoder LMs due to their flexibility in application across a range of tasks (Guo et al., 2022;Wang et al., 2023b). On the other perspective of LM researches rather than performances, efficiency of LMs (e.g.,computational and memory cost) have been intensively studied because of their huge computational requirements. This research direction is called model compression. Among the various model compression techniques (Jiao et al., 2020;Yao et al., 2022), pruning (Frankle and Carbin, 2018;Sanh et al., 2020;Wang et al., 2020c;Xia et al., 2022) is a promising method that aims to remove redundant weights from networks, resulting in improved efficiency by saving storage capacity and enhancing inference speed. Between structured pruning and unstructured pruning approaches, structured pruning is typically preferred in practice due to its relative ease of deployment on various types of hardware platforms compared to unstructured pruning (Han et al., 2016;Gupta and Agrawal, 2020). \n\nTherefore, we focus on the structured pruning method specifically tailored for encoder-decoder LMs. Despite the remarkable advancements in encoder-decoder models, little attention has been given to structured pruning methods for encoderdecoder LMs.",
            "score": 0.6038858535560847,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1843
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 303,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 399,
                    "end": 419,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 459,
                    "end": 476,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 685,
                    "end": 703,
                    "matchedPaperCorpusId": "247315559"
                },
                {
                    "start": 1027,
                    "end": 1046,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1046,
                    "end": 1063,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 1117,
                    "end": 1136,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 1136,
                    "end": 1153,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1549,
                    "end": 1567,
                    "matchedPaperCorpusId": "1663491"
                },
                {
                    "start": 1567,
                    "end": 1591,
                    "matchedPaperCorpusId": "221112343"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64501953125
        },
        {
            "corpus_id": "256662734",
            "title": "What Matters In The Structured Pruning of Generative Language Models?",
            "text": "Large language models (LLMs), such as the state-of-the-art GPT-3 model (Brown et al., 2020) with up to 175 billion parameters, have achieved remarkable performance in natural language processing (NLP) tasks. However, training and deploying such massive models also poses significant challenges in terms of computational cost, energy consumption, and environmental impact. Therefore, it is crucial to develop effective methods to reduce the size of LLMs without compromising their quality. \n\nNeural network pruning is a long-standing model compression method (Janowsky, 1989;Mozer & Smolensky, 1988;Frankle & Carbin, 2018;Karnin, 1990;Blalock et al., 2020). It can be broadly classified into two types: unstructured and structured. Unstructured pruning removes individual weights from the network based on some criteria, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2020b;Xu et al., 2021;Zafrir et al., 2021;Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast services based on LLMs. \n\nExisting work on structured pruning for LMs focuses on BERT-like networks (Devlin et al., 2018) that consist of an encoder-decoder or an encoder-only architecture (Li et al., 2020;Xia et al., 2022;Zhang et al., 2022;Yao et al., 2021). These models are mainly used for natural language understanding (NLU) tasks, such as question answering, sentiment analysis, or natural language inference. Among the various methods, Block Movement Pruning (Lagunas et al., 2021) is a recent and popular technique that removes weight blocks based on movement.",
            "score": 0.6010975172748023,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 574,
                    "matchedPaperCorpusId": "31375995"
                },
                {
                    "start": 574,
                    "end": 598,
                    "matchedPaperCorpusId": "17651092"
                },
                {
                    "start": 621,
                    "end": 634,
                    "matchedPaperCorpusId": "1101832"
                },
                {
                    "start": 1162,
                    "end": 1182,
                    "matchedPaperCorpusId": "204009154"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77490234375
        },
        {
            "corpus_id": "277955463",
            "title": "NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models",
            "text": "Large language models (LLMs) (Brown et al., 2020) have demonstrated remarkable capabilities across a wide range of natural language processing tasks, but their immense computational and memory requirements during inference pose significant challenges for deployment. Consequently, post-training compression techniques have emerged as a promising tool to reduce model size and computational overhead while maintaining accuracy. Two promising families of methods for post-training compression are Pruning (Lecun et al., 1989;Hassibi et al., 1993;Han et al., 2015) and Quantization (Yao et al., 2022;Dettmers et al., 2022;Ahmadian et al., 2023). \n\nPruning aims to remove redundant parameters from LLMs while preserving performance. We will focus on two forms of pruning, unstructured pruning (Liao et al., 2023), which removes zeroed out, and N:M semi-structured pruning (Huang et al., 2024b), where N of every M elements are zeroed out. SparseGPT (Frantar & Alistarh, 2023) introduced an efficient, unstructured and semi-structured pruning method that leverages Hessian-based weight updates to minimize performance loss. More recently, Wanda (Sun et al., 2024) demonstrated a simple yet effective unstructured and semi-structured pruning method that requires no weight updates or hessian computation, making it significantly faster and easier to apply than SparseGPT. However, current hardware only supports 2:4 semistructured sparsity, which results in significant post compression performance loss. \n\nA more effective compression method is quantization, which reduces the number of bits used to store each weight (Kuzmin et al., 2023). For the scope of this paper we focus on a common form of quantization, weight only Post Training Quantization (PTQ). Pioneering works (Frantar et al., 2023;Lin et al., 2024;Kim et al., 2024) focused on scalar quantization.",
            "score": 0.6007335269303196,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 523,
                    "end": 544,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 579,
                    "end": 597,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 597,
                    "end": 619,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 619,
                    "end": 641,
                    "matchedPaperCorpusId": "258967189"
                },
                {
                    "start": 789,
                    "end": 808,
                    "matchedPaperCorpusId": "260887085"
                },
                {
                    "start": 1613,
                    "end": 1634,
                    "matchedPaperCorpusId": "259360935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "271909421",
            "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
            "text": "To illustrate the enhanced efficiency of pruned models, we present the inference speed of dense and sparse models on AMD CPU. We use the DeepSparse library [12] and apply 50% unstructured pruning on OPTs and LLaMAs in this experiment. Table 7 indicates that the pruned models can achieve 1.19x \u223c 1.87x speedup compared to their dense counterparts. This significant boost in inference speed underscores the critical importance of model pruning in practical applications. Pruning for Language Model Compression. The surging complexity of Transformer-based language models, which now feature hundreds of billions of parameters, has accentuated the urgent need for effective and efficient model pruning methods [6,8,9]. These pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,14]. Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]. Techniques such as SparseGPT [3] extend the OBS [9] methodology to column-wise prune weights, allowing the modification of values in the unpruned columns to compensate for the pruning errors. Aaquib et al. [28] enhance SparseGPT by incorporating minimal iterative task fine-tuning during the pruning process, demonstrating performance improvements at high sparsity levels. Wanda [27] introduces a simple yet effective no-retraining-needed pruning strategy that prunes weights based on their magnitudes and corresponding activations. OWL [39] considers the inhomogeneous distribution of interlayer outliers and further extends Wanda to a non-uniform sparsity distribution, achieving better performance. DS\u2205T [42] and SPP [16], as fine-tuning methods designed for sparse models, can improve the performance of pruned PLMs within limited complexity. \n\nWeight Distribution Optimization. Various techniques have been employed to understand and optimize weight distributions in the quest for more efficient neural networks.",
            "score": 0.5955480025336286,
            "section_title": "Efficiency Analysis",
            "char_start_offset": 19063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1995
                },
                {
                    "start": 1998,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 707,
                    "end": 710,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 710,
                    "end": 712,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 712,
                    "end": 714,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "260815690"
                },
                {
                    "start": 1178,
                    "end": 1181,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1197,
                    "end": 1200,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "259950394"
                },
                {
                    "start": 1686,
                    "end": 1690,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 1856,
                    "end": 1860,
                    "matchedPaperCorpusId": "264128029"
                },
                {
                    "start": 1869,
                    "end": 1873,
                    "matchedPaperCorpusId": "270063400"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "276107452",
            "title": "Progressive Binarization with Semi-Structured Pruning for LLMs",
            "text": "Large language models (LLMs) have achieved remarkable success in natural language processing tasks, but their high computational and memory demands pose challenges for deployment on resource-constrained devices. Binarization, as an efficient compression method that reduces model weights to just 1 bit, significantly lowers both computational and memory requirements. Despite this, the binarized LLM still contains redundancy, which can be further compressed. Semi-structured pruning provides a promising approach to achieve this, which offers a better trade-off between model performance and hardware efficiency. However, simply combining binarization with semi-structured pruning can lead to a significant performance drop. To address this issue, we propose a Progressive Binarization with Semi-Structured Pruning (PBS$^2$P) method for LLM compression. We first propose a Stepwise semi-structured Pruning with Binarization Optimization (SPBO). Our optimization strategy significantly reduces the total error caused by pruning and binarization, even below that of the no-pruning scenario. Furthermore, we design a Coarse-to-Fine Search (CFS) method to select pruning elements more effectively. Extensive experiments demonstrate that PBS$^2$P achieves superior accuracy across various LLM families and evaluation metrics, noticeably outperforming state-of-the-art (SOTA) binary PTQ methods. The code and models will be available at https://github.com/XIANGLONGYAN/PBS2P.",
            "score": 0.5943379384795645,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68896484375
        },
        {
            "corpus_id": "276482745",
            "title": "MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures",
            "text": "Unstructured pruning compresses models by removing individual parameters, resulting in sparse weight matrices that consume less memory. However, without dedicated hardware support, the updated models do not achieve faster inference, thereby still imposing computational burdens during the inference process. Semi-structured pruning offers some speed improvements, but these are limited compared to those achieved by structured pruning. Structured pruning adopts a more modular approach to remove modules from models, typically targeting attention heads, embedding dimensions, FFN intermediate dimensions, experts in Mixture-of-Experts (MoE) networks, or layers. After structured pruning, the weight matrices of the models remain dense, and their reduced dimensions typically lead to greater inference acceleration. However, the coarser granularity of this pruning method makes it more challenging to preserve model capabilities after pruning. Currently, most pruning techniques employ metricbased methods, which determine the modules to be pruned by introducing specific pruning metrics. These metrics are usually designed heuristically and often perform poorly at high pruning rates. Moreover, a single metric cannot fully capture the importance of model weights, making it difficult to identify superior local optimal solutions. In contrast, optimization-based pruning methods determine which weights to prune by learning a pruning mask, thereby avoiding the performance degradation associated with manually designed metrics. This paper primarily focuses on optimization-based pruning methods. \n\nGiven the large scale of Large Language Models (LLMs), existing optimization-based pruning methods employ structured pruning, wherein a single mask prunes entire modules of the model. Methods such as CoFi (Xia et al., 2022), Compresso (Guo et al., 2023) and NutePrune (Li et al., 2024) follow the L 0 regularization (Louizos et al., 2018) training paradigm during the training of pruning masks, learning masks by setting a total sparsity without additional constraints. This approach results in a lack of uniformity between layers during training, causing each layer to have a different number of attention heads and FFN intermediate dimensions, as illustrated in Figure 1, which leads to suboptimal inference speed.",
            "score": 0.594204205290018,
            "section_title": "Introduction",
            "char_start_offset": 1761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1595
                },
                {
                    "start": 1598,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2314
                }
            ],
            "ref_mentions": [
                {
                    "start": 1803,
                    "end": 1821,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1914,
                    "end": 1936,
                    "matchedPaperCorpusId": "30535508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "272910976",
            "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
            "text": "Pruning Large Language Models. Network Pruning [15,30,17,18,41] have been proven an efficient approach to compress pre-trained language models via the removal of redundant parameters. \n\nAccording to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning [26,43,24], Unstructured Pruning [17,15], and Semi-Structured Pruning [12,38,29,32,33]. Structured pruning physically eliminates substructures like attention heads [26], embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters [26,43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy [12,38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32,33,12]. \n\nIn this study, we focus on N:M semi-structured sparsity within Large Language Models and present a learnable framework to obtain high-quality masks via end-to-end training. \n\nLearnable Semi-Structured Sparsity. On another hand, a burgeoning interest exists in developing learnable masks [49,25,47], especially in the field of vision models. Markedly contrasted with traditional one-shot pruning methods that rely on a predetermined metric of importance, learnable sparsity can fully leverage the rich information in training data, enabling the identification of more effective sparsity masks.",
            "score": 0.5938160519062461,
            "section_title": "Related Works",
            "char_start_offset": 5105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 54,
                    "matchedPaperCorpusId": "195657904"
                },
                {
                    "start": 54,
                    "end": 57,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 60,
                    "end": 63,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "260815690"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "245002847"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 748,
                    "end": 752,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 948,
                    "end": 952,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1469,
                    "end": 1472,
                    "matchedPaperCorpusId": "245002847"
                },
                {
                    "start": 1472,
                    "end": 1475,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1770,
                    "end": 1773,
                    "matchedPaperCorpusId": "256503843"
                },
                {
                    "start": 1773,
                    "end": 1776,
                    "matchedPaperCorpusId": "249642429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "274776083",
            "title": "P$^2$ Law: Scaling Law for Post-Training After Model Pruning",
            "text": "Large language models (LLMs) based on the Transformer architecture (Vaswani et al., 2017) have been applied across diverse domains and tasks. However, as LLMs grow in size, their hardware demands increase substantially, limiting their practical deployment in real-world scenarios. To address this challenge, researchers have focused on developing compact models through model pruning techniques (Han et al., 2016) that maintain high performance while reducing hardware requirements. \n\nModel pruning can be broadly categorized into unstructured pruning (Frantar and Alistarh, 2023;Zhang et al., 2024;Sun et al., 2024) and structured pruning (Chen et al., 2024;Hu et al., 2024; 10 3 10 4 C = 6ND 2.2 2.4 2.6 2.8 3.0",
            "score": 0.5912654516276699,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40771484375
        },
        {
            "corpus_id": "274982336",
            "title": "GQSA: Group Quantization and Sparsity for Accelerating Large Language Model Inference",
            "text": "Compressing Large Language Models. Pruning and quantization are the two primary techniques for compressing LLMs. Pruning methods can be classified into structured (Chen et al., 2024;Ma et al., 2023;Ashkboos et al., 2024), semistructuredcite (Frantar and Alistarh, 2023;Sun et al., 2023;Fang et al., 2024), and unstructured (Han et al., 2016(Han et al., , 2015;;Sun et al., 2023) pruning, depending on the granularity of pruning. Structured pruning operates at a coarser granularity and offers significant acceleration, but it often results in a substantial loss of accuracy (Wang et al., 2024), limiting its application in LLMs. Unstructured pruning better preserves accuracy but provides limited improvements in inference speed (Frantar and Alistarh, 2023). Semi-structured pruning strikes a balance between accuracy retention and acceleration, though it is constrained by a sparsity of 50%, reducing its flexibility. Quantization reduces model size by replacing floating-point numbers with low-precision integers, which accelerates memory access during inference. Currently, highbit quantization techniques such as AWQ (Lin et al., 2024), GPTQ (Frantar et al., 2022), QuIP (Chee et al., 2024), OmniQuant (Shao et al., 2023), and OWQ (Lee et al., 2024) are widely adopted. However, extremely low-bit quantization poses significant challenges, with mainstream methods struggling to maintain performance at low-bit levels. \n\nWhile techniques like AQLM (Egiazarian et al., 2024) and QuIP# (Tseng et al., 2024) aim to enhance low-bit quantization, they rely on vector quantization and complex codebooks, which hinder inference acceleration. Overall, existing model compression techniques continue to face substantial challenges in achieving an optimal balance between flexibility and compression rate. Advantages of GQSA. Quantization and sparsity address model redundancy in different ways.",
            "score": 0.5912481464772765,
            "section_title": "Related work",
            "char_start_offset": 4029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 269,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 729,
                    "end": 757,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1121,
                    "end": 1139,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 1175,
                    "end": 1194,
                    "matchedPaperCorpusId": "260154775"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "267095435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72314453125
        },
        {
            "corpus_id": "257361022",
            "title": "DNN Pruning and Its Effects on Robustness",
            "text": "Pruning methods reduce the size of an already trained model to correspondingly decrease the runtime, memory footprint, and power consumption. For this purpose, redundant parameters are removed from the model while trying to preserve the model accuracy compared to the baseline model. Since pruning has a regularization effect (Bartoldson et al., 2020), it is sometimes even possible to gain some improvement in accuracy by pruning, especially when the initial network is over-parameterized. \n\nIn general, pruning methods can be divided into two main categories: structured and unstructured pruning. In structured pruning, complete structures such as layers (Wang et al., 2017), filters (Zeng and Urtasun, 2019) or channels (He et al., 2017) are removed. On the other hand, in unstructured pruning (Lee et al., 2018;Kwon et al., 2020), individual weights are set to zero (Han et al., 2015;Hayou et al., 2021). Unstructured pruning methods suffer significant drawbacks, such as particular frameworks and chip architectures are required, as not all algorithms and hardware architectures can exploit weight sparsity to improve performance. Therefore, in this work we consider only structured pruning methods for our experiments.",
            "score": 0.5909344124999674,
            "section_title": "Neural Network Pruning",
            "char_start_offset": 4658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1224
                }
            ],
            "ref_mentions": [
                {
                    "start": 723,
                    "end": 740,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 815,
                    "end": 833,
                    "matchedPaperCorpusId": "212415101"
                },
                {
                    "start": 870,
                    "end": 888,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 888,
                    "end": 907,
                    "matchedPaperCorpusId": "235078790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28369140625
        },
        {
            "corpus_id": "264904333",
            "title": "The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models",
            "text": "For encoder-only models (Fig. 9), L1unstructured pruning has a smaller impact compared to L1-structured pruning. We hypothesize that the final layer of the encoder-only models might encode knowledge in a structured or modular manner, and any form of structured compression would disrupt this encoding, resulting in a larger performance drop. Such a result would be consistent with existing approaches that enable editing knowledge in language models and rely on structure (Mitchell et al., 2021). \n\nFinding: For encoder-only models, L1unstructured leads to a smaller decrease in performance than L1-structured. For decoder-only (Fig. 10) and encoder-decoder (Fig. 14) models, even at a sparsity level of 20%, the predicted accuracy is very close to the majority baseline. This finding aligns with the claims made in Mitchell et al. (2022) that final layers encode significant amount of information. The drastic performance drop observed suggests that the final layers play a crucial role in encoding knowledge. Additional results for pruning the final layer are shown in Fig. 26, 27, 28.",
            "score": 0.5904336054137632,
            "section_title": "Final Dense layer Pruning",
            "char_start_offset": 19058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1087
                }
            ],
            "ref_mentions": [
                {
                    "start": 816,
                    "end": 838,
                    "matchedPaperCorpusId": "249642147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4287109375
        },
        {
            "corpus_id": "265220879",
            "title": "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization",
            "text": "\u2022 2:4 semi-structured sparsity: Two weights in every contiguous block of four must be zero, providing a total of 50% sparsity. This sparsity pattern is required to enable hardware acceleration on GPUs (Mishra et al., 2021). \n\n\u2022 50% unstructured sparsity: To enable comparison, we use a sparsity level of 50% for unstructured pruning, unless otherwise stated. \n\nWe do not explore pruning above 50% sparsity as language modeling performance collapses shortly beyond this threshold (Frantar and Alistarh, 2023;Sun et al., 2024). Maintaining language modeling performance is essential for the generation of highquality summaries, enabling comparison between the models and their pruned counterparts.",
            "score": 0.5895643655696077,
            "section_title": "Pruning Methods",
            "char_start_offset": 11200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 695
                }
            ],
            "ref_mentions": [
                {
                    "start": 507,
                    "end": 524,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29931640625
        },
        {
            "corpus_id": "276557680",
            "title": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing",
            "text": "Large Language Models (LLMs) (Vaswani et al., 2017;Zhang et al., 2022;Touvron et al., 2023;Diao et al., 2024) have recently achieved significant success, leading to the development of numerous applications (OpenAI, 2023;Anand et al., 2023). However, the inference for these models, often containing billions of parameters, presents challenges. These challenges primarily arise from the substantial computational demands and the risk of high latency (Ma et al., 2023). \n\nStructured pruning is a promising hardware-friendly approach to reduce computational consumption and accelerate inference (Yuan et al., 2021). It removes complete structures from models, such as weight channels and attention heads. Compared with other methods like unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2023), parameter sharing (Diao et al., 2019), offloading (Rasley et al., 2020;Diao et al., 2022;2024), and quantization (Dettmers et al., 2022;Lin et al., 2023;Frantar et al., 2022), structured pruning reduces computational resources and speeds up inference without requiring specific hardware. However, when applied to LLMs, structured pruning often results in a performance gap compared to dense models (Wang et al., 2024b). \n\nA major factor contributing to the performance gap in LLMs may be the emergence of significant outlier phenomena in internal representations (Dettmers et al., 2022;Liu et al., 2024;Sun et al., 2024). Current advanced structured pruning methods typically utilize calibration datasets to assess the importance of weights using pruning metrics. For example, the FLAP method (An et al., 2024) uses a calibration dataset to compute fluctuation metrics for each input feature and its corresponding channel in attention or MLP block weight matrices, specifically in the output projection (O) or fully connected layer 2 (FC2).",
            "score": 0.5886763004589037,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 466,
                    "matchedPaperCorpusId": "256390345"
                },
                {
                    "start": 592,
                    "end": 611,
                    "matchedPaperCorpusId": "239998338"
                },
                {
                    "start": 819,
                    "end": 838,
                    "matchedPaperCorpusId": "201125864"
                },
                {
                    "start": 851,
                    "end": 872,
                    "matchedPaperCorpusId": "221191193"
                },
                {
                    "start": 872,
                    "end": 890,
                    "matchedPaperCorpusId": "252815953"
                },
                {
                    "start": 914,
                    "end": 937,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 1364,
                    "end": 1387,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "267103825",
            "title": "A Review of Neural Network Lightweighting Techniques",
            "text": "By analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67]. \n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation.",
            "score": 0.5886747204199544,
            "section_title": "Analysis and discussion",
            "char_start_offset": 17510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2410
                },
                {
                    "start": 2411,
                    "end": 2503
                }
            ],
            "ref_mentions": [
                {
                    "start": 1534,
                    "end": 1538,
                    "matchedPaperCorpusId": "202558809"
                },
                {
                    "start": 1538,
                    "end": 1542,
                    "matchedPaperCorpusId": "15663507"
                },
                {
                    "start": 1546,
                    "end": 1550,
                    "matchedPaperCorpusId": "5423629"
                },
                {
                    "start": 1550,
                    "end": 1554,
                    "matchedPaperCorpusId": "256390345"
                },
                {
                    "start": 1554,
                    "end": 1558,
                    "matchedPaperCorpusId": "249205107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "272770793",
            "title": "CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information",
            "text": "Although scaling up Large Language Models (LLMs) brings remarkable performance (Brown et al., 2020;OpenAI, 2023;Gemini Team et al., 2023;Meta, 2024;DeepSeek-AI et al., 2024;Yang et al., 2024a), increasing parameters brings more computations and memory consumption, posing a significant challenge of deploying in practical applications. To address this, various model compression methods for LLMs are proposed (Dettmers et al., 2022;Frantar et al., 2022;Lin et al., 2023;Muralidharan et al., 2024). Existing LLM pruning work (Frantar and Alistarh, 2023;Sun et al., 2023;Xu et al., 2024a;Zhang et al., 2024b) focuses mainly on unstructured or semi-structured sparsity. However, these paradigms require specific hardware to achieve practical acceleration. \n\nIn contrast, structured pruning, which imposes structured sparsity by removing groups of consecutive parameters (Louizos et al., 2017;Wang et al., 2020;Xia et al., 2022), is more hardware-friendly on general devices. However, there are some challenges involved in existing structured pruning methods for LLMs: (1) They typically introduce learnable masks to search (Xia et al., 2023;Dery et al., 2024) or utilize gradients to guide pruning (Ma et al., 2023;Zhang et al., 2023a). Unfortunately, they require significant computational overhead, especially for large-scale (e.g., 70B) models. (2) It is also worth noting that they usually assign a uniform sparsity budget per block, which is suboptimal since LLM blocks have different significance in the representation functionality (Gromov et al., 2024a). Moreover, they usually involve a recovery fine-tuning with Low-Rank Adapter (LoRA) (Hu et al., 2022) to enhance pruned models, which also introduce training overhead and overlook the varying importance of blocks.",
            "score": 0.5878192833423119,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1772
                }
            ],
            "ref_mentions": [
                {
                    "start": 889,
                    "end": 907,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 907,
                    "end": 924,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1195,
                    "end": 1212,
                    "matchedPaperCorpusId": "268253513"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "269149156",
            "title": "Structured Model Pruning for Efficient Inference in Computational Pathology",
            "text": "Model Compression.Numerous strategies have been explored in the literature for model compression and four major categories include pruning, knowledge distillation (KD), quantization and neural architecture search (NAS).Pruning refers to removing relatively 'less important' model components based on certain heuristics to measure importance (Blalock et al., 2020).Quantization refers to adopting less precise byte representation for model parameters and thus requiring less memory footprint, like reducing weight/gradient data type from float32 to int8.KD (Gou et al., 2021) involves a larger model (teacher) and a smaller model (student).The teacher model is trained first and later used to guide the training of the smaller student model via distillation loss.NAS (Ren et al., 2021;Baymurzina et al., 2022) aims at automatically finding an efficient model architecture by searching through a design space.Many of these techniques are complementary and can be combined, among which is pruning.Here we focus on model pruning (Blalock et al., 2020), which are further categorised into unstructured, semi-structured (See notes in Appendix B) and structured pruning, based on the sparsity patterns that a model pruning strategy targets.\n\nUnstructured Pruning.Determine the importance of each weight parameter and prune the relatively unimportant ones by replacing them with zero values.Unstructured pruning is straightforward to implement and usually does not harm model performance.However, current hardware is not optimized for unstructured sparse matrices and thus inference speed cannot be achieved without customized software/hardware systems.Therefore, we do not focus on such pruning methods.\n\nStructured Pruning.Structured pruning follows a fixed pattern.For example, an entire filter of a convolution layer or an entire layer can be removed.Contrary to unstructured pruning, since the entire layer (Ding et al., 2021;Fu et al., 2022a) or filter (Li et al., 2016;Liu et al., 2017) is removed, model speedup is achieved.Various pruning heuristics have been proposed.",
            "score": 0.586914685315544,
            "section_title": "Background",
            "char_start_offset": 4671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 18,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1233
                },
                {
                    "start": 1235,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1383
                },
                {
                    "start": 1383,
                    "end": 1480
                },
                {
                    "start": 1480,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1696
                },
                {
                    "start": 1698,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1760
                },
                {
                    "start": 1760,
                    "end": 1847
                },
                {
                    "start": 1847,
                    "end": 2024
                },
                {
                    "start": 2024,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 556,
                    "end": 574,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 784,
                    "end": 808,
                    "matchedPaperCorpusId": "245175474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4140625
        },
        {
            "corpus_id": "274981759",
            "title": "Lillama: Large Language Models Compression via Low-Rank Feature Distillation",
            "text": "We discuss next previous compression methods, highlight their limitations, and explain how our approach addresses them. Table 1 positions our approach to recent compression methods. \n\nPruning methods remove unimportant weights in the pre-trained model (LeCun et al., 1989;Han et al., 2015). Structured Pruning removes entire groups of parameters, which results in a smaller and faster model (Xia et al., 2024;Ma et al., 2023). Ma et al. (2023) propose a new gradient-based criterion to eliminate substructures in LLMs, while Xia et al. (2024) use a joint loss combining a pruning mask loss with the language modeling loss. However, optimizing these criteria can be computationally intensive. For example, the pruning step of Sheared-LLaMA (Xia et al., 2023) is 5x expensive compared to standard LM training, according to the authors. In contrast, thanks to the local gradient updates, our approach is computationally efficient, allowing us to compress a 47B model within minutes on a single A100 GPU. Regarding unstructured pruning, these methods do not provide any gains in terms of memory or speedup, at least with current algorithmic implementations. Semi-structured pruning (e.g., 2:4 and 4:8) (Sun et al., 2024;Frantar and Alistarh, 2023;Liu et al., 2024) does not lead to memory gain but can speed up processing on kernels optimized for such matrix structures. On the other hand, our method, which directly shrinks matrices, saves memory across all hardware and leads to speed up, as fewer computations are performed. \n\nLow-Rank Decomposition compresses a model by approximating its pre-trained matrices with lowerdimension ones. Based on the observation by Li et al. (2018) that Neural Networks have lower intrinsic dimensions, Aghajanyan et al. (2020) show that Transformer language models also require lower intrinsic dimensions depending on the language task.",
            "score": 0.5857756351247755,
            "section_title": "Related Works",
            "char_start_offset": 2705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1869
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 272,
                    "matchedPaperCorpusId": "7785881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "268253513",
            "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect",
            "text": "To reduce the inference cost of large language models and increase their practical applications, there have been many recent works on compressing models, which can be classified into two categories: model pruning and quantization. Besides, there are some works aim to study the redundancy of model which is essential for compressing models. \n\nModel pruning: model pruning (LeCun et al., 1989;Han et al., 2015) is a classic and effective method of reducing model redundancy modules to compress models. The model pruning methods mainly include unstructured pruning and structured pruning. The unstructured pruning simplifies an LLM by removing specific parameters without considering its internal structure, such as SparseGPT (Frantar & Alistarh, 2023) and LoRAPrune (Zhang et al., 2023). However, this method disregards the overall LLM structure, resulting in an irregular sparse model composition. Another more practical approach is structured pruning, GUM (Syed et al., 2023) makes an analysis of several structured pruning methods for decoder-only LLMs. LLM-Pruner (Ma et al., 2024) selectively removes non-critical structures according to gradient information. ShearedLLaMA (Xia et al., 2023) employs targeted structured pruning and dynamic batch loading. LaCo (Yang et al., 2024) used layer merging to compress the model. Compared to the previous method, our method is a simple and efficient structured pruning method. \n\nQuantization: quantization (Liu et al., 2021;Gholami et al., 2022;Dettmers et al., 2022;2024) is a widely accepted technique in the field of model compression, which can significantly save the storage and computational costs of deep learning models. Traditional models are generally stored as floating-point numbers, but quantization converts them into integers or other discrete forms. LUT-GEMM (Park et al., 2022) quantifies only weights and optimizes matrix multiplication in LLM using BCQ format.",
            "score": 0.585278114531763,
            "section_title": "Related works",
            "char_start_offset": 17732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 392,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 392,
                    "end": 409,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 765,
                    "end": 785,
                    "matchedPaperCorpusId": "174798153"
                },
                {
                    "start": 957,
                    "end": 975,
                    "matchedPaperCorpusId": "259950394"
                },
                {
                    "start": 1067,
                    "end": 1084,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1452,
                    "end": 1470,
                    "matchedPaperCorpusId": "235658553"
                },
                {
                    "start": 1470,
                    "end": 1491,
                    "matchedPaperCorpusId": "232352683"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        },
        {
            "corpus_id": "269899537",
            "title": "Adaptive Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization",
            "text": "In recent years, the emergence and application of large language models (LLMs) have served as a powerful stimulant for natural language processing and artificial intelligence (OpenAI, 2022(OpenAI, , 2023;;Bubeck et al., 2023;Yang et al., 2023). Adhering to the scaling law (Kaplan et al., 2020;Hoffmann et al., 2022), researchers are continually seeking LLMs with more parameters and training data, aiming to achieve general models closer to human capabilities. However, larger language models imply a larger overhead of computing resources. Therefore, when deploying LLMs, it is necessary to strike a balance between efficiency and performance (Wan et al., 2024). To achieve efficient LLMs, many compression techniques for LLMs are proposed, such as pruning (Frantar and Alistarh, 2023a;Sun et al., 2024;Ma et al., 2023), quantization (Frantar et al., 2023;Lin et al., 2023;Liu et al., 2023) and knowledge distillation (Gu et al., 2024). \n\nAmong these methods, unstructured pruning and quantization can reduce the number of parameters or memory requirements by half or even more without significant performance degradation, but they require specialized GPU kernels to fully realize their acceleration potential. In contrast, structured pruning can produce lightweight models that do not rely on specialized hardware. Despite extensive research, the performance of structured pruning still lags significantly behind that of the original model. Low-rank compression (LRC) (Ben Noach and Goldberg, 2020;Li et al., 2023) is another promising compression technique. It decomposes the weight matrix into the product of two dense low-rank matrices, discarding unimportant parameter information during the decomposition process. However, LRC remains under-explored in LLMs. \n\nThe keys to LRC are low-rank decomposition methods and low-rank dimension allocation. Existing decomposition methods can generally be categorized into two types: weight-based and featurebased decomposition.",
            "score": 0.585176870647931,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 759,
                    "end": 788,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 788,
                    "end": 805,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 805,
                    "end": 821,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 836,
                    "end": 858,
                    "matchedPaperCorpusId": "259298689"
                },
                {
                    "start": 1471,
                    "end": 1501,
                    "matchedPaperCorpusId": "227905681"
                },
                {
                    "start": 1501,
                    "end": 1517,
                    "matchedPaperCorpusId": "259203385"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6318359375
        },
        {
            "corpus_id": "258015063",
            "title": "Benchmarking Neural Network Compression Techniques for Ocular-Based User Authentication on Smartphones",
            "text": "This experiment aims to compare the performance of the models (ResNet-50, VGG-13, and VGG-11) pruned using structured and unstructured pruning techniques based on L2-norm-based Channel Pruning (Table 4) and Layerwise Gradient Magnitude (Table 5) based pruning techniques, respectively. \n\nAs can be seen, the performance of the ResNet-50 model using unstructured pruning outperformed the structured pruning counterpart. It was evident as the mean EERs of 9.65% and 17.17% were obtained for the unstructured and structured pruning techniques across all CR. Overall, the AUC of ResNet-50 using the unstructured pruning dropped by about 0.08. The difference in EER of 1.18% and 0.16% was obtained for ResNet-50 using unstructured and structured pruning techniques, respectively, at a compression rate of 10% and 50%. The reason could be structured pruning on the residual connections prunes away filters inside the residual connections resulting in the number of output channels left unchanged [61]. \n\nAcross both VGG models, the unstructured and structured pruning methods obtained an identical mean performance across GMR and AUC, and EER was lower for the unstructured pruning technique. VGG-8 and VGG-13 models obtained a difference of 1.65% in EER between the structured and unstructured pruning. Specifically, the difference in EER on VGG-11 between the 10% and 50% CR points ranged from [9.11-9.03]% and [8.60-10.08]% for the unstructured and structured pruning techniques, respectively. VGG-13 obtained a minimal performance drop between the two pruning methods. To investigate the impact of backbone architectures on the performance of structured pruning, we applied the L2-norm-based channel pruning on different variations of VGGs and ResNet (namely, ResNet-50, VGG-13, and VGG-11) as illustrated in Table 4.",
            "score": 0.584016745503194,
            "section_title": "C. STRUCTURED VS. UNSTRUCTURED PRUNING",
            "char_start_offset": 35920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1815
                }
            ],
            "ref_mentions": [
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "208158386"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226806640625
        },
        {
            "corpus_id": "277452419",
            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
            "text": "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks [27], while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]. However, unstructured pruning often results in irregular sparse patterns in the weight matrix, necessitating specialized hardware accelerators (e.g., sparse matrix multiplication units) to efficiently handle sparse matrix computations and fully exploit the benefits of sparsity in terms of storage and computation. Among various unstructured pruning methods, Magnitude Pruning is the most basic, directly removing weights with small magnitudes. While simple to implement, it does not account for the contextual importance of weights. SparseGPT [29], on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda [27] simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA [30] introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM [31] builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers.",
            "score": 0.5835271244969228,
            "section_title": "Unstructured Pruning",
            "char_start_offset": 21938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 442,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1813,
                    "end": 1817,
                    "matchedPaperCorpusId": "271745835"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "266818263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "271720097",
            "title": "Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations",
            "text": "It is essential to consider the constraints of using pruned models for inference. To achieve a speed-up, the hardware in the production environment has to support sparse models. LoRA-Prune [51] emphasizes that specific unique hardware configurations are needed for unstructured pruning to utilize the improvement. SpraseGPT [50] mention specific hardware support for structured pruning for their performance evaluation. Therefore, identifying if the targeted hardware supports models structured or unstructured pruned models is important. \n\nIn Conclusion. \n\n\u2022 Pruning involves identifying and removing redundant weights while maintaining the properties of the uncompressed baseline.",
            "score": 0.581858114915078,
            "section_title": "B. Pruning",
            "char_start_offset": 21658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 682
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "274776787",
            "title": "QPruner: Probabilistic Decision Quantization for Structured Pruning in Large Language Models",
            "text": "The advent of large language models (LLMs) has revolutionized various natural language processing (NLP) tasks, such as machine translation (Zhang et al., 2023a;Sato et al., 2020), sentiment analysis (Zhang et al., 2023b;Deng et al., 2023), and speech recognition (Min and Wang, 2023). Despite their impressive capabilities, the resource consumption required to obtain a fine-tuned model suitable for specific tasks remains substantial due to the large number of parameters and high computational demands of LLMs (Frantar and Alistarh, 2023). To address these issues, various compression techniques, including pruning (Molchanov et al., 2019;Liu et al., 2018), quantization (Shao et al., 2023;Lee et al., 2023), and distillation (Gu et al., 2023;Tan et al., 2023), have been proposed. \n\nStructured pruning (Ma et al., 2023;Xia et al., 2023) is a widely used approach that reduces model size by removing less important parameters in a structured manner, preserving the overall architecture compatibility with hardware requirements. However, the disruption of computational graph uniformity and the removal of parameters can significantly reduce the accuracy of LLMs, which are inherently information-dense networks. To mitigate this degradation, fine-tuning is often used to recover the accuracy of pruned models. This finetuning step, while effective, is memory-intensive and presents substantial challenges in terms of resource consumption. \n\nTo further reduce memory usage during the finetuning and inference phases, we introduce quantization into the structured pruning framework. Specifically, after performing structured pruning, we quantize the pruned model and then apply different fine-tuning strategies. Quantization effectively reduces the bit-width of model parameters, thereby lowering the resource consumption during both fine-tuning and inference. However, integrating quantization with structured pruning introduces additional complexities. Structured pruning applies different pruning intensities across model layers, which exacerbates the uneven distribution of layer importance, making some layers more critical for maintaining model performance.",
            "score": 0.5812626528133387,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 160,
                    "matchedPaperCorpusId": "255942578"
                },
                {
                    "start": 160,
                    "end": 178,
                    "matchedPaperCorpusId": "226283659"
                },
                {
                    "start": 199,
                    "end": 220,
                    "matchedPaperCorpusId": "263829356"
                },
                {
                    "start": 220,
                    "end": 238,
                    "matchedPaperCorpusId": "254974468"
                },
                {
                    "start": 263,
                    "end": 283,
                    "matchedPaperCorpusId": "259847517"
                },
                {
                    "start": 512,
                    "end": 540,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 617,
                    "end": 641,
                    "matchedPaperCorpusId": "195657904"
                },
                {
                    "start": 673,
                    "end": 692,
                    "matchedPaperCorpusId": "261214575"
                },
                {
                    "start": 692,
                    "end": 709,
                    "matchedPaperCorpusId": "265066991"
                },
                {
                    "start": 745,
                    "end": 762,
                    "matchedPaperCorpusId": "259137871"
                },
                {
                    "start": 822,
                    "end": 839,
                    "matchedPaperCorpusId": "263830786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6298828125
        },
        {
            "corpus_id": "249395329",
            "title": "Differentially Private Model Compression",
            "text": "Pruning algorithms are a broad class of model compression techniques where one drops the parameters from a model during or after the training process. Many works have shown that eliminating unnecessary parameters of neural networks via pruning can lead to sparser and compressed models that have shorter inference times without loss in performance [30,25,24]. For example, in magnitude pruning, one of the most widely used pruning techniques, we prune a fraction of parameters with the lowest magnitude. However, there are several pruning strategies, and we refer the readers to [33] for more details and references. \n\nPruning can be implemented in both structured and unstructured ways. In structured pruning, all the pruned weights belong to a single building block of the model. For example, a 6-layer 1 2 -BERT can be obtained by pruning 6 layers from the full BERT model, which consists of 12 transformer blocks. On the other hand, in unstructured pruning, pruned weights may be spread across all the layers of the network. In unstructured pruning, it is possible to obtain a 50% sparse student model while still having all the 12 layers of BERT. Depending on the hardware architectures, inference latency between models with structured and unstructured sparsity could be quite different. However, in this section, we use sparsity as the main measure of model compression, which is also well accepted in the community [33,27].",
            "score": 0.5807195803113567,
            "section_title": "Model Compression via Pruning",
            "char_start_offset": 17428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1431
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1427,
                    "end": 1430,
                    "matchedPaperCorpusId": "231740691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4345703125
        },
        {
            "corpus_id": "268692211",
            "title": "The Unreasonable Ineffectiveness of the Deeper Layers",
            "text": "Pruning is a method for reducing the size of a trained machine-learning model by removing unnecessary parameters, either individually or together as a group. Pruning for neural networks has a long history (LeCun et al., 1989;Hassibi and Stork, 1992), and, as originally conceived, unstructured pruning techniques sparsify networks by removing individual parameters based on pre-defined criteria. For instance, if a parameter of the model has a very small value, then removing it -i.e. by setting it to exactly zero -will likely have minimal impact on performance. Inspired by this early work, modern researchers began exploring different criteria for such unstructured pruning, focusing mostly on computer vision models (Han et al., 2015;Chen et al., 2015;Srinivas and Babu, 2015). In particular, Han et al. (2015) developed an iterative pruning method for alternatively pruning and finetuning a network in order to reach better compression ratios and performance. \n\nWhile these models were smaller, they were not necessarily more efficient: sparsifying networks by removing individual parameters according to a criterion leads to irregular or pseudorandom sparsification patterns that are difficult to accelerate without specialized hardware or libraries designed for sparsity (Li et al., 2016). To that end, structured pruning techniques were developed to remove irrelevant groups of parameters together, such as particular channels or filters in convolutional networks. As this increased their practical relevance, researchers then began exploring structured pruning across computer vision (Li et al., 2016;Wen et al., 2016;Hu et al., 2016;He et al., 2017;Huang et al., 2018) and pre-transformer NLP architectures (Murray and Chiang, 2015;See et al., 2016;Kim and Rush, 2016). \n\nFollowing unprecedented progress in language modeling, recent work has focused on applying structured pruning methods to the Transformer (Vaswani et al., 2017).",
            "score": 0.5793009912781337,
            "section_title": "A.1 PRUNING",
            "char_start_offset": 29852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 225,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 225,
                    "end": 249,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 720,
                    "end": 738,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 738,
                    "end": 756,
                    "matchedPaperCorpusId": "543597"
                },
                {
                    "start": 797,
                    "end": 814,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1610,
                    "end": 1627,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 1627,
                    "end": 1643,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 1659,
                    "end": 1678,
                    "matchedPaperCorpusId": "31409561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53857421875
        },
        {
            "corpus_id": "276576138",
            "title": "Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability",
            "text": "Methods like SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023) use sophisticated weight updates and pruning without retraining, respectively, while PST (Li et al., 2022) combines unstructured pruning with efficient fine-tuning. Structured pruning methods (Chen et al., 2021b(Chen et al., , 2023) ) remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency. Techniques such as LLM-Pruner (Ma et al., 2023) and LoRAPrune (Zhang et al., 2023) focus on efficient deployment and inference acceleration, with Sheared-LLaMA (Xia et al., 2023) aiming to prune models to a target architecture and train them dynamically. Furthermore, the compression of language models has garnered significant attention, leading to various methods like network pruning, knowledge distillation, and quantization (Bai et al., 2020;Brown et al., 2020;Devlin, 2018). Pruning, especially structural pruning, remains a crucial focus due to its hardware-friendly nature, with methods varying from l1-dependent pruning (Zafrir et al., 2021) to more advanced techniques like the optimal brain surgeon (LeCun et al., 1989). Efficient compression and low-resource strategies are increasingly essential, with advancements in layer-wise optimal brain surgeon and data-free pruning approaches aiming to optimize the balance between compression efficiency and training data availability (Kurti\u0107 et al., 2024;Srinivas and Babu, 2015).",
            "score": 0.5790977005991614,
            "section_title": "Related Work",
            "char_start_offset": 5555,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1457
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 51,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 273,
                    "end": 292,
                    "matchedPaperCorpusId": "235899080"
                },
                {
                    "start": 451,
                    "end": 468,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 868,
                    "end": 887,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1131,
                    "end": 1151,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1411,
                    "end": 1432,
                    "matchedPaperCorpusId": "256662263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "277272083",
            "title": "Energy-Aware LLMs: A step towards sustainable AI for downstream applications",
            "text": "Overall, structured pruning applied to these models consistently delivered energy efficiency improvements across all configurations. However, the 32-bit model showed more noticeable performance degradation on both 2:4 and 4:8 sparsity patterns compared to unstructured pruning and the baseline 32-bit model. In contrast, the 16-bit model with structured pruning significantly enhanced performance across metrics such as BLEU, ROUGE, METEOR, cosine similarity, and the number of tokens generated per second when compared to the unpruned 16-bit model and the unstructuredpruned models. Impressed is the 2:4 sparsity pattern on the 16-bit model which increases both model performance and energy efficiency in comparison to the baseline model and 16-bit model without pruning. This suggests that applying a combination of 16-bit quantization and 2:4 structured pruning achieves the best compromise between model performance and energy efficiency, making it an ideal approach for toward sustainable AI system on our hardware configuration.",
            "score": 0.5788175721110584,
            "section_title": "IV. RESULTS AND FINDINGS",
            "char_start_offset": 22340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 1034
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305419921875
        },
        {
            "corpus_id": "260232381",
            "title": "Recursive least squares method for training and pruning convolutional neural networks",
            "text": "In theory, unstructured pruning methods can prune arbitrary redundant nodes in convolutional layers and achieve high compression ratios. However, unstructured pruning methods are difficult to implement since they destroy the form of weight matrices. \n\nTo address this issue, almost all existing unstructured pruning algorithms, such as Optimal Brain Damage [23], Soft Channel Pruning [24] and 0 Minimization [25], zero out the unimportant weights instead of really pruning the redundant nodes in simulation experiments. In contrast, structured pruning methods aim to remove unimportant channels. These methods preserve the structure of weight matrices and are thus more practical and popular, although their pruning granularity is coarse. \n\nStructured pruning methods usually include three stages: training, pruning and fine-tuning (also called retraining in some papers) [26,27]. According to the number of pruning operations, these methods can be divided into one-shot structured pruning and iterative structured pruning [28]. The former performs pruning and fine-tuning only once and thus requires fewer epochs to obtain the compressed model. However, its compression ratio and accuracy rely heavily on the given pruning ratio. In other words, it is often difficult to obtain the optimal compressed model with one-shot pruning. In contrast, the latter performs multiple pruning and finetuning operations, which may lead to better results; however, multiple operations are very time-consuming, especially for large-scale neural networks. There is still much debate about what kind of structured pruning approach is best for different scenarios. \n\nIn recent years, researchers have proposed many structured pruning algorithms. For example, Li et al. proposed a one-shot pruning algorithm called the 1 -norm [29], which evaluates and prunes unimportant output channels by using 1 regularization for the weights of the convolutional layers. Liu et al. proposed another one-shot pruning algorithm called Network Slimming [30], which prunes channels by using 1 regularization for the scaling factors in the batch normalization layers.",
            "score": 0.5778974199266257,
            "section_title": "Introduction",
            "char_start_offset": 1982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 249
                },
                {
                    "start": 252,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 384,
                    "end": 388,
                    "matchedPaperCorpusId": "201805442"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "230566586"
                },
                {
                    "start": 876,
                    "end": 879,
                    "matchedPaperCorpusId": "254490473"
                },
                {
                    "start": 1023,
                    "end": 1027,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "14089312"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.495361328125
        },
        {
            "corpus_id": "272524767",
            "title": "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation",
            "text": "The assumption that higher-resource languages have smaller intrinsic subspaces naturally leads to the following question: How small can these subspaces be? Therefore, we adopt unstructured pruning 1 to explore the minimal intrinsic languagespecific subspaces exhaustively. Compared with structured pruning which directly removes entire rows or columns from a weight matrix, we choose unstructured pruning without the above limitation to achieve a higher pruning ratio. To ensure the stability of the model during training under a high pruning ratio, we introduce a Gradual Pruning Schedule (Zhu and Gupta, 2017;He et al., 2023) during training. Specifically, the pruning ratio for each epoch P e is gradually increased from 0 to the predefined target pruning ratio P . The entire training process is divided into three stages as denoted in equation 4. Given a predefined pruning ratio P and the total training process has T epochs. E is the starting epoch for pruning, and the pruning process will last for k epochs. \n\n4) During the first E epochs (e \u2264 E), no pruning is applied denoted by P = 0; for stage 2, the pruning ratio of the current e epoch is gradually increased until reaching the target ratio P for the next k epochs; for stage 3, the pruning ratio P is kept to the end. For the following experiments, we empirically choose the start epoch E = 2 and the duration k = 8. We provide a detailed description of the target pruning ratio P used in each experiment, as well as the impact of different choices of E and k on performance in Appendix B.",
            "score": 0.5776778242503615,
            "section_title": "Unstructured Pruning",
            "char_start_offset": 6935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1555
                }
            ],
            "ref_mentions": [
                {
                    "start": 611,
                    "end": 627,
                    "matchedPaperCorpusId": "266163850"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2379150390625
        },
        {
            "corpus_id": "277275922",
            "title": "Efficient self-attention with smart pruning for sustainable large language models",
            "text": "The compression of language models has gained significant attention, leading to the development of various methods, including network pruning 12 , knowledge distillation, and quantization 13 . LLM-Pruner 14 and FLAP 15 aim to reduce network width by pruning coupled structures. Sheared-LLaMA 16 takes a more comprehensive approach, reducing network width and depth by removing entire layers. While methods that address both width and depth aspects exist 17,18 , there is still a need for in-depth analysis comparing the impact of these factors on LLM inference efficiency. Applying traditional pruning techniques to LLMs presents unique challenges due to their vast number of parameters and substantial computational requirements 19 . Pruning methods for LLMs can be broadly categorized into unstructured and structured approaches. \n\nStructured pruning methods 20 focus on removing entire groups of parameters, maintaining dense weight matrices, and improving hardware efficiency. Techniques such as LLM-Pruner 14 and LoRAPrune 11 emphasize efficient deployment and inference acceleration. Sheared-LLaMA 16 aims to prune models to a target architecture and train them dynamically. The work of Tao introduces a novel compression technique called QuantGPT. This method focused on quantization and module adaptation, allowing for systematic compression while maintaining the integrity of the model's architecture and performance 25 . LightPAFF focuses on transferring knowledge from a more prominent (teacher) model to a smaller (student) model. This process inherently involves structuring the model to retain important features learned by the teacher rather than simply removing individual weights, as seen in unstructured pruning 21 . Unstructured pruning methods 22 target individual weights, maintaining performance. Notable examples include SparseGPT 23 , which employs sophisticated weight updates and pruning without retraining. Edalati et al. developed KnGPT2 for compressing the linear mappings of the GPT-2 model, focusing on reducing the number of parameters flexibly without drastically altering the overall architecture. This technique allows for representing weight matrices in a more compact form while maintaining performance, which aligns with the characteristics of unstructured pruning 24 . \n\nStructure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility 25 .",
            "score": 0.577265962792162,
            "section_title": "Related work",
            "char_start_offset": 4650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2307
                },
                {
                    "start": 2310,
                    "end": 2460
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 218,
                    "matchedPaperCorpusId": "266362404"
                },
                {
                    "start": 457,
                    "end": 459,
                    "matchedPaperCorpusId": "256662263"
                },
                {
                    "start": 730,
                    "end": 732,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 861,
                    "end": 863,
                    "matchedPaperCorpusId": "235899080"
                },
                {
                    "start": 1426,
                    "end": 1428,
                    "matchedPaperCorpusId": "204009154"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.802734375
        },
        {
            "corpus_id": "247794014",
            "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models",
            "text": "Large pre-trained language models (PLMs) (Devlin et al., 2019;Liu et al., 2019) have achieved great success in a variety of NLP tasks. However, it is difficult to deploy them for real-world applications where computation and memory resources are limited. Reducing the pre-trained model size and speeding up the inference have become a critical issue.\n\nPruning is a common technique for model compression. It identifies and removes redundant or less important neurons from the networks. From the view of the model structure, pruning methods can be categorized into unstructured pruning and structured pruning. In the unstructured pruning, each model parameter is individually removed if it reaches some criteria based on the magnitude or importance score (Han et al., 2015;Zhu and Gupta, 2018;. The unstructured pruning results in sparse matrices and allows for significant model compression, but the inference speed can hardly be improved without specialized devices. While in the structured pruning, rows or columns of the parameters are removed from the weight matrices (McCarley, 2019;Michel et al., 2019;Voita et al., 2019;Lagunas et al., 2021;Hou et al., 2020). Thus, the resulting model speeds up on the common CPU and GPU devices.\n\nPruning methods can also be classified into optimization-free methods (Michel et al., 2019) and the ones that involve optimization (Frankle and Carbin, 2019;Lagunas et al., 2021). The latter usually achieves higher performance, but the former runs faster and is more convenient to use.\n\nPruning PLMs has been of growing interest. Most of the works focus on reducing transformer size while ignoring the vocabulary (Abdaoui et al., 2020). Pruning vocabulary can greatly reduce the model size for multilingual PLMs.\n\nIn this paper, we present TextPruner, a model pruning toolkit for PLMs. It combines both transformer pruning and vocabulary pruning. The purpose of TextPruner is to offer a universal, fast, and easy-to-use tool for model compression. We expect it can be accessible to users with little model training experience. Therefore, we implement the structured optimization-free",
            "score": 0.5756872739559102,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 62,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 772,
                    "end": 792,
                    "matchedPaperCorpusId": "27494814"
                },
                {
                    "start": 1088,
                    "end": 1108,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1108,
                    "end": 1127,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 1127,
                    "end": 1148,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 1148,
                    "end": 1165,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 1309,
                    "end": 1330,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1370,
                    "end": 1396,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 1396,
                    "end": 1417,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 1652,
                    "end": 1674,
                    "matchedPaperCorpusId": "222291680"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "276902790",
            "title": "Sample-aware Adaptive Structured Pruning for Large Language Models",
            "text": "For example, LLM-Pruner (Ma, Fang, and Wang 2023) is the first attempt at structured pruning of LLMs, reducing model computation and memory usage while keeping the overall structure of the LLMs. Specifically, it utilizes a dependency detection algorithm to identify coupled structures in the LLMs, scores the importance of the coupled structures, and selectively removes non-critical structures according to first-order and approximate Hessianbased information. Finally, LLM-Pruner uses Low-Rank-Adaptor (LoRA) for fine-tuning to recover the pruned model weights. Further, LoRAShear (Chen et al. 2023) utilizes the Lora Half-Space Projected Gradient (LH-SPG) technique for incremental structured pruning and knowledge recovery. LoRAShear can be applied to various LLMs through dependency graph analysis and sparsity optimization. Recently, Sheared-LLaMA (Xia et al. 2024) is a directed structured pruning that prunes a large model to a specified target structure. It then trains the pruned model using dynamically loaded data based on the loss reduction ratio in each domain, thus improving data usage efficiency and model performance. However, Sheared-LLaMA uses substantial computational resources for subsequent pre-training to performance recovery. Like LLM-Pruner, the number of layers is constant. Shortened LLaMA (Kim et al. 2024) proposes that a simple deep pruning method can achieve promising performance and improve inference speed in a zero-shot task. \u2022 Unstructured pruning. It performs pruning operations on weights or neurons instead of structured blocks. Although unstructured pruning typically results in higher compression ratios, the need for specific hardware support leads to the failure to achieve proper inference acceleration or storage reduction. SparseGPT (Frantar and Alistarh 2023) is the first approach to perform unstructured pruning on LLMs. It applies the Hessian inverse to pruning and uses a complex weight-updating process that includes synchronized second-order Hessian updates. The  strategy is computationally expensive to execute iteratively between weight pruning and weight updating at each layer.",
            "score": 0.5742489573271503,
            "section_title": "Model Quantization Model quantization compresses",
            "char_start_offset": 25661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 49,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 854,
                    "end": 870,
                    "matchedPaperCorpusId": "263830786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8193359375
        },
        {
            "corpus_id": "276482745",
            "title": "MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures",
            "text": "Large Language Models (LLMs), such as Ope-nAI's GPT series (Achiam et al., 2023) and Meta's LLaMA (Touvron et al., 2023a,b), have made substantial advancements in the domain of Natural Language Processing (NLP). These models exhibit robust capabilities in language understanding and generation, facilitated by extensive pre-training and fine-tuning. However, as the size of these models continues to expand, their computational and storage demands increase sharply, presenting significant challenges for practical applications. Model compression, a vital approach to reducing memory footprint and computational load during model deployment, offers unique benefits across various domains. Techniques such as pruning (Frantar and Figure 1: Compresso/NutePrune results in heterogeneous inter-layer structures, whereas MaskPrune achieves uniform inter-layer structures, which is friendly to inference deployment and continual training. Alistarh, 2023;Ma et al., 2023;Sun et al., 2023), quantization (Frantar et al., 2023;Xiao et al., 2023;Lin et al., 2024), knowledge distillation (Gu et al., 2024;Agarwal et al., 2023), and low-rank factorization (Yuan et al., 2023;Wang et al., 2024) can significantly decrease the number of model parameters and computational complexity, thereby enabling large-scale language models to function efficiently in resource-constrained environments. \n\nThe pruning technique reduces the size and computational complexity of the models by eliminating redundant parameters, which can generally be categorized into unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024), semi-structured pruning (Mishra et al., 2021), and structured pruning (Ma et al., 2023;Xia et al., 2023;An et al., 2023). Unstructured pruning compresses models by removing individual parameters, resulting in sparse weight matrices that consume less memory.",
            "score": 0.5725009511160947,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 932,
                    "end": 947,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 995,
                    "end": 1017,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 1017,
                    "end": 1035,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1035,
                    "end": 1052,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 1559,
                    "end": 1587,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 1604,
                    "end": 1622,
                    "matchedPaperCorpusId": "270257857"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "The toolkit is flexible and may be used for a variety of applications including pre-trained language models and NLP tasks.In addition to structured pruning, the authors propose a self-supervised pruning technique that does not need any kind of labeled data.This method allows for further reduction of the model size by removing unnecessary parameters without compromising performance.For the effective evaluation of TextPruner, the authors conduct experiments on several NLP tasks.The results demonstrate that TextPruner effectively reduces the model size without retraining, thus addressing the computational resource limitations.The toolkit proves to be valuable in enabling the wider application of pre-trained language models by making them more resource-efficient.\n\nIn their paper Wang et al. [32] explore the need for large language models and proposes a structured pruning approach to reduce their size without sacrificing performance.As language models have become larger, their resource requirements and latency have also increased, leading to higher costs.The authors address this issue by investigating model compression techniques.Their purposed method focuses on structured pruning, which entails parameterizing each weight matrix with a low-rank factorization and deleting rank-1 components selectively during training.By doing so, the authors achieve significant compression levels while outperforming unstructured and block-structured pruning techniques in language modeling tasks.Moreover, their approach offers notable speed improvements during both training and inference stages.The paper also highlights the applicability of their method to other aspects of large language models.They demonstrate its effectiveness in pruning adaptive word embeddings, which are crucial for language understanding.Furthermore, they apply their structured pruning approach to the BERT model and evaluate its performance on various downstream fine-tuning classification benchmarks.",
            "score": 0.5709354593759046,
            "section_title": "IV. METHODS",
            "char_start_offset": 23040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 769
                },
                {
                    "start": 771,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1598
                },
                {
                    "start": 1598,
                    "end": 1700
                },
                {
                    "start": 1700,
                    "end": 1817
                },
                {
                    "start": 1817,
                    "end": 1982
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "267412232",
            "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
            "text": "Pruning can help optimize the model for deployment and make the model more efficient in terms of computation complexity and memory usage. Accordingly, pruning can be considered as an approach to enable a language model to support longer sequence length, while maintaining the desirable complexity and performance. In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters. LLM-Pruner [Ma et al., 2023a] is a structural task-agnostic pruning approach that selectively removes non-critical connection structures considering both first-order information and an approximated Hessian information gradient information. As an alternative, Sheared LLaMA [Xia et al., 2024] utilizes a two-stage approach for pruning an LLM. In the first stage, it exploits targeted structured pruning to prune a large model to a targeted shape by pruning layers, heads, and intermediate connections. In the second stage, the batches of data are loaded dynamically and the model structure is modified in each training iteration based on losses in various domains. As a result, Sheared LLaMA achieves a compressed model that can outperform the LLMs, with the same size but trained from scratch. \n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models. However, SparseGPT heavily relies on weight updates.",
            "score": 0.5702139366846237,
            "section_title": "Model Compression",
            "char_start_offset": 28041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1434
                },
                {
                    "start": 1437,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2076
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "278310893",
            "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
            "text": "A branch of research has focused on compressing LLMs to reduce computational and hardware requirements using various pruning techniques. Unstructured pruning (Sun et al., 2023) removes individual weights, producing sparse matrices that maintain performance but are less hardwareefficient. Semi-structured pruning (Frantar & Alistarh, 2023), such as the 2:4 sparsity pattern (Pool et al., 2021), introduces a hardware-friendly structured sparsity that accelerates computation. Structured pruning (Ashkboos et al., 2024;Yuan et al., 2023;Sengupta et al., 2025) takes a broader approach by removing entire components, such as Transformer layers (depth pruning) (Fan et al., 2019) or reducing embedding dimensions and attention heads (width pruning) (Zhu et al., 2021). After pruning, post-training is crucial to mitigate performance degradation. This involves fine-tuning or continual pre-training on datasets tailored to enhance performance recovery while maintaining efficiency. The P2 law (Chen et al., 2024b), highlighted in Equation 11, provides a predictive framework for the posttraining loss, considering factors such as pruning rate, model size, pre-pruning loss, and the number of training tokens. This law enables to balance computational costs with performance recovery by identifying optimal post-training data sizes. While higher pruning rates inevitably lead to larger initial losses, effective post-training strategies significantly minimize this impact, allowing smaller models to perform on par with their larger counterparts in many scenarios. \n\nwhere L 0 is the uncompressed model loss, \u03c1 is the pruning rate, N 0 is the model size before pruning, D is the number of post-training tokens, and N C , D C , E, \u03b1, \u03b2, \u03b3 are fitting constants.",
            "score": 0.570205845990089,
            "section_title": "Model Downscaling",
            "char_start_offset": 16924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1755
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "263830468",
            "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
            "text": "Compression of Small Language Models. In the era of small language models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019;Raffel et al., 2020), various compression techniques have been proposed to reduce the model size and inference costs, including weight pruning (Sanh et al., 2020b;Gordon et al., 2020;Zhang et al., 2022a;Xia et al., 2022), input token pruning (Li et al., 2023;Kim et al., 2022;Guan et al., 2022), quantization (Shen et al., 2020;Kim et al., 2021) and distillation (Sanh et al., 2020a;Jiao et al., 2020). We focus on weight pruning, particularly structured pruning, as it can directly reduce inference costs without special hardware support. Most state-of-the-art pruning methods involve a training process to update gradients and utilize them to estimate weight importance. Notable examples include CoFi (Xia et al., 2022) and nn pruning (Lagunas et al., 2021). However, these approaches cannot be directly applied to LLMs for two primary reasons. First, they are taskspecific pruning methods requiring downstream training datasets. Therefore, the pruned models do not retain the generalization capabilities across different tasks. Second, the pruning process for LLMs demands substantial training resources (e.g., expensive GPU memory). \n\nPruning Large Language Model. Given the above challenges, training-based pruning for LLMs remains unexplored. Existing efforts, such as SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2023) and LLM-Pruner (Ma et al., 2023), all adopt low-resource, one-shot pruning methods without training. SparseGPT is the first unstructured pruning approach specifically developed to be fast enough for pruning LLMs within a few hours.",
            "score": 0.5682368495283484,
            "section_title": "RELATED WORKS",
            "char_start_offset": 3857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1265
                },
                {
                    "start": 1268,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1698
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 149,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 312,
                    "end": 332,
                    "matchedPaperCorpusId": "251979775"
                },
                {
                    "start": 332,
                    "end": 349,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 371,
                    "end": 388,
                    "matchedPaperCorpusId": "259251699"
                },
                {
                    "start": 388,
                    "end": 405,
                    "matchedPaperCorpusId": "235727659"
                },
                {
                    "start": 405,
                    "end": 423,
                    "matchedPaperCorpusId": "248780407"
                },
                {
                    "start": 438,
                    "end": 457,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 832,
                    "end": 850,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 866,
                    "end": 888,
                    "matchedPaperCorpusId": "237485472"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "269588232",
            "title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search",
            "text": "Pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b) are widely used for natural language understanding (NLU) tasks when large amount of labelled data is available for fine-tuning. However, deploying PLMs for inference can be challenging due to their large parameter count. They demand significant GPU memory and exhibit high inference latency, making them impractical for many real-world applications, for example when used in an end-point for a web service or deployed on an embedded systems. Recent work (Blalock et al., 2020;Kwon et al., 2022;Michel et al., 2019;Sajjad et al., 2022) demonstrated that in many cases only a subset of the pre-trained model's parameters significantly contributes to the downstream task performance. This allows for compressing the model by pruning parts of the network while minimizing performance deterioration. \n\nUnstructured pruning (Blalock et al., 2020) computes a score for each weight in the network, such as the weight's magnitude, and removes weights with scores below a predetermined threshold. This approach often achieves high pruning rates with minimal performance degradation, but it also leads to sparse weight matrices, which are not well-supported by commonly used machine learning frameworks. Structured pruning (Michel et al., 2019;Sajjad et al., 2022) removes larger components of the network, such as layers or  Afterwards, we run a multi-objective search to select the optimal set of sub-networks that balance parameter count and validation error. \n\nheads. Although it typically does not achieve the same pruning rates as unstructured pruning, it only prunes entire columns/rows of the weight matrix, making it compatible with popular deep learning frameworks and hardware. \n\nNeural architecture search (Zoph & Le, 2017;Real et al., 2017;Bergstra et al., 2013) (NAS) finds more resource efficient neural network architectures in a data-driven way by casting it as an optimization problem.",
            "score": 0.5672338544232967,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 69,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 595,
                    "end": 615,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1313,
                    "end": 1334,
                    "matchedPaperCorpusId": "166227946"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "278501529",
            "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?",
            "text": "While training-aware pruning traditionally achieved better results, its computational demands proved impractical for billion-parameter models [3]. This led to efficient post-training approaches like SparseGPT [8], which enables one-shot pruning through sparse regression. Unstructured pruning offers maximum theoretical compression by removing individual weights, but often results in irregular sparsity patterns that are challenging to accelerate on current hardware. Semi-structured approaches, like N:M sparsity patterns [10], balance compression rates with hardware efficiency by enforcing regular pruning patterns, where N out of every M consecutive weights are pruned. Structured pruning [11] takes this further by removing entire structures, channels, or attention heads, with recent work showing that up to 50% of attention layers in large models can be removed while preserving performance [12].",
            "score": 0.5670426122199868,
            "section_title": "A. Pruning",
            "char_start_offset": 4253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 904
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 145,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 209,
                    "end": 212,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 524,
                    "end": 528,
                    "matchedPaperCorpusId": "231847094"
                },
                {
                    "start": 694,
                    "end": 698,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "268875867",
            "title": "Rethinking Pruning for Vision-Language Models: Strategies for Effective Sparsity and Performance Restoration",
            "text": "Unstructured Sparsity.In Table 1, We compare the zero-shot performance on various datasets using BLIP-2 pruned by different pruning techniques at unstructured 50% sparsity ratios.Among all pruning methods, while Wanda and ECoFLaP achieve the best performance, Wanda does not require multiple forward passes and is much more time-efficient.On the other hand, considering, EcoFLaP does not apply for N:M sparsity, we use Wanda as the default pruning method.\n\nCompared to DS\u25cbT that focuses on reconstruction errors, SparseLoRA also considers taskspecific performance and knowledge distillation from original full models, consistently outperforming the baselines on all tasks.Notably, the average performance of SparseLoRA is comparable to that of the full model.\n\nN:M Sparsity.In addition to unstructured sparsity, we also conduct experiments on N: M sparsity (andYukun Ma et al., 2021;Zhang et al., 2022), which can be applied to specific GPU cores and has more practical applications (Mishra et al., 2021).Compared to unstructured pruning, structured pruning causes a more significant performance drop and requires more extensive restores.Under more structured patterns, SparseLoRA recovers more performance, achieving a 10.5% improvement for 2:4 sparsity compared to 3.4% for unstructured sparsity.After restoring, all structured pruned models maintain over 90% of the performance of the original models, demonstrating the universality and effectiveness of SparseLoRA.\n\nSingle Model Pruning.Language models typically have much larger parameter sizes compared Table 2: Performance comparison at different sparse patterns (i.e., unstructured 50%, 2:4 and 4:8).using InstructBLIP (Dai et al., 2023) as the backbone.",
            "score": 0.5665857455163168,
            "section_title": "Main Experimental Results",
            "char_start_offset": 14159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 22,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 455
                },
                {
                    "start": 457,
                    "end": 672
                },
                {
                    "start": 672,
                    "end": 759
                },
                {
                    "start": 761,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1298
                },
                {
                    "start": 1298,
                    "end": 1468
                },
                {
                    "start": 1470,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1658
                },
                {
                    "start": 1658,
                    "end": 1712
                }
            ],
            "ref_mentions": [
                {
                    "start": 867,
                    "end": 883,
                    "matchedPaperCorpusId": "231847094"
                },
                {
                    "start": 883,
                    "end": 902,
                    "matchedPaperCorpusId": "249642429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "277876026",
            "title": "Adaptive Depth-Wise Pruning for Efficient Environmental Sound Classification",
            "text": "In general, structured pruning not only reduces model size but also improves computational efficiency by removing unimportant components in a hardware-friendly manner. In contrast, unstructured pruning enhances inference speed by increasing sparsity without explicitly simplifying the model architecture. \n\nTo achieve a better trade-off, fine-grained structured pruning systematically removes interdependent neurons or channels at a certain ratio, relying on importance estimation to guide pruning decisions. However, these static pruning methods do not adaptively adjust the model based on its actual performance and instead require trial-and-error to identify an optimal subnetwork that meets a given performance requirement. As a result, achieving a more efficient trade-off remains an open challenge.",
            "score": 0.5655865203383497,
            "section_title": "E. SUMMARY OF PRUNING",
            "char_start_offset": 11236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 804
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "270062857",
            "title": "A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts",
            "text": "Mixture-of-Experts.The sparsely gated MoE sends different tokens of an input sequence to different experts in language models (Shazeer et al., 2017;Lepikhin et al., 2020;Fedus et al., 2022;Du et al., 2022) and vision models (Riquelme et al., 2021;Puigcerver et al., 2022;Allingham et al., 2022) Pruning deep neural networks.Network pruning has been widely explored recently to reduce the computation and memory cost of deep neural networks (DNN) (Han et al., 2016a;Luo et al., 2017;Lee et al., 2019b;Liu et al., 2021a;Jaiswal et al., 2023) Unstructured pruning methods prune individual weights (Han et al., 2015;2016b;Frankle & Carbin, 2018;Wang et al., 2020;Liu et al., 2021b), while structured pruning methods remove neurons, channels, and layers (Li et al., 2016;Tung & Mori, 2018;Nonnenmacher et al., 2021).Although unstructured pruning can lead to a smaller model size because of the higher flexibility in pruning, the resulting irregular sparsity can lead to computational and memory overhead in practice.In contrast, structured pruning produces more regular and structured architecture, leading to possibly better hardware utilization and computational efficiency.\n\nFor large Transformer-based models, some focus on pruning pre-trained models (Chen et al., 2020;Zafrir et al., 2021;Li et al., 2024a) while others focus on task-specific pruning during or after the fine-tuning stage (Wang et al., 2020;Li et al., 2020b;Sanh et al., 2020).Note that all of these methods are compatible with our method and can be implemented together to further compress MoE models.\n\nConvergence and Generalization Analyses of Neural Networks.",
            "score": 0.5650120243887744,
            "section_title": "Related Works",
            "char_start_offset": 5975,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 19,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 811
                },
                {
                    "start": 811,
                    "end": 1011
                },
                {
                    "start": 1011,
                    "end": 1171
                },
                {
                    "start": 1173,
                    "end": 1444
                },
                {
                    "start": 1444,
                    "end": 1569
                },
                {
                    "start": 1571,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 148,
                    "matchedPaperCorpusId": "12462234"
                },
                {
                    "start": 170,
                    "end": 189,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 189,
                    "end": 205,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 224,
                    "end": 247,
                    "matchedPaperCorpusId": "235417196"
                },
                {
                    "start": 247,
                    "end": 271,
                    "matchedPaperCorpusId": "252992497"
                },
                {
                    "start": 482,
                    "end": 500,
                    "matchedPaperCorpusId": "52920837"
                },
                {
                    "start": 518,
                    "end": 539,
                    "matchedPaperCorpusId": "259203662"
                },
                {
                    "start": 594,
                    "end": 612,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 612,
                    "end": 618,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 618,
                    "end": 641,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 641,
                    "end": 659,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 766,
                    "end": 784,
                    "matchedPaperCorpusId": "49241074"
                },
                {
                    "start": 1250,
                    "end": 1269,
                    "matchedPaperCorpusId": "220768628"
                },
                {
                    "start": 1389,
                    "end": 1408,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 1425,
                    "end": 1443,
                    "matchedPaperCorpusId": "218665313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45068359375
        },
        {
            "corpus_id": "264146875",
            "title": "NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models",
            "text": "Language Model Compression. With the advancement of NLP, LMs have grown in size, making it difficult to deploy them on edge devices and resulting in slower inference speed. As a result, there has been active research on language model compression which has three main approaches: quantization, knowledge distillation, pruning. Quantization (He et al., 2016;Alom et al., 2018;Zafrir et al., 2019;Shen et al., 2020;Yao et al., 2022) minimizes the storage requirements for weight values by reducing the number of bits needed to represent them. Knowledge distillation (Sanh et al., 2019;Jiao et al., 2019;Sun et al., 2019Sun et al., , 2020;;Wang et al., 2020b,a) transfers the knowledge of a large-scale teacher model with high performance to a smaller-scale student model, enabling the student model to replicate the behavior of the teacher model. Pruning (Chen et al., 2020;Sanh et al., 2020;Kwon et al., 2022;Frantar and Alistarh, 2023) reduces the size of a model by removing unnecessary parts of large networks such as neurons, weights, or layers. \n\nPruning. Pruning can be categorized into two parts: (1) unstructured pruning and (2) structured pruning. In unstructured pruning (Chen et al., 2020;Prasanna et al., 2020), weights, which are connections between neurons, are removed from the network based on various criteria. However, this line of methods produces sparse weight matrices, requiring specific hardware support. On the other hand, structured pruning (Xia et al., 2022;Kwon et al., 2022;Kurtic et al., 2023), prunes away structures such as neurons, weight matrix blocks, or layers.",
            "score": 0.5640140982940962,
            "section_title": "Related Works",
            "char_start_offset": 24475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1595
                }
            ],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 375,
                    "matchedPaperCorpusId": "3651397"
                },
                {
                    "start": 375,
                    "end": 395,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 395,
                    "end": 413,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 413,
                    "end": 430,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 853,
                    "end": 872,
                    "matchedPaperCorpusId": "220768628"
                },
                {
                    "start": 1180,
                    "end": 1199,
                    "matchedPaperCorpusId": "220768628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69580078125
        },
        {
            "corpus_id": "274166121",
            "title": "DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization",
            "text": "Pruning. Unstructured pruning (Frankle and Carbin, 2019;Frantar and Alistarh, 2023;Sun et al., 2024) removes individual weights but offers limited speedup. This study focuses on structured pruning (Han et al., 2015;Wen et al., 2016), which removes entire structural components, such as neurons or layers, making it more effective for simplifying models and improving efficiency. In task-specific models, extensive pruning can retain performance (Liu et al., 2017;Wang et al., 2020a;Lagunas et al., 2021;Xia et al., 2022;Kurtic et al., 2023). However, for large language models (LLMs), as the ratio of training data to model size increases (Hoffmann et al., 2022b;Touvron et al., 2023a,b), fewer redundant parameters remain, leading to significant performance degradation after pruning. \n\nTo counter this, performance recovery techniques like continued pretraining are essential (Ma et al., 2023;Zhang et al., 2024b). However, continued pretraining of pruned models reduces loss at different rates across domains compared to training from scratch, resulting in less efficient data utilization (Xia et al., 2024). To address this, we employ a distributionally robust pruning method that dynamically adjusts the data distribution during training, ensuring balanced performance across domains.",
            "score": 0.563218107646713,
            "section_title": "Related Work",
            "char_start_offset": 23338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1289
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 56,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 56,
                    "end": 83,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 83,
                    "end": 100,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 215,
                    "end": 232,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 445,
                    "end": 463,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 463,
                    "end": 482,
                    "matchedPaperCorpusId": "211146532"
                },
                {
                    "start": 482,
                    "end": 503,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 503,
                    "end": 520,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "256662263"
                },
                {
                    "start": 878,
                    "end": 895,
                    "matchedPaperCorpusId": "262044134"
                },
                {
                    "start": 1092,
                    "end": 1110,
                    "matchedPaperCorpusId": "263830786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.640625
        },
        {
            "corpus_id": "257233142",
            "title": "Automatic Attention Pruning: Improving and Automating Model Pruning using Attentions",
            "text": "Unstructured Pruning vs. Structured Pruning. Unstructured pruning (e.g., [LeCun et al., 1990, Han et al., 2015a, Molchanov et al., 2017]) prunes individual elements in the weight tensors of a model. It has less impact on model accuracy, compared to structured pruning, because it is finergrained, but unstructured pruned models are hard to accelerate on commodity hardware. Structured pruning is a coarser-grained approach that prunes entire regular regions of the weight tensors of a model. It is more difficult to prune a model without causing accuracy loss using structured pruning, because by removing entire regions, it might remove weight elements that are important to the final accuracy. However, structured pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries [He et al., 2018b]. \n\nOne Shot Pruning vs. Iterative Pruning. One-shot pruning prunes a pre-trained model and then retrains it once, whereas iterative pruning prunes and retrains the model in multiple rounds. Iterative pruning generally achieves much better performance than one-shot pruning because multiple retraining phases help recover the accuracy lost during pruning. In particular, recent works based on the Lottery Ticket Hypothesis (LTH) have achieved great results in creating smaller and more accurate models through iterative pruning with rewinding [Frankle and Carbin, 2018]. LTH posits that a dense network has a sub-network, termed as a \"winning ticket\", which can achieve an accuracy comparable to the original network. However, existing LTH-based works consider only unstructured pruning, e.g., Iterative Magnitude Pruning (IMP) [Frankle andCarbin, 2018, Frankle et al., 2019] and Synflow [Tanaka et al., 2020], which, as discussed above, are hardware-inefficient. \n\nAutomatic Pruning.",
            "score": 0.5628255169304792,
            "section_title": "Background and Related Works",
            "char_start_offset": 4784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1827
                },
                {
                    "start": 1830,
                    "end": 1848
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 92,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 111,
                    "end": 136,
                    "matchedPaperCorpusId": "206592664"
                },
                {
                    "start": 846,
                    "end": 864,
                    "matchedPaperCorpusId": "51608028"
                },
                {
                    "start": 1407,
                    "end": 1433,
                    "matchedPaperCorpusId": "219965256"
                },
                {
                    "start": 1692,
                    "end": 1704,
                    "matchedPaperCorpusId": "219558821"
                },
                {
                    "start": 1704,
                    "end": 1739,
                    "matchedPaperCorpusId": "219965256"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68701171875
        },
        {
            "corpus_id": "266573164",
            "title": "The LLM Surgeon",
            "text": "For 2:4 semi-structured pruning, we compare LLM Surgeon with magnitude pruning, which only uses weight magnitudes, single-shot L-OBD, which only uses activations, and single-shot K-OBD, which also uses Kronecker-factored curvature but assumes full independence and thus only prunes without updating remaining weights as well as the recent state-of-the-art SparseGPT (Frantar & Alistarh, 2023). We report test performance after 50 % (2:4) semi-structured compression on wikitext-2 data in table 3. We empirically find that considering more weight correlations results in improved final performance after compression. Our proposed LLM Surgeon is competitive with prior work outperforming all baselines in terms of test set perplexity (PPL). \n\nTable 3: Semi-structured 2:4 compression for large language models on wikitext-2 data. The proposed method can dynamically allocate sparsity across layers through global thresholds described in section 3.3. In Fig. 4.5, we compare total allocated sparsity levels per layer depth and per layer type after compressing a pretrained OPT-125m model. We find that the LLM Surgeon prunes relatively more in the first layer and less in middle layers. Further, we observe that a larger portions of weights are removed in fully-connected compared to attention blocks, but deviations are less compared to other methods. Dynamic allocation allows for most pruning where it hurts least.",
            "score": 0.5597644712358267,
            "section_title": "SEMI-STRUCTURED COMPRESSION",
            "char_start_offset": 23743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1414
                }
            ],
            "ref_mentions": [
                {
                    "start": 366,
                    "end": 392,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.393798828125
        },
        {
            "corpus_id": "271544038",
            "title": "Pruning Large Language Models with Semi-Structural Adaptive Sparse Training",
            "text": "Network Pruning Pruning is a prevalent technique aimed at reducing model size and computational costs. It originated from methods like OBD (LeCun, Denker, and Solla 1990) and OBS (Hassibi, Stork, and Wolff 1993). Based on sparsity patterns, pruning methods can be broadly categorized into unstructured, structured, and semi-structured pruning. Unstructured pruning removes individual weights (Han, Mao, and Dally 2016;Paul et al. 2023), which can maintain performance even with high sparsity. However, due to its random pattern, unstructured models are difficult to accelerate. Structured pruning (Liu et al. 2017;Molchanov et al. 2019;Nova, Dai, and Schuurmans 2023;Shen et al. 2022), on the other hand, removes entire neurons, filters, or attention heads, resulting in models that are easier to accelerate on standard hardware but often suffer from severe performance loss. Semi-structured sparsity (e.g., N:M sparsity) (Hubara et al. 2021) has been applied as a trade-off between performance and achieving actual speedup. Recently, a series of works (Frantar and Alistarh 2023;Sun et al. 2023;Zhang et al. 2024a,b) have made progress in pruning LLMs with billions of parameters. However, these pruned models from training-free methods still fall short in complex zeroshot performance. \n\nRetraining Pruned Models Another line of research (Han et al. 2015;Singh and Alistarh 2020;Renda, Frankle, and Carbin 2020;Zhou et al. 2023) has focused on retraining pruned models to enhance performance. While retraining has been shown to be effective with smaller models and simpler tasks (Kurtic and Alistarh 2022;Zhu and Gupta 2018), it often involves additional training steps (Frankle and Carbin 2019) or introduces extra parameters for the pruning process (Shi et al. 2023), which limits its application to large-scale models due to the substantial computational resources required.",
            "score": 0.5595884046044033,
            "section_title": "Related Work",
            "char_start_offset": 5051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 211,
                    "matchedPaperCorpusId": "13740328"
                },
                {
                    "start": 392,
                    "end": 418,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 418,
                    "end": 435,
                    "matchedPaperCorpusId": "252735281"
                },
                {
                    "start": 597,
                    "end": 614,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 614,
                    "end": 636,
                    "matchedPaperCorpusId": "195657904"
                },
                {
                    "start": 636,
                    "end": 667,
                    "matchedPaperCorpusId": "257404900"
                },
                {
                    "start": 667,
                    "end": 684,
                    "matchedPaperCorpusId": "252873091"
                },
                {
                    "start": 1080,
                    "end": 1096,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1340,
                    "end": 1357,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1357,
                    "end": 1381,
                    "matchedPaperCorpusId": "220364055"
                },
                {
                    "start": 1413,
                    "end": 1429,
                    "matchedPaperCorpusId": "258967212"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5224609375
        },
        {
            "corpus_id": "272693912",
            "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models",
            "text": "Large language models (LLMs) refer to natural language processing (NLP) models with a massive number of parameters [1]- [5], commonly based on the Transformer architecture [5]. These models have also found widespread applications in fields such as speech processing [6]- [8] and computer vision [9]- [11]. In recent years, LLMs have demonstrated remarkable capabilities in handling complex tasks in applications like dialogue systems [12], [13] and knowledge-based question answering [14]- [16], significantly accelerating the development of downstream applications. However, as model sizes continue to grow, the challenges related to inference efficiency have become more pronounced. \n\nCurrently, optimization methods for large models include pruning (structured pruning [17]- [19] and unstructured pruning [20], [21]), quantization [22]- [24], and distillation [25], [26]. This work focuses on structured pruning, making it more deployment-friendly and hardware-friendly. In modern structured pruning algorithms for LLMs, LLM-Pruner [17] achieves model size reduction by removing inter-group dependencies in the network. Sheared-LLaMA [19] not only removes structures within groups but also prunes less important blocks to achieve compression. \n\nThese methods can employ LoRA [27] to recover performance, but the gains in runtime memory efficiency and inference performance are still minimal. Shortened-LLM [18] aggressively removes entire blocks to speed up inference, but The top left compares perplexity(PPL) across different strategies under the same pruning ratio and fine-tuning steps, where our method demonstrates superior performance. The bottom left shows the keyvalue (KV) cache usage, where our approach achieves more significant KV memory pruning at both strategy-level and model parameter-level pruning ratios. Right side: Under the same model parameter settings, KVPruner achieves faster inference speeds compared to Shortened-LLM [18] and LLM-Pruner [17] pruning method. \n\nit shows that directly removing blocks requires retraining with CPT [18], which can take several weeks to restore the pruned model.",
            "score": 0.5581889037805969,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1988
                },
                {
                    "start": 1991,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "258546397"
                },
                {
                    "start": 120,
                    "end": 123,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "239024736"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "233444273"
                },
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "1820614"
                },
                {
                    "start": 484,
                    "end": 488,
                    "matchedPaperCorpusId": "261048772"
                },
                {
                    "start": 490,
                    "end": 494,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 772,
                    "end": 776,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 814,
                    "end": 818,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1035,
                    "end": 1039,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1968,
                    "end": 1972,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "273345395",
            "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
            "text": "Pruning for Model Compression Pruning is a wellestablished method for reducing the complexity of overparameterized models in both computer vision and NLP (Le-Cun et al., 1989;Hassibi et al., 1993). It is typically classified into structured and unstructured pruning. Unstructured pruning removes individual weights and can achieve high compression rates in LLMs, particularly when paired with hardware accelerators like the Cerebras CS-3 (Lie, 2022;Thangarasa et al., 2024a) or Neural Magic DeepSparse (Neural Magic, 2021), which exploit sparsity for significant speedups. However, without specialized infrastructure, unstructured pruning can result in inefficient acceleration. Structured pruning, which removes entire channels, layers, or attention heads, is more effective in models with architectural redundancy, but can degrade model quality, especially in complex tasks which require multi-step reasoning (Kurtic et al., 2023;Ma et al., 2023;Sun et al., 2024). \n\nTo address these challenges, several metrics have been developed to guide pruning decisions more effectively. For instance, Shortened Llama (Kim et al., 2024) demonstrated that depth pruning (removing layers) can be as effective as width pruning (removing units within layers), or even a combination of both. The Block Influence (BI) score (Men et al., 2024), applied in Llama-2 (Touvron et al., 2023b), measures block importance by evaluating changes in hidden state magnitudes. Additionally, the angular cosine similarity metric (Gromov et al., 2024) identifies layers with redundant activations, allowing for selective pruning in models such as Llama-2 and Mistral (Jiang et al., 2023). Gromov et al. (2024) also proposed a healing method using low-rank adapters (Hu et al., 2022) to recover lost quality.",
            "score": 0.5577695285990814,
            "section_title": "RELATED WORK",
            "char_start_offset": 29434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 196,
                    "matchedPaperCorpusId": "7001469"
                },
                {
                    "start": 438,
                    "end": 449,
                    "matchedPaperCorpusId": "252559073"
                },
                {
                    "start": 449,
                    "end": 474,
                    "matchedPaperCorpusId": "257636503"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75927734375
        },
        {
            "corpus_id": "271544324",
            "title": "CELLM: An Efficient Communication in Large Language Models Training for Federated Learning",
            "text": "Model compression techniques reduce the size of an ML model by altering weights or structure, improving training efficiency, decreasing storage requirements, and reducing inference latency [50], [60]. We focus on pruning/sparsity, which sets a large fraction of model weights to zero, representing the model in a sparse matrix format [10]. Pruning is categorized into structured and unstructured methods. Structured pruning ranks and prunes entire structures like filters or layers, enhancing training speed on GPUs but often reducing model accuracy [52]. Unstructured pruning ranks and prunes individual weights, offer more flexible sparsity patterns and typically better accuracy but limited efficiency benefits on consumer hardware [57], [49]. Pruning can also be performed at initialization, setting weights to zero before training [14], or iteratively, progressively increasing sparsity during training [37]. Sparsity is emphasized due to its ability to significantly compress models while maintaining performance, unlike quantization, which often reduces accuracy [50].",
            "score": 0.5577560520807279,
            "section_title": "Sparsity and Model Compression",
            "char_start_offset": 10510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1075
                }
            ],
            "ref_mentions": [
                {
                    "start": 195,
                    "end": 199,
                    "matchedPaperCorpusId": "254877120"
                },
                {
                    "start": 836,
                    "end": 840,
                    "matchedPaperCorpusId": "221802286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68017578125
        },
        {
            "corpus_id": "268364208",
            "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks such as natural language understanding and generation (Zhao et al., 2023;Gozalo-Brizuela and Garrido-Merch\u00e1n, 2023). Despite such capabilities, the democratization of LLMs is primarily restricted by their substantial resource demands, which motivates the design of LLM compression methods (Wan et al., 2024a;Wang et al., 2024;Zhu et al., 2024;Zhou et al., 2024). These methods are often performed in a post-training manner without requiring retraining from scratch. Post-training LLM compression methods based on quantization (Yuan et al., 2024;Huang et al., 2024), unstructured pruning (Frantar and Alistarh, 2023), and structured pruning (Ma et al., 2023;Ashkboos et al., 2024;Zhong et al., 2024) have been intensively studied. Despite their success, these methods have certain limitations, such as dependence on specific hardware and low inference speedup. In contrast, compression methods based on low-rank approximation, such as Singular Value Decomposition (SVD) are not limited by those constraints. Moreover, the KV cache of LLMs compressed via SVD at runtime can also be reduced. \n\nDespite these advantages, the potential of SVD for LLM compression has not been fully explored. Several SVD-based LLM compression methods, such as FWSVD (Hsu et al., 2022) and ASVD (Yuan et al., 2023) have been proposed. However, these methods exhibit severe performance degradation when model compression ratio 1 increases. Such limitation can be attributed to two fundamental issues involved in their approaches. \u2776 Misalignment between SVD truncation and compression loss: both FWSVD and ASVD fail to establish a direct relationship between singular values and model compression loss. As a consequence, truncating smaller singular values in SVD could lead to higher compression loss. \u2777 Lack of model parameter update after SVD truncation: as model compression ratio increases, the number of singular values that need to be truncated in SVD increases as well.",
            "score": 0.5569359069077965,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 404,
                    "matchedPaperCorpusId": "266044196"
                },
                {
                    "start": 439,
                    "end": 457,
                    "matchedPaperCorpusId": "269293007"
                },
                {
                    "start": 622,
                    "end": 641,
                    "matchedPaperCorpusId": "263333921"
                },
                {
                    "start": 641,
                    "end": 660,
                    "matchedPaperCorpusId": "267523201"
                },
                {
                    "start": 683,
                    "end": 711,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 753,
                    "end": 775,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 1340,
                    "end": 1358,
                    "matchedPaperCorpusId": "250243971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50927734375
        },
        {
            "corpus_id": "274992799",
            "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
            "text": "However, there exist some challenges in its application to structured pruning: \n\n\u2022 The OBS is a fine-grained compression framework that compresses one parameter at each iteration, whereas structured pruning has a minimum granularity of either a column or head. Directly applying the OBS framework will result in high numerical errors, impairing model performance. \u2022 The OBS is essentially a layer-wise compression method. It focuses on each individual layer, thus failing to allocate pruning ratios for each layer rationally using global information (such as global gradients). This is crucial for LLM structured pruning, which relies on a non-uniform strategy to reduce the impact on performance. \n\nTo address these issues, we propose a new structured pruning method for LLMs. We introduce Batched Greedy Pruning to achieve low-cost and rapid pruning for LLMs. Specifically, for attention heads, we propose grouped Cholesky decomposition to select nearly optimal heads for pruning in each iteration, thereby maintaining an approximately locally optimal pruning result. For Feed-Forward Networks (FFNs), we achieve near-optimal and efficient pruning results through Dynamic Group Size. Furthermore, since the OBS is essentially a layer-wise compression framework, we investigate the error accumulation phenomenon in layer-wise pruning and propose pruning by Incremental Pruning Ratio, a straightforward non-uniform strategy to control the pruning rate of each layer, further mitigating performance loss under a given overall pruning ratio. \n\nContribution. In this paper, we propose SlimGPT, a layer-wise pruning approach that extends the classical OBS framework to structured pruning for LLMs. The characteristics of SlimGPT can be summarized as follows: (i) Task-agnostic pruning scheme. Only a random sample of data from generic pre-training corpora is needed as a calibration set, and we can obtain a compressed model with most performance preserved; (ii) Low-cost, low-resource, and time-efficient compression scheme. The model can be compressed using just a single GPU, a few hundred of calibration data, and about one hour; (iii) A universal pruning method for Transformer-based models.",
            "score": 0.554386606456799,
            "section_title": "Introduction",
            "char_start_offset": 2357,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2192
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "269149429",
            "title": "LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models",
            "text": "As we know, unstructured pruning has poor generality but strong performance, whereas structured pruning has strong generality but typically slightly worse performance.In this study, we aim to design a structured compression method that could achieve promising performance closer to unstructured pruning, meanwhile ensure its generalization on common devices.The weights obtained from unstructured pruning represent the crucial weights that impact the performance of model.Analyzing the structural patterns therein can better guide structured compression.\n\nAfter unstructured pruning with Wanda under 50% sparsity, the weight matrices of the first MHA sub-layer are visualized in Fig. 2. We observe an interesting phenomenon, that is, the distribution of retained weights is more concentrated in some certain rows or columns, We further present the To quantitatively analyze the low-rank property, we perform SVD on the weight matrices of MHA and FFN sub-layers.\n\nThe first row in Table 1 shows the percentage of the number of singular values when 80% energy is retained.The results demonstrate that compared to FFN sub-layer, the weights in MHA sub-layer exhibit a more pronounced low-rank structure.This suggests that low-rank approximation is more suitable to the MHA sub-layer and the FFN sub-layer deserves other compression strategies.Based on this insight, in this paper, we propose a weighted low-rank approximation method to compress the MHA sub-layer and adopt the structured pruning to compress the FFN sub-layer.In the following, we will elaborate our proposal.",
            "score": 0.554367380329649,
            "section_title": "Weight Distribution in MHA and FFN Sub-Layers",
            "char_start_offset": 10830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 472
                },
                {
                    "start": 472,
                    "end": 554
                },
                {
                    "start": 556,
                    "end": 961
                },
                {
                    "start": 963,
                    "end": 1070
                },
                {
                    "start": 1070,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1572
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6650390625
        },
        {
            "corpus_id": "268041812",
            "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
            "text": "Large language models (LLMs) Touvron et al. [2023], OpenAI [2023] have recently transformed the field of natural language processing (NLP) by delivering exceptional results across a variety of intricate language benchmarks Wei et al. [2022], Bommarito II and Katz [2022], Bubeck et al. [2023]. Nonetheless, these models, with billions of parameters, generally necessitate significant computational resources. To make LLMs more accessible, extensive efforts have been devoted to model compression of LLMs Xu and McAuley [2023], Bai et al. [2024a], including pruning, quantization, knowledge distillation, and low-rank factorization. Pruning, by introducing sparsity, jointly enhances memory and computational efficiency and offers unparalleled flexibility, seamlessly integrating with any LLMs, thus standing out as a highly effective and widely adopted compression strategy. \n\nModel pruning has a long history LeCun et al. [1989] and has proven effective in applications related to vision and smaller language models Hoefler et al. [2021]. However, conventional pruning techniques, which rely on global pruning and require loading the entire model into the same GPU Mallya and Lazebnik [2018], Singh and Alistarh [2020], become impractical for today's LLMs due to their vast size. Recently, several local pruning methods have been proposed for billion-scale LLMs. These methods compress each layer separately, and the overall compressed model is then obtained by \"stitching together\" the individually compressed layers. SparseGPT Frantar and Alistarh [2023], an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves parameter reduction of up to 60% with minimal performance loss. Another approach, Wanda Sun et al. [2023], introduces a novel pruning criterion that evaluates weights by considering both magnitude 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2402.17946v4 [cs.CL] 31 Oct 2024 and related input activations.",
            "score": 0.5537930428660336,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 504,
                    "end": 525,
                    "matchedPaperCorpusId": "254069544"
                },
                {
                    "start": 910,
                    "end": 929,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1017,
                    "end": 1038,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 1166,
                    "end": 1192,
                    "matchedPaperCorpusId": "35249701"
                },
                {
                    "start": 1194,
                    "end": 1219,
                    "matchedPaperCorpusId": "220364055"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "259287257",
            "title": "An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs",
            "text": "Large Transformer-based Language Models (LMs) are evolving rapidly from millions of parameters, e.g., BERT-Large (Devlin et al., 2018), to billions of parameters, e.g., Turing-Megatron (Smith et al., 2022), and GPT3 (Brown et al., 2020). Transformer-based LMs are currently used for solving almost all natural language processing (NLP) tasks, and those large models have demonstrated promising stateof-the-art (SoTA) accuracy on a wide range of NLP tasks. However, it's still quite challenging to deploy these models in production due to the demands of large computation resources and strict latency constraints in real applications. \n\nTo enable the deployment of Transformer models at scale, model compression and optimization are usually required to make model inference more efficient. Pruning (Sanh et al., 2020) and quantization (Zafrir et al., 2019) are two well-known approaches that have been widely used to compress Transformer models. There are two kinds of pruning methods: unstructured pruning (Han et al., 2015) (Gordon et al., 2020) (Wang, 2020) and structured pruning (Pool et al., 2021), where unstructured pruning does not require any special sparsity pattern while structured one requires applying the same sparsity pattern in the weights. In general, it is difficult to demonstrate the performance gains for an unstructured sparse model without high sparsity ratio. Even for a structured sparse model, speedup might be difficult without special hardware support (e.g., NVIDIA Ampere architecture and beyond). On the other hand, there are some recent works (Yao et al., 2022) (Kim et al., 2021) that have demonstrated the performance of 8-bit integer (INT8) quantization as 8-bit fused multiply-add (FMA) or matrix multiplication instruction has been supported in majority modern hardwares. However, there is a lack of work that demonstrate the efficient inference on modern CPUs combining quantization and pruning.",
            "score": 0.5533983652734261,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 816,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 834,
                    "end": 855,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 1006,
                    "end": 1024,
                    "matchedPaperCorpusId": "2238772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "264171146",
            "title": "Learnable Sparsity Structured Pruning for Acoustic Pre-trained Models",
            "text": "Self-supervised learning (SSL) methods for learning representations of speech have been successful in many tasks in the past few years. They can obtain the learning of meaningful and distinctive features from unlabeled speech data. Pre-trained models based on Transfomer structures such as Wav2Vec2 [21], HuBERT [9], and WavLM [2] have made this representation an important component and have achieved significant results on tasks related to the speech domain in this way. These models, however, typically have a large number of parameters and take long inference time, which requires more storage space and more computational power. The limitations make these models unsuitable for low-resource devices and difficult for researchers in academia. Therefore, model compression has become a research hot-spot and focus in the field of deep learning.\n\nIn order to reduce the model size and improve accuracy, a variety of model compression techniques are proposed [7]. Most commonly used techniques include low-rank approximation [13,16], weight sharing [4], knowledge distillation [10,11,23], quantization [1,6,22,28], and pruning [3,17,18,25]. In this paper, we focus on pruning. Pruning methods have been shown to be extremely effective at reducing the storage size of models fine-tuned for a specific task. It aims to search for an accurate sub-network in a larger pre-trained model. It can be broadly classified into two categories based on the granularity of removed components: (i) Unstructured pruning, that is, zeroing out insignificant parameters based on heuristic methods such as magnitude pruning [8], regularization of L0 [15]. (ii) Structured pruning refers to the structural pruning of Transformer networks [24], from pruning heads [18], to removing entire layers [5]. Although various pruning techniques have been proposed, no matter which kind of pruning methods, there are some defects. Unstructured pruning can lead to competitive performance, but it is difficult to accelerate due to irregular sparsity. As for structured pruning, in order to obtain a reasonable accuracy, complex hyper-parameter tuning operations are required",
            "score": 0.5531958624013806,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 303,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 312,
                    "end": 315,
                    "matchedPaperCorpusId": "235421619"
                },
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "239885872"
                },
                {
                    "start": 960,
                    "end": 963,
                    "matchedPaperCorpusId": "1663491"
                },
                {
                    "start": 1128,
                    "end": 1131,
                    "matchedPaperCorpusId": "202771940"
                },
                {
                    "start": 1131,
                    "end": 1134,
                    "matchedPaperCorpusId": "195345467"
                },
                {
                    "start": 1606,
                    "end": 1609,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1719,
                    "end": 1723,
                    "matchedPaperCorpusId": "202565587"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "In the context of NLP, unstructured pruning can be applied to various components of neural network models, including word embeddings, recurrent connections, and fully connected layers.The main objective is to identify and eliminate parameters that are deemed less important or redundant, thereby reducing the model's memory requirements and computational demands while striving to maintain or minimize the impact on performance.\n\nUnstructured pruning in NLP seeks to establish a compromise between model reduction and acceptable performance.It allows for more aggressive parameter removal compared to structured pruning methods but may also pose challenges FIGURE 6. Unstructured Pruning reduces the network by irregular removal of neuron units [54].\n\nsuch as potential accuracy loss or the need for careful fine-tuning.\n\nUnstructured Pruning is depicted in Fig 6 .Li et al. [7] present an unstructured weight pruning approach for CNNs.The authors offer a metric for measuring the relevance of filters and prune filters systematically depending on their importance.The results show that unstructured pruning can minimise the number of parameters while preserving performance.\n\nNarang et al. [50] investigate the use of unstructured pruning in the context of recurrent neural networks (RNNs).The authors want to know how weight pruning affects RNNs and suggest a structured regularisation strategy to encourage sparsity in these networks.By exploring the potential of unstructured pruning, they aim to reduce the number of parameters in RNNs while minimizing the impact on performance.The study begins by highlighting the significance of sparsity in deep learning models, especially in scenarios where computational and memory resources are limited.RNNs, being widely used in various NLP tasks such as language modeling and sequence generation, can benefit from pruning techniques to alleviate the computational burden and improve efficiency.However, the unique recurrent structure of RNNs poses challenges in applying traditional pruning methods effectively.To address these challenges, Narang et al. propose a structured regularization technique that encourages sparsity in RNNs.This regularization technique introduces a penalty term based on the L1 norm of the weights, encouraging a large number of them to be set to zero.",
            "score": 0.5530693106463315,
            "section_title": "B. UNSTRUCTURED PRUNING",
            "char_start_offset": 47298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 428
                },
                {
                    "start": 430,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 750
                },
                {
                    "start": 752,
                    "end": 820
                },
                {
                    "start": 822,
                    "end": 865
                },
                {
                    "start": 865,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1175
                },
                {
                    "start": 1177,
                    "end": 1291
                },
                {
                    "start": 1291,
                    "end": 1437
                },
                {
                    "start": 1437,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1748
                },
                {
                    "start": 1748,
                    "end": 1941
                },
                {
                    "start": 1941,
                    "end": 2058
                },
                {
                    "start": 2058,
                    "end": 2180
                },
                {
                    "start": 2180,
                    "end": 2326
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475830078125
        },
        {
            "corpus_id": "269149429",
            "title": "LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models",
            "text": "In (Noach & Goldberg, 2020), the authors perform Singular Value Decomposition (SVD) on BERT and utilize knowledge distillation to recovery model performance.DRONE (Chen et al., 2021) approximates input activations through SVD in specific tasks.FWSVD (Hsu et al., 2022) evaluates the importance of weights with the Fisher information and conducts SVD on the weighted matrix.AFM (Yu & Wu, 2023) decomposes the weight matrix based on the lowrank property of the output activations.LORD (Kaushal et al., 2023) compresses a 16B code model with AFM and then restores its performance by LoRA fine-tuning.Very recently, several efforts attempt to incorporate low-rank approximation with other compression techniques.LoftQ (Li et al., 2023a) utilizes a quantized matrix and two low-rank matrices to approximate the original high-precision weight matrix.In (Li et al., 2023b), the weight matrix is approximated by the sum of a low-rank matrix and a sparse matrix.LPAF (Ren & Zhu, 2023) performs SVD on the pruned model obtained through movement pruning.Despite the brilliant achievements, the present works typically compress each module of Transformer layer in the same manner, while ignoring a fundamental problem, that is, whether the modules in transformer have the same property.This beneficial exploration is expected to provide important guidance for the improvement of compression methods.\n\nPruning aims to remove unimportant parts of the weights, which can be categorized into unstructured pruning and structured pruning.Unstructured pruning entails the removal of less important individual weights based on their importance scores.The representative unstructured pruning methods for LLMs include SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2023), GBLM-pruner (Das et al., 2023).Although unstructured pruning yields favorable performance, the acceleration in inference is only achievable on specific hardware due to the irregular sparsity, which makes it difficult to migrate across different platforms and environments.",
            "score": 0.5523344051422053,
            "section_title": "Introduction",
            "char_start_offset": 1836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 244
                },
                {
                    "start": 244,
                    "end": 373
                },
                {
                    "start": 373,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1043
                },
                {
                    "start": 1043,
                    "end": 1274
                },
                {
                    "start": 1274,
                    "end": 1387
                },
                {
                    "start": 1389,
                    "end": 1520
                },
                {
                    "start": 1520,
                    "end": 1631
                },
                {
                    "start": 1631,
                    "end": 1791
                },
                {
                    "start": 1791,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 27,
                    "matchedPaperCorpusId": "227905681"
                },
                {
                    "start": 163,
                    "end": 182,
                    "matchedPaperCorpusId": "245003869"
                },
                {
                    "start": 250,
                    "end": 268,
                    "matchedPaperCorpusId": "250243971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.469970703125
        },
        {
            "corpus_id": "273962638",
            "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
            "text": "In this section, we provide a comprehensive discussion about network pruning applied to LLMs. We first introduce structured and unstructured network pruning; then, we focus on the latter, introducing the latest approaches proposed for improving sparse model performance. \n\nStructured Network Pruning. Given a layer's weight matrix W \u2208 R n\u00d7m to sparsify, structured pruning removes either entire rows (n) or columns (m) (see the next section) aiming at speeding up both training and inference time. The first approach that applies structured pruning to LLMs has been proposed in [28], and focuses on the dependency of Transformers, i.e., it removes components of the networks while maximizing their original functionality. In [24], a pruning mechanism has been devised to remove components with the worst balance between loss and runtime. Other structured pruning approaches have been proposed based on combinatorial optimization [30], perturbative forward-pass only [9], and reduction of the embedding dimension through PCA [1]. Finally, in [17] it has been found that the last Transformer blocks are redundant, hence they can be completely removed with minor performance drops. The reason behind this phenomenon lies in the similarity between the learnable representation of consecutive blocks, which turns out to increase when the block depth increases. While all these approaches can achieve valuable inference speed-ups, the performance of the resulting sparse models w.r.t. their dense counterparts can be matched only at low sparsity values, such as 20% in [28] or 30% in [1]. This somehow limits the applicability of these methods, since in the case of models with billions of parameters one may need more aggressive pruning strategies to meet stringent hardware requirements. \n\nUnstructured Network Pruning. Differently from structure pruning, unstructured pruning works by removing weights in a scattered (i.e., non-structured) way.",
            "score": 0.5523312836031858,
            "section_title": "Related Work",
            "char_start_offset": 3549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "256662263"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 1563,
                    "end": 1567,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1578,
                    "end": 1581,
                    "matchedPaperCorpusId": "267301573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359375
        },
        {
            "corpus_id": "259357156",
            "title": "PET: Parameter-efficient Knowledge Distillation on Transformer",
            "text": "Pruning, quantization, and matrix decompositions are widely known compression methods. Unstructured pruning [38] and quantization [39] reduce the memory usage of models by removing redundant parameters and representing model weights with fewer bits, respectively. However, these methods require hardware or acceleration libraries to achieve speedup gains. Structured pruning [26,40] reduces attention heads and layers in the Transformer, while matrix decomposition [41,42] is efficient for reducing parameters in embedding, feedforward, and attention layers. MDN [28] unifies existing compression techniques and improves Transformer inference speed, utilizing several training strategies to compensate for accuracy loss resulting from model size reduction. These techniques are orthogonal to our work; combining them with our work to achieve more compression and performance enhancement is a future work.",
            "score": 0.5520423425373023,
            "section_title": "Other compression techniques",
            "char_start_offset": 13943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 904
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "222134166"
                },
                {
                    "start": 379,
                    "end": 382,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 465,
                    "end": 469,
                    "matchedPaperCorpusId": "231698843"
                },
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "237452653"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.468017578125
        },
        {
            "corpus_id": "266363467",
            "title": "Multiply-and-Fire: An Event-Driven Sparse Neural Network Accelerator",
            "text": "Many prior accelerators aim to benefit from the high compression ratio of unstructured pruning by designing specific dataflow and hardware to handle irregular weight and activation sparsity [1,7,8,12]. However, they cannot fully benefit from the high compression ratio due to the irregularity in the sparse data. More recent works shift the focus to structured pruning to leverage the regular sparsity in weights [21,32,45,59]. Although the compression ratio of structured pruning can be lower than unstructured pruning, the storage requirement of the structurally pruned model can be smaller or comparable as additional indices are not required to identify sparse data in the compressed format [35]. Both prior structured and unstructured sparsity-aware accelerators manage to improve the performance of sparse processing, but they suffer from a number of drawbacks, such as ineffectual computations and costly pairing logic. In the following, we discuss the designs and drawbacks of two representative works.",
            "score": 0.5512689135292405,
            "section_title": "Sparsity-aware Accelerator",
            "char_start_offset": 10912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1010
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "3746338"
                },
                {
                    "start": 193,
                    "end": 195,
                    "matchedPaperCorpusId": "207882941"
                },
                {
                    "start": 195,
                    "end": 197,
                    "matchedPaperCorpusId": "131771552"
                },
                {
                    "start": 197,
                    "end": 200,
                    "matchedPaperCorpusId": "235415831"
                },
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "249205107"
                },
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "236034289"
                },
                {
                    "start": 420,
                    "end": 423,
                    "matchedPaperCorpusId": "248385124"
                },
                {
                    "start": 423,
                    "end": 426,
                    "matchedPaperCorpusId": "56174616"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "210064442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "275921475",
            "title": "You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning",
            "text": "Model pruning is a widely-used technique to reduce the number of parameters in a model, enhancing both speed and efficiency. It can be broadly categorized into two classes -unstructured and structured pruning. Unstructured pruning removes individual weights, as seen in SparseGPT (Frantar & Alistarh, 2023), which leverages Hessian matrix inversion to identify and eliminate less critical weights. However, unstructured pruning often requires hardware-specific optimizations and may not always result in significant computational gains (Yang et al., 2024;Wang et al., 2024b). In contrast, structured pruning removes entire channels or components, making it more compatible with various hardware setups. For example, LLM-Pruner (Ma et al., 2023b) evaluates weight group importance and uses LoRA fine-tuning to recover lost accuracy. While structured pruning is more hardware-friendly, it can lead to greater accuracy loss at higher compression ratios. Methods like Layer Collapse (Yang et al., 2024) take a layer-wise approach, merging parameters of adjacent lay-ers to achieve up to 50% compression without extensive retraining. Recent advancements in posttraining compression methods, such as SliceGPT (Ashkboos et al., 2024) and SVD-LLM (Wang et al., 2024b), aim to maintain performance while reducing model size. SliceGPT is a structured pruning method that compresses LLMs by slicing off entire rows and columns of weight matrices, using orthogonal transformations to reduce the embedding dimensions. SVD-LLM, on the other hand, applies singular value decomposition with layer-wise updates, ensuring minimal accuracy loss even under high compression, outperforming previous methods like ASVD (Yuan et al., 2023) and FWSVD (Hsu et al., 2022). \n\nMajority of these structured pruning methods rely heavily on external calibration datasets, making them sensitive to data quality.",
            "score": 0.5510358091239363,
            "section_title": "RELATED WORK",
            "char_start_offset": 4717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 727,
                    "end": 745,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "We systematically examine three fundamental components of the FLAP method: the pruning metric, the global compression structure, and bias compensation. Additionally, we evaluate the robustness of our pruning approach in relation to calibration samples. Table 2: Zero-shot performance of the compressed LLaMA-7B. Bold results highlight the best performance. Underscored results denote the second-best performance for each pruning ratio. \n\nPruning Metric. Both the pruning metric and compressed model structure are critical factors in the pruning process. \n\nFLAP is specifically designed to address these two dimensions in the structured pruning of Large Language Models (LLMs). To evaluate their effectiveness, we conducted experiments employing various structured pruning metrics and global compression structures. We investigated three structured pruning metrics in this study: 1) Weighted Input Feature Norm (WIFN), a lowdamage metric assessing the effect of weight columns on the output feature map; 2) Input Feature Variance (IFV), used to gauge the variability among input features; and 3) Weighted Input Feature Variance (WIFV), utilized by FLAP to assist in determining the potential for recovery of the output feature map after a column of the weight matrix is removed. \n\nTo underscore the importance of global adaptive compression structure, we defined four configurations: 'UL-UM' (Uniform across Layers and Modules, employed in unstructured pruning for LLMs like Wanda); 'UL-MM' (Uniform across Layers, Manual ratio for Modules); 'AL-MM' (Adaptive across Layers, Manual for Modules); and 'AL-AM' (Adaptive across both Layers and Modules), the structure chosen by FLAP . Results in this section include bias compensation, with bias-compensated ablation experiments detailed later. \n\nIn our experiments, we structurally pruned the LLaMA-7B model with a 50% pruning ratio and evaluated the model using the perplexity metric on the WikiText2 dataset. The detailed results are presented in Table 3. Notably, the most effective pruning model was obtained using the default configuration of FLAP , achieving a perplexity of 31.80.",
            "score": 0.5509795689133101,
            "section_title": "Ablation Study",
            "char_start_offset": 18442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2134
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50439453125
        },
        {
            "corpus_id": "249063170",
            "title": "Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models",
            "text": "Unstructured pruning can achieve some of the highest sparsity levels using various criteria and schedules for determining which parameters to prune (Frankle and Carbin, 2019;Chen et al., 2020;Sanh et al., 2020;Guo et al., 2021), though sparsity patterns resulting from unstructured pruning often do not result in latency reduction on modern accelerator hardware. Work in structured pruning has explored removing entire parameter matrices such as self-attention heads, hidden units, and entire layers (Michel et al., 2019;Lagunas et al., 2021;Xia et al., 2022), with basic underlying hardware constraints in mind. Pruning is often combined with a distillation objective, which provides complementary gains, likely by reducing complexity of the dataset (Zhou et al., 2020). \n\nDistillation is a prominent, practical method for compression that is widely used in NLP. Work on distillation in NLP has focused largely on the taskagnostic setting of compressing general-purpose pre-trained models such as BERT (Sanh et al., 2019;Sun et al., 2019Sun et al., , 2020) ) but task-specific distillation has also been reported to work well (Jiao et al., 2020). \n\nApproaches for model quantization can be categorized into post-training quantization, where general-purpose models are quantized at test-time (Jacob et al., 2018;Bhandare et al., 2019;Kim et al., 2021), and quantization-aware training, where models incorporate simulated quantization error during training in order to learn more quantizable parameters (Zafrir et al., 2019;Bai et al., 2021). Quantization-aware training tends to lead to higher accuracy quantized inference, but post-training quantization can be applied on-the-fly to any model at inference time.",
            "score": 0.5508387207287118,
            "section_title": "Ethics Statement",
            "char_start_offset": 31936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1712
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 192,
                    "matchedPaperCorpusId": "220768628"
                },
                {
                    "start": 192,
                    "end": 210,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 210,
                    "end": 227,
                    "matchedPaperCorpusId": "229152766"
                },
                {
                    "start": 500,
                    "end": 521,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 521,
                    "end": 542,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 542,
                    "end": 559,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 751,
                    "end": 770,
                    "matchedPaperCorpusId": "207847275"
                },
                {
                    "start": 1022,
                    "end": 1038,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1038,
                    "end": 1059,
                    "matchedPaperCorpusId": "215238853"
                },
                {
                    "start": 1127,
                    "end": 1146,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1292,
                    "end": 1312,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1523,
                    "end": 1540,
                    "matchedPaperCorpusId": "229923538"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74755859375
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "This regularization technique introduces a penalty term based on the L1 norm of the weights, encouraging a large number of them to be set to zero.By incorporating this regularization term into the training objective, the authors aim to promote sparsity during the learning process.The experimental evaluation conducted by the authors showcases the effectiveness of unstructured pruning in reducing the number of parameters in RNNs.By applying the proposed structured regularization technique, they successfully induce sparsity in the RNNs without a significant loss in performance.The experiments demonstrate that the pruned RNNs achieve comparable or even improved performance on tasks such as language modeling and speech recognition.Furthermore, the authors examine how different sparsity levels affect the performance of pruned RNNs.They discover that even at high sparsity levels, trimmed models may outperform their dense counterparts.This discovery demonstrates the potential of unstructured pruning to dramatically lower RNN memory and compute needs while maintaining RNN efficacy in NLP workloads.\n\nChen et al. [51] addresses the challenges posed by unstructured sparsity after pruning in the implementation of deep learning models.The authors suggest a technique of compression that combines unstructured pruning with a unique weight permutation mechanism.The sparse weight matrix is further compressed into a tiny and dense shape by permuting it, maximising hardware resource utilisation.When compared to state-of-the-art approaches, the suggested method yields a 10.28x improvement in matrix compression rate.As a result, throughput and energy efficiency are increased by 2.12 and 1.57 times, respectively.The approach is named ''Tight Compression'' and offers a promising solution for efficiently compressing CNN models.",
            "score": 0.5502693310165969,
            "section_title": "B. UNSTRUCTURED PRUNING",
            "char_start_offset": 49478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 581
                },
                {
                    "start": 581,
                    "end": 736
                },
                {
                    "start": 736,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 941
                },
                {
                    "start": 941,
                    "end": 1106
                },
                {
                    "start": 1108,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1366
                },
                {
                    "start": 1366,
                    "end": 1499
                },
                {
                    "start": 1499,
                    "end": 1621
                },
                {
                    "start": 1621,
                    "end": 1718
                },
                {
                    "start": 1718,
                    "end": 1833
                }
            ],
            "ref_mentions": [
                {
                    "start": 1120,
                    "end": 1124,
                    "matchedPaperCorpusId": "222297215"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "265050936",
            "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
            "text": "Large Language Models (LLMs) based on transformer architecture (Vaswani et al., 2017) have ushered in a transformative era in the realm of natural language processing, achieving outstanding success. Their consistent and remarkable performance spans a wide array of tasks (Brown et al., 2020b;Chung et al., 2022;Touvron et al., 2023a;b;Rozi\u00e8re et al., 2023;OpenAI, 2023;Anil et al., 2023). For a long time, pruning has been identified as a powerful technique for reducing the size or complexity of a model by removing unnecessary or redundant components (LeCun et al., 1989;Hassibi et al., 1993a). Pruning can be divided into structured and unstructured pruning. Structured pruning targets at removing a set of weights from a network at once such as channels or layers to reduce the model size and complexity while maintaining the network structure intact. In the realm of pruning LLMs, several studies (Frantar & Alistarh, 2022;2023;Sun et al., 2023) have been undertaken in this area. Our work provides a unique angle from gradient along this direction.",
            "score": 0.5501365536577495,
            "section_title": "Related Work",
            "char_start_offset": 22924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1054
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 85,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 553,
                    "end": 573,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 573,
                    "end": 595,
                    "matchedPaperCorpusId": "61815367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275634765625
        },
        {
            "corpus_id": "263620656",
            "title": "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models",
            "text": "the expense of model performance. Unstructured pruning (Sun et al., 2023a;Frantar & Alistarh, 2023), on the other hand, can be advantageous in preserving performance even with high model sparsity given AI acceleration software or sparse matrix computation schemes (Han et al., 2016;Mishra et al., 2021;Das & Ramamoorthy, 2022;NeuralMagic, 2022). Recently, Frantar & Alistarh (2023) suggest a one-shot pruning technique, SparseGPT, for generated pre-trained transformers (GPTs) in an unstructured manner. They newly employ a sparse regression solver that prunes weights at each layer based on row-wise Hessian reconstruction as formulated by a closed-form solution. Wanda (Sun et al., 2023a) proposes a magnitude-based unstructured pruning approach for large language models (LLMs). It promotes layer-wise weight sparsification based on the importance, computed by multiplying weights and input activations. However, these SoTA pruning methods rely on the pre-defined pruning ratio that all layers resort to the same sparsity, restricting the upper bound on model compression. In addition, their methods are tailored to language models without concern for different modalities. On the other hand, our proposed method allows adaptive pruning at each layer without heavy computation of global gradients. Further, to the best of our knowledge, we propose a first unified sparse approximate solver for vision-language multimodal models. \n\nTransformers for vision-language multimodal learning. Vision-language multimodal learning has shown remarkable achievement on various tasks, such as classification (Liu et al., 2018;Liang et al., 2022a), retrieval (Fei et al., 2021), few-shot learning (Tsimpoukelli et al., 2021;Alayrac et al., 2022), visual QA (Kim et al., 2016;Liu et al., 2023), and image/video generation (Zhou et al., 2022b;Singer et al., 2022;Lee et al., 2023).",
            "score": 0.5498421279097743,
            "section_title": "INTRODUCTION",
            "char_start_offset": 5604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 282,
                    "matchedPaperCorpusId": "1663491"
                },
                {
                    "start": 302,
                    "end": 326,
                    "matchedPaperCorpusId": "228372945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "266163403",
            "title": "Neural Architecture Codesign for Fast Bragg Peak Analysis",
            "text": "Structured pruning removes weights associated to larger structures in the network, such as neurons, channels, or attention heads, while unstructured pruning removes individual weights with no specific structure requirements. Unstructured pruning leads to sparse matrices, which can often limit gains in actual inference speed on GPUs despite the reduced number of parameters. However, on more versatile or flexible hardware like FPGAs and CPUs, unstructured pruning can provide significant speed up with a negligible drop in performance. While structured pruning provides definitive speed ups on general hardware, it can lead to a larger decline in performance. To get around this, newer hardware supports N : M or mixed sparsity (Zhang et al. 2022), such as Nvidia's A100 that supports 2:4 sparsity, which alleviates the need to prune entire rows or columns as done with dropping neurons or filters. Therefore, the choice of pruning algorithm is closely tied to the target hardware for deployment. \n\nResearchers also employ quantization to reduce the number of bits needed to represent the weights or activations. Like pruning, quantization can be done post-training or during training with quantization-aware training (QAT) (Coelho et al. 2021). With QAT, the weights are quantized on the forward pass, but use full-precision gradients on the backward pass, allowing for further fine-tuning of the low-bit representations. The effectiveness of quantization is heavily dependent on hardware support of low-bit data types. This framework aims to eventually support deployment on FP-GAs, which can support sparse operations and a wide range of reduced precision data types, thus unstructured pruning with sub-8 bit QAT is used.",
            "score": 0.5483924371178928,
            "section_title": "Related Work",
            "char_start_offset": 4622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1726
                }
            ],
            "ref_mentions": [
                {
                    "start": 730,
                    "end": 749,
                    "matchedPaperCorpusId": "249642429"
                },
                {
                    "start": 1226,
                    "end": 1246,
                    "matchedPaperCorpusId": "235490171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46875
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "The detailed results are presented in Table 3. Notably, the most effective pruning model was obtained using the default configuration of FLAP , achieving a perplexity of 31.80. The AL-AM global adaptive compression structure consistently outperformed other configurations under all evaluated pruning metrics, thereby effectively validating our proposed Adaptive Structure Search strategy. When analyzing the effectiveness of different global compression structures, we observed that various metrics present distinct strengths and weaknesses. Nevertheless, our proposed WIFV structured pruning metric displayed superior adaptability to the global compression structure. \n\nBaseline Bias Compensation. In structured pruning of large language models, restoring model performance after the pruning process is a crucial aspect. Our approach uniquely leverages bias compensation as a strategy to re- cover the performance of pruned models, circumventing the need for expensive and time-consuming retraining procedures. Figure 5 vividly illustrates the performance of the FLAP method on the WikiText2 dataset, comparing the perplexity scores with and without bias compensation at varying pruning ratios for the LLaMA-7B model. Evident from the figure, bias compensation plays a significant role in mitigating the performance degradation associated with pruning. Furthermore, this compensatory effect becomes more pronounced as the pruning ratio increases, highlighting the growing importance of bias compensation in more aggressively pruned models. \n\nRobustness to Calibration Samples. Our method utilizes a calibration dataset to estimate the input variance at each layer of the language model. This makes it critical to investigate the impact of the size of this calibration dataset on the pruning performance. Figure 6 delineates the effects of varying the number of calibration samples on the pruning outcome. For this analysis, we set a pruning ratio of 50% for the LLaMa-7B model and observed the resultant perplexity on the WikiText2 dataset.",
            "score": 0.547655927393962,
            "section_title": "Ablation Study",
            "char_start_offset": 20400,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2041
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.487060546875
        },
        {
            "corpus_id": "266550821",
            "title": "Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention",
            "text": "Sparsity-driven techniques, often associated with model pruning, form an energetic subset of research primarily in the pursuit of model compression. At their core, these methods focus on the elimination of less influential neurons while retaining the more critical ones, thereby sustaining optimal model performance (LeCun, Denker, and Solla 1990a;Han, Mao, and Dally 2016;Han et al. 2015;LeCun, Denker, and Solla 1990b;Liu et al. 2017;He, Zhang, and Sun 2017;Zhou, Alvarez, and Porikli 2016). Contemporary research has shed light on the heightened robustness of pruned models against adversarial conditions, such as overfitting and distribution shifts. Typical pruning methods for language models encompass structured pruning (Michel, Levy, and Neubig 2019), fine-grained structured pruning (Lagunas et al. 2021), and unstructured pruning (Gale, Elsen, and Hooker 2019). In brief, unstructured pruning removes individual weights in a network, leading to a sparse matrix, structured pruning eliminates entire structures like neurons or layers for a dense model, while fine-grained structured pruning prunes smaller structures like channels or weight vectors, offering a balance between the previous two. We direct the readers to the benchmark (Liu et al. 2023) for a comprehensive overview. In our case, we focus on unstructured pruning for its effectiveness and better interpretability. \n\nRecently, studies have underscored the interpretability afforded by sparse networks (Subramanian et al. 2018). For instance, Meister et al. (2021) delve into the interpretability of sparse attention mechanisms in language models, Liu et al. (2022) incorporate sparse contrastive learning in an ancillary sparse coding layer to facilitate word-level interpretability, and Oikarinen et al. (2023) demonstrate that a sparsity constraint on the final linear predictor enhances concept-level interpretation of CBMs. Despite their effectiveness, these frameworks restrict sparsity to a handful of layers, leading to unidimensional interpretability that falls short of the desired comprehensiveness.",
            "score": 0.5474361687616677,
            "section_title": "Sparsity Mining for Language Models",
            "char_start_offset": 7613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 373,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 436,
                    "end": 460,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 460,
                    "end": 492,
                    "matchedPaperCorpusId": "14666124"
                },
                {
                    "start": 1474,
                    "end": 1499,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 1620,
                    "end": 1637,
                    "matchedPaperCorpusId": "256631032"
                },
                {
                    "start": 1761,
                    "end": 1784,
                    "matchedPaperCorpusId": "13029170"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328369140625
        },
        {
            "corpus_id": "276107759",
            "title": "AdaSVD: Adaptive Singular Value Decomposition for Large Language Models",
            "text": "Recent advancements in model compression techniques have significantly enhanced the efficiency of deploying LLMs while maintaining their performance. Widely explored approaches include weight quantization [11,22], network pruning [1,10,14,25,41], and hybrid methods [8]. In unstruc- X \u2190 GET CALIB(C ) \u25b7 Randomly collect samples as calibration data 5: \n\n\u25b7 Shuffle samples and utilize stack-of-batch (SOB) strategy 6: \n\nSet S \u2190 WHITENING(M, X \u2032 ), Set SVD \u2190 \u2205, Set W \u2190 M \u25b7 Initialize sets of decomposed matrices and weights 7: \n\nSet CR \u2190 LAYER CR(M, X \u2032 , trr, mrr) \u25b7 Calculate layerwise importance and compression ratio 8: \n\nfor layer i in language model M do 9: \n\n\u25b7 Extract the whitening matrix of current weight W i 10: \n\n\u25b7 Apply adaptive compression ratio and truncation 12: \n\nend for 14: \n\nreturn M \u2032 16: end procedure tured pruning, SparseGPT [10] prunes weights based on their importance, as determined by the Hessian matrix. However, it faces challenges in achieving optimal speedup, particularly due to hardware compatibility issues. Structured pruning methods, in contrast, are more hardware-friendly. LLM-Pruner [25] selectively removes non-critical coupled structures using gradient information. LaCo [41] introduces a layer-wise pruning strategy, where subsequent layers collapse into preceding ones. Gromov et al. [14] explores the effectiveness of basic layer-pruning techniques combined with parameter-efficient fine-tuning (PEFT). Additionally, SliceGPT [1] has pioneered post-training sparsification, emphasizing the importance of layer removal order for optimal performance. Quantization techniques offer another significant avenue for compression. GPTQ [11] applies layer-wise quantization and reduces quantization errors through secondorder error compensation. AWQ [22] introduces activationaware weight quantization, employing a scale transformation between weights and activations. Moreover, BiLLM [19] and ARB-LLM [21] achieve further compression to 1-bit while maintaining remarkable performance.",
            "score": 0.5474078033498295,
            "section_title": "LLM Compression Techniques",
            "char_start_offset": 5054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2019
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 209,
                    "end": 212,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "268692211"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "267751181"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1211,
                    "end": 1215,
                    "matchedPaperCorpusId": "267751181"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "268692211"
                },
                {
                    "start": 1469,
                    "end": 1472,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 1671,
                    "end": 1675,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "258999941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "278602414",
            "title": "Onboard Optimization and Learning: A Survey",
            "text": "Hardware-Aware NAS [41][42][43] Auto-optimized for edge devices. \n\nNAS Compression Challenges [44,45] High search cost, memory use. \n\nFew-Shot NAS [46,47] Fast, memory-efficient architecture search. \n\npredefined pruning rate. Selecting an optimal pruning rate is critical to maintaining accuracy while minimizing computational overhead. Unstructured vs. Structured Pruning. Pruning can be broadly classified into unstructured and structured approaches [16]. Unstructured pruning removes individual weights across the network, leading to sparse weight matrices that can improve efficiency when supported by dedicated accelerators. However, since standard deep learning frameworks and hardware do not efficiently support sparse computation, unstructured pruning often requires specialized hardware or software libraries for performance gains. In contrast, structured pruning removes entire groups of weights, such as neurons, channels, or filters, reducing model dimensions in a way that aligns well with conventional deep learning frameworks. Structured pruning is generally more suitable for onboard learning, as it produces a compact, hardware-friendly model compatible with edge devices [17,18]. \n\nPruning at Different Stages. Pruning can be applied at various points in the model lifecycle: before training (pruning at initialization), during training (dynamic pruning), or after training (post-training pruning). Each stage offers distinct trade-offs in terms of computational efficiency and model accuracy. \n\nPruning at Initialization. Pruning at initialization removes unnecessary parameters before training begins, reducing model complexity and enabling efficient training on resource-constrained devices [17,[48][49][50]. A common approach is layerwise pruning, where entire layers or channels are removed based on their contribution to model performance [17,18,51,52]. \n\nSeveral methods have been proposed to enhance pruning at initialization. SynFlow prevents layer collapse by selectively pruning while maintaining model trainability [19]. Single-shot network pruning (SNIP) assigns sensitivity scores to each weight and removes unimportant ones before training, ensuring critical connections are preserved [20,21].",
            "score": 0.5470404089524404,
            "section_title": "Neural Architecture Search",
            "char_start_offset": 5865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 67,
                    "end": 131
                },
                {
                    "start": 134,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1877
                },
                {
                    "start": 1880,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 23,
                    "matchedPaperCorpusId": "234004712"
                },
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "253097864"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "208268598"
                },
                {
                    "start": 151,
                    "end": 154,
                    "matchedPaperCorpusId": "269419038"
                },
                {
                    "start": 1189,
                    "end": 1193,
                    "matchedPaperCorpusId": "229190662"
                },
                {
                    "start": 1193,
                    "end": 1196,
                    "matchedPaperCorpusId": "235306148"
                },
                {
                    "start": 1712,
                    "end": 1716,
                    "matchedPaperCorpusId": "229190662"
                },
                {
                    "start": 1716,
                    "end": 1720,
                    "matchedPaperCorpusId": "261706173"
                },
                {
                    "start": 1724,
                    "end": 1728,
                    "matchedPaperCorpusId": "203592134"
                },
                {
                    "start": 1863,
                    "end": 1867,
                    "matchedPaperCorpusId": "229190662"
                },
                {
                    "start": 1867,
                    "end": 1870,
                    "matchedPaperCorpusId": "235306148"
                },
                {
                    "start": 1870,
                    "end": 1873,
                    "matchedPaperCorpusId": "257496775"
                },
                {
                    "start": 1873,
                    "end": 1876,
                    "matchedPaperCorpusId": "249297809"
                },
                {
                    "start": 2045,
                    "end": 2049,
                    "matchedPaperCorpusId": "219558821"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "247318780",
            "title": "Data-Efficient Structured Pruning via Submodular Optimization",
            "text": "It is also possible to combine different compression approaches to compound their benefits, see e.g., [Kuzmin et al., 2019, Section 4.3.4]. \n\nExisting pruning methods fall into two main categories: unstructured pruning methods which prune individual weights leading to irregular sparsity patterns, and structured pruning methods which prune regular regions of weights, such as neurons, channels, or attention heads. Structured pruning methods are generally preferable as the resulting pruned models can work with off-the-shelf hardware or kernels, as opposed to models pruned with unstructured pruning which require specialized ones. \n\nThe majority of existing structured pruning methods are heuristics that do not offer any theoretical guarantees. Moreover, most pruning methods are inapplicable in the limited-data regime, as they rely on fine-tuning with large training data for at least a few epochs to recover some of the accuracy lost with pruning. Mariet and Sra [2015] proposed a \"reweighting\" procedure applicable to any pruning method, which optimize the remaining weights of the next layer to minimize the change in the input to the next layer. Their empirical results on pruning single linear layers suggest that reweighting can provide a similar boost to performance as fine-tuning, without the need for data labels. \n\nOur contributions We propose a principled data-efficient structured pruning method based on submodular optimization. In each layer, our method simultaneously selects neurons to prune and new weights for the next layer, that minimize the change in the next layer's input induced by pruning. The optimization with respect to the weights, for a fixed selection of neurons, is the same one used for reweighting in [Mariet and Sra, 2015]. The resulting subset selection problem is intractable, but we show that it can be formulated as a weakly submodular maximization problem (see Definition 2.1). We can thus use the standard greedy algorithm to obtain a (1 \u2212 e \u2212\u03b3 )-approximation to the optimal solution, where \u03b3 is non-zero if we use sufficient training data.",
            "score": 0.5457373379675039,
            "section_title": "Introduction",
            "char_start_offset": 1712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2089
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "269741380",
            "title": "Pruning as a Domain-specific LLM Extractor",
            "text": "Model compression involves transforming a large, resource-intensive model into a compact version suitable for low-resource deployment (Deng et al., 2020;Zhu et al., 2023).There are mainly three techniques for model compression, which are pruning, knowledge distillation, and quantization.\n\nPruning.Pruning techniques in neural networks can be broadly classified into structured pruning and unstructured pruning (Xia et al., 2022;Sanh et al., 2020;Du et al., 2021).Structured pruning entails the removal of entire network components, such as channels or layers, guided by specific criteria, while maintaining the overall network architecture.In contrast, unstructured pruning targets individual weights, leading to an irregular sparse structure.\n\nWhile numerous attempts have been made to prune language models of relatively small scales, such as BERT (Kenton and Toutanova, 2019), scant attention has been devoted to pruning LLMs containing billions of parameters.These larger models possess 100-1000 times more weights, rendering the pruning task significantly more challenging.SparseGPT (Frantar and Alistarh, 2023), a post-training method for Large Language Models (LLMs), lacks the capability to identify crucial weights tailored to specific domains or tasks as it refrains from fine-tuning.On the other hand, LLM-Pruner (Ma et al., 2023) employs gradient-based techniques for pruning.However, it falls short in identifying pivotal weights essential for domainshared knowledge, resulting in pruned models that lack the desired level of generality.\n\nThe existing pruning methods either focus on general or domain-specific weights, yet none of them consider preserving both at the same time.\n\nTo the best of our knowledge, we are the first to work on pruning LLMs while preserving weights important to both generality and specificity.\n\nKnowledge Distillation.Knowledge Distillation (KD) has emerged as a powerful technique, drawing considerable interest for its ability to augment model performance and enhance generalization capacities (Hinton et al., 2015;Zhu et al., 2023).",
            "score": 0.5453880237540467,
            "section_title": "Related Work",
            "char_start_offset": 5200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 288
                },
                {
                    "start": 290,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 641
                },
                {
                    "start": 641,
                    "end": 744
                },
                {
                    "start": 746,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1079
                },
                {
                    "start": 1079,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1551
                },
                {
                    "start": 1553,
                    "end": 1693
                },
                {
                    "start": 1695,
                    "end": 1836
                },
                {
                    "start": 1838,
                    "end": 1861
                },
                {
                    "start": 1861,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 153,
                    "matchedPaperCorpusId": "215799572"
                },
                {
                    "start": 411,
                    "end": 429,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 429,
                    "end": 447,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 1089,
                    "end": 1117,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "270870796",
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "text": "Pruning techniques remove redundant parameters and connections in a model to enhance computational efficiency and reduce model size.Pruning techniques can be categorized into unstructured pruning, structured pruning, and hybrid pruning.\n\nUnstructured pruning operates at a fine granularity, removing arbitrary \"redundant\" parameters in the network.However, this method may result in irregular network structures that are difficult to accelerate effectively.LeCun proposed the Optimal Brain Damage (OBD) algorithm [39] in the late 1980s, using the second-order derivatives of the loss function to determine parameter importance.Hassibi et al. [40], [41] extended this with the Optimal Brain Surgeon (OBS) algorithm, not limited by OBD's diagonal assumption, zeroing out less important weights and recalculating others to compensate for activation values, achieving better compression results.Srinivas et al. [42] proposed methods to remove dense connections in fully connected layers without relying on training data, significantly reducing computational complexity.\n\nStructured pruning removes structural components based on predefined criteria, such as attention heads or layers.X-Pruner [43] utilizes explainable masks learned end-to-end, measuring each unit's contribution to predicted target classes, and adaptively searches layer-wise thresholds to retain the most informative units while determining pruning rates.\n\nHybrid pruning combines unstructured and structured pruning methods, balancing their advantages for better performance optimization.For example, SPViT [44] developed a multihead token selector based on dynamic attention for adaptive instance-level token selection, introducing a soft pruning technique that merges less important tokens into packet tokens rather than discarding them.ViT-Slim [45] introduced learnable and unified sparsity constraints with predefined factors to represent global importance within various dimensions.\n\nThese pruning techniques demonstrate significant effectiveness in various applications.In image classification tasks, structured pruning can significantly reduce convolutional neural networks' computational costs while maintaining high classification accuracy.In natural language processing tasks, unstructured and hybrid pruning effectively reduce Transformer model complexity, enabling inference and training with lower resource consumption.",
            "score": 0.5451773417746657,
            "section_title": "B. Model Pruning",
            "char_start_offset": 22529,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 236
                },
                {
                    "start": 238,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 627
                },
                {
                    "start": 627,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1065
                },
                {
                    "start": 1067,
                    "end": 1420
                },
                {
                    "start": 1422,
                    "end": 1554
                },
                {
                    "start": 1554,
                    "end": 1805
                },
                {
                    "start": 1805,
                    "end": 1954
                },
                {
                    "start": 1956,
                    "end": 2043
                },
                {
                    "start": 2043,
                    "end": 2216
                },
                {
                    "start": 2216,
                    "end": 2399
                }
            ],
            "ref_mentions": [
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 642,
                    "end": 646,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 648,
                    "end": 652,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 1189,
                    "end": 1193,
                    "matchedPaperCorpusId": "257427497"
                },
                {
                    "start": 1573,
                    "end": 1577,
                    "matchedPaperCorpusId": "245537400"
                },
                {
                    "start": 1814,
                    "end": 1818,
                    "matchedPaperCorpusId": "258509611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.484375
        },
        {
            "corpus_id": "258015063",
            "title": "Benchmarking Neural Network Compression Techniques for Ocular-Based User Authentication on Smartphones",
            "text": "Tables 9 and 10 show the performance of subject-independent analysis of the PTQ technique applied on the pruned version of ResNet-50, VGG-13, and VGG-11 models. L2-norm-based channel structured pruning and Layerwise Gradient Magnitude-based unstructured pruning were applied for model compression. Overall, the combination of unstructured pruning and quantization obtained a lower EER score compared to structured pruning with quantization. Mostly, the average EER, GMR, and AUC of standalone pruned and those quantized after pruning, i.e., TABLE 9. Structured Pruning-Quantization: Subject-independent analysis of PTQ-based quantization applied on the pruned models using L2-norm-based channel pruning method on ResNet-50, VGG-13 and VGG-11 models using UFPR [25] and VISOB 2.0 [21] datasets. \n\nthe Pruning-Quantization combination was equivalent. For instance, ResNet-50 obtained no change across EERs (an average of 9.39%) between the standalone pruned using structured pruning and those quantized after being pruned across the compression rates. The average size of the combination of pruned and quantized models was 1 MB smaller than the model pruned using the unstructured pruning technique. Recall that unstructured pruning does not have any impact on the model size. The combination of quantization and structured pruning on VGG-11 and VGG-13 models obtained an increased EER of 0.11% over the standalone pruned model using structured pruning, while the size of the TABLE 10. Unstructured Pruning-Quantization: Subject-independent analysis of the PTQ-based quantized models after using layer-wise magnitude-based unstructured pruning on ResNet-50, VGG-13, and VGG-11 models using UFPR [25] VISOB 2.0 [21] and datasets. combination of pruned and quantized models decreased by about 0.44 MB.",
            "score": 0.5444133525229697,
            "section_title": "F. EVALUATION OF THE COMBINATION OF PRUNING AND QUANTIZATION",
            "char_start_offset": 45173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 779,
                    "end": 783,
                    "matchedPaperCorpusId": "232022597"
                },
                {
                    "start": 1708,
                    "end": 1712,
                    "matchedPaperCorpusId": "232022597"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.136962890625
        },
        {
            "corpus_id": "256662734",
            "title": "What Matters In The Structured Pruning of Generative Language Models?",
            "text": "Among the various methods, Block Movement Pruning (Lagunas et al., 2021) is a recent and popular technique that removes weight blocks based on movement. However, there is a lack of systematic research on structured pruning for decoder-only architectures such as GPT-2 Radford et al. (2019), GPT-3 Brown et al. (2020), or GPT-Neo Black et al. (2021), which are mainly used for natural language generation (NLG) tasks, such as text summarization, machine translation, or text completion. While there are some works that apply unstructured pruning (Li et al., 2022) or many kinds of orthogonal compression techniques to decoder-only LLMs (Wang et al., 2020a;Li et al., 2021;Edalati et al., 2022;Tao et al., 2022;Xu & Hu, 2022;Chen et al., 2021), there is no comprehensive evaluation of traditional structured pruning for these models on NLG tasks. \n\nIn this work, we compress decoder-only auto-regressive language models. Due to the lack of prior literature towards the same goal, we evaluate the performance of several general-domain pruning methods on NLG tasks, including magnitude and movement pruning. However, we find these methods can struggle or under-perform compared to na\u00efve baselines, leading to the following question: \n\nWhat determines the performance of structured pruning on generative language models? \n\nWe aim to fill this gap by conducting a systematic study of structured fine-pruning (pruning while finetuning) methods for decoder-only LLMs on NLG tasks1 , and further proposing a novel method that combines the strengths of different existing methods. Our main contributions are: \n\n\u2022 To our knowledge, we perform the first systematic evaluation of several structured pruning methods to decoder-only LLMs on NLG tasks. However, we find that these established methods only achieve marginal improvements over a simple baseline in which we randomly prune neurons during the finetuning process, which is vastly different from the cases of pruning BERT-like models.",
            "score": 0.5440266410585872,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 671,
                    "end": 692,
                    "matchedPaperCorpusId": "239009526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43994140625
        },
        {
            "corpus_id": "277244595",
            "title": "Temporal Action Detection Model Compression by Progressive Block Drop",
            "text": "Model compression techniques are typically categorized into pruning [21], knowledge distillation [24], model decomposition [10], and quantization [31]. This paper focuses on model pruning, which can be either unstructured [21] or structured [35]. Unstructured pruning removes network connections to create sparsity, but it requires specialized hardware for sparse matrix operations [20,21,56], limiting its practical deployment. Structured pruning, which includes width-reduction methods like channel pruning [22,35] and filter pruning [23], results in a dense weight matrix, enabling GPU acceleration. However, it may not fully exploit GPU parallelism [40]. \n\nDepth-wise pruning techniques [26,58] have been used for model acceleration, although they may suffer from insufficient memory occupancy. Various methods, including layer importance evaluation [8,13], parameter optimization [66,75], and reparameterization [12,16], can also guide pruning to compress the model from the perspective of depth. Previous work [39,72] has explored the concept of progressive block dropping; however, there are significant differences between their approaches and ours. Specifically, [39] focuses on compressing and integrating the layers within the block, which is a layer-level pruning technique. The dynamically skipping blocks method proposed in [72] accelerates training by dropping blocks, but during inference, the full model is still employed. Additionally, skipping blocks to reduce the depth of Transformer models [11,47] often leads to a loss in accuracy. In contrast, our progressive drop block method reduces depth at the block level, improving both inference speed and detection accuracy. Our method can be combined with structured pruning [14] and unstructured activation sparsity [33,48] to further compress the model and accelerate inference while improving performance.",
            "score": 0.5440069220434869,
            "section_title": "Model Compression",
            "char_start_offset": 7464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1875
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 97,
                    "end": 101,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "7340116"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 222,
                    "end": 226,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 389,
                    "end": 392,
                    "matchedPaperCorpusId": "18053655"
                },
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "102350938"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "202558809"
                },
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "51891697"
                },
                {
                    "start": 857,
                    "end": 860,
                    "matchedPaperCorpusId": "220496508"
                },
                {
                    "start": 889,
                    "end": 892,
                    "matchedPaperCorpusId": "232065185"
                },
                {
                    "start": 921,
                    "end": 924,
                    "matchedPaperCorpusId": "249282255"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "266977311"
                },
                {
                    "start": 1020,
                    "end": 1023,
                    "matchedPaperCorpusId": "225066769"
                },
                {
                    "start": 1172,
                    "end": 1176,
                    "matchedPaperCorpusId": "266977311"
                },
                {
                    "start": 1338,
                    "end": 1342,
                    "matchedPaperCorpusId": "225066769"
                },
                {
                    "start": 1512,
                    "end": 1516,
                    "matchedPaperCorpusId": "232134936"
                },
                {
                    "start": 1516,
                    "end": 1519,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1742,
                    "end": 1746,
                    "matchedPaperCorpusId": "256390345"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "225629735"
                },
                {
                    "start": 1788,
                    "end": 1791,
                    "matchedPaperCorpusId": "263830421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.712890625
        },
        {
            "corpus_id": "256846987",
            "title": "Workload-Balanced Pruning for Sparse Spiking Neural Networks",
            "text": "In this work, we target to solve the workload imbalance problem associated with unstructured pruning. In contrast to unstructured pruning, structured pruning has also been a popular network compression method [52], [58], [59]. In structured pruning, the networks are pruned in a pattern that aims to leverage the hardware's power to process the pruned networks more efficiently. The nature of the structured pruning does not make it suffer from the workload imbalance problem we discussed in this work. Although the structured pruning methods take advantage of efficient hardware processing, they do suffer from a relatively lower weight sparsity. For example, on the VGG-16 network, the structured pruning, on average, achieves around 85% weight sparsity, while our LTH-based unstructured pruning gets over 95% weight sparsity.",
            "score": 0.5437921849373293,
            "section_title": "A. Comparison with Structured Pruning",
            "char_start_offset": 31040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 828
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 215,
                    "end": 219,
                    "matchedPaperCorpusId": "249205107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6181640625
        },
        {
            "corpus_id": "267312283",
            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
            "text": "However, pruning decoder-only models is much more difficult than encoder-only models. For instance, structured pruning algorithms [77,134] for GPT-2 show performance degradation at a low compression rate smaller than 50% while those [77,82,155] for BERT succeed in pruning up to 75% of weights with minimal accuracy degradation. Structured pruning approaches [105] for billion-scale LLMs show harsher accuracy degradation at a low compression rate of 20% since the excessive size of LLMs limits the use of fine-tuning processes. SparseGPT [40] demonstrates relatively better performance using an unstructured pruning pattern and expands its algorithm to cover 2:4 pruning granularity which supports acceleration on customized hardwares; 2:4 pruning granularity represents a semi-structured pruning pattern that there are 2 zeros in 4 consecutive elements. Improving the accuracy of structured pruning with large granularities for decoder-only models is a promising future work.",
            "score": 0.5436426302042634,
            "section_title": "\ud835\udc40 (\ud835\udc4b",
            "char_start_offset": 25637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 977
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 138,
                    "matchedPaperCorpusId": "259858812"
                },
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 240,
                    "end": 244,
                    "matchedPaperCorpusId": "247922354"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "267312283",
            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
            "text": "After selecting the granularity for pruning, we need to determine the strategy for pruning which includes methodologies of identifying unnecessary components and compensating for pruning errors induced by pruning of the identified components. We categorize pruning algorithms into two groups of high-cost [47,56,73,76,77,82,97,100,120,122,134,155,159,162] and low-cost [40,81,105,110,112,129,163] algorithms according to the cost of pruning. We give an overview of highcost and low-cost pruning algorithms in Fig. 6 where the size of datasets and the running time are based on the pruning algorithms [56,112] [103] for details). FLOP [146] utilizes the stochastic mask variables for structured pruning of language models and substitutes the  0 regularization with an augmented Lagrangian to ensure the compressed model has a preset sparsity. CoFi [155] achieves both high accuracy and fast inference speed via jointly pruning coarse and fine-grained granularities using an augmented Lagrangian in FLOP [146]. \n\nGiven a weight vector  and an input vector , OBS [52] algorithm finds an optimal weight-update  () after pruning th weight   that minimizes the amount || \u2212 \u0175 || 2 2 of a pruning error where \u0175 =  +  () is a vector of weights after pruning. OBS is applied to language model compression by formulating the compression problem as a sublayer-wise output reconstruction problem after pruning [40,77]. OBS is crucial for pruning PLMs since OBS-based algorithms demonstrate the state-of-the-art performance in both settings of high-cost [76,77] and low-cost pruning algorithms [40]. oBERT [76] modifies the objective function of OBS into the cross-entropy and proposes an efficient algorithm to approximate the inverse of Hessian which is the computational bottleneck of OBS.",
            "score": 0.5410353885333906,
            "section_title": "Pruning Strategies: High-cost vs. Low-cost",
            "char_start_offset": 26661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1778
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "219792793"
                },
                {
                    "start": 315,
                    "end": 318,
                    "matchedPaperCorpusId": "247446572"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "222134166"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "236477807"
                },
                {
                    "start": 331,
                    "end": 335,
                    "matchedPaperCorpusId": "251005814"
                },
                {
                    "start": 339,
                    "end": 343,
                    "matchedPaperCorpusId": "259858812"
                },
                {
                    "start": 343,
                    "end": 347,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "245131431"
                },
                {
                    "start": 373,
                    "end": 376,
                    "matchedPaperCorpusId": "248266822"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "257404900"
                },
                {
                    "start": 634,
                    "end": 639,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 847,
                    "end": 852,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1002,
                    "end": 1007,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 1540,
                    "end": 1544,
                    "matchedPaperCorpusId": "247446572"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34228515625
        },
        {
            "corpus_id": "268253513",
            "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect",
            "text": "To validate the efficacy of our proposed method, we conducted comparative experiments against baseline techniques commonly employed in large language model evaluation. \n\nConsidering the current structured pruning methods generally reduce parameters by no more than 30%, we performed experiments with approximately 1/4 of the parameters pruned. The experimental results are presented in Table 2. Additional experiments exploring different parameter reduction proportions will be discussed in the subsequent section. \n\nThe results demonstrate that the performance of the model pruned by our method significantly surpasses that of the baseline methods, maintaining most of the large language model's capabilities. Furthermore, we note that the approach of reducing the number of layers (ShortGPT/LaCo) outperforms the method of reducing the embedding dimensions (LLMPru./SliceGPT), implying that the model exhibits more redundancy in depth than in width. Further experimental analysis will be presented in the ensuing section. \n\nIn Table 2, we fully adopted the benchmark, model, and pruning ratio in the LaCo paper. \n\nIn order to make a more fair comparison with LLMprun. and SliceGPT, we compared them with the same benchmark, model, and pruning ratio in their original paper. The experimental results are shown in Appendix C. Consistent with our findings in Table 2, these experiments further demonstrate the significant layer redundancy present in existing large language models, and ShortGPT achieves superior performance compared to other pruning methods. \n\nThe results show that coarse-grained pruning methods, such as removing entire layers, often outperform fine-grained approaches like Slice GPT or LLM Pruner. We speculate that the reason is that the large language model is actually very robust, as shown in Figure 1, removing any deep layer individually actually has very little impact on the final output, which means it is difficult to define the importance of a finer grained module and perform pruning.",
            "score": 0.5409945738859531,
            "section_title": "Main Results",
            "char_start_offset": 10694,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 514
                },
                {
                    "start": 517,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 2016
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328125
        },
        {
            "corpus_id": "272689601",
            "title": "ELSA: Exploiting Layer-wise N:M Sparsity for Vision Transformer Acceleration",
            "text": "In recent years, transformer-based neural networks have been modified for artificial intelligence tasks that include not only natural language processing but also computer vision, such as image classification [5,32,18] and object detection [3,40]. Consisting of a series of transformer blocks that can effectively capture dependencies between patches in a given image, transformer-based neural networks demonstrate outstanding performance on vision tasks and replace convolutional neural networks (CNNs) as the state-of-the-art. For example, DeiT-B can achieve a Top-1 accuracy of 81.8% on the ImageNet dataset by utilizing 12 transformer blocks and 17.6G parameters. However, it is challenging to deploy such huge models on smartphones or embedded devices, given their limited memory budget and computational resources. \n\nVarious model compression methods have been proposed to reduce the requirements of memory usage and computational cost for model inference, such as quantization [19,17,36] and pruning/sparsifying [10,34,14,37,4]. To maintain the application accuracy, unstructured pruning methods [10,8,25,6] have been presented to remove neurons or connections from a deep neural network (DNN) that do not significantly impact accuracy and only retain important weights for computation. On the other hand, the remaining data in the weight matrix are irregularly distributed and require a high cost of encoding/indexing which induces overhead when these compressed data are sent from memory to processing units. In contrast, structured pruning methods [23,34,14] remove rows, columns, or N:M sparsity Given N & M Find a better sparse mask [15,38] Training algorithm [24,39,21] Find N & M [31] Figure 1: Three categories of methodologies for N :M semi-structured pruning channels that do not significantly impact the accuracy. Although structured pruning methods have little or no memory overhead on data encoding, the application accuracy decreases dramatically when a larger compression ratio is applied for reducing the model size.",
            "score": 0.5399273628974601,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 212,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 243,
                    "end": 246,
                    "matchedPaperCorpusId": "222208633"
                },
                {
                    "start": 984,
                    "end": 988,
                    "matchedPaperCorpusId": "235658553"
                },
                {
                    "start": 991,
                    "end": 994,
                    "matchedPaperCorpusId": "244527659"
                },
                {
                    "start": 1019,
                    "end": 1023,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1023,
                    "end": 1026,
                    "matchedPaperCorpusId": "250294994"
                },
                {
                    "start": 1026,
                    "end": 1029,
                    "matchedPaperCorpusId": "245537400"
                },
                {
                    "start": 1032,
                    "end": 1034,
                    "matchedPaperCorpusId": "235367934"
                },
                {
                    "start": 1103,
                    "end": 1107,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1107,
                    "end": 1109,
                    "matchedPaperCorpusId": "251765570"
                },
                {
                    "start": 1109,
                    "end": 1112,
                    "matchedPaperCorpusId": "49310977"
                },
                {
                    "start": 1112,
                    "end": 1114,
                    "matchedPaperCorpusId": "208267757"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1562,
                    "end": 1565,
                    "matchedPaperCorpusId": "250294994"
                },
                {
                    "start": 1565,
                    "end": 1568,
                    "matchedPaperCorpusId": "245537400"
                },
                {
                    "start": 1645,
                    "end": 1649,
                    "matchedPaperCorpusId": "258987321"
                },
                {
                    "start": 1649,
                    "end": 1652,
                    "matchedPaperCorpusId": "249642429"
                },
                {
                    "start": 1679,
                    "end": 1682,
                    "matchedPaperCorpusId": "256503843"
                },
                {
                    "start": 1694,
                    "end": 1698,
                    "matchedPaperCorpusId": "245117261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.248291015625
        },
        {
            "corpus_id": "248299756",
            "title": "Multiply-and-Fire (MNF): An Event-driven Sparse Neural Network Accelerator",
            "text": "To reduce the overall storage needed to process deep neural network (DNN) workloads, several compression techniques [10,17,18,20,26,38] have been proposed and widely investigated in recent years. Pruning, in particular, has emerged as a promising compression technique for drastically reducing DNN model size and processing requirements while maintaining accuracy [4,18]. Pruning methods can be broadly classified as either unstructured or structured. Unstructured pruning techniques [18] remove neurons and weights with lower importance from wherever they exist. Although these methods achieve very high compression rates by removing nearly 95% of the model parameters, they produce irregular sparsity, meaning that zeros are distributed randomly across the model parameters [18]. Due to such irregular sparsity, it is difficult to reap the benefits of unstructured pruning methods through traditional hardware solutions such as CPUs, GPUs, TPUs, and ASIC DNN accelerators [6,9,13]. \n\nStructured pruning [4], on the other hand, entails removing a group of weights or neurons or an entire convolutional Proposal SCNN [31] Cambricon-S [43] Eyeriss-v2 [8] SNAP [41] GoSPA [12] This kernel or filter. Although structured pruning methods have not achieved the compression ratios of unstructured pruning, they produce regular sparsity, which means that zero values appear in groups [4]. When the sparsity is regular, it is relatively simple to reap the benefits using traditional hardware solutions [6,9,13]. \n\nIn addition to pruning, the rectified linear activation function (ReLU) used in DNN layers reduces DNN processing requirements further. Because all negative activations are treated as zero following ReLU, the number of activations that must be processed after ReLU can be significantly reduced. However, ReLU, like unstructured pruning, produces irregular sparsity [22]. Moreover, the sparsity is input-dependent, which means that the level of sparsity and the location of zeros varies with each input.",
            "score": 0.5399185918824732,
            "section_title": "BACKGROUND",
            "char_start_offset": 6411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 120,
                    "end": 123,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "2779809"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "7333079"
                },
                {
                    "start": 367,
                    "end": 370,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 484,
                    "end": 488,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 776,
                    "end": 780,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 974,
                    "end": 977,
                    "matchedPaperCorpusId": "207209696"
                },
                {
                    "start": 977,
                    "end": 979,
                    "matchedPaperCorpusId": "6838992"
                },
                {
                    "start": 979,
                    "end": 982,
                    "matchedPaperCorpusId": "11504619"
                },
                {
                    "start": 1005,
                    "end": 1008,
                    "matchedPaperCorpusId": "7333079"
                },
                {
                    "start": 1117,
                    "end": 1121,
                    "matchedPaperCorpusId": "1775161"
                },
                {
                    "start": 1134,
                    "end": 1138,
                    "matchedPaperCorpusId": "56174616"
                },
                {
                    "start": 1150,
                    "end": 1153,
                    "matchedPaperCorpusId": "131771552"
                },
                {
                    "start": 1159,
                    "end": 1163,
                    "matchedPaperCorpusId": "231735450"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "matchedPaperCorpusId": "235415831"
                },
                {
                    "start": 1377,
                    "end": 1380,
                    "matchedPaperCorpusId": "7333079"
                },
                {
                    "start": 1494,
                    "end": 1497,
                    "matchedPaperCorpusId": "207209696"
                },
                {
                    "start": 1497,
                    "end": 1499,
                    "matchedPaperCorpusId": "6838992"
                },
                {
                    "start": 1499,
                    "end": 1502,
                    "matchedPaperCorpusId": "11504619"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "225629735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5634765625
        },
        {
            "corpus_id": "276259010",
            "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
            "text": "The high accuracy of Transformer-based models on a wide range of tasks comes with massive computational requirements, which hinders deployability. A long line of research has been conducted to reduce the computational cost of large language models via methods such as quantization (Frantar et al., 2022;Dettmers et al., 2023), pruning (Xia et al., 2024;Frantar & Alistarh, 2023) and distillation (Hsieh et al., 2023). In this work, we explore structured pruning on large language models (LLMs) by removing whole rows or columns in a weight matrix, resulting in regular but smaller matrices. Unlike unstructured pruning (Frantar & Alistarh, 2023), the model produced by structured pruning can be accelerated on mainstream hardware without any specific design for computation. \n\nWhile conventional pruning methods generally prune each layer or block uniformly, non-uniform compression methods, e.g. (Yin et al., 2023;Sieberling et al., 2024), showed that different LLM layers can have significantly different \"sensitivities\" to pruning, which can be leveraged to obtain higher compression while preserving accuracy. To address this, smaller-scale pruning methods such as ZipLM (Kurti\u0107 et al., 2024) propose to utilize a dynamic-programmingbased search (Frantar & Alistarh, 2022) to obtain a sparse model with runtime guarantees. However, there are several challenges for methods such as ZipLM on large-scale models such as Llama-2-7B (Touvron et al., 2023): for instance, ZipLM only considers the local layer-wise error during the search, which is not consistent with performance on in-context learning (ICL) or downstream tasks. \n\nOverview. In this paper, we propose a new structured pruning method based on evolutionary search called DarwinLM, which addresses these issues. Specifically, the search starts from a \"parent\" model, generated by pruning the original model using second-order information.",
            "score": 0.5399151981622122,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1627
                },
                {
                    "start": 1630,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1900
                }
            ],
            "ref_mentions": [
                {
                    "start": 915,
                    "end": 939,
                    "matchedPaperCorpusId": "273482154"
                },
                {
                    "start": 1175,
                    "end": 1196,
                    "matchedPaperCorpusId": "247446572"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "267312283",
            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
            "text": "Unstructured pruning algorithms [76,122,159,162] have demonstrated high accuracy in compressing language models achieving up to 97% compression rate with a small accuracy degradation. However, their drawback lies in the acceleration difficulty due to the resulting sparse pruning patterns. Therefore, researchers have focused on extending the findings in unstructured pruning algorithms to structured pruning algorithms to achieve acceleration. For example, BMP [82] extends the findings of MvP [122] considering that the magnitude of gradient of an objective function with respect to a weight is more important than the magnitude of the weight. Similarly, ZipLM [77] achieves a good performance on structured pruning of BERT by developing the findings in oBERT [76] that the optimal error compensation strategy of OBS [52] is useful for pruning BERT. [129], *: includes token embeddings, \u2720: a compressed model \u2020: geometric mean of speedups in individual affine transformations When extending unstructured pruning algorithms to structured pruning algorithms, it is crucial to determine the granularity for pruning, considering the characteristics of the model. Michel et al. [107] experimentally demonstrate that MHA sublayers are robust to the pruning of attention heads by showing that approximately 40% of attention heads are prunable with negligible accuracy degradation. Similarly, BMP [82] discovers the pattern that parameters in certain attention heads in MHA sublayers are entirely removed when we perform block-wise pruning with 16x16 weight blocks. \n\nFor FFN sublayers, the weight blocks within specific neurons in intermediate layers are entirely removed, while weight blocks in other neurons are preserved. Based on these findings, subsequent structured pruning algorithms [56,77,81,97,100,105,110,112,155,159] employ attention heads and intermediate neurons as the pruning granularities for MHA and FFN sublayers, respectively.",
            "score": 0.5391201424548863,
            "section_title": "Pruning Granularity: Unstructured vs. Structured",
            "char_start_offset": 20978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "247446572"
                },
                {
                    "start": 40,
                    "end": 44,
                    "matchedPaperCorpusId": "245131431"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "247446572"
                },
                {
                    "start": 1391,
                    "end": 1395,
                    "matchedPaperCorpusId": "237485472"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.806640625
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "After activation of a filter for a training set is passed through the max pooling operation, variation in resultant values across all training sets is used to calculate the utility score of each filter.The method achieved a reduction of a third less network weight with 7% enchantment in the micro-averaged F1-score and 22% in the macro-averaged F1-score.\n\nXu and Hu [46] proposed block pruning for BERT, RoBERTa, and DistilBERT models.The method rearranged the weight matrix into a row or column blocks format and performed the pruning of blocks based on the l 2 norm value against a predefined value.This structure pruning supports parallel processing.The technique achieves 5x weight pruning with negligible accuracy degradation on the GLUE benchmark.Additional 1.79x compression is achieved using knowledge distillation for little accuracy loss.\n\nWang [68] explore the fairness of the Natural Language Processing  about knowledge distillation being less biased and toxic while model compression may be viewed as regularization.\n\nYang et al. [69] proposed the code generator (SparseRT) for hardware implementation of unstructured pruning e.g., GPUs.Traditionally, structured pruning introduces more loss of accuracy as par unstructured pruning.However, their efficient hardware implementation remains a question.The proposed technique attempted to take advantage of unstructured sparsity to increase the efficiency of sparse linear algebra operations for transformer using the WMT English-to-German 2014 dataset BLEU score.The technique achieved 95% sparsity with 6.6 points accuracy loss.\n\nJoniak and Aizawa [49] proposed the debiasing of the Language model using Movement Pruning for gender.The pruning of Attention heads and entire square blocks of the BERT model is utilized to reduce the bias by finding the subset of the existing Language model.Sentence Encoder Association Test (SEAT) and Stereotype Score metric are used for measuring bias.The finding determined the performance and bias trade-off.A model having high performance also has a high gender bias.\n\nLi et.al. [60] presented stage-wise pruning of recurrent neural networks (RNN) for NLP.",
            "score": 0.5389337516986423,
            "section_title": "V. COMPARISON OF DIFFERENT PRUNING TECHNIQUES",
            "char_start_offset": 69835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 202,
                    "end": 355
                },
                {
                    "start": 357,
                    "end": 436
                },
                {
                    "start": 436,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 754
                },
                {
                    "start": 754,
                    "end": 849
                },
                {
                    "start": 851,
                    "end": 1031
                },
                {
                    "start": 1033,
                    "end": 1152
                },
                {
                    "start": 1152,
                    "end": 1247
                },
                {
                    "start": 1247,
                    "end": 1315
                },
                {
                    "start": 1315,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1592
                },
                {
                    "start": 1594,
                    "end": 1696
                },
                {
                    "start": 1696,
                    "end": 1854
                },
                {
                    "start": 1854,
                    "end": 1951
                },
                {
                    "start": 1951,
                    "end": 2009
                },
                {
                    "start": 2009,
                    "end": 2069
                },
                {
                    "start": 2071,
                    "end": 2077
                },
                {
                    "start": 2077,
                    "end": 2158
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3828125
        },
        {
            "corpus_id": "258865964",
            "title": "PruMUX: Augmenting Data Multiplexing with Model Compression",
            "text": "Model compression reduces the number of model parameters with minimal loss in task performance. \n\nA well-studied method is network pruning, which removes unimportant connections from a network with minimal or no accuracy loss (LeCun et al., 1989;Hanson and Pratt, 1989;Hassibi et al., 1993). Unstructured pruning (Han et al., 2015b,a;Zhu and Gupta, 2017;Frankle and Carbin, 2019;Chen et al., 2020a;Huang et al., 2021;Sanh et al., 2020) does not impose any constraints on the locations of non-zero weights. The resulting network can achieve high sparsity but may not run efficiently on common hardware such as GPUs. \n\nStructured pruning produces structured sparse matrices that can take better advantage of the parallelism in existing hardware, but its sparsity is rela-tively lower than the unstructured pruning method for the same accuracy loss budget (Yu et al., 2017;Narang et al., 2017;Wen et al., 2017;Mao et al., 2017;Wang et al., 2019;McDanel et al., 2022). Structured pruning has been applied to transformers to improve inference throughput (Fan et al., 2019;Sajjad et al., 2023;Voita et al., 2019;Michel et al., 2019;Prasanna et al., 2020;Chen et al., 2020b;McCarley et al., 2019;Hou et al., 2020;Yao et al., 2021). \n\nDistillation compresses a model by transferring knowledge from a large teacher model to a small student model (Hinton et al., 2015). General distillation for Transformer models learn from unlabeled corpus (Sanh et al., 2019;Sun et al., 2020;Wang et al., 2020;Turc et al., 2019;Jiao et al., 2019). Task-specific distillation for Transformer models learns on task-specific data (Sun et al., 2019). (Jiao et al., 2019) combines the two distillation methods to improve performance.",
            "score": 0.5386393436389136,
            "section_title": "Related Work Model Compression",
            "char_start_offset": 20064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 98,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1704
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 246,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 246,
                    "end": 269,
                    "matchedPaperCorpusId": "9344018"
                },
                {
                    "start": 269,
                    "end": 290,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 354,
                    "end": 379,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 379,
                    "end": 398,
                    "matchedPaperCorpusId": "220768628"
                },
                {
                    "start": 417,
                    "end": 435,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 853,
                    "end": 870,
                    "matchedPaperCorpusId": "4497748"
                },
                {
                    "start": 1067,
                    "end": 1087,
                    "matchedPaperCorpusId": "251005814"
                },
                {
                    "start": 1189,
                    "end": 1206,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 1468,
                    "end": 1486,
                    "matchedPaperCorpusId": "211296536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3984375
        },
        {
            "corpus_id": "264171146",
            "title": "Learnable Sparsity Structured Pruning for Acoustic Pre-trained Models",
            "text": ", no matter which kind of pruning methods, there are some defects. Unstructured pruning can lead to competitive performance, but it is difficult to accelerate due to irregular sparsity. As for structured pruning, in order to obtain a reasonable accuracy, complex hyper-parameter tuning operations are required. Under the conditions of high sparsity, the accuracy loss is still relatively high, although they produce hardware-friendly sub-networks.\n\nIn this work, we propose a structured pruning approach method named Learnable Sparsity Structured Pruning(LSSP). In our method, we introduced the upper confidence bound of importance scores, to evaluate which components should be pruned out in the model. In addition, in order to simplify the hyperparameter tuning process, we introduce a set of learnable pruning ratio parameters that can be learned by stochastic gradient descent, and a new regularization term. We apply our method for HuBERT models on ASR tasks. The results show for all pruning granularity, and pruning ratios, our methods all yield higher accuracy and speedup ratios. Especially, when the sparsity is 60%, the accuracy only decreases by 0.63% and the speedup ratio reaches 2.4.Next,we will demonstrate from a variety of novel methods, experiments, and ablation experiments.",
            "score": 0.5377170600411865,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1850,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36865234375
        },
        {
            "corpus_id": "274992799",
            "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
            "text": "Large Language Models (LLMs) [1,2,3] have made significant strides in various natural language processing tasks, leading to the emergence of novel applications such as AI agents [4]. One of the factors contributing to the exceptional capabilities of LLMs is their massive parameter scales. However, these extensive parameters also introduce increased inference costs and deployment challenges, hindering the widespread application and adoption of LLMs. Accelerating inference for LLMs has become a focal point of current research. Model compression [5], as one of the strategies for inference acceleration, including techniques like pruning and quantization [6,7], has been extensively researched. Nevertheless, earlier model compression techniques, particularly model pruning, typically rely on heavy post-training to recover the model's capabilities, which typically involves retraining with the entire training dataset. Given the constraints of current computational resources, the above approaches are not feasible for LLMs. \n\nIn the domain of LLM pruning, recent studies have largely focused on unstructured (or semistructured) pruning [8], a method that shrinks models by selectively zeroing out weights considered non-critical. Despite its advancements, unstructured pruning falls short in substantially reducing parameter count, which is crucial for accelerating LLM inference as it is often bottlenecked on memory bandwidth and communication [9]. To accelerate inference speed, unstructured pruning models are often paired with specialized frameworks or hardware solutions. Conversely, structured pruning [10,11] effectively decreases the model's parameter count by systematically eliminating columns or rows from weight matrices, enabling significant improvements in inference speed, and reduce deployment cost on conventional hardware. Yet, structured pruning often entails more pronounced compromises in model performance, which poses a greater challenge. \n\nRecently, researchers have applied the classic Optimal Brain Surgeon (OBS) framework to the compression of LLMs. This approach includes parameter compensation which can mitigate the loss incurred during compression and reduce the dependence on post-training. The OBS framework is currently applied in the areas of unstructured pruning [12] and quantization [13] for LLMs. However, there exist some challenges in its application to structured pruning:",
            "score": 0.5377063307875437,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2341
                },
                {
                    "start": 2342,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 552,
                    "matchedPaperCorpusId": "211062209"
                },
                {
                    "start": 658,
                    "end": 661,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 661,
                    "end": 663,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 1451,
                    "end": 1454,
                    "matchedPaperCorpusId": "254096365"
                },
                {
                    "start": 1614,
                    "end": 1618,
                    "matchedPaperCorpusId": "7333079"
                },
                {
                    "start": 2305,
                    "end": 2309,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "274710980",
            "title": "Re-Parameterization After Pruning: Lightweight Algorithm Based on UAV Remote Sensing Target Detection",
            "text": "Pruning methods play a crucial role in reducing redundant parameters within networks, and they can be categorized into unstructured pruning and structured pruning. \n\nUnstructured pruning can obtain higher pruning rates, and any weight in the model can be pruned. However, unstructured pruning can result in irregular sparsity in the weight matrix, which is difficult to accelerate by general hardware acceleration. Conversely, the application of unstructured pruning maintains the network's depth and width intact. Therefore, the acceleration effect of the pruned model using this method is limited. \n\nStructured pruning prunes the entire channels or filters of weights. The filter pruning removes the whole filter of the weight matrix, while channel pruning removes channels with identical sequences in different filters. Therefore, structured pruning can effectively compress network parameters, thereby improving the inference speed of the model.",
            "score": 0.5376957453117148,
            "section_title": "Model Pruning",
            "char_start_offset": 10146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 166,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 949
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "The recent LLM-Pruner (Ma, Fang, and Wang 2023) attempted structured pruning for LLMs, but its dependence on LoRA finetuning (Hu et al. 2021) creates a tough trade-off between high computation and effective pruning, limiting its use in larger models. \n\nPruning essentially involves two key aspects: discovering redundancy and recovering performance. For an effective structured pruning method tailored to LLMs, three fundamental criteria must be satisfied: a) a structured importance metric to discover structured redundancy; b) a mechanism for adaptively searching the optimal global compression model structure; and c) a compensation strategy to minimize performance degradation. \n\nIn response to these three essential criteria, we introduce FLAP (FLuctuation-based Adaptive Structured Pruning), a novel structured pruning framework. We find that certain channels of hidden state features exhibit structured sample stability. This observation enables us to compensate for bias within the model using baseline values. Specifically, we design a structured pruning metric that estimates the fluctuation of each input feature relative to the baseline value, utilizing a set of calibration samples. This metric assists in determining whether the output feature map can be recovered when a column of the weight matrix is removed. We then standardize these fluctuation metric scores across lay-ers and modules separately, allowing for the adaptive determination of the global compressed model structure. Finally, FLAP employs the baseline values to add additional biases, recovering the output feature maps for the corresponding layers. Remarkably, our method avoids the need for the retraining process and requires only a single forward pass for both pruning and bias compensation, thereby maintaining low memory overhead. \n\nWe evaluate the effectiveness of FLAP on the LLaMA model family, and FLAP achieves remarkable performance on a variety of language benchmarks. Impressively, without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. \n\nOur main contributions are listed as follows: \u2022 We propose a novel retraining-free structured pruning framework for LLMs named FLAP. To our best knowledge, this is the first work that identifies the characteristic of structured sample stability in LLMs.",
            "score": 0.5364290063674599,
            "section_title": "Introduction",
            "char_start_offset": 1708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2139
                },
                {
                    "start": 2142,
                    "end": 2274
                },
                {
                    "start": 2275,
                    "end": 2395
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 47,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 125,
                    "end": 140,
                    "matchedPaperCorpusId": "7785881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "262054394",
            "title": "Flash-LLM: Enabling Low-Cost and Highly-Efficient Large Generative Model Inference With Unstructured Sparsity",
            "text": "There are two typical types of pruning principals. The most flexible pruning strategy (unstructured sparsity) is to remove less salient elements without considering the distribution of the pruned elements in the weight matrix. Taking magnitude pruning for example, we can rank all the elements in the matrix based on their absolute 3 For these MatMuls,  and  are integer multiples of hidden size while  equals inference batch size (typically orders of magnitude smaller than hidden size). values and then remove the weights with the smallest magnitude. \n\nAnother strategy (structured sparsity) is to prune the less salient weights, but at the same time, some kind of structural criteria must be enforced. For example, the weight matrices can be split into non-overlapping 8 \u00d7 1 vectors [4,25] or 32 \u00d7 32 blocks [13], and then each vector/block is either kept or removed during pruning. In short, the major difference between structured and unstructured pruning is that extra constraints must be satisfied for structured pruning. Even though structured sparsity is friendly for hardware acceleration, it suffers from more severe model accuracy degradation [8,12,14,16,51,54] as it limits the freedom of deciding which element to prune. As shown in [28], compared to structured sparsity which has 5% accuracy drop, unstructured sparsity only results in 1% accuracy drop. In our experiments, the OPT-like models could greatly preserve accuracy through retraining-based unstructured pruning [15,29] at 80% sparsity (e.g., the accuracy only decreases from 85.55% to 84.11% for OPT-30B). \n\nHowever, the conventional techniques for supporting random unstructured sparsity in SpMM execution are not effective since they focus on leveraging SIMT cores without a sophisticated way of utilizing high-performance tensor cores. Fig. 3 shows the performance comparison of different techniques for SpMM on an OPT-66B inference task with batch size 8. Note that the standard pruning for LLM inference typically requires a moderate level of sparsity (e.g., 80%) to preserve model quality while reducing memory footprint.",
            "score": 0.5363574494894257,
            "section_title": "Unstructured Sparsity on Tensor Cores",
            "char_start_offset": 10448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "252280661"
                },
                {
                    "start": 1164,
                    "end": 1167,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "2238772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "248986195",
            "title": "Parameter-Efficient Sparsity for Large Language Models Fine-Tuning",
            "text": "Many applications in natural language processing have been following a paradigm, which first pre-trains a large language model and then fine-tunes it towards multiple downstream tasks. Despite its great success, such large-scale language models with millions to billions of parameters need a huge memory footprint and computational overhead in fine-tuning downstream datasets and also the inference stage, which prevents them from being directly applied to various tasks. MvP represent the representative data-free and data-driven methods, respectively. W represents the weights, G represents the corresponding gradient. A, B, R and C denote our proposed small matrices. We simplify the importance criteria for clear analysis. \n\nTo mitigate the computational and memory burden in the language model inference, one promising direction is pruning [McCarley et al., 2019;Zhang and He, 2020], which removes unimportant weights/channels/layers independently to reduce the computation and memory overhead. Among these, unstructured pruning, i.e. sparsity, is widely studied since it can achieve a higher compression ratio with competitive performance. \n\nPrevious sparsity methods propose various criteria to compute the importance of each weight, which can be roughly classified to two categories, data-free [Han et al., 2015;Tanaka et al., 2020] and data-driven [Sanh et al., 2020;Wang et al., 2020a]. The comparison is shown in Table 1. Data-free criterion methods compute the importance of weight based on the weight itself without any data involved, such as magnitude pruning (MaP) [Han et al., 2015]. Although data-free criteria have high computational and memory efficiency, they ignore that the role of each weight varies widely across different downstream tasks, which leads to degradation in model performance. Typical data-driven criteria methods focus on designing precise important criteria to compute the importance scores based on the specific dataset, which is proved to succeed in reducing the computation inference cost of the language model without a performance drop. However, these data-driven criteria introduce extra computation and trainable parameters to obtain the importance measurement, which dramatically increases the memory footprint and computational overhead during sparsity-aware training.",
            "score": 0.5360596374504292,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2316
                }
            ],
            "ref_mentions": [
                {
                    "start": 845,
                    "end": 868,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 868,
                    "end": 887,
                    "matchedPaperCorpusId": "225066769"
                },
                {
                    "start": 1320,
                    "end": 1340,
                    "matchedPaperCorpusId": "219558821"
                },
                {
                    "start": 1357,
                    "end": 1376,
                    "matchedPaperCorpusId": "218665313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "268536948",
            "title": "OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization",
            "text": "Network pruning can be generally categorized into unstructured and structured methods.(a) Unstructured methods (LeCun et al., 1989;Hassibi & Stork, 1992;Han et al., 2015;2016;Guo et al., 2016) prune unimportant weights in the model, but efficiency of the pruned sparse network cannot be realized on general-purpose GPU hardware.(b) Structured methods prune channels, neurons etc. (Lebedev & Lempitsky, 2016;Wen et al., 2016), and thus actual speedup can be realized without the need for sparse accelerators.In this work, we consider structured pruning.Next, we sum-1 OSSCAR: One-Shot Structured Compression AlgoRithm marize structured pruning methods in vision and language models.\n\nStructured pruning in Vision Models.Lebedev & Lempitsky (2016); Wen et al. (2016); Zhou et al. (2016) consider \u2113 21 norm to prune filters in convolutional layers.Li et al. (2017) and He et al. (2018a) 2022) considers a local model based on the second-order (Hessian) information of the loss function to prune attention layer heads and hidden neurons in FFN.Although effective, this approach can be prohibitively expensive in terms of runtime and/or memory for billion-parameter models as the ones we consider.Kurti\u0107 et al. (2023) consider a layer-wise reconstruction error (He et al., 2017;Luo et al., 2017) to prune attention heads and hidden neurons in FFN.Kwon et al. (2022) and Kurti\u0107 et al. (2023) consider BERT-Large and GPT2 models with sizes < 340 million.\n\nPaper Organization.The rest of the paper is organized as follows.We present our problem formulation in Section 3 and our proposed algorithm in Section 4. We provide empirical validation of our proposals on large vision and language models in Section 5.",
            "score": 0.5359027050566737,
            "section_title": "Related Work",
            "char_start_offset": 4751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 86,
                    "end": 328
                },
                {
                    "start": 328,
                    "end": 507
                },
                {
                    "start": 507,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 681
                },
                {
                    "start": 683,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1192
                },
                {
                    "start": 1192,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1447
                },
                {
                    "start": 1449,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1514
                },
                {
                    "start": 1514,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 131,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 131,
                    "end": 153,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 153,
                    "end": 170,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 175,
                    "end": 192,
                    "matchedPaperCorpusId": "744803"
                },
                {
                    "start": 380,
                    "end": 407,
                    "matchedPaperCorpusId": "7204133"
                },
                {
                    "start": 407,
                    "end": 424,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 719,
                    "end": 745,
                    "matchedPaperCorpusId": "7204133"
                },
                {
                    "start": 747,
                    "end": 764,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 766,
                    "end": 784,
                    "matchedPaperCorpusId": "14666124"
                },
                {
                    "start": 845,
                    "end": 861,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 866,
                    "end": 883,
                    "matchedPaperCorpusId": "51608028"
                },
                {
                    "start": 1256,
                    "end": 1273,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 1273,
                    "end": 1290,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 1342,
                    "end": 1360,
                    "matchedPaperCorpusId": "248266822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66064453125
        },
        {
            "corpus_id": "264146580",
            "title": "The Road to On-board Change Detection: A Lightweight Patch-Level Change Detection Network via Exploring the Potential of Pruning and Pooling",
            "text": "In order to reduce the high computational cost of convolutional neural networks (CNNs) and realize the on-board deployment, various types of model compression methods have been proposed, including network pruning [39][40][41][42][43][44][45][46][47][48][49], parameter quantization [50][51][52], low-rank decomposition [53][54][55], knowledge distillation [56,57], etc. Among them, network pruning is the most popular and extensively studied model compression technique in both academia and industry [47]. \n\nBased on the differences in pruning granularity, network pruning methods can be divided into two types: unstructured pruning (or weight pruning) [39][40][41] and structured pruning (or filter pruning) [42][43][44][45][46][47][48][49]. Unstructured pruning method concentrates on individual weight pruning for the original model according to magnitude or gradient criterions. This strategy usually causes unstructured sparsity patterns. The existing hardware architecture cannot directly accelerate the process of unstructured pruning, and special algorithms need to be designed to support the corresponding sparse operations [39]. \n\nContrarily, structured pruning method prunes part of the network structures (e.g., channels, layers) instead of individual weights. It does not require specialized libraries for sparse computing, which is easy to be implemented and hardware-friendly. Among structured pruning methods, channel pruning is most popular, since it operates at the most fine-grained level while still fitting in conventional deep learning frameworks [42]. To determine the channels to be pruned, some criterion should be utilized to measure the importance of each channel. A commonly-used strategy is to use the norms of filters to evaluate their importance [43,44]. Specifically, the filters with small norm correspond to small values in intermediate output and are identified to be less important than those with large norm values.",
            "score": 0.5352269489816368,
            "section_title": "II. RELATED WORKS A. Network Pruning",
            "char_start_offset": 12666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "1663491"
                },
                {
                    "start": 217,
                    "end": 221,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 229,
                    "end": 233,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "51608028"
                },
                {
                    "start": 237,
                    "end": 241,
                    "matchedPaperCorpusId": "102350938"
                },
                {
                    "start": 241,
                    "end": 245,
                    "matchedPaperCorpusId": "258779280"
                },
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "211258761"
                },
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "239998772"
                },
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "543597"
                },
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "215716065"
                },
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "1437449"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "213005191"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "226841849"
                },
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "211258761"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "1663491"
                },
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 713,
                    "end": 717,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "51608028"
                },
                {
                    "start": 721,
                    "end": 725,
                    "matchedPaperCorpusId": "102350938"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "258779280"
                },
                {
                    "start": 729,
                    "end": 733,
                    "matchedPaperCorpusId": "211258761"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "239998772"
                },
                {
                    "start": 1133,
                    "end": 1137,
                    "matchedPaperCorpusId": "1663491"
                },
                {
                    "start": 1777,
                    "end": 1781,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 1781,
                    "end": 1784,
                    "matchedPaperCorpusId": "51608028"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418212890625
        },
        {
            "corpus_id": "274992799",
            "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
            "text": "The early works [26,27] provide a layer-wise compression framework with a locally optimal solution named Optimal Brain Surgeon (OBS). And then OBC [28] reduces the computational burden by converting layer-wise pruning into row-wise pruning and updating the inverse Hessian using a proposed formula. Furthermore, GPTQ [13] accelerates the process with Lazy Batch-Updates and Cholesky Reformulation, enabling the application of this method to the quantization of LLMs. SparseGPT [12] also adapts this approach for unstructured pruning of LLMs. However, there appears to be no existing research that has implemented OBS in structured pruning for LLMs. \n\nStructured Pruning vs. Other Techniques. Given that OBS has previously been used in both quantization and unstructured pruning, and is now being applied to structured pruning, there is an inherent consistency across these three compression schemes. These methods actually compress the model at varying levels of granularity. Quantization, which \"trims\" floating-point precision, represents the finest granularity and delivers excellent compression outcomes. Structured pruning, on the other hand, involves trimming weight vectors and represents the coarsest granularity, naturally resulting in higher performance losses compared to other methods, which poses significant challenges. For small models, it is possible to recover most of the performance with post-training, but this is challenging to achieve in LLMs due to resource constraints. Nonetheless, structured pruning effectively reduces the number of parameters without needing special inference framework support and is compatible with the other two methods, thus still holding considerable potential for application.",
            "score": 0.5347527054089969,
            "section_title": "Related Work",
            "char_start_offset": 6932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1727
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 20,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "251765570"
                },
                {
                    "start": 477,
                    "end": 481,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "269214244",
            "title": "SNP: Structured Neuron-level Pruning to Preserve Attention Scores",
            "text": "Pruning methods can be broadly categorized into two types, unstructured and structured pruning.Unstructured pruning sets individual weights or parameters to zero, resulting in irregular sparse matrices [9,14].Compressed models using unstructured pruning tend to maintain relatively high performance for a given pruning ratio.However, they necessitate additional libraries, such as cuSPARSE [5], Automatic SParsity [20], or SparseDNN [26] to accelerate sparse matrix computations.\n\nStructured pruning, on the other hand, involves the removal of entire groups of units, such as filters or attention heads.This can be implemented using \"masking\" (zeroing out) [11,12,31,32], or by \"pruning\" [8,15].Structured pruning by masking [11,12,31,32] simply sets the group of units to zero, which requires additional libraries to accelerate the model, as unstructured pruning.\"Pruning\" [8,15], on the other hand, requires a comprehensive understanding of the network's graphical connectivity, including elementwise operations that enforce the same input shape.By considering the graphical connectivity and pruning identical filter indices for interconnected layers, structured pruning can achieve acceleration on any devices.",
            "score": 0.5342002502423543,
            "section_title": "Unstructured and structured pruning",
            "char_start_offset": 3983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 479
                },
                {
                    "start": 481,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 864
                },
                {
                    "start": 864,
                    "end": 1048
                },
                {
                    "start": 1048,
                    "end": 1213
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 393,
                    "matchedPaperCorpusId": "230447302"
                },
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 661,
                    "end": 664,
                    "matchedPaperCorpusId": "102350938"
                },
                {
                    "start": 667,
                    "end": 670,
                    "matchedPaperCorpusId": "247318543"
                },
                {
                    "start": 688,
                    "end": 691,
                    "matchedPaperCorpusId": "256390345"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "102350938"
                },
                {
                    "start": 735,
                    "end": 738,
                    "matchedPaperCorpusId": "247318543"
                },
                {
                    "start": 874,
                    "end": 877,
                    "matchedPaperCorpusId": "256390345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52978515625
        },
        {
            "corpus_id": "271270022",
            "title": "Reconstruct the Pruned Model without Any Retraining",
            "text": "Network pruning is a widely applicable compression technique, whose key point is to remove the redundant weight or modules from the original network [18] and reserve the salient ones [11,12].It is broadly categorized from the granularity aspect into structured and unstructured pruning.Unstructured pruning [19,20,21] performs at the individual weight level, which brings about larger sparsity but fails to accelerate the model and reduce the storage cost without requiring additional hardware support.By contrast, structured pruning [22,23,24] removes a group of weights, such as an entire channel, head, layer, and so on, therefore providing a more hardware-friendly solution, enhancing the lower inference latency and memory demands, so we focus on structured pruning in this paper.\n\nThe conventional retraining-based paradigm involves compressing the original model using various criteria followed by retraining to restore performance [25,26,27,28].However, as the size and complexity of LLMs rapidly increase [29,30,31], this conventional approach becomes impractical and costly, prompting the need for retraining-free compression techniques.Recent developments in this area have primarily centered around quantization [32,33,34] and have expanded to include pruning methods [13,15,14] that eliminate the need for retraining.In this paper, our work targets enhancing the performance of the retraining-free pruning paradigm, which can reduce the model size, lower the memory consumption, accelerate the inference, and be orthogonal and compatible with quantization for further compression simultaneously.",
            "score": 0.5339086214858795,
            "section_title": "Network Pruning for Language Models",
            "char_start_offset": 4907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 785
                },
                {
                    "start": 787,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1608
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 187,
                    "end": 190,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 314,
                    "end": 317,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1280,
                    "end": 1284,
                    "matchedPaperCorpusId": "248266822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "260125212",
            "title": "Model Compression Methods for YOLOv5: A Review",
            "text": "Unstructured or fine-grained pruning refers to when goal parameters for pruning are weights of the model without considering their location in the associated tensor or layer. In weight pruning, unnecessary weights are identified through saliency evaluation and masked out or removed afterward. Since eliminating the weights may impair the model architecture, they are mostly masked out during this process [37]. While masking out the weights instead of removing them increases the memory usage during training, the information of the masked weights can be used at each step to compare the pruned model with the original one. fine-grained pruning is not always beneficial as it requires special hardware to take advantage of such irregular sparse operations [41]. While higher compression ratios could be achieved using unstructured pruning, storing the index of pruned weights may result in higher storage usage [42,43].",
            "score": 0.5337652405286641,
            "section_title": "Unstructured pruning",
            "char_start_offset": 13328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 920
                }
            ],
            "ref_mentions": [
                {
                    "start": 757,
                    "end": 761,
                    "matchedPaperCorpusId": "202558809"
                },
                {
                    "start": 912,
                    "end": 916,
                    "matchedPaperCorpusId": "210064442"
                },
                {
                    "start": 916,
                    "end": 919,
                    "matchedPaperCorpusId": "231740691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.479736328125
        },
        {
            "corpus_id": "272397797",
            "title": "Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks",
            "text": "During the evaluation of l1 unstructured and random unstructured pruning techniques, expected trends were observed regarding accuracy and compression. As the pruning percentages increased for both linear and convolutional layers, a corresponding decrease in accuracy was noted, while the model size, parameters, and MACs remained unaltered. This outcome aligns with the inherent nature of unstructured pruning, wherein weights are zeroed out but not entirely eliminated, resulting in sparse models without substantial reductions in computational complexity. However, the lack of significant reductions in computational complexity may constrain their effectiveness, particularly in resource-constrained environments where efficient utilization of computational resources is essential. This highlights the importance of considering the tradeoffs between model compression and computational efficiency when selecting pruning techniques for deployment in real- world applications, especially in edge computing scenarios where computational resources are limited.",
            "score": 0.5327893415149406,
            "section_title": "C. Unstructured Pruning Techniques",
            "char_start_offset": 38854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1058
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.223388671875
        },
        {
            "corpus_id": "259501534",
            "title": "CrAFT: Compression-Aware Fine-Tuning for Efficient Visual Task Adaptation",
            "text": "For better hardware efficiency, structured pruning [21,22,36,48] seeks to trade pruning granularity for computational regularity. Our work benchmark unstructured weight pruning with one-shot methods since they do not require expensive retraining.",
            "score": 0.5326697976522508,
            "section_title": "Related Works",
            "char_start_offset": 20087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 246
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 55,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 55,
                    "end": 58,
                    "matchedPaperCorpusId": "250244042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "The development of Large Language Models (LLMs) has witnessed a surge in model and dataset sizes, necessitating distributed training across numerous devices (Tang et al., 2020;2023;2024).This distributed approach, while effective, demands substantial computational and storage resources, with LLMs incurring higher energy costs compared to their smaller counterparts (Luccioni et al., 2023;Schwartz et al., 2020;Tang et al., 2019).Consequently, energy-efficient LLM training and inference are crucial for green computing, with LLM pruning emerging as a key technique for achieving this goal.Post-training pruning, in particular, has gained prominence due to its minimal resource requirements, making it a cost-effective approach for democratizing access to LLMs (Lu et al., 2022;Frantar & Alistarh, 2023;Sun et al., 2024).This method's efficiency and accessibility contribute significantly to the broader impact and applicability of LLMs.\n\nNetwork Pruning Network pruning is an effective technique for reducing model complexity while preserving performance, although it often requires extensive retraining.However, traditional pruning methods (Hoang et al., 2023;Sreenivasan et al., 2022;Liu et al., 2019;Chen et al., 2023;Chijiwa et al., 2021) become impractical when dealing with the substantial parameter sizes and vast datasets of Large Language Models (LLMs).Deep Compression (Han et al., 2016a) popularized magnitude-based pruning for deep neural networks, which removes the weights with the smallest absolute values, assuming that they have the least impact on the network's output.Network pruning techniques can be broadly categorized into two main approaches: unstructured pruning and structured pruning.\n\n(1) Unstructured Pruning involves removing individual weights or connections based on certain criteria.SparseGPT (Frantar & Alistarh, 2023) is the first post-training quantization method that performs unstructured pruning using an approximated Hessian matrix.",
            "score": 0.5326441830429092,
            "section_title": "A. Related Work",
            "char_start_offset": 33826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 187,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 938
                },
                {
                    "start": 940,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1364
                },
                {
                    "start": 1364,
                    "end": 1589
                },
                {
                    "start": 1589,
                    "end": 1713
                },
                {
                    "start": 1715,
                    "end": 1818
                },
                {
                    "start": 1818,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 390,
                    "matchedPaperCorpusId": "253265387"
                },
                {
                    "start": 412,
                    "end": 430,
                    "matchedPaperCorpusId": "166228547"
                },
                {
                    "start": 762,
                    "end": 779,
                    "matchedPaperCorpusId": "251648051"
                },
                {
                    "start": 1163,
                    "end": 1188,
                    "matchedPaperCorpusId": "247084008"
                },
                {
                    "start": 1205,
                    "end": 1223,
                    "matchedPaperCorpusId": "257495957"
                },
                {
                    "start": 1223,
                    "end": 1244,
                    "matchedPaperCorpusId": "235458499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.493896484375
        },
        {
            "corpus_id": "271709495",
            "title": "An approach to optimize inference of the DIART speaker diarization pipeline",
            "text": "The main goal of pruning is to reduce the size of a neural network. This is achieved by reducing the connections in the network by deleting neurons or zeroing out weights [12]. Pruning methods can be divided into structured and unstructured [12]. Structured pruning methods [13] [14] remove entire channels or layers. Unstructured pruning methods [15] [16], on the other hand, deactivate individual neurons by setting their weight to zero. However, this means that unstructured pruning does not automatically make the model smaller or faster [17]. This requires additional libraries that can generate performance gains from the zeroed neurons [17].",
            "score": 0.5326089081487302,
            "section_title": "B. Pruning",
            "char_start_offset": 3349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 648
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "38486148"
                },
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "18201582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.157470703125
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "Network Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive\nStructured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP.",
            "score": 0.5319784462385577,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "272770793",
            "title": "CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information",
            "text": "The colossal parameters and computational overhead of Large Language Models (LLMs) challenge their real-world applications. Network pruning, which targets unstructured or structured sparsity by removing redundant parameters, has recently been explored for LLM acceleration. Existing LLM pruning works focus on unstructured pruning, which typically requires special hardware support for a practical speed-up. In contrast, structured pruning can reduce latency on general devices. However, it remains a challenge to perform structured pruning efficiently and maintain performance, especially at high sparsity ratios. To this end, we introduce an efficient structured pruning framework named CFSP, which leverages both Coarse (interblock) and Fine-grained (intrablock) activation information as an importance criterion to guide pruning. The pruning is highly efficient, as it only requires one forward pass to compute feature activations. Specifically, we first allocate the sparsity budget across blocks based on their importance and then retain important weights within each block. In addition, we introduce a recovery fine-tuning strategy that adaptively allocates training overhead based on coarse-grained importance to further improve performance. Experimental results demonstrate that CFSP outperforms existing methods on diverse models across various sparsity budgets. Our code will be available at https://github.com/wyxscir/CFSP.",
            "score": 0.531964122101348,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "276902790",
            "title": "Sample-aware Adaptive Structured Pruning for Large Language Models",
            "text": "Large language models (LLMs), such as GPT-3 (Brown et al. 2020), OPT (Zhang et al. 2022), LLaMA (Touvron et al. 2023) and Vicuna (Chiang et al. 2023), have demonstrated significant accomplishments in the realm of natural language processing (Wei et al. 2022;Wu et al. 2020). Nevertheless, their exceptional capabilities are coupled with substantial model sizes and elevated computational expenses. Furthermore, owing to the scaling law (Hoffmann et al. 2022;Kaplan et al. 2020), LLMs tend to enhance model performance by progressively augmenting model parameters. Regrettably, larger model sizes entail heightened consumption of computational resources, presenting a notable obstacle to their practical deployment, particularly in settings with limited resources. \n\nStructured pruning (Xia et al. 2024) emerges as a pivotal technique for mitigating resource demands in the deployment of LLMs. In comparison to other strategies (Zhu et al. 2023) like unstructured pruning (Yin et al. 2023;Frantar and Alistarh 2023;Jaiswal et al. 2023), model quantization (Liu et al. 2023;Xiao et al. 2023), and knowledge distillation (Gu et al. 2024;Yuan et al. 2023;Hsieh et al. 2023), structured pruning not only provides a practical and hardwareindependent solution but also offers an effective approach to streamline LLMs implementation on devices with limited computational resources. Its unique ability to selectively remove redundant model parameters while maintaining model integrity positions structured pruning as a cornerstone in optimizing the efficiency of LLMs deployment. \n\nHowever, existing work on structured pruning commonly employs Taylor expansion as the metric for estimating the importance of structures. These methods hinge on a localized approximation of the loss function, necessitating additional calibration data for gradient information computation. As a result, the precision of the gradient is directly tied to the calibration data's quality, thereby influencing both the Taylor expansion approximation and the pruning decision. Subpar calibration data and unsuitable importance estimation metrics can lead to substantial performance degradation in pruned models.",
            "score": 0.5311747838013455,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 63,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 69,
                    "end": 88,
                    "matchedPaperCorpusId": "248496292"
                },
                {
                    "start": 241,
                    "end": 258,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 258,
                    "end": 272,
                    "matchedPaperCorpusId": "226262321"
                },
                {
                    "start": 785,
                    "end": 801,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 927,
                    "end": 944,
                    "matchedPaperCorpusId": "248496292"
                },
                {
                    "start": 971,
                    "end": 988,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 988,
                    "end": 1014,
                    "matchedPaperCorpusId": "3922816"
                },
                {
                    "start": 1014,
                    "end": 1034,
                    "matchedPaperCorpusId": "259088941"
                },
                {
                    "start": 1072,
                    "end": 1089,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1134,
                    "end": 1151,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 1151,
                    "end": 1169,
                    "matchedPaperCorpusId": "258461606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66162109375
        },
        {
            "corpus_id": "271051518",
            "title": "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression",
            "text": "Semi-structured pruning aims to achieve specific N:M sparsity patterns (N elements are non-zero for every M consecutive elements) to allow for inference speed-up with hardware support (Nvidia, 2021). In this work, we include both unstructured pruning and semi-structured pruning. \n\nQuantization aims to compress a neural network by reducing the number of bits (i.e., precision) in the weights of the model (Dettmers et al., 2022;Xu and McAuley, 2023;Dettmers et al., 2024, inter alia). Post-training quantization rescales the weights of a trained language model, while quantization-aware training rounds the weights during the training process. We should note quantization and pruning are two orthogonal compression directions -pruned models can be further quantized for extreme compression.",
            "score": 0.5310628443838334,
            "section_title": "Compression Methods for LLMs.",
            "char_start_offset": 6434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 279
                },
                {
                    "start": 282,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 791
                }
            ],
            "ref_mentions": [
                {
                    "start": 406,
                    "end": 429,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40673828125
        },
        {
            "corpus_id": "274982216",
            "title": "Lightweight Design and Optimization methods for DCNNs: Progress and Futures",
            "text": "Training a small network (student) to mimic the behavior of a large network (teacher). \n\nReduce the model size while maintaining performance effectively. \n\nThe distillation process can be complex and time-consuming. \n\nmodel size can be effectively reduced without compromising network performance. The process of model pruning is shown in Figure 8. Model pruning primarily divides into structured pruning and unstructured pruning. Unstructured pruning removes insignificant neurons, and correspondingly, the connections between pruned neurons and other neurons are ignored during computation. Since the model after unstructured pruning is typically sparse and disrupts the original structure of the model, this method is also known as fine-grained pruning. Unstructured pruning can significantly reduce the number of model parameters and theoretical computational load, but existing hardware architectures, especially edge devices with constrained resources, cannot get acceleration benefit from unstructured pruning due to sparse models. In contrast to unstructured pruning, structured pruning typically operates at the level of filters or entire network layers. When a filter is pruned, the previous feature map and the next feature map undergo corresponding changes, but the model structure remains intact, still allowing acceleration through GPUs or other general hardware.",
            "score": 0.530098452962023,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 43876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1377
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.248291015625
        },
        {
            "corpus_id": "273185970",
            "title": "Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression",
            "text": "Large Language Model Compression LLM compression techniques include model distillation, pruning and quantization, etc. Gu et al. (2024); Huang et al. (2022); Magister et al. (2023); Jiang et al. (2023b) successfully applied model distillation to LLM by retraining, which incurs high computational cost. Frantar & Alistarh (2023;2022); Sun et al. (2024); Ma et al. (2023) pruned weights that are less sensitive to outliers. However, the resulting unstructured weight matrices do not provide meaningful compression benefits on real hardware. Structured pruning techniques, such as 2:4 or 4:8 pruning, can achieve effective compression but restrict a fixed 50% pruning ratio, which limits flexibility in balancing performance and compression ratio. Zhao et al. (2024); Ashkboos et al. (2024); Lin et al. (2024); Xiao et al. (2023) allocated higher quantization bits to weights with larger influence on outliers, but it does not reduce the number of parameters, limiting its impact on overall compression. \n\nSVD-based Weight Compression SVD-based weight compression has a flexible compression ratio to maintain performance without retraining. Golub et al. (1987) were the first to apply SVD for neural network compression, and Lv et al. (2023); Wu et al. (2023) extended this approach to shallow transformer models (Vaswani, 2017). However, in LLM compression, these methods incur significant errors since they do not consider outliers in activations. FWSVD (Hsu et al., 2022) addresses this issue by incorporating the impact of outliers through the Fisher information analysis of weight matrices. However, this method requires gradient information during training process, which is computationally prohibitive for LLMs. ASVD (Yuan et al., 2023) alleviates this problem by selecting key channels in the weight matrix based on their sensitivity to outliers and minimizing compression error in these channels.",
            "score": 0.5294732039858882,
            "section_title": "RELATED WORK",
            "char_start_offset": 1574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1903
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 135,
                    "matchedPaperCorpusId": "259164722"
                },
                {
                    "start": 158,
                    "end": 180,
                    "matchedPaperCorpusId": "254823156"
                },
                {
                    "start": 182,
                    "end": 202,
                    "matchedPaperCorpusId": "258833333"
                },
                {
                    "start": 303,
                    "end": 328,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 335,
                    "end": 352,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 354,
                    "end": 370,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 746,
                    "end": 764,
                    "matchedPaperCorpusId": "264828796"
                },
                {
                    "start": 790,
                    "end": 807,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 809,
                    "end": 827,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1139,
                    "end": 1158,
                    "matchedPaperCorpusId": "121324775"
                },
                {
                    "start": 1223,
                    "end": 1239,
                    "matchedPaperCorpusId": "259859167"
                },
                {
                    "start": 1241,
                    "end": 1257,
                    "matchedPaperCorpusId": "260848415"
                },
                {
                    "start": 1311,
                    "end": 1326,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1454,
                    "end": 1472,
                    "matchedPaperCorpusId": "250243971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6630859375
        },
        {
            "corpus_id": "251476392",
            "title": "Heuristic Method for Minimizing Model Size of CNN by Combining Multiple Pruning Techniques",
            "text": "Let us present our proposed method in detail including the pruning scheme and the approach for the compression. We setup the approach to achieve the best compression ratio with the structured and unstructured pruning methods. We also introduce the parameters in our proposed method to be defined by application. \n\nPruning scheme. We first apply the structured pruning method to a CNN model to compress the convolutional layers. The reduction ratio is gradually increased, and each ratio is applied to the original network until the model accuracy reaches beneath a given target accuracy. Here, the reduction ratio is defined as the percentage for compression given to the pruning process. Then, the pruning method switches to the unstructured pruning method for compressing the fully connected layers while also increasing the reduction ratio until the accuracy reaches beneath the target accuracy again. Through these two pruning methods, we can finally identify the model that is minimized in size. As we express mathematically above, assume SP(m, p) is a compression of the structured pruning for a model m in a percentage of the reduction ratio p. It returns a pair of the compressed model m and the accuracy Acc. By increasing p and comparing Acc with the target accuracy, m is passed to the unstructured pruning USP(m , p ) when Acc is less than the target accuracy. The unstructured pruning also returns the compressed model m and the accuracy Acc . Finally, by increasing the reduction ratio p and comparing Acc with the target accuracy, it finishes the compression at the reduction ratio right before p results in worse accuracy than the target one. \n\nInitial Margin. As illustrated above, the structured pruning method compresses a CNN model with every reduction ratio gradually increased until the accuracy does not meet the target one. The compressed model gained after the pruning process is minimized by this structured pruning. Here, if we directly pass the compressed model to the unstructured pruning method, it might not leave enough room for further compression by the unstructured pruning method. Therefore, our method introduces a margin for the reduction ratio after the former pruning method when m is passed to the unstructured pruning.",
            "score": 0.5291567079749872,
            "section_title": "Strategy for Minimizing the Model Size",
            "char_start_offset": 13926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2260
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1517333984375
        },
        {
            "corpus_id": "256188839",
            "title": "A 1.6-mW Sparse Deep Learning Accelerator for Speech Separation",
            "text": "Many model compression techniques [12]- [20] have been presented for deep learning models to reduce memory size and computational complexity. The model compression techniques can be classified into two approaches: pruning and quantization. \n\nThe pruning technique often includes structured pruning and unstructured pruning schemes to remove unimportant weights [12], [13]. The structured pruning technique performs the filter, channel and layer pruning, while the unstructured pruning technique performs the element-wise pruning. In the case of structured pruning, the network can either be pruned using a predefined target network and directly trained the small target model from random initialization, or the pruned network model can also be auto-discovered and trained from scratch [20]. In the case of the unstructured pruning, the unstructured pruned model can be obtained from the iterative procedures of training, element-wise pruning, and fine-tuning. \n\nThe second compression technique is the quantization of the model. The quantization technique reduces the number of bit widths for weights and activations and thus achieves the low requirements of memory bandwidth and SRAM storage [15]- [18]. With the appropriate selection of quantization data format and data size, the retraining network model can maintain comparative performance. The numerical representations of a quantized value can be divided into two classes. The fixedpoint representation possesses the less dynamic range, but an arithmetic unit using the fixed-point representation is more area-efficient, while the floating-point representation owns the larger dynamic range, but the floating-point arithmetic unit may consume more area overhead. When using the floatingpoint format, only fewer bits are needed to represent a specified value, and thus gain the benefit from the reduction of memory bandwidth and memory storage. In addition to the model compression technique, the decomposition scheme can be used to reduce the computational complexity of a 2-D network model [19]. The operations such as the depthwise dilated convolution and transposed convolution may introduce many zeros computations. The decomposition technique enables one to skip the zero computations and reduce the computation complexity.",
            "score": 0.5290505865048662,
            "section_title": "B. Model compression",
            "char_start_offset": 3065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2285
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 38,
                    "matchedPaperCorpusId": "215799572"
                },
                {
                    "start": 40,
                    "end": 44,
                    "matchedPaperCorpusId": "52978527"
                },
                {
                    "start": 361,
                    "end": 365,
                    "matchedPaperCorpusId": "215799572"
                },
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "235339978"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "52978527"
                },
                {
                    "start": 1193,
                    "end": 1197,
                    "matchedPaperCorpusId": "230512323"
                },
                {
                    "start": 1199,
                    "end": 1203,
                    "matchedPaperCorpusId": "202779157"
                },
                {
                    "start": 2048,
                    "end": 2052,
                    "matchedPaperCorpusId": "224977140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2462158203125
        },
        {
            "corpus_id": "278367624",
            "title": "AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design",
            "text": "Pruning boosts model compactness at the architectural level by removing redundant parameters, ranging from individual weights (unstructured pruning [32]- [34]) to entire channels or layers (structured pruning [35]- [37]). Although unstructured pruning can achieve significant compression ratios, the resulting irregular sparsity is not conducive to hardware implementation [38]. In contrast, structured pruning is more compatible with hardware acceleration but often results in model accuracy degradation and limited sparsity [39]. To balance model accuracy and hardware efficiency, N :M semi-structured pruning [17], [18], where N out of every M elements are pruned, is commonly adopted in prevalent LLMs [16], [40]. For example, SparseGPT [17] effectively prunes GPT-family models [3] to achieve 2:4 and 4:8 sparsity in a one-shot manner without any retraining. Moreover, Wanda [18] leverages the product of weights and input activations to achieve 2:4 semi-structured pruning, demonstrating improved perplexity in certain cases, such as Llama-2-13B [4].",
            "score": 0.5285495852796835,
            "section_title": "A. Pruning for LLMs",
            "char_start_offset": 5562,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1056
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "264146174"
                },
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "268032346"
                },
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 215,
                    "end": 219,
                    "matchedPaperCorpusId": "259203385"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "16167970"
                },
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 618,
                    "end": 622,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 706,
                    "end": 710,
                    "matchedPaperCorpusId": "266844224"
                },
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "233296249"
                },
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 783,
                    "end": 786,
                    "matchedPaperCorpusId": "248496292"
                },
                {
                    "start": 880,
                    "end": 884,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "259950998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "270562464",
            "title": "A Comprehensive Study of Structural Pruning for Vision Models",
            "text": "Model compression is an essential pursuit in the domain of machine learning, motivated by the necessity to strike a balance between model accuracy and computational efficiency. Various approaches have been developed to create more efficient models, including pruning [1], quantization [2], decomposition [3], and knowledge distillation [4][5][6]. Among the multitude of compression paradigms, pruning has proven itself to be remarkably effective and practical [7][8][9][10][11][12][13][14][15][16]. The aim of network pruning is to eliminate redundant parameters of a network to produce sparse models and potentially speed up the inference. Mainstream pruning approaches can be categorized into structurual pruning and unstructurual pruning. Unstructured pruning typically involves directly zeroing partial weights without modifying the network structure; whereas structured pruning methods, although some require specific hardware support, can physically remove grouped parameters from the network, they effectively compress the network size, thus getting a wider domain of applications in practice. \n\nDespite the extensive research on structural pruning, the community still suffers from a lack of standardized benchmarks and metrics, leaving the progress in this area not fully comprehended [17,18]. Table 1 provides the experimental settings used in some representative papers on network pruning, which unveils three pitfalls in structure pruning evaluations in the current literature: \n\nPitfall 1: Limited comparisons with SOTA. Many works (e.g., [11,22,27,29,30]) limit their evaluations to a comparison between the original and pruned models, without benchmarking against state-of-the-art methodologies. Similarly, certain approaches (e.g., [19,20,24,28,28,31,32]) restrict their assessments to a single competitor.",
            "score": 0.5282942112494512,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1822
                }
            ],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 304,
                    "end": 307,
                    "matchedPaperCorpusId": "7340116"
                },
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "237108363"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "235703417"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "231699188"
                },
                {
                    "start": 469,
                    "end": 473,
                    "matchedPaperCorpusId": "211258761"
                },
                {
                    "start": 477,
                    "end": 481,
                    "matchedPaperCorpusId": "229297917"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "231854993"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "4142619"
                },
                {
                    "start": 493,
                    "end": 497,
                    "matchedPaperCorpusId": "256390345"
                },
                {
                    "start": 1556,
                    "end": 1559,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 1559,
                    "end": 1562,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 1565,
                    "end": 1568,
                    "matchedPaperCorpusId": "221082307"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33251953125
        },
        {
            "corpus_id": "267547625",
            "title": "Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes",
            "text": "Although we focus on pruning in this work, we note that prior research has explored various other compression schemes such as distillation (Hinton et al., 2015;Gu et al., 2023) and quantization (Xiao et al., 2023) to create smaller models from larger pre-trained ones. Similar to structured pruning, these compression methods themselves often impose significant computational burdens. For example, distillation-based techniques require using LLMs to generate large amounts of teacher data (Jiao et al., 2019;Hsieh et al., 2023). Although unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2023) and quantization have lower training-time resource demands, the models they produce either require specialized hardware to achieve speedups (Mishra et al., 2021) or may actually slow down inference due to additional computational overhead (Dettmers et al., 2022). Table 7 summarizes these existing limitations across different compression approaches. We also cover additional background in distillation and quantization below. \n\nDistillation: Originally proposed by (Hinton et al., 2015;Sanh et al., 2019) have shown that it is possible to reach similar performances on many downstream tasks using much smaller language models pre-trained with knowledge distillation. The resulting models are lighter and faster at inference time. Unfortunately, as noted by (Xia et al., 2022b), distilling a pre-trained model to a student model that has been initialized from scratch requires lots of compute and data to recover reasonable levels of teacher performance. To reduce this resource burden, (Sanh et al., 2020;Lagunas et al., 2021;Xia et al., 2022b) combine distillation with gradient-based structured pruning, so that the student model does not have to be initialized from scratch. Bonsai also leverages distillation, but only during the post-pruning adaptation phase. This maintains the constraint that the pruning process is gradient-free.",
            "score": 0.5278813258051315,
            "section_title": "B Background: Additional Methods for LLM Compression",
            "char_start_offset": 26745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 213,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 559,
                    "end": 585,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1590,
                    "end": 1609,
                    "matchedPaperCorpusId": "218665313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58447265625
        },
        {
            "corpus_id": "277781301",
            "title": "PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products in Neural Network Computations",
            "text": "Pruning is a process for reducing model size by setting low-magnitude weights to zero during training. The resulting weight matrices are sparse and each layer's computation amounts to one or more sparsematrix-times-dense-matrix multiplications (SpMM) between sparse weights and dense activations. Structured pruning entails pruning away groups of weights within a network based on some criteria such as L1 or L2 norm of the group. Since structured sparsity yields predictable nonzero distributions in weight matrices, it is amenable to software and/or hardware acceleration for skipping unnecessary computation. However, structured pruning limits the degrees of freedom in the sparsification process and may produce models with lower accuracy than the dense baseline [18,26]. \n\nOn the other hand, unstructured pruning entails removing individual neuron connections, producing a model with high accuracy that approaches the dense baseline. However, accelerating unstructured sparse matrix operations is challenging as non-zero values may be arbitrarily distributed and must be addressed individually [38]. Unstructured sparse data formats, such as compressed sparse row (CSR) or compressed sparse column (CSC) formats [9] may incur significant overheads from index storage [35] and irregular memory accesses, especially on resource constrained systems [38]. Even though the pruned network has only a fraction of the compute requirements, these overheads may cause performance of a pruned network to be worse than the corresponding dense network [35,38]. \n\nIn this work we focus on N:M pruning [29,39], a middle-ground between structured and unstructured pruning that allows for unstructured sparsity within fixed-sized groups of weights. In N:M pruning, the smallest N out of every M weights are pruned away and set to 0. The resulting semi-structured sparsity is amenable to both software and hardware acceleration while incurring less compute and indexing overhead than unstructured sparse formats [27,29].",
            "score": 0.5272215505798016,
            "section_title": "Network Pruning",
            "char_start_offset": 6240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 767,
                    "end": 771,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "4497748"
                },
                {
                    "start": 1272,
                    "end": 1276,
                    "matchedPaperCorpusId": "244527383"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "4497748"
                },
                {
                    "start": 1544,
                    "end": 1548,
                    "matchedPaperCorpusId": "244527383"
                },
                {
                    "start": 1548,
                    "end": 1551,
                    "matchedPaperCorpusId": "4497748"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2607421875
        },
        {
            "corpus_id": "272368391",
            "title": "On-Device Language Models: A Comprehensive Review",
            "text": "Pruning in neural networks involves selectively removing weights or neurons to reduce complexity and enhance computational efficiency without significantly compromising performance. This process targets the less crucial components of a model, focusing on efficiency and functional integrity. \n\n1. Structured Pruning: This approach removes entire subsets of parameters like layers, channels, or filters, which is beneficial for hardware optimization due to more regular memory access patterns and simplified computations. The 'LLM-Pruner' (Kaddour et al., 2023) employs structured pruning to eliminate non-essential groups based on gradient data, thus maintaining critical functionalities. It also facilitates performance recovery through techniques such as LoRA, allowing efficient restoration with minimal data. 2. Unstructured Pruning: Unlike structured pruning, unstructured pruning removes individual weights across the model, offering finer granularity and potentially higher compression rates (Li et al., 2023a). However, this method typically results in sparse matrices, which can be less compatible with traditional hardware architectures, compromising computational efficiency. It is most suitable where maximum compression is needed without constraints on structural preservation. 3. Contextual Pruning: This advanced method prunes based on the operational context of the model, targeting weights or neurons that are only relevant under specific conditions or for particular tasks. Contextual pruning ensures that reductions align dynamically with the model's operational needs, thereby preserving performance where it matters most.",
            "score": 0.5261804646117343,
            "section_title": "Pruning",
            "char_start_offset": 36780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1642
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85986328125
        },
        {
            "corpus_id": "267617160",
            "title": "A Survey on Transformer Compression",
            "text": "Pruning granularity. In response to the considerable parameter increase in the LLM era, there have been early attempts of pruning arise with the focus on unstructured and semi-structured pruning [143], [144], [145]. SparesGPT, for instance, conducts unstructured and semi-structured (mainly 2:4) pruning starting from OPT-175B [146] and BLOOM-176B [147] and achieve 50-60% sparsity with moderate perplexity increase and performance decrease on downstream datasets. This work demonstrates the feasibility of low-resource pruning in >100 B models with moderate sparsity. \n\nAnother line of work mainly focuses on the structured pruning method to achieve a more significant speed-up. Language structural pruning has been conducted at multiple granularities. For instance, Michel et al. [148] discuss the redundancy of attention head, Fan et al. [149] propose a task-specific extraction of pre-trained sub-networks, Santacroce et al. [150] focus on FFN layers, and Block Pruning [151] is introduced to prune MHA and FFN parameters separately. In comparison, CoFi [152] proposes a more comprehensive coarse-and fine-grained pruning method that learns to generate masks at different granularity, i.e., FFN layers, FFN intermediate dimensions, MHA layers, Attention heads, and hidden dimensions to encourage the optimization in the high sparsity domain. In this way, the pruning of a specific parameter can be determined at different granularity in an end-to-end learnable manner. When pruning the general domain pre-trained BERT model to typical downstream datasets, CoFi achieves more than 10\u00d7 speedups with a limited accuracy gap. Later, this method was extended to a larger decoder-based LLM with a 7B parameter size [30]. With a pre-defined model shape, the pruned model shows better instruction tuning performance than models of similar size but trained from scratch. Also, specifying the model configuration with uniform size avoids irregularities in model shape and further increases the inference throughput.",
            "score": 0.5246447709345533,
            "section_title": "Pruning for Transformer-Based Large Language Models",
            "char_start_offset": 42434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 214,
                    "matchedPaperCorpusId": "259950394"
                },
                {
                    "start": 782,
                    "end": 787,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 974,
                    "end": 979,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 1058,
                    "end": 1063,
                    "matchedPaperCorpusId": "247922354"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.460693359375
        },
        {
            "corpus_id": "276107452",
            "title": "Progressive Binarization with Semi-Structured Pruning for LLMs",
            "text": "Ablation for SPBO Strategy. To validate the effectiveness of our SPBO strategy, we provide the performance of PBS 2 P with and without its application. As shown in Table 3a, the performance of SPBO surpasses that of the vanilla pruning followed by binarization approach. In the vanilla method, pruning is done in a single step, where all elements are pruned simultaneously, and binarization is applied to the remaining elements afterward. However, the vanilla approach makes it hard to effectively minimize the combined errors. These results demonstrate the performance improvement achieved through our SPBO strategy. Ablation for Group Size. Table 3b presents the results of our ablation study on the group size configuration. It indicates that a smaller group size, meaning finer-grained grouping, leads to better performance. However, this also comes with increased computational and storage costs. To strike a balance between performance and resource efficiency, we select a group size of 128. \n\nAblation for Metric in Coarse-Stage Search. Table 3c shows the performance of the coarse-stage search under three conditions: without the coarse-stage search, using the relative importance (RI) metric from STBLLM, and applying our layer redundancy (LR) score. The results reveal that using the RI metric causes a performance drop compared to the baseline without coarse-stage search. In contrast, applying our LR score leads to a clear improvement, demonstrating not only the effectiveness of our coarse-stage search but also the superiority of the LR score metric. \n\nAblation for Pruning Type. In Table 3d, we present the impact of different pruning types on the results, comparing structured pruning, unstructured pruning, and semistructured pruning under the same pruning ratio (50%). For semi-structured pruning, we use an N :M sparsity of 4:8, while for structured pruning, we apply column pruning. \n\nFor unstructured pruning, we perform elementwise pruning based on weight importance. Semi-structured pruning outperforms structured pruning for LLM compression. It also maintains hardware-friendliness while achieving performance levels similar to unstructured pruning.",
            "score": 0.5241566490028609,
            "section_title": "Ablation Study",
            "char_start_offset": 21525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.462158203125
        },
        {
            "corpus_id": "266550954",
            "title": "Fairness-Aware Structured Pruning in Transformers",
            "text": "Pruning of large language models can be split into two main categories: structured and unstructured pruning (Behnke and Heafield 2021b). Structured pruning involves removing specific building blocks within the model, such as attention heads or layers, which alters the overall model structure. On the other hand, unstructured pruning is more fine-grained, entailing the removal of certain model weights (Narang et al. 2017;Zhu and Gupta 2018), while retaining the original structure of the network. Structured pruning typically leads to faster models, while unstructured pruning results in less performance degradation (Behnke and Heafield 2021b). In this study, we focus on structured pruning to explore the impact of attention heads on fairness through targeted removal, which represents a relatively unexplored research avenue. Some of the pioneering works in the application of structural pruning were conducted by Voita et al. (2019) and Michel, Levy, and Neubig (2019), where the authors explored the removal of attention heads from transformerbased models. Their findings revealed the presence of important heads in terms of performance. While the removal of important heads led to model collapse, less critical heads had minimal impact on performance. Building upon these works, He and Choi (2021) conducted a detailed analysis of the important heads, demonstrating their interpretable roles in task-solving. \n\nMeanwhile, Bian et al. (2021) focused on investigating the non-important heads and concluded that these heads were redundant since their output exhibited a high correlation with other heads, making them inconsequential for final predictions. To address this, Zhang et al. (2021) proposed an approach for transforming non-important heads into important heads by injecting task-specific prior knowledge, thereby increasing their contribution to the output. In a separate study, Sajjad et al. (2023) examined layer removal in BERT (Devlin et al. 2019) with fine-tuning and showcased the importance of preserving lower layers to maintain performance. Furthermore, Fan, Grave, and Joulin (2020) investigated layer removal without fine-tuning and achieved considerable performance preservation through the implementation of layer dropout during training.",
            "score": 0.5230831212099122,
            "section_title": "Pruning of Large Language Models",
            "char_start_offset": 4177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 135,
                    "matchedPaperCorpusId": "245855868"
                },
                {
                    "start": 403,
                    "end": 423,
                    "matchedPaperCorpusId": "10135357"
                },
                {
                    "start": 619,
                    "end": 646,
                    "matchedPaperCorpusId": "245855868"
                },
                {
                    "start": 919,
                    "end": 938,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 943,
                    "end": 974,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1430,
                    "end": 1448,
                    "matchedPaperCorpusId": "235097467"
                },
                {
                    "start": 1895,
                    "end": 1915,
                    "matchedPaperCorpusId": "251005814"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.421142578125
        },
        {
            "corpus_id": "254685796",
            "title": "Gradient-based Intra-attention Pruning on Pre-trained Language Models",
            "text": "Transformer-based (Vaswani et al., 2017) pretrained language models (PLMs) have achieved great success and become the backbones of various natural language processing tasks. However, PLMs are computationally expensive and slow in inference due to their large sizes, which limits their applications in real-world scenarios. Hence, a growing interest has been in developing compression and acceleration methodologies for PLMs.\n\nA common approach to model compression is structured pruning, which compresses the model by removing groups of consecutive parameters, namely the pruning units. In applying structured 1 Code is available at https://github.com/airaria/ GRAIN. pruning on PLMs, recent works have investigated removing units such as hidden dimensions in feedforward layers, attention heads in the multi-head attention (Michel et al., 2019;Li et al., 2022), and coarse-grained units such as multi-head attention layers and feed-forward layers (Xia et al., 2022). However, these pruning units only span a small space of model structures and limit the exploration for better structures. For example, in the pruning of BERT base (Devlin et al., 2019), which contains 144 attention heads, the possible choices of attention heads for the pruned model are limited. Block Pruning (Lagunas et al., 2021) extends pruning units by considering blocks in the weight matrices, but Block Pruning is not a fully structured pruning method and can not achieve large speedups.\n\nIn this work, we propose GRAIN (Gradientbased Intra-attention pruning), a structured pruning method that prunes PLMs with finer pruning units. In the following, we present the method from three aspects: pruning units, pruning algorithm, and training objectives. Pruning Units Unlike attention heads pruning where the pruning unit is a single head, we propose intra-attention pruning, which inspects and prunes the structures inside attention heads. Intra-attention pruning greatly expands the search space of model structures, making the resulting models more likely to find better structures. However, directly applying intra-attention pruning yields fragmented models, i.e.",
            "score": 0.5230058364608261,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 40,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 824,
                    "end": 845,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 845,
                    "end": 861,
                    "matchedPaperCorpusId": "247996697"
                },
                {
                    "start": 948,
                    "end": 966,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1131,
                    "end": 1152,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1278,
                    "end": 1300,
                    "matchedPaperCorpusId": "237485472"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "267412953",
            "title": "Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward",
            "text": "The compression of LLMs represents a distinctive challenge compared to traditional deep learning models, primarily due to the substantial scale of the former. Many established compression methods rely on the paradigm of executing finetuning steps to regain lost performance during the compression stage. However, this approach encounters significant limitations when applied to LLMs owing to their considerable size, necessitating a paradigm shift in the treatment of LLM compression as an independent and new research domain. \n\nArchitecture pruning. LLM-Pruner [Ma et al., 2023] used Taylor series expansion by leveraging a single gradient step to estimate important parts of a pre-trained LLM. LoRAPrune [Zhang et al., 2023] outperformed LLM-Pruner by using gradients of LoRA [Hu et al., 2021] weights, offering computational efficiency. LoRAShear [Chen et al., 2023a] identified dependencies in LLMs, separated trainable variables into groups, and achieved compression through pruning and finetuning. Sheared LLaMA [Xia et al., 2023] introduced targeted structured pruning and dynamic batch loading for endto-end component removal. FLaP [An et al., 2023] a finetuning free structured pruning method which used a fluctuation based metric to determine the importance score of various weight columns. \n\nUnstructured pruning methods, such as SparseGPT [Frantar and Alistarh, 2023], adopted a one-shot technique without the need for fine-tuning. WANDA [Sun et al., 2023] pruned weights based on the product of weight values and activation inputs, eliminating the need for fine-tuning. Another recent work suggested fusing of OBS [Hassibi et al., 1993] and OBD [LeCun et al., 1989] criteria for weight selection and determining layer sparsity based on sensitivities derived from Hessian matrices [S. et al., 2023]. While the structured and unstructured methods mentioned above show promise, the observed performance drop for the achieved compression level remains relatively high. Further efforts are required in developing pruning methods that can lead to efficient LLMs. \n\nQuantization.",
            "score": 0.5224400378151615,
            "section_title": "Compression of LLMs",
            "char_start_offset": 10430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2069
                },
                {
                    "start": 2072,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 562,
                    "end": 579,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 773,
                    "end": 795,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 1450,
                    "end": 1468,
                    "matchedPaperCorpusId": "266435969"
                },
                {
                    "start": 1627,
                    "end": 1649,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 1658,
                    "end": 1677,
                    "matchedPaperCorpusId": "262460763"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5693359375
        },
        {
            "corpus_id": "277634221",
            "title": "Mosaic: Composite Projection Pruning for Resource-efficient LLMs",
            "text": "Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models.",
            "score": 0.5223648822871075,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "267547625",
            "title": "Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes",
            "text": "Here we discuss relevant work in LLM pruning, and defer a longer discussion compression methods to Appendix B. \n\nUnstructured pruning: Whilst structured pruning approaches remove entire model components like layers (Xu et al., 2020;Xia et al., 2022b), dimensions of linear layers (Wang et al., 2019) or attention heads (Michel et al., 2019;Held & Yang, 2022), unstructured pruning (Han et al., 2015;Frankle & Carbin, 2018;Benbaki et al., 2023;Sun et al., 2023) removes individual parameters of the model. These approaches achieve memory savings by inducing sparsity in the model weights, but they generally do not result in actual model speedups except when specialized hardware is available (Mishra et al., 2021). Proposed semi-structured sparsity methods (Mishra et al., 2021) such as 2:4 and 4:8 patterns do result in faster inference, but the speedup gains they achieve are far from the idealized 2\u00d7.",
            "score": 0.5223006585308329,
            "section_title": "Related Work",
            "char_start_offset": 20745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 113,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 904
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6845703125
        },
        {
            "corpus_id": "275094749",
            "title": "A multi-agent reinforcement learning based approach for automatic filter pruning",
            "text": "Network pruning has attracted significant attention in deep learning as an effective model compression technique. It reduces the model scale and computational complexity by removing redundant weights or entire structural components within the neural network while maintaining the performance of the original model to the greatest extent. Research on network pruning has experienced rapid development, including exploring pruning strategies, optimization objectives, automation methods, and so on 23,24 . \n\nAccording to the granularity of the pruning unit, network pruning methods can be divided into unstructured pruning (fine-grained) and structured pruning (coarse-grained) 14 . Unstructured pruning methods can remove neuron connections (weights) independently. In 2015, Han et al. 25 conducted the first real research on deep neural network pruning. It prunes the connections whose absolute values of weight parameters are lower than the threshold and proposes the three-stage iterative pruning process for the first time. This method, however, can lead to highly sparse models that are difficult to deploy on standard hardware. This team further proposed the \"Deep Compression\" method 26 . They implemented weight quantization to facilitate weight sharing and applied Huffman coding to achieve better compression. Despite its effectiveness, the method requires additional steps like quantization and coding, which can be complex and time-consuming. Yang et al. 27 proposed the energy-aware weight pruning algorithm for CNNs that directly uses energy consumption estimation to guide the pruning process and then locally fine-tuned with a closed-form least-square solution to quickly restore the accuracy. Sanh et al. 28 proposed movement pruning, a method that selects weights that tend to move away from zero and applies it to pre-trained language representations (such as BERT). However, these approaches may not generalize well to different hardware platforms. \n\nConversely, structured pruning focuses on removing entire channels, filters, or layers. Structured pruning methods maintain the regularity of the model structure, making them more hardware-friendly and easier to implement on existing platforms. However, they may not achieve the same level of compression as unstructured pruning. He et al. 29 introduced an iterative two-step algorithm to channel prune by a LASSO regression-based channel selection and least square reconstruction.",
            "score": 0.521853868479951,
            "section_title": "Related work",
            "char_start_offset": 4941,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2214
                },
                {
                    "start": 2215,
                    "end": 2299
                },
                {
                    "start": 2300,
                    "end": 2451
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 499,
                    "matchedPaperCorpusId": "253881571"
                },
                {
                    "start": 499,
                    "end": 501,
                    "matchedPaperCorpusId": "247350878"
                },
                {
                    "start": 676,
                    "end": 678,
                    "matchedPaperCorpusId": "257255597"
                },
                {
                    "start": 785,
                    "end": 787,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1190,
                    "end": 1192,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 1466,
                    "end": 1468,
                    "matchedPaperCorpusId": "2779809"
                },
                {
                    "start": 1721,
                    "end": 1723,
                    "matchedPaperCorpusId": "218665313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70703125
        },
        {
            "corpus_id": "263671908",
            "title": "Neural Language Model Pruning for Automatic Speech Recognition",
            "text": "More recently, [10] showed that inducing sparsity incrementally while training can let the model recover from pruning losses, yielding better classification rates. \n\nIn this paper, we investigate different choices involved in model pruning, and to what extent generic pruning assumptions are true in the context of language modeling for automatic speech recognition (ASR). Our main contributions are: \n\n\u2022 Perform an in-depth evaluation of the aforementioned aspects of pruning, namely: pruning criteria (magnitude and data driven); pruning methods (unstructured, structured and pruning factorized layers); and pruning scheduling (one-shot and incremental). \n\n\u2022 Propose a variant of low-ranking approximation suitable for training pipelines delivering multiple models with varied sizes. \n\n\u2022 Benchmark pruning approaches applied to language models used in state-of-the-art ASR systems in terms of accuracy, model size and inference complexity.",
            "score": 0.5198921553077261,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 166,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 941
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "27494814"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3203125
        },
        {
            "corpus_id": "252846445",
            "title": "GMP*: Well-Tuned Gradual Magnitude Pruning Can Outperform Most BERT-Pruning Methods",
            "text": "As any academic study, our work is not without its limitations. Following the literature, our extensive empirical studies were conducted only on the standard BERT BASE model, giving us opportunity to compare against a vast amount of different pruning techniques. Throughout the literature, this model emerged as a consistent benchmark for unstructured pruning methods. However, the current results don't directly imply that our findings will be generally applicable to other language models as well. Sparse pre-trained language models. arXiv preprint arXiv:2111.05754. \n\nMichael Zhu and Suyog Gupta. 2017. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878.",
            "score": 0.519045918111915,
            "section_title": "Limitations",
            "char_start_offset": 12222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 722
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447265625
        },
        {
            "corpus_id": "270214634",
            "title": "Large Language Model Pruning",
            "text": "The computation time may also be saved by such a design.The last approach, called low-rank factorization [25,26], emphasizes replacing the large weight matrix with a small weight matrix to reach the goal of model compression.\n\nTo focus on the family of network pruning techniques, we consider two types of methods, the structured and unstructured ones.The overall goal is to remove redundant components from the models.First, the structured pruning simplifies an LLM by removing the entire structural components, such as neurons, channels, or layers while keeping the network structures [27,28].On the other hand, the unstructured pruning [18,29] is aimed at pruning the redundant neurons or links.Given the approach of deleting individual parameters, we may suffer from the irregular sparse structure problems.To compare between the two, the structured pruning can be deployed to various edge devices directly, while the unstructured pruning needs to be accompanied by the assistance of extra software or hardware treatment to complete the task [28].\n\nWe have to keep in mind a few items when designing an appropriate compression method for LLMs.First, quite a few LLMs were trained mainly based on unlabeled data.Some of these types of pre-trained LLMs can be used by third-party developers for various downstream tasks.Therefore, we do not know and we cannot assume any label or contextual information provided when building the LLMs.That is to say, we do not know what could be the real mission when building the models or applying any compression to the models [30].After all, we better assume only an unsupervised type of compression applied to LLMs.Second, most pruning methods need retraining to maintain the models' performance.This retraining procedure inevitably needs to consume various resources [30] and should be avoided if possible.In LLMs, the Feedforward Network (FFN) is the major part of the models in terms of the models' size and it attracts the most computation [31].Therefore compressing this part of a network means a lot if refers to the saving of space and time complexity.",
            "score": 0.5187378898446833,
            "section_title": "Introduction",
            "char_start_offset": 4016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 56,
                    "end": 225
                },
                {
                    "start": 227,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 595
                },
                {
                    "start": 595,
                    "end": 698
                },
                {
                    "start": 698,
                    "end": 811
                },
                {
                    "start": 811,
                    "end": 1051
                },
                {
                    "start": 1053,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1215
                },
                {
                    "start": 1215,
                    "end": 1322
                },
                {
                    "start": 1322,
                    "end": 1437
                },
                {
                    "start": 1437,
                    "end": 1571
                },
                {
                    "start": 1571,
                    "end": 1656
                },
                {
                    "start": 1656,
                    "end": 1737
                },
                {
                    "start": 1737,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 1990
                },
                {
                    "start": 1990,
                    "end": 2100
                }
            ],
            "ref_mentions": [
                {
                    "start": 1985,
                    "end": 1989,
                    "matchedPaperCorpusId": "211532645"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "267760017",
            "title": "Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers",
            "text": "There are many metrics to evaluate which neurons to drop, but here we will use the L2-norm of the vector corresponding to the connections attached to the neuron. To further understand different metrics to perform pruning see these comprehensive surveys (Liang et al., 2021;Xu et al., 2020;Deng et al., 2020). \n\nThe second question is also more engineering work. In fully connected layers, we represent connections with just a vector, so performing unstructured or structured pruning has the same effect. However, in convolutional layers, the connections are represented with matrices, so while unstructured pruning removes single elements, structured pruning is more harmful because it removes entire rows. The trade-off evaluation to balance these methods is that, besides being harsher, structured pruning has a direct impact on the model size reduction (removing neurons, filters, and channels decreases matrices dimensions), whereas unstructured just increases sparsity. Clearly, if the sparsity is very pronounced, we can use different algorithms to perform sparse matrix multiplication and also other matrix representations that require less hardware. In the end, this question is also solved by trial and error with different percentages of unstructured and structured pruning.",
            "score": 0.5177845598009356,
            "section_title": "Compressing the model",
            "char_start_offset": 18886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1284
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 273,
                    "matchedPaperCorpusId": "231699188"
                },
                {
                    "start": 273,
                    "end": 289,
                    "matchedPaperCorpusId": "221591820"
                },
                {
                    "start": 289,
                    "end": 307,
                    "matchedPaperCorpusId": "215799572"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12322998046875
        },
        {
            "corpus_id": "275920740",
            "title": "On Accelerating Edge AI: Optimizing Resource-Constrained Environments",
            "text": "While structured pruning offers consistent inference acceleration across hardware, unstructured pruning often achieves higher compression ratios but requires specialized hardware or compiler optimizations to realize speedups. \n\nStructured pruning maximizes speed improvements by carefully selecting architectural elements to prune in a way that minimizes performance degradation. Semi-structured pruning is a catch-all category for many methods that blend elements of structured and unstructured pruning Xu et al. [2024a], Ma et al. [2020], Meng et al. [2020]. By sequentially performing coarse pruning and then fine-grained pruning of the remaining structures, authors achieve greater compression levels while retaining performance. \n\nAs we mentioned, pruning seeks to decrease neural networks' size and computational expenses by eliminating weights or structural elements that minimally affect overall performance. Following this, we will introduce various pruning strategies-which differ mainly by the timing of their application-and emphasize significant research gaps. \n\nPre-Training Pruning: One forward-looking method is to prune networks based on randomly initialized weights Wang et al. [2020a]. Though appealing for saving training time (because pruned weights require no subsequent computation), this approach risks issues like layer collapse Tanaka et al. [2020]. It has been applied to convolutional neural nets (CNNs) Lee et al. [2019Lee et al. [ , 2020]], but scaling it up to large models remains challenging, given the expense of training even pruned networks. \n\nPruning During Training: Another strategy embeds pruning into the training loop, iteratively updating which weights remain active. For instance, the RigL algorithm Evci et al. [2021] periodically removes and regrows weights to maintain model capacity. Structured sparsity learning (SSL) Wen et al. [2016], network slimming Liu et al. [2017], and differentiable methods like Differential Sparsity Allocation (DSA) Ning et al. [2020] similarly integrate pruning decisions with gradient-based updates. In neural architecture search, a related concept called Progressive Shrinking alternates between pruning and training to explore potential architectures Wang et al. [2020b].",
            "score": 0.5169669868563971,
            "section_title": "Pruning",
            "char_start_offset": 6723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 228,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1577
                },
                {
                    "start": 1580,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "275336918",
            "title": "Swift Cross-Dataset Pruning: Enhancing Fine-Tuning Efficiency in Natural Language Understanding",
            "text": "Main Experiments. The evaluation results of the baseline methods are compared to our proposed method. We conduct experiments on multiple pruning rates to investigate how our method perform at different data compression rates. In Table 2, we present the performance on all datasets at 50% and 70% pruning rates. Full results at 10%, 30%, 50% and 70% over all datasets are presented in Appendix A.4. In overall, our proposed method performs the best compared to all other baselines methods and outperforms baseline methods. \n\nWhen compared to SOTA baselines, our method has best performance overall. Our method consistently outperforms state-of-the-art baselines AUM, EL2N, Forgetting and CCS. For high post-pruning coreset size, distance-based stratified sampling is particularly effective because it ensures that the pruned dataset remains both diverse and informative, eliminating redundant data while preserving the core structure of the dataset. At small postpruning coreset size, furthest samples prove to be effective, since most distant from the central tendency can help maintain the diversity and richness of the data. \n\nComparison between our method and random baseline is shown in Table 4. At 10% pruning rate, the overall improvement to random is 2.57%, and at 70% pruning rate, the overall improvement to random is 1.19%. \n\nWe tested our methods on other language models, specifically BERT (Devlin et al., 2019), AL-BERT (Lan et al., 2020), XLNet (Yang et al., 2019) and RoBERTa (Liu, 2019)    across all models, which prove that our method is not a phenomenon with respect to a particular language model. Dataset size-adaptive pruning. In Figure 3, we compare three different pruning strategies from distance-based scores: only keeping the closest samples to geometric median, only keeping the furthest samples to geometric median, and using stratified sampling to select the coreset. \n\nWith regard to the proposed dataset size-adaptive pruning, there are three cases where keeping furthest samples are applied: for MRPC dataset at 70% pruning rate, and for RTE dataset at 50% and 70% pruning rates.",
            "score": 0.5166069528325725,
            "section_title": "Experimental Results",
            "char_start_offset": 12506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 1402,
                    "end": 1423,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1433,
                    "end": 1451,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 1459,
                    "end": 1478,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09088134765625
        },
        {
            "corpus_id": "259360935",
            "title": "Pruning vs Quantization: Which is Better?",
            "text": "Other types of pruning While we solely focused in our comparison on unstructured pruning in which individual weights are removed, our results translate to semi-structured and structured pruning. Unstructured pruning has more degrees of freedom and is a strict superset of what can be represented by (semi-)structured pruning. Therefore, unstructured pruning gives an upper bound of the accuracy for all pruning methods. This means that for the cases in which quantization is better than unstructured pruning, quantization will also be better than (semi-)structured pruning. However, we can not make any claims for (semi-)structured pruning for the few scenarios in which pruning is better than quantization. \n\nNatural sparsity in quantized tensors In our comparison, we used a theoretical compression ratio for quantization, which depends on the bitwidth. However, we also observe that quantized tensors naturally contain many zeros; for example, 8-bit tensors from PyTorch model zoo have an average sparsity of 13% while 4-bit tensors are 35% sparse. We give more details on this in appendix C.",
            "score": 0.5165481861312131,
            "section_title": "Discussion",
            "char_start_offset": 21366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1095
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11993408203125
        },
        {
            "corpus_id": "275550746",
            "title": "Tailored Channel Pruning: Achieve Targeted Model Complexity Through Adaptive Sparsity Regularization",
            "text": "Unlike unstructured pruning, which removes connections with individual weights [8], [9], [10], structured pruning removes whole structured components, such as channels or layers from a neural network. Although structured pruning generally has a lower compression rate than unstructured pruning, it provides practical performance improvements on modern GPUs where computations are performed at the tensor level due to computational efficiency. For structured pruning, some studies [18], [19], [21], [27], [28], [29] have focused on defining suitable importance measures for selecting parameters to remove from a pre-trained model. These methods have a weakness due to the way they rank the importance of groups of parameters: even if they are perfectly ranked, pruning can degrade performance if lower-ranked groups contain meaningful activations.",
            "score": 0.5165431484690157,
            "section_title": "II. RELATED WORKS A. STRUCTURED PRUNING",
            "char_start_offset": 6469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 846
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 89,
                    "end": 93,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 480,
                    "end": 484,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 486,
                    "end": 490,
                    "matchedPaperCorpusId": "51608028"
                },
                {
                    "start": 492,
                    "end": 496,
                    "matchedPaperCorpusId": "4142619"
                },
                {
                    "start": 498,
                    "end": 502,
                    "matchedPaperCorpusId": "258549779"
                },
                {
                    "start": 504,
                    "end": 508,
                    "matchedPaperCorpusId": "174800733"
                },
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "233394250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "269635222",
            "title": "OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning",
            "text": "Another way to achieve lightweight LLMs is model compression.Model compression, including pruning, quantization, and distillation, achieves the trade-off between performance and efficiency based on existing outstanding LLMs.Pruning reduces model parameters by eliminating modules, neurons, or individual connections that have a minimal impact on performance.Pruning can be categorized into unstructured and structured pruning.Unstructured pruning targets individual connections between neurons, resulting in sparse weight matrices that require specific hardware for efficiency gains (Frantar & Alistarh, 2023;Sun et al., 2024;van der Ouderaa et al., 2024).Structured pruning removes entire rows or columns of weights, creating smaller dense matrices that are more hardware-friendly.However, it faces significant performance degradation at high compression ratios (> 30%) (Ma et al., 2023b;Zhang et al., 2023;An et al., 2023).To improve the performance of the structured pruned model, Xia et al. (2024) introduce the dynamic continual pre-training strategy, which resamples critical data for performance recovery for training.Quantization uses lower-bitwidth integers (Frantar et al., 2023;Dettmers et al., 2024;Liu et al., 2024;Li et al., 2024b) or floats (Liu et al., 2023;Perez et al., 2023) to represent weights, activations and KV caches.It reduces the memory required for LLMs, which is essential when memory is a bottleneck for model deployment.Currently, 8-bit quantization achieves nearly lossless performance compression and is compatible with most GPUs (Li et al., 2024a).However, researchers are not satisfied with this and continue to push the boundaries of ultra-low quantization precisions below 4-bit (Yuan et al., 2023;Ma et al., 2024).",
            "score": 0.5165001537935789,
            "section_title": "Model Compression for LLMs",
            "char_start_offset": 5798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 61,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 426
                },
                {
                    "start": 426,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 925
                },
                {
                    "start": 925,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1451
                },
                {
                    "start": 1451,
                    "end": 1582
                },
                {
                    "start": 1582,
                    "end": 1752
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 626,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 626,
                    "end": 655,
                    "matchedPaperCorpusId": "266573164"
                },
                {
                    "start": 871,
                    "end": 889,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 984,
                    "end": 1001,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 1189,
                    "end": 1211,
                    "matchedPaperCorpusId": "259076379"
                },
                {
                    "start": 1211,
                    "end": 1228,
                    "matchedPaperCorpusId": "263908852"
                },
                {
                    "start": 1228,
                    "end": 1245,
                    "matchedPaperCorpusId": "264128197"
                },
                {
                    "start": 1256,
                    "end": 1274,
                    "matchedPaperCorpusId": "264451753"
                },
                {
                    "start": 1274,
                    "end": 1293,
                    "matchedPaperCorpusId": "263310873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.572265625
        },
        {
            "corpus_id": "273963228",
            "title": "CULL-MT: Compression Using Language and Layer pruning for Machine Translation",
            "text": "Pruning (LeCun et al., 1989) is a key approach for compressing neural networks, aiming to identify and remove less important weights using various methods. While unstructured pruning allows for the sparsification of specific weights, it often necessitates specialized hardware for speed improve- ments (Cheng et al., 2024). In contrast, structural pruning (Molchanov et al., 2017) focuses on removing entire filters, blocks, or layers, providing efficiency that is not dependent on specific devices (Cheng et al., 2024). Voita et al. (2019) indicated that the top layers of pre-trained transformer (Vaswani et al., 2017) models are often specialized for the original training objectives. Therefore, Sajjad et al. (2023) showed that the higher layers are not essential for finetuning on downstream tasks and they can be pruned. Peer et al. (2022) introduce a Greedy Layer Pruning (GLP) method that iteratively selects layers to remove, optimizing performance on GLUE tasks with encoder-only models. While their focus is on GLUE tasks, we concentrate on translation tasks involving encoder-decoder and decoder-only architectures. \n\nRecent works concentrate on pruning LLMs. Ma et al. (2023) developed LLM-Pruner, which uses structural pruning to remove non-critical coupled structures based on gradient information, effectively compressing large language models while maintaining their core functionality. This method allows for performance recovery with tuning techniques like LoRA and demonstrates satisfactory zero-shot task capabilities across various models. Furthermore, Gromov et al. (2024) highlighted that deeper layers often exhibit highly similar representations. This similarity allows for significant model compression, particularly by pruning deeper layers starting from the penultimate layer, which can be achieved without major performance loss when combined with light fine-tuning.",
            "score": 0.5162307891948567,
            "section_title": "Structural Pruning",
            "char_start_offset": 6917,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 27,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 302,
                    "end": 322,
                    "matchedPaperCorpusId": "260887757"
                },
                {
                    "start": 356,
                    "end": 380,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 499,
                    "end": 519,
                    "matchedPaperCorpusId": "260887757"
                },
                {
                    "start": 521,
                    "end": 540,
                    "matchedPaperCorpusId": "202541078"
                },
                {
                    "start": 598,
                    "end": 620,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 699,
                    "end": 719,
                    "matchedPaperCorpusId": "251005814"
                },
                {
                    "start": 827,
                    "end": 845,
                    "matchedPaperCorpusId": "247771234"
                },
                {
                    "start": 1172,
                    "end": 1188,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77880859375
        },
        {
            "corpus_id": "271744772",
            "title": "A Convex-optimization-based Layer-wise Post-training Pruner for Large Language Models",
            "text": "These methods prune LLMs in a single step, simplifying implementation and reducing both time and computational demands. Consequently, various one-shot pruning algorithms have been developed under different sparsity frameworks. For structured pruning, SliceGPT (Ashkboos et al., 2024) and Eigenpruning (Vergara-Browne et al., 2024) utilize singular value decompositions to prune singular values of weight matrices and reduce model dimensions. ZipLM (Kurti\u0107 et al., 2024) adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Our proposed FISTAPruner focuses on unstructured and semi-structured pruning, and thus is orthogonal to these structured pruning methods, enabling further model compression. For unstructured and semi-structured pruning, SparseGPT (Frantar and Alistarh, 2023) and ISC (Shao et al., 2024) leverage the OBS framework to calculate saliency for each entry using the inverse Hessian of the loss metric, based on which pruning masks are generated and weights updated. Wanda (Sun et al., 2023) implements a heuristic approach, removing weights based on the product of their magnitudes and activations without compensation. (Bo\u017ea, 2024) employs ADMM to optimize weight updates under iteratively refined pruning masks chosen through heuristic methods. These strategies adopt a layer-wise pruning strategy, where errors between the pruned output and the original output of each operator accumulates. Moreover, due to their heuristic nature, the performances of the pruned models are unstable and compromised. In contrast, our proposed FISTAPruner introduces a novel layer-wise one-shot pruning approach by formulating it as a convex optimization problem with intra-layer error corrections. It employs the FISTA solver to efficiently compute optimal pruned weights, providing theoretical guarantees for performance stability and effectiveness.",
            "score": 0.5160946817326528,
            "section_title": "Background and Related Work",
            "char_start_offset": 6720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 469,
                    "matchedPaperCorpusId": "256662263"
                },
                {
                    "start": 807,
                    "end": 835,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 844,
                    "end": 863,
                    "matchedPaperCorpusId": "264146174"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "277244079",
            "title": "Large Language Model Compression via the Nested Activation-Aware Decomposition",
            "text": "However, the irregular sparsification achieved through unstructured pruning often fails to achieve the desired speedup or memory savings. Structured pruning, in contrast, removes entire channels or components from LLMs, making it easier to implement on hardware but potentially leading to significant accuracy degradation, especially under high compression ratios as seen with LLM-Pruner (Ma et al., 2023). \n\nQuantization techniques reduce the precision of weight matrices, offering limited compression options typically ranging from 3 to 8 bits, which may not fully utilize available memory budgets. GPTQ is a notable example that uses layer-wise quantization and updates weights with inverse Hessian information (Frantar et al., 2022). \n\n\u25b6 Low-rank approximation for LLMs. Despite the popularity of low-rank factorization as a neural network compression technique, its application in LLMs remains relatively underexplored. This gap is addressed through the introduction of novel low-rank decomposition methods designed specifically for LLMs, such as ASVD (Yuan et al., 2023). To be more specific, standard SVD focuses on compressing original weight matrices without considering parameter importance, which can result in larger compression errors. To address this, FWSVD incorporates Fisher information to weigh parameter importance, ASVD scales the weight matrix by a diagonal matrix representing input channel impact to account for activation distribution effects, while SVD-LLM further scales the weight matrix by a Cholesky decomposition of the activation matrix to upper bound the reconstruction error (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024). However, all of these methods face challenges in balancing compression efficiency and accuracy, especially under medium to high compression scenarios for datasets with different languages or tasks. \n\nDespite advancements such as the introduction of rank-adaptive methods like FWSVD, ASVD, and SVD-LLM, which aim to optimize compression efficiency and accuracy by considering parameter importance and activation distribution effects, significant challenges remain. These methods struggle with severe accuracy degradation under medium to high compression ratios due to unawareness of the discrepancy between different datasets.",
            "score": 0.5158140781148913,
            "section_title": "Related Work",
            "char_start_offset": 5237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2289
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "252815687",
            "title": "CWP: Instance complexity weighted channel-wise soft masks for network pruning",
            "text": "Convolutional neural networks have shown excellent performance in computer vision tasks Dong et al. (2015); Girshick et al. (2014); Xu et al. (2020); Zhao et al. (2021); Bosquet et al. (2023); Wu et al. (2022); Ma et al. (2022b) such as object detection and image recognition. To make the network perform well in various tasks, the size of network has been continuously increased, which results in extensive computation consumption. Deploying such large networks to edge devices is impossible due to limited computing resources. To cope with this issue, model compression has been introduced to obtain compact and efficient subnetworks with little damage to the model performance. \n\nThere are many excellent model compression methods including knowledge distillation Xu et al. (2019); Hinton et al. (2015), low-rank decomposition Yu et al. (2017), and network quantization Wan et al. (2020); Wei et al. (2022). They all have their own advantages and disadvantages while complement each other. A plenty of recent approaches have focused on another model compression method -Pruning. Pruning can be divided into structured pruning and unstructured pruning. Unstructured pruning operates on weights or connections of network, and requires specific hardware devices or libraries to speed up computation after pruning. Structured network pruning is a widely used model compression technique, it aims to find and remove redundant structures from a pre-trained or baseline network. Current differentiable pruning methods Lin et al. (2018); You et al. (2019a,b); Gao et al. (2020); Kim et al. (2020); Kang and zhyu@nxu.edu.cn (Z. Yu) ORCID(s): Han (2020); Tang et al. (2021) often introduce channelwise masks (or gates) to indicate pruning status of each filter, and employ sparsity constraints such as 1 regularizer to make some gates or masks approach zero.",
            "score": 0.515411410481714,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1851
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 106,
                    "matchedPaperCorpusId": "6593498"
                },
                {
                    "start": 108,
                    "end": 130,
                    "matchedPaperCorpusId": "215827080"
                },
                {
                    "start": 132,
                    "end": 148,
                    "matchedPaperCorpusId": "208857690"
                },
                {
                    "start": 150,
                    "end": 168,
                    "matchedPaperCorpusId": "226321639"
                },
                {
                    "start": 170,
                    "end": 191,
                    "matchedPaperCorpusId": "251985240"
                },
                {
                    "start": 193,
                    "end": 209,
                    "matchedPaperCorpusId": "250272290"
                },
                {
                    "start": 211,
                    "end": 228,
                    "matchedPaperCorpusId": "203591996"
                },
                {
                    "start": 767,
                    "end": 783,
                    "matchedPaperCorpusId": "58952128"
                },
                {
                    "start": 830,
                    "end": 846,
                    "matchedPaperCorpusId": "24553488"
                },
                {
                    "start": 873,
                    "end": 890,
                    "matchedPaperCorpusId": "216445229"
                },
                {
                    "start": 1514,
                    "end": 1531,
                    "matchedPaperCorpusId": "51607445"
                },
                {
                    "start": 1555,
                    "end": 1572,
                    "matchedPaperCorpusId": "219965256"
                },
                {
                    "start": 1574,
                    "end": 1591,
                    "matchedPaperCorpusId": "208006068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140380859375
        },
        {
            "corpus_id": "258015063",
            "title": "Benchmarking Neural Network Compression Techniques for Ocular-Based User Authentication on Smartphones",
            "text": "combination of pruned and quantized models decreased by about 0.44 MB. \n\nFurther, Table 11 shows the subject-independent analysis of the quantized models after being compressed using a combination of KD and pruning techniques (i.e., Pruning-KD-Quantization and KD-Pruning-Quantization). The models were quantized using PTQ after being distilled and pruned using vanilla KD and L2-norm-based structured pruning. The results suggested that the PTQ method could preserve the equivalent performance across all points (EER, GMR, and AUC). The quantized models resulted in a further size reduction of 23.66 KB for ResNet-20 and 0.43 MB for VGG-8 over both KD-Pruning and Pruning-KD. Note that KD does not impact the original size of the model since it is a training and learning mechanism for performance enhancement of the compact student models. \n\nOverall, the combination of quantized and pruned models (using unstructured pruning) obtained almost close to zero performance difference (about 0.04% difference) compared to the pruned models. Likewise, applying quantization to models compressed using structured pruning obtained an EER decrease of approximately 0.14%. This analysis suggested that the combination of pruning and quantization techniques further reduced the models' sizes while maintaining a performance similar to those of pruned models.",
            "score": 0.5152594472539945,
            "section_title": "F. EVALUATION OF THE COMBINATION OF PRUNING AND QUANTIZATION",
            "char_start_offset": 46900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 73,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1349
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0982666015625
        },
        {
            "corpus_id": "276551557",
            "title": "A Survey on Real-Time Object Detection on FPGAs",
            "text": "Pruning is typically performed in software before deploying the model on hardware. However, performance can be further enhanced if the pruning process accounts for the capabilities and custom hardware features of the target FPGA, as highlighted in some of the works discussed below. \n\nPruning is defined as identifying and eliminating neurons, kernels, weights, and channels that have minimal or negligible impact on the final accuracy of an AI model to reduce network complexity [225]. It offers several advantages, including reducing computational load and required memory and improving accuracy per parameter and per operation [150], [203]. To compensate for the possible effect of this on the accuracy, the pruned model, also known as a sparse network [225], needs to be retrained [226]. To avoid slow convergence during the retraining of a sparse network, pruning should be conducted incrementally, with each group of layers pruned in a separate stage [225]. \n\nPruning methods can be broadly classified into two categories [227]: unstructured pruning and structured pruning. Figure 8 illustrates the distinction between these approaches in a fully connected layer. \n\nIn unstructured pruning [228], [229], [230], weights with low sensitivity are selectively removed throughout the network. This method allows for aggressive pruning, removing a significant portion of neural network parameters with insignificance accuracy loss. Wang et al. [73] apply unstructured pruning to YOLOv2 [71], demonstrating its effectiveness in enabling real-time object detection on FPGAs. Detailed results are provided in Sections V-A and V-B. However, unstructured pruning leads to sparse matrix operations, which are challenging to accelerate and are often memory-bound [231]. \n\nStructured pruning [190], [232], [233], on the other hand, involves removing a group of parameters, such as the entire kernel. This approach alters the input and output shapes of layers and weight matrices, allowing for dense matrix operations to continue. However, aggressive structured pruning often results in considerable accuracy degradation.",
            "score": 0.514710801928359,
            "section_title": "a: PRUNING",
            "char_start_offset": 73629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1762
                },
                {
                    "start": 1765,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 630,
                    "end": 635,
                    "matchedPaperCorpusId": "245634301"
                },
                {
                    "start": 637,
                    "end": 642,
                    "matchedPaperCorpusId": "207220904"
                },
                {
                    "start": 785,
                    "end": 790,
                    "matchedPaperCorpusId": "744803"
                },
                {
                    "start": 1210,
                    "end": 1215,
                    "matchedPaperCorpusId": "202787073"
                },
                {
                    "start": 1444,
                    "end": 1448,
                    "matchedPaperCorpusId": "220308105"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "786357"
                },
                {
                    "start": 1784,
                    "end": 1789,
                    "matchedPaperCorpusId": "5073700"
                },
                {
                    "start": 1791,
                    "end": 1796,
                    "matchedPaperCorpusId": "575794"
                },
                {
                    "start": 1798,
                    "end": 1803,
                    "matchedPaperCorpusId": "11169209"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.178955078125
        },
        {
            "corpus_id": "257255597",
            "title": "Structured Pruning for Deep Convolutional Neural Networks: A Survey",
            "text": "ResNet [8] and DenseNet [9] have been proposed. These architectures contain millions of parameters and require large computing power, making deployments on resource-limited hardware challenging. Model compression is a solution for this problem, aiming to reduce the number of parameters, computational cost, and memory consumption. As such, its study has gained importance. \n\nTo generate more efficient models, model compression techniques including pruning [10], quantization [11], decomposition [12], and knowledge distillation [13] have been proposed. The term \"pruning\" refers to removing components of a network to produce sparse models for acceleration and compression. The objective of pruning is to minimize the number of parameters without significantly affecting the performance of the models. Most research on pruning has been conducted on CNNs for the image classification task, which is the foundation for other computer vision tasks. \n\nPruning can be categorized into unstructured [10] and structured pruning [14]. Unstructured pruning removes connections (weights) of neural networks, resulting in unstructured sparsity. Unstructured pruning often leads to a high compression rate, but requires specific hardware or library support for realistic acceleration. Structured pruning removes entire filters of neural networks, and can achieve realistic acceleration and compression with standard hardware by taking advantage of a highly efficient library such as the Basic Linear Algebra Subprograms (BLAS) library. \n\nRevisiting the properties of CNNs from the perspective of structured pruning is meaningful in the era of Transformers [15]. Recently, there has been an increasing trend of incorporating the architectural design of CNNs into Transformer-based models [16], [17], [18], [19], [20]. Although the self-attention [21] in Transformers is effective in computing a representation of the sequence, an enormous amount of training data is still needed since Transformers often lack induction biases [18], [22], [23]. In contrast, the structure of CNNs enforces two key inductive biases on the weights: locality and weight sharing, to influence the generalization of learning algorithms and independent of data [18].",
            "score": 0.5146823133501232,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 947
                },
                {
                    "start": 950,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 7,
                    "end": 10,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 24,
                    "end": 27,
                    "matchedPaperCorpusId": "9433631"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 477,
                    "end": 481,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "7340116"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 995,
                    "end": 999,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1023,
                    "end": 1027,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 1646,
                    "end": 1650,
                    "matchedPaperCorpusId": "245837420"
                },
                {
                    "start": 1777,
                    "end": 1781,
                    "matchedPaperCorpusId": "244920703"
                },
                {
                    "start": 1783,
                    "end": 1787,
                    "matchedPaperCorpusId": "232290742"
                },
                {
                    "start": 1789,
                    "end": 1793,
                    "matchedPaperCorpusId": "248406101"
                },
                {
                    "start": 1795,
                    "end": 1799,
                    "matchedPaperCorpusId": "247011605"
                },
                {
                    "start": 1801,
                    "end": 1805,
                    "matchedPaperCorpusId": "235606268"
                },
                {
                    "start": 1835,
                    "end": 1839,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 2015,
                    "end": 2019,
                    "matchedPaperCorpusId": "248406101"
                },
                {
                    "start": 2021,
                    "end": 2025,
                    "matchedPaperCorpusId": "227239152"
                },
                {
                    "start": 2027,
                    "end": 2031,
                    "matchedPaperCorpusId": "380052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40869140625
        },
        {
            "corpus_id": "256662263",
            "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
            "text": "The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning [2], quantization [3], and distillation [4]. In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy. In addition, structural compression significantly complicates the use of knowledge distillation [5], which is usually done via manual or dynamic layer mapping [6,7]. On the practical side, another challenge is that most existing techniques do not provide runtime speedup guarantees: the model is pruned to a fixed sparsity or FLOPS target, and then must be evaluated in the target inference environment. If the pruned model fails to meet the target inference specifications, the whole process must be repeated from scratch. \n\nOverview. In this paper, we resolve these issues and provide a novel structured pruning approach called ZipLM, which achieves state-of-the-art performance, both in the post-training/one-shot setting, where retraining is not desirable, as well as in the popular gradual compression setting, where retraining is possible. We accomplish this via an inference-aware algorithm, which successfully balances the loss-runtime trade-off at each pruning step. By taking runtime into account, we avoid removing components that do not bring significant speedup gains. Additionally, our algorithm provides speedup guarantees for compressed models, a highly-desirable property in practical applications. \n\nWe summarize our contributions as follows:",
            "score": 0.5145846601366911,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1584
                },
                {
                    "start": 1587,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2276
                },
                {
                    "start": 2279,
                    "end": 2321
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 308,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 345,
                    "end": 348,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1220,
                    "end": 1223,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1223,
                    "end": 1225,
                    "matchedPaperCorpusId": "247922354"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90576171875
        },
        {
            "corpus_id": "258987626",
            "title": "Accurate and Structured Pruning for Efficient Automatic Speech Recognition",
            "text": "We compare our approach with other widely-used compression methods: SVD [30] and two magnitude pruning methods (One-shot Magnitude Pruning (OMP), PARD [16]). In particular, OMP and PARD are originally implemented for unstructured pruning, which do not provide any benefits to inference latency. For a fair comparison, we implement OMP and PARD using our structured pruning granularity setting. \n\nTo evaluate the performance of the pruned models, we report the WER results on the LibriSpeech test, as well as the encoder model size and the inference real-time factor (RTF) of the encoder part on a single-core Intel(R) Xeon(R) CPU (2.60GHz).",
            "score": 0.5139203794953715,
            "section_title": "Experimental Setup",
            "char_start_offset": 12551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 640
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "4949673"
                },
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "235390847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5771484375
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "Deep neural networks (DNN) are in high demand because of their widespread applications in natural language processing, image processing, and a lot of other domains. However, due to their computational expense, over-parameterization, and large memory requirements, DNN applications often require the use of substantial model resources. This strict requirement of latency and limited memory availability are hurdles in the device deployment of these technologies. Therefore, a common idea could be to mitigate the DNN-based models\u2019 size without any performance degradation using different compression techniques. During the last few years, a great deal of progress has been made in the field of Natural Language Processing (NLP) using deep learning approaches. The objective of this research is to offer a thorough overview of the various pruning methods applied in the context of NLP. In this paper, we review several recent pruning-based schemes used for converting standard networks into their compact and accelerated versions. Traditionally, pruning is a technique for improving latency, reducing model size, and computational complexity which is a viable approach to deal with the above-mentioned challenges. In general, these techniques are divided into two main categories: structural and unstructured pruning methods. Structural pruning methods are further classified into filter, channel, layer, block, and movement pruning. Whereas, neuron, magnitude-based, and iterative pruning lie in the category of unstructured pruning. For each method, we discuss the related metrics and benchmarks. Then recent work on each method is discussed in detail, which provides insightful analysis of the performance, related applications, and pros and cons. Then, a comparative analysis is provided to analyze the differences among approaches. Finally, the paper concludes with possible future directions and some technical challenges.",
            "score": 0.5136347987521396,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "256000028",
            "title": "HCE: Improving Performance and Efficiency with Heterogeneously Compressed Neural Network Ensemble",
            "text": "Model compression for DNNs have been widely used to accelerate models on resource-limited devices. Two popular methods are pruning and quantization. \n\nNeural network pruning aims to zero out redundant parameters to reduce both model size and computation cost. Generally it can be categorized into 1) unstructured pruning and 2) structured pruning. Unstructured pruning can achieve high sparse ration with negligible accuracy drop [18]- [23]. However, unstructured pruning can hardly bring actual speedup on hardware because of the irregular non-zero data structure. On the other hand, coarse-gained structured pruning can benefit real-life hardware inference such as filter pruning [9] and channel pruning [11]. Although structured pruning can generate hardware-friendly weight structures, it cannot achieve relatively high sparse ratio compared to unstructured pruning. \n\nQuantization is another common and effective way to compress the deep learning models. It reduces the DNN model size and lower computation cost by replacing the floating point weights with low precision fixed-point data. Common quantization methods including directly apply uniform quantizers [24], [25], quantization-aware fine-tuning [26] and mixedprecision quantization [7], [8], [27]. Quantization can largely decrease DNN's arithmetic intensity but still cause significant accuracy degradation for ultra-low data precision. \n\nOur work explores the intrinsic diversity in the compressed model decision boundary induced by different compression methods. HCE utilizes the diversity between pruned and quantized model to build efficient ensemble, and further improves the ensemble performance with a diversity-aware sub-model training objective.",
            "score": 0.5133974188956792,
            "section_title": "B. Model Compression",
            "char_start_offset": 6619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1719
                }
            ],
            "ref_mentions": [
                {
                    "start": 682,
                    "end": 685,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 1166,
                    "end": 1170,
                    "matchedPaperCorpusId": "9897483"
                },
                {
                    "start": 1172,
                    "end": 1176,
                    "matchedPaperCorpusId": "9183542"
                },
                {
                    "start": 1246,
                    "end": 1249,
                    "matchedPaperCorpusId": "207852310"
                },
                {
                    "start": 1256,
                    "end": 1260,
                    "matchedPaperCorpusId": "231693233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39990234375
        },
        {
            "corpus_id": "278221202",
            "title": "SV-SAE: Layer-Wise Pruning for Autoencoder Based on Link Contributions",
            "text": "There are two primary methods for pruning DNNs: structured pruning and unstructured pruning. Structured pruning removes entire filters, channels, layers, or neurons, which simplifies the architecture. However, this method imposes structural constraints that make it difficult to consistently identify unimportant components across multiple layers and can potentially lead to a greater performance drop. In contrast, unstructured pruning selectively removes individual weights without following a fixed structural pattern. This allows for higher sparsity levels while minimizing accuracy loss. Studies have shown that unstructured pruning offers greater robustness in performance and sparsity optimization, which makes it particularly useful for applications that require precise compression and efficient resource utilization [9]. \n\nIn this paper, we propose an unstructured pruning method designed for multilayer perceptron (MLP)-based autoencoders, a type of DNNs well-suited for unsupervised learning tasks especially for feature extraction and dimensionality reduction [10], [11]. Autoencoders have been applied across various research domains, including network security, time series forecasting, image processing, and natural language processing [12], [13], [14], [15], [16]. Unlike convolutional neural networks (CNNs), which are optimized for tasks with spatial dependencies, MLP-based autoencoders do not rely on spatial hierarchies. This makes them more versatile for tasks where spatial features are less critical. In addition, their fully connected layers can capture relationships across all input features. The encoder-decoder structure enables efficient feature extraction and dimensionality reduction without the added computational cost of convolutional layers. \n\nAutoencoders have become increasingly complex, with a primary focus on effectively compressing input data. However, the effective compression of the autoencoder itself has received less attention [17], [18], [19]. While some CNN pruning algorithms can potentially be applied to MLP-based autoencoders, most do not explicitly account for their unique characteristics.",
            "score": 0.5131887559440637,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 826,
                    "end": 829,
                    "matchedPaperCorpusId": "260887757"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "267466773"
                },
                {
                    "start": 1079,
                    "end": 1083,
                    "matchedPaperCorpusId": "257448471"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "202537509"
                },
                {
                    "start": 1258,
                    "end": 1262,
                    "matchedPaperCorpusId": "248006978"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "253880803"
                },
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "272262363"
                },
                {
                    "start": 1276,
                    "end": 1280,
                    "matchedPaperCorpusId": "271225765"
                },
                {
                    "start": 1977,
                    "end": 1981,
                    "matchedPaperCorpusId": "232417114"
                },
                {
                    "start": 1983,
                    "end": 1987,
                    "matchedPaperCorpusId": "207111259"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4970703125
        },
        {
            "corpus_id": "270621063",
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "text": "Recent years have witnessed the great success of Large Language Models (LLMs) across various challenging tasks, such as mathematical reasoning, code generation. However, the practical use of these models for inference has faced a major obstacle due to the substantial computational resources they consume. To tackle this, many of the key developments up to now have revolved around weight quantization. It is possible to quantize LLMs down to 4 bits per weight with little impact on accuracy, which aids in memory reduction and speeds up inference (Lin et al., 2024). Nonetheless, maintaining accuracy becomes problematic when quantizing to around 3 bits per weight with existing methods (Dettmers et al., 2024;Egiazarian et al., 2024). \n\nA complementary method is neural network pruning (Han et al., 2015b), which can be combined with quantization to further improve the inference efficiency of LLMs (Kurtic et al., 2023;Frantar & Alistarh, 2023). Pruning can be categorized into two main approaches: unstructured pruning (Sun et al., 2024;Frantar & Alistarh, 2023), which involves the removal of specific weights, and structured pruning (Ma et al., 2023), which entails the removal of complete rows or columns of weights. In contrast to structured pruning, which struggles with performance in LLMs even at low sparsity levels, unstructured pruning methods like SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2024) exhibit promising results without additional retraining, and achieves practical speedup in both CPU and GPU through the recent engineering advancements (Agarwalla et al., 2024). They also have the benefit in reducing hallucinations of LLMs (Chrysostomou et al., 2024).",
            "score": 0.5121303009122097,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1697
                }
            ],
            "ref_mentions": [
                {
                    "start": 548,
                    "end": 566,
                    "matchedPaperCorpusId": "271271084"
                },
                {
                    "start": 688,
                    "end": 711,
                    "matchedPaperCorpusId": "259076379"
                },
                {
                    "start": 788,
                    "end": 807,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1023,
                    "end": 1041,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1139,
                    "end": 1156,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1410,
                    "end": 1428,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5849609375
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "Network pruning is a model compression technique that identifies and eliminates redundancy in the structure or parameters of a neural network, based on specific pruning metrics, and incorporates methods to recover model performance (LeCun, Denker, and Solla 1989;Hassibi, Stork, and Wolff 1993;Han et al. 2015). Pruning methods fall into two categories: unstructured pruning and structured pruning. Unstructured pruning is performed at the individual weight level, allowing for a large sparsity but failing to achieve real inference acceleration or storage reduction (Zafrir et al. 2021;Han, Mao, and Dally 2016). Within unstructured pruning, there exists a specialized variant known as semi-structured pruning. This approach enforces exactly N non-zero values in each block of M consecutive weights (Zhou et al. 2021). This approach has gained traction recently, particularly with support on newer NVIDIA hardware (Mishra et al. 2021). Structured pruning, by contrast, operates on entire rows or columns of weights, providing a more hardware-friendly solution that reduces storage requirements and enhances inference speed (Xia, Zhong, and Chen 2022;Molchanov et al. 2017). However, conventional structured pruning methods typically rely on retraining (sometimes iteratively) to regain the performance of the pruned model (Han et al. 2015;Tan and Motani 2020;Han, Mao, and Dally 2016). Such methods pose scalability challenges for billion-scale LLMs due to constraints on memory and computational resources. Therefore a retraining-free structured pruning method for LLMs is very critical.",
            "score": 0.5121273017549195,
            "section_title": "Related Works Network Pruning Methods",
            "char_start_offset": 4505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1589
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 294,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 294,
                    "end": 310,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 587,
                    "end": 612,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 915,
                    "end": 935,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 1124,
                    "end": 1151,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1323,
                    "end": 1340,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1340,
                    "end": 1360,
                    "matchedPaperCorpusId": "221082307"
                },
                {
                    "start": 1360,
                    "end": 1385,
                    "matchedPaperCorpusId": "2134321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "250301941",
            "title": "BATUDE: Budget-Aware Neural Network Compression Based on Tucker Decomposition",
            "text": "Pruning. Pruning, as the most popular model compression approach, realizes model size reduction via setting parts of the weight as zeros. According to different sparsity pattern after pruning, pruning approach can be categorized as unstructured pruning and structured pruning. Unstructured pruning (Han, Mao, and Dally 2015;Zhang et al. 2018) can provide high compression ratio and accuracy, but the inherent unstructured sparsity pattern prevents this approach from delivering its theoretical speedup on the practical hardware platforms. Structured pruning, such as filter (Luo, Wu, and Lin 2017;He et al. 2019;Lin et al. 2020;Sui et al. 2021) and channel (He, Zhang, and Sun 2017;Zhuang et al. 2018;Peng et al. 2019) pruning are hardware-friendly solutions. However, the purposely imposed structure pattern, on the other hand, limits the performance of pruned models in terms of compression ratio and accuracy. \n\nQuantization. Quantization, as a compression approach that uses limited number of bits to represent weight and/or activation (Han, Mao, and Dally 2015;Rastegari et al. 2016;Jacob et al. 2018;Gong et al. 2019), is widely adopted in the DNN deployment on practical devices, especially for specialized DNN chips (Han et al. 2016;Chen, Emer, and Sze 2016). The number precision used in the quantization scheme typically highly depends on the resource budget and accuracy requirement. In (Rastegari et al. 2016;Courbariaux, Bengio, and David 2015), 1-bit weight, as a very aggressive quantization scheme, is proposed to realize very low storage and computational costs. \n\nTensor Decomposition. In tensor theory tensor decomposition is used for compact representation of large-scale tensor-format data objective. Starting from (Lebedev et al. 2015;Kim et al. 2016), many existing tensor decomposition approaches have been introduced to compresses DNN models by factorizing the original weight matrices/tensors into small core tensors.",
            "score": 0.51210931260407,
            "section_title": "Related Work Model Compression",
            "char_start_offset": 6163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 341,
                    "matchedPaperCorpusId": "4752389"
                },
                {
                    "start": 574,
                    "end": 597,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 597,
                    "end": 612,
                    "matchedPaperCorpusId": "102350938"
                },
                {
                    "start": 628,
                    "end": 643,
                    "matchedPaperCorpusId": "239998772"
                },
                {
                    "start": 657,
                    "end": 682,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 682,
                    "end": 701,
                    "matchedPaperCorpusId": "53102564"
                },
                {
                    "start": 701,
                    "end": 718,
                    "matchedPaperCorpusId": "174800733"
                },
                {
                    "start": 1066,
                    "end": 1088,
                    "matchedPaperCorpusId": "221103977"
                },
                {
                    "start": 1088,
                    "end": 1106,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1106,
                    "end": 1123,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 1224,
                    "end": 1241,
                    "matchedPaperCorpusId": "1663491"
                },
                {
                    "start": 1241,
                    "end": 1266,
                    "matchedPaperCorpusId": "3291270"
                },
                {
                    "start": 1398,
                    "end": 1421,
                    "matchedPaperCorpusId": "221103977"
                },
                {
                    "start": 1421,
                    "end": 1457,
                    "matchedPaperCorpusId": "1518846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        },
        {
            "corpus_id": "269318104",
            "title": "Edge Federated Optimization for Heterogeneous Data",
            "text": "This pruning method typically adheres to the structure and topology of the model to ensure that the connectivity and hierarchical structure of the network remain unchanged. The structured pruning used in this work involves setting the weights of certain neurons to zero, thereby disabling the associated connections. On the other hand, unstructured pruning involves pruning individual weights within the model, regardless of their location or structure. In this scenario, pruning is typically achieved by setting weights with smaller values to zero. This may result in some weights within the model being entirely eliminated, thereby reducing the model's size. Unstructured pruning is often used to further reduce the model's storage requirements and computational costs, but it may require sophisticated techniques to minimize its impact on model performance. In this work, we utilized the sparse optimization framework provided by DessiLB [14] to transform the original unstructured architecture into a structured one and make it iteratively adapt to our model. \n\nFigure 2a shows the initial network. The unstructured (weight) pruning (Figure 2b) and structured (filter) pruning (Figure 2c) approaches are techniques for reducing the size of deep learning models, improving efficiency, and mitigating overfitting. Structured pruning cuts out full filters and kernel rows, leading to fewer intermediate feature maps, unlike just pruning connections [49]. The primary distinction between them lies in their focus on pruning different types of elements within the model (Figure 3). In [10], it is mentioned that conventional training is applied to specific parameters (weak nodes), while non-specific parameters (strong nodes) are not subjected to training. Similar to iterative structured pruning, pruning is applied to strong nodes while iteratively training weak nodes.",
            "score": 0.5118753117857753,
            "section_title": "Pruning",
            "char_start_offset": 27269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 945,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1584,
                    "end": 1588,
                    "matchedPaperCorpusId": "3586416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322265625
        },
        {
            "corpus_id": "277621577",
            "title": "Compression Laws for Large Language Models",
            "text": "Neural scaling laws. The study of scaling laws dates back several decades, with Cortes et al. (1993) (Vaswani, 2017). While most of these scaling laws manage to capture the expected test-time behaviors of pre-trained LLMs, they fail to explain the scaling behaviors of models in parameter and data-efficient settings. \n\nModel compression for parameter efficiency. Despite the remarkable performance of LLMs such as LLaMA (Dubey et al., 2024) and Deepseek (DeepSeek-AI et al., 2024) on a wide range of tasks, including natural language inference, complex reasoning, summarization, translation, and code generation, large-scale utilization of these models remains challenging due to high computational resource requirements. Model compression (aka pruning) is a common technique to reduce the parameter count in pre-trained models, improving their computational efficiency and speed. It generally falls into two main categories: unstructured and structured pruning. Unstructured pruning focuses on removing individual weights (Frantar & Alistarh, 2023;Sun et al., 2023) from pre-trained models. Despite their ability to retain performance post-compression, unstructured pruning often demands hardware-specific optimizations and may not always lead to substantial computational benefits. Conversely, structured pruning eliminates entire channels or components, making it more suitable for a broader range of hardware configurations. Contemporary structure pruning methods like SliceGPT (Ashkboos et al., 2024), layer collapse (Yang et al., 2024) use a small calibration dataset to assess the importance of different components of a pretrained model and removes them subsequently, if found unimportant. Sengupta et al. (2025) proposed a policydriven calibration-free model compression method and argued that LLMs can withstand even when pruned by a ran-dom subset of the pre-trained components. \n\nScaling laws for parameter-efficient LLMs. Kumar et al. (2024) introduced precision-aware scaling laws, demonstrating that training in lower precision effectively reduces the parameter count of LLMs. Their findings suggest that larger LLMs scale more efficiently when trained at lower precision.",
            "score": 0.5117345787518071,
            "section_title": "Related Work",
            "char_start_offset": 4244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1890
                },
                {
                    "start": 1893,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 100,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 101,
                    "end": 116,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "278327238",
            "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
            "text": "Large language models (LLMs) have emerged as a transformative technology, demonstrating remarkable capabilities in natural language understanding and generation tasks [1]. LLMs have showcased significant adaptability through fine-tuning, enabling their deployment in highly specialized applications. These advantages underscore the critical role of LLMs in solving real-world challenges [2]. Despite these capabilities, the deployment of LLMs has been hindered by their immense computational demands. Modern LLMs often consist of billions or even trillions of parameters [3], as seen in models like GPT-3 (175 billion parameters) and PaLM (540 billion parameters). The immense scale incurs significant memory, storage and power costs, making it challenging to run these models on resource-constrained devices. To address these issues, researchers have increasingly turned to model compression techniques, particularly pruning, to reduce the size and computational requirements of LLMs, while retaining their performance [4]. Pruning [5; 6; 7] is one of the most prominent model compression techniques. It aims to remove redundant or less important parameters from neural networks, thereby reducing their sizes and computational complexity. Recent advances in pruning have progressed from unstructured sparsity (i.e., individual weight removal) to structured sparsity (i.e., eliminating entire neurons, heads or layers), thereby enabling hardware-efficient implementation [4; 8; 9; 10]. For LLMs, several stateof-the-art methods have been proposed, including magnitude pruning, lottery ticket hypothesis approaches, and structured pruning based on attention mechanisms. These methods demonstrate the potential of pruning to enable LLM deployment on resource-constrained devices without causing substantial performance degradation. \n\nExisting pruning methods often use a layer-wise strategy that applies uniform sparsity across all layers, ignoring their varying importance. While simple to implement, this approach overlooks the inherent differences in the contributions of different layers to the overall performance of the model. Thus, it can only find the local optimal pruning solution, but not the global optimal solution. Empirical evidence also suggests that certain layers are more critical than others, and uniformly pruning across all layers may lead to the removal of essential parameters, ultimately impairing the pruned model's performance [11; 12].",
            "score": 0.5116534570184916,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1829
                },
                {
                    "start": 1832,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2461
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 571,
                    "end": 574,
                    "matchedPaperCorpusId": "253265387"
                },
                {
                    "start": 1020,
                    "end": 1023,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "260401806",
            "title": "Summary of Deep Neural Network Pruning Algorithms",
            "text": "The primary purpose of pruning is to decrease the parameters of a network and expedite network computations. In this paper, the pruning algorithms of neural networks in recent years are classified and summarized. The unstructured pruning algorithm and the structured pruning algorithm are introduced. According to the pruning granularity and starting point, the structured pruning algorithms are divided into four categories: Filter-wise pruning\u3001Group-wise pruning\u3001Stripe-wise pruning, and Channel-wise pruning. \n\nUnstructured pruning can delete the parameters with negligible contribution from the neural network to the greatest extent. However, it needs the support and optimization of hardware and library to exert its advantages. \n\nCompared with unstructured pruning, structured pruning has a larger granularity, which can optimize the network's structure and does not need additional hardware and library support because it directly reduces parameters. The main disadvantage is that the change of input and output dimensions in the middle layer of the network may cause accuracy loss or some deviation to the network, and even the disappearance of the middle layer of the network may occur. \n\nThe practical and easy-to-implement structured pruning algorithms are mainly about convolutional neural networks. However, with the popularity of language processing models such as ChatGPT in recent years, it can be predicted that natural language processing tasks will receive more attention. As the backbone network of most language processing models, Transformer [25] has excellent context processing capabilities and heavy parameters and calculations to neural networks. Therefore, implementing an algorithm that can effectively compress the Transformer skeleton network will be a promising direction. \n\nIn addition, graph neural networks are more in line with the human brain structure in theory and may have significant development in the future. However, there are few pruning algorithms for neural graph networks, so that pruning algorithms can be developed for such networks in the future. \n\nSeveral automated network structure search algorithms have emerged, such as the SPOS [26] and the ENAS algorithm [27]. They are committed to finding the optimal network sub-model through search algorithms, simplifying the network structure.",
            "score": 0.5112117845121891,
            "section_title": "Conclusion and discussion",
            "char_start_offset": 19684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1803
                },
                {
                    "start": 1806,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419189453125
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "Experimental results on three LLMs, Large Language Model Meta AI (LLaMA), Vicuna, and General Language Model (ChatGLM), demonstrate that the compressed models perform well in zero-shot classification and generation tasks.\n\nMcCarley et al. [30] investigate the utilization of structured pruning in a Bidirectional Encoder Representations 89422 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.\n\nfrom Transformers (BERT)-based Question Answering (QA) model.The primary objective is to reduce the model's size by removing redundant parameters while preserving its QA performance.They present a novel approach to optimize the computational efficiency of BERT-based question-answering models through structured pruning.BERT is a popular transformer-based approach for applications involving natural language processing.Deploying it on devices with limited resources or in situations where real-time inference is necessary, however, is difficult due to its vast size and computing requirements.The proposed method focuses on identifying and removing redundant parameters in BERT by leveraging the structured sparsity pattern present in the model's attention heads.By pruning attention heads that contribute minimally to the model's performance, significant model size reduction is achieved without sacrificing accuracy.When compared to the original BERT model, the suggested technique gains considerable pruning ratios while keeping comparable performance, which is demonstrated by testing on benchmarks for answering questions.This work contributes to the development of more efficient and deployable BERT-based question-answering systems.\n\nYang et al. [31] address the computational resource limitations associated with pre-trained language models used in NLP by introducing TextPruner which is a dedicated open-source toolkit developed to facilitate model pruning, aiming to enable efficient and straightforward compression of models.It provides structured post-training pruning techniques, such as vocabulary pruning and transformer pruning, for streamlined implementation.These methods enable the reduction of the model size without the need for retraining, thus making the pruning process more efficient.The toolkit is flexible and may be used for a variety of applications including pre-trained language models and NLP tasks.",
            "score": 0.5105308897471098,
            "section_title": "IV. METHODS",
            "char_start_offset": 20759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 223,
                    "end": 450
                },
                {
                    "start": 450,
                    "end": 469
                },
                {
                    "start": 471,
                    "end": 532
                },
                {
                    "start": 532,
                    "end": 653
                },
                {
                    "start": 653,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1390
                },
                {
                    "start": 1390,
                    "end": 1599
                },
                {
                    "start": 1599,
                    "end": 1711
                },
                {
                    "start": 1713,
                    "end": 2008
                },
                {
                    "start": 2008,
                    "end": 2148
                },
                {
                    "start": 2148,
                    "end": 2281
                },
                {
                    "start": 2281,
                    "end": 2403
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2880859375
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "Limitations: Our Pruner-Zero framework, while pioneering in unstructured pruning at a 50% sparsity level, encounters specific limitations in its specialization and evaluative scope.It primarily excels within its tested scenario but lacks empirical support for its effectiveness in alternative configurations such as structured pruning or higher sparsity levels, where challenges escalate due to the complexity of removing structured network components or a greater proportion of weights.It is possible to have better performance under higher sparsity levels or structured pruning.Furthermore, the evaluation of Pruner-Zero has predominantly focused on perplexity and accuracy within constrained contexts, specifically on the Wikitext2 dataset and zero-shot tasks.While these measures offer insights into language modeling and general reasoning, they fall short of capturing the full breadth of capabilities essential for Large Language Models (LLMs), such as in-context learning, commonsense reasoning, instruction following, and self-calibration.These abilities are vital for gauging the comprehensive intelligence and versatility of LLMs, particularly in assessing their operational efficacy post-pruning.\n\nFuture Work: Recognizing these gaps, future research will aim to broaden the evaluative framework of the Pruner-Zero to encompass a wider array of capabilities intrinsic to LLMs.This expansion will involve devising tests and benchmarks specifically tailored to accurately measure the impacts of pruning on aspects like in-context learning, commonsense reasoning, and other advanced functionalities.The goal is to ensure that pruning not only mitigates model size and computational demands but also preserves or enhances the model's capacity for complex tasks and exhibiting human-like intelligence.(Chen et al., 2020) Unstructured Magnitude Fine-tuning BERT Movement (Sanh et al., 2020) Unstructured Loss Sensitivity Fine-tuning BERT Platon (Zhang et al., 2022a) Unstructured Loss Sensitivity Fine-tuning BERT PINS (Ren & Zhu, 2023) Unstructured Loss Sensitivity Fine-tuning BERT",
            "score": 0.5103475001991726,
            "section_title": "G. Limitations and Future Works",
            "char_start_offset": 69725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 181,
                    "end": 487
                },
                {
                    "start": 487,
                    "end": 580
                },
                {
                    "start": 580,
                    "end": 763
                },
                {
                    "start": 763,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1207
                },
                {
                    "start": 1209,
                    "end": 1387
                },
                {
                    "start": 1387,
                    "end": 1607
                },
                {
                    "start": 1607,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 1876,
                    "end": 1895,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 1950,
                    "end": 1971,
                    "matchedPaperCorpusId": "250072480"
                },
                {
                    "start": 2024,
                    "end": 2041,
                    "matchedPaperCorpusId": "258833212"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255615234375
        },
        {
            "corpus_id": "251476392",
            "title": "Heuristic Method for Minimizing Model Size of CNN by Combining Multiple Pruning Techniques",
            "text": "Therefore, our method introduces a margin for the reduction ratio after the former pruning method when m is passed to the unstructured pruning. The reduction ratio p when m is derived is drawn back for an amount of the margin value before the pruning method is switched to unstructured pruning. \n\nParameters to be configured. There are three parameters defined for our proposed method. The parameters are defined by application side. The first one is the target accuracy for the final compressed model. It affects the size of the final compressed model. The second one is the step_width to be used for increasing the reduction ratios p and p for the structured and the unstructured pruning methods. A bigger step_width will help us accelerate the compression. However, it might miss the reduction ratio that achieves the minimum size of the compressed model because it passes over the reduction ratio by the large step_width used for incrementing the ratio. On the other hand, when the step_width is too small, we inevitably need to invoke a large number of iterations to find the minimized compressed model. This will result in a waste of calculation resources and execution time. The final parameter is the margin. As we mentioned above, the margin is used for finding a suitable reduction ratio for the structured pruning method. Hence, if the margin is configured to a large value, our method switches to invoke the unstructured pruning method to reduce the model size that is not compressed adequately by the structured pruning phase. In addition, when the margin is small, the model would be over-compressed by the structured pruning as it is difficult for further compression through the unstructured pruning. \n\nOur proposed method employs the three-step procedure as shown in Figure 4. Figure 5 illustrates the accuracy changes in every execution of two pruning methods during the procedure. \n\nStep 1 applies network slimming to the initial model until the model's accuracy becomes beneath the target accuracy (dotted line in gray color). Step 2 draws back (dotted line in yellow color) the reduction ratio during Step 1 according to the margin value (the square in red color) for obtaining the model that can be further pruned (the square in green color).",
            "score": 0.5102833870732429,
            "section_title": "Strategy for Minimizing the Model Size",
            "char_start_offset": 16043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298095703125
        },
        {
            "corpus_id": "277621577",
            "title": "Compression Laws for Large Language Models",
            "text": "In recent years, there has been growing interest in understanding how the size of pre-training models and datasets impacts the downstream performance of large language models In attempts to make large pre-trained models more compute efficient, model compression (aka model pruning) has been widely adopted for compressing large models into smaller and computationally more feasible variants. Post-training model compression methods (Ashkboos et al., 2024;Wang et al., 2024;Sengupta et al., 2025) prune various components of pre-trained LLMs to reduce their size, often with minimal impact on performance post-compression. Despite the growing adoption of model compression techniques, there is still no systematic study on how these methods scale across different LLMs. To address this gap, our work introduces compression laws for LLMs, providing a structured framework to understand the effectiveness and scalability of structured compression methods. Through a multifaceted approach, we analyze the key factors that influence the performance stability of LLMs after compression, both with and without recovery fine-tuning (van der Ouderaa et al.; Ma et al., 2023). Our goal is to offer both empirical and analytical insights into five critical research questions surrounding model compression as follows. \n\nRQ1. What is the impact of model compression on downstream performance? RQ2. What computational benefits does compression provide? RQ3. How much performance can be regained with recovery fine-tuning? RQ4. How can we determine which LLM to compress and at what compression ratio to achieve comparable performance? RQ5. Is calibration necessary during model compression? \n\nWe conduct over 1000 experiments using Qwen-2.5 (Qwen et al., 2025) and LLaMA-3 (Dubey et al., 2024), with pa- rameter size ranging from 0.5B to 14B. These models are compressed using both calibration-free (Sengupta et al., 2025) and calibration-based (Ashkboos et al., 2024) structured pruning methods, with compression ratios ranging from 10% to 90% and recovery fine-tuning token sizes varying from 1M to 25M .",
            "score": 0.5096416063638143,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 2093
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "253474561",
            "title": "Tiny Machine Learning for Resource-Constrained Microcontrollers",
            "text": "Pruning. An NN's computational complexity can be lowered by pruning its unused features. Pruning techniques can be divided into two main categories: structured and unstructured pruning [36]. Structured pruning means removing entire channels or filters, and unstructured pruning means removing individual weight connections by setting them to zero [37]. In addition, it is possible to combine different pruning approaches. For example, in [38], the authors presented a method whereby unstructured and structured pruning approaches were combined with neural architecture search, which automatically finds accurate, lightweight, and sparse CNN architecture. \n\nThe process of zeroing out the NN model's weights is also called magnitude-based pruning, and it leads to a sparse model and can bring a sixfold improvement in model compression [39]. The downside of this method is that it also leads to sparse matrix multiplications that need extra computation power and the use of sparse convolution libraries [40,41]. Still, by weight pruning the deep neural network (DNN) model's internal redundancy, the model can be downsized, and its performance can be increased without a decrease in the prediction accuracy [40]. The weight pruning method suits the use of microcontrollers in particular because the benefits of model size reduction are more significant than the extra computational load from sparse multiplications. \n\nStructured pruning changes the shapes of layers and weight matrices by removing groups of weight connections [37]. When whole channels or filters are removed, the network's inference speed increases, and the model size decreases. A channel-level pruning produces a lightweight network, but it can lower the model's performance and accuracy when the width of the entire network is reduced. Hence, it is recommended that unstructured pruning methods are used whenever possible.",
            "score": 0.5095208404510106,
            "section_title": "3.3.",
            "char_start_offset": 11094,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1892
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 189,
                    "matchedPaperCorpusId": "7333079"
                },
                {
                    "start": 438,
                    "end": 442,
                    "matchedPaperCorpusId": "168169895"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "4497748"
                },
                {
                    "start": 1206,
                    "end": 1210,
                    "matchedPaperCorpusId": "4497748"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31689453125
        },
        {
            "corpus_id": "276107452",
            "title": "Progressive Binarization with Semi-Structured Pruning for LLMs",
            "text": "Pruning is a widely used technique for compressing neural networks by removing less significant parameters, reducing the number of active weights. This results in sparse networks that are more efficient in memory, computation, and size. In LLMs, pruning methods are generally divided into structured, unstructured, and semi-structured approaches. Structured pruning (Ma et al., 2023;Ashkboos et al., 2024;Xia et al., 2024;An et al., 2024) eliminates entire structured model components to improve efficiency. However, this approach can lead to substantial performance degradation, often requiring retraining to restore lost functionality. Unstructured pruning (Dong et al., 2024), removes weight elements individually based on their importance, maintaining high performance even at higher sparsity levels, but the resulting sparsity patterns are not hardware-efficient. Semi-structured pruning strikes an optimal balance by keeping regular sparsity patterns, such as N :M sparsity, which is optimized for hardware, as seen in methods like SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2024), and STBLLM (Dong et al., 2025). Our approach leverages semi-structured pruning with N :M sparsity, aiming to minimize performance degradation while maintaining hardware efficiency.",
            "score": 0.5092820441126038,
            "section_title": "LLM Pruning",
            "char_start_offset": 6710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1282
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 678,
                    "matchedPaperCorpusId": "270257857"
                },
                {
                    "start": 1113,
                    "end": 1132,
                    "matchedPaperCorpusId": "271710591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59326171875
        },
        {
            "corpus_id": "255125037",
            "title": "Hyperspherical Quantization: Toward Smaller and More Accurate Models",
            "text": "Pruning consists of structured and unstructured methods. It can greatly compress redundancy and maintain high accuracy. Unstructured pruning brings high sparsity, but can-not take advantage of acceleration without customized hardware [25]. Only structured pruning methods can reduce the inference latency and are easier to accelerate [30,38] because the original weight structures of the model are preserved. Unstructured pruning uses criteria, such as gradient [56,35], and magnitude [26,55] information, to remove individual weights; structured pruning [38,32,1] aims to remove unimportant channels of the convolutional layer based on similar criteria. The lottery ticket hypothesis [20] shows that there exists sparse subnetworks that can be trained from scratch and achieve the same performance as the full network. [81] studies the lottery ticket hypothesis from the perspective of weight reinitialization and points out that the key premise is the sign of weight values. \n\nRe-training after pruning [20,81] reveals the link between the network structure and performance. Furthermore, our findings show that training after pruning and reinitialization can be used to produce more accurate and highly compressed ternary weights, which surpasses the current model compression methods and has a wide range of application scenarios.",
            "score": 0.5091746181902188,
            "section_title": "Pruning",
            "char_start_offset": 5430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1333
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "20140944"
                },
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 338,
                    "end": 341,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "17934228"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "18201582"
                },
                {
                    "start": 555,
                    "end": 559,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 562,
                    "end": 564,
                    "matchedPaperCorpusId": "3512041"
                },
                {
                    "start": 685,
                    "end": 689,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 820,
                    "end": 824,
                    "matchedPaperCorpusId": "145047837"
                },
                {
                    "start": 1005,
                    "end": 1009,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 1009,
                    "end": 1012,
                    "matchedPaperCorpusId": "145047837"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.488037109375
        },
        {
            "corpus_id": "258167346",
            "title": "The Optimization Strategy of Deep Learning Algorithm Based on Image Recognition and Object Detection",
            "text": "The types of pruning in deep learning are divided into structured pruning and unstructured pruning. Structured pruning is aiming the channel of convolution kernels or feature maps, and unstructured pruning targets specific neurons. The advantage of structured pruning is that it can be accelerated directly on the GPU, because the convolution calculation is equivalent to matrix multiplication. After the feature map and convolution kernel channel are structured pruned, the matrix multiplication and addition operation are not required for the pruned channel so that directly improve the operation speed. The advantage of unstructured is that the pruning granularity is fine, the secondary neurons can be accurately found, the model compression ratio is larger. However, each channel still needs to be calculated during the multiply-add operation, so the acceleration cannot be directly reflected on the GPU. Therefore, if need to reflect the intuitive acceleration on the GPU and make the model smaller, it is necessary to design an AI chip to specifically accelerate the pruned model.",
            "score": 0.5087242995052661,
            "section_title": "Types of pruning in deep learning.",
            "char_start_offset": 13687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "270870796",
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "text": "In natural language processing tasks, unstructured and hybrid pruning effectively reduce Transformer model complexity, enabling inference and training with lower resource consumption.By applying these pruning techniques, models maintain high performance while significantly reducing computational and storage costs, enhancing their usability and efficiency in practical applications.",
            "score": 0.5082760682617598,
            "section_title": "B. Model Pruning",
            "char_start_offset": 24745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.377197265625
        },
        {
            "corpus_id": "277065819",
            "title": "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
            "text": "Despite the outstanding performance Large Language Models (LLMs) exhibit in various tasks (Zhao et al., 2023;Gozalo-Brizuela and Garrido-Merch\u00e1n, 2023;Wan et al., 2024b;Shen et al., 2024;Wan et al., 2025), the significant resources consumed limit their widespread accessibility (Wan et al., 2024a;Wang et al., 2024a;Zhou et al., 2024). Model compression (Zhu et al., 2023;Shen et al., 2025) is one effective approach to reduce resource consumption. To avoid resourceintensive retraining, LLM compression is often conducted in a post-training manner. Techniques such as LLM quantization (Yuan et al., 2024; Huang et al., 2024), unstructured pruning (Frantar and Alistarh, 2023), and structured pruning (Ma et al., 2023;Ashkboos et al., 2024;Zhong et al., 2024) have been proposed. \n\nLow-rank approximation, such as Singular Value Decomposition (SVD) is also an effective technique for compressing LLMs. Compared with quantization and unstructured pruning, SVD compression is more hardware-friendly. Recently, a few SVD-based LLM compression methods have been proposed. At a high level, these methods all focus on reducing the truncation loss during SVD compression to reserve accuracy. Specifically, FWSVD (Hsu et al., 2022) reduces truncation loss by estimating weight importance and preserving more important weights. ASVD (Yuan et al., 2023) injects a scaling matrix to reduce the truncation loss but was not able to achieve theoretical minimum truncation loss at each LLM layer. SVD-LLM (Wang et al., 2024b), on the other hand, fills this gap by proposing a whitening matrix that obtains theoretical minimum truncation loss at each LLM layer, demonstrating superior performance. \n\nDespite such advantage, SVD-LLM has two limitations. First, SVD-LLM applies a homogeneous compression ratio to all the weight matrices.",
            "score": 0.5082468576259077,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1819
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 297,
                    "matchedPaperCorpusId": "266044196"
                },
                {
                    "start": 297,
                    "end": 316,
                    "matchedPaperCorpusId": "266755609"
                },
                {
                    "start": 648,
                    "end": 676,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 701,
                    "end": 718,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54248046875
        },
        {
            "corpus_id": "266573164",
            "title": "The LLM Surgeon",
            "text": "We report computational cost in terms of pruning time in table 11 and GPU memory in table 12. Our method is most efficient for structured pruning, but it must be noted that engineering efforts may further improve speed for unstructured pruning. The focus of the paper is structured pruning, on which we achieve state-of-the-art compression rates. Importantly, compression of LLMs only needs to happen once after which a pruned model can be deployed infinitely many times without further cost. This motivates our method which takes longer to run but reaches better final test performance. Structured LLM-Surgeon (ours) Llama-7B 16h58m / 4xH100 80 GB 17h08m / 4xH100 80 GB Llama-13B 1d6h5m / 8xH100 80 GB 1d9h26m / 8xH100 80 GB We argue that differences in the performance and the runtime of pruning methods can largely be attributed to underlying assumptions on correlations between weights. Notably, algorithms that consider few correlations, sometimes to the extent of completely disregarding all gradient information, can result in very fast pruning algorithms for unstructured and semi-structured pruning but are often not flexible enough to perform structured pruning of rows and columns. Examples of such lightweight algorithms for LLMs are (Sun et al., 2023) and SparseGPT (Frantar & Alistarh, 2023), as can also be observed from table 11. Our approach makes less strong assumptions on the curvature of the loss and as a result outperforms all baselines on all unstructured, semi-structured and structured pruning. Further, the improved curvature is also eligible for dynamic allocation of weight removal and improved correlated weight updates. In practice, we always recommend using our method for structured pruning. For unstructured and semi-structured pruning, we note an important trade-off between the desired final test accuracy and the available computational budget. Here, our proposed method can achieve the highest final model performance but requires more computational resources and takes longer to run.",
            "score": 0.5082326317741678,
            "section_title": "H COMPUTATIONAL PERFORMANCE",
            "char_start_offset": 36300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 1279,
                    "end": 1305,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "270209927",
            "title": "Sparsity-Accelerated Training for Large Language Models",
            "text": "Transformer Pruning Pruning has demonstrated considerable potential in augmenting the inference speed and reducing the memory footprint of transformers, as highlighted in prior research (Kwon et al., 2022).Broadly speaking, transformer pruning techniques can be delineated into two principal categories (Chitty-Venkata et al., 2023): unstructured pruning and structured pruning.Unstructured pruning methods (Gordon et al., 2020;Campos et al., 2022;Zhang et al., 2022a) possess the capability to substantially diminish the number of parameters due to their fine-grained pruning granularity.Nonetheless, they frequently encounter challenges in achieving significant improvements in inference speed across diverse hardware platforms (Sanh et al., 2020).\n\nHence, numerous researchers are embracing structured pruning methods at the granularity level of entire layers, filters, channels, or heads (Li et al., 2021).For instance, Liu et al. prunes the unimportant heads within multi-head attention layers and insignificant channels in feed-forward networks on BERT (Kenton and Toutanova, 2019).Liu et al. and Song et al. extend such a method and scale it for large language models.In this study, distinct from their focus on the inference of large language models (LLMs), we draw upon the principles of structured pruning and apply them to the training of LLMs.\n\nSparse Fine-Tuning Sparse fine-tuning is a technique aimed at expediting model fine-tuning and diminishing model memory demands by selectively updating only a small subset of parameters (Ansell et al., 2022;Li et al., 2024).For relatively small language models, such as BERT, Guo et al. devise a binary mask controlled by a regularization term to manage the parameters for updating.Sung et al. induce the mask by retaining parameters exhibiting higher Fisher information over numerous iterations and subsequently maintain the mask to sparsely fine-tune models.Additionally, Ansell et al. retain parameters that undergo the most significant changes during an initial round of full fine-tuning.",
            "score": 0.5081016157908198,
            "section_title": "Related Works",
            "char_start_offset": 20106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 750
                },
                {
                    "start": 752,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1355
                },
                {
                    "start": 1357,
                    "end": 1581
                },
                {
                    "start": 1581,
                    "end": 1739
                },
                {
                    "start": 1739,
                    "end": 1917
                },
                {
                    "start": 1917,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 205,
                    "matchedPaperCorpusId": "248266822"
                },
                {
                    "start": 303,
                    "end": 332,
                    "matchedPaperCorpusId": "259937180"
                },
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "250072480"
                },
                {
                    "start": 730,
                    "end": 749,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 892,
                    "end": 909,
                    "matchedPaperCorpusId": "236965871"
                },
                {
                    "start": 1543,
                    "end": 1564,
                    "matchedPaperCorpusId": "238856900"
                },
                {
                    "start": 1564,
                    "end": 1580,
                    "matchedPaperCorpusId": "270380069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6826171875
        },
        {
            "corpus_id": "273186301",
            "title": "On Efficient Variants of Segment Anything Model: A Survey",
            "text": "Model pruning reduces a model's size and complexity by eliminating redundant weights or connections, while aiming to maintain accuracy as much as possible. Pruning methods are typically categorized into two types: structured pruning [4,135] and unstructured pruning [18,63]. \n\nStructured pruning removes parameters in groups based on specific criteria, targeting substructures such as channels, layers, or blocks in a systematic manner. In contrast, unstructured pruning focuses on individual weights, often resulting in a sparse and fragmented network. However, unstructured pruning may not lead to effective acceleration on general hardware due to the irregularity of the remaining network structure. In [20], structured pruning is applied to lighten SAM, significantly reducing the model's size while preserving most of SAM's capabilities by removing large amounts of redundant weights.",
            "score": 0.5079337367045358,
            "section_title": "Pruning",
            "char_start_offset": 20618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 889
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "7333079"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "222297215"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36279296875
        },
        {
            "corpus_id": "268512967",
            "title": "Toward Adaptive Large Language Models Structured Pruning via Hybrid-grained Weight Importance Assessment",
            "text": "Structured pruning for large language models (LLMs) has garnered significant academic interest due to its ability to efficiently compress and accelerate LLMs by eliminating redundant weight groups at a coarse-grained granularity. Current structured pruning methods for LLMs typically depend on a singular granularity for assessing weight importance, resulting in notable performance degradation in downstream tasks. Intriguingly, our empirical investigations reveal that utilizing unstructured pruning, which achieves better performance retention by pruning weights at a finer granularity, \\emph{i.e.}, individual weights, yields significantly varied sparse LLM structures when juxtaposed to structured pruning. This suggests that evaluating both holistic and individual assessments for weight importance are essential for LLM pruning. Building on this insight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA), a novel method that merges fine-grained and coarse-grained evaluations of weight importance for the pruning of LLMs. Leveraging an attention mechanism, HyWIA adaptively determines the optimal blend of granularity in weight importance assessments in an end-to-end pruning manner. Extensive experiments on LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks demonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA surpasses the cutting-edge LLM-Pruner by an average margin of 2.82% in accuracy across seven downstream tasks when pruning LLaMA-7B by 50%.",
            "score": 0.5075799486066296,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.642578125
        },
        {
            "corpus_id": "270621063",
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "text": "Pruning LLM. Neural network pruning in LLM can be broadly categorized into two groups: structured pruning (Ma et al., 2023;Zhang et al., 2023) and unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2024). Ma et al. (2023) proposes a dependency detection algorithm to detect and prune non-critical grouped structures followed by LoRA fine-tuning. Although structured pruning can usually achieve better hardware efficiency, the accuracy drops a lot even at a low compression rate. Unstructured pruning can yield a higher compression rate and achieve acceleration on Nvidia's GPUs by employing a hardware-friendly N:M sparsity pattern. SparseGPT (Frantar & Alistarh, 2023) leverages the Hessian inverse for pruning and reduces reconstruction error of dense and sparse weights by subsequent weight updates. Wanda (Sun et al., 2024) employs an efficient method that augments input activations into weight magnitudes, and matches the performance of SparseGPT at medium sparsity. Our work incorporates dependency information into unstructured pruning, achieving a novel pruning paradigm. \n\nInherent Sparsity of Transformer MLP. Interestingly, sparsity within the MLP activations of trained Transformer-based models occurs innately even without applying explicit regularizations or constraints (Zhang et al., 2022;Li et al., 2023;Dong et al., 2023). Such a phenomenon is prevalent in learned Transformers, including other zero-saturating functions. Outlier-dependent LLM Compression. Outlier features, defined as features with magnitudes substantially larger than others, are a notable characteristic of LLMs (Dettmers et al., 2022). Despite making up only a small fraction of all feature dimensions, these outliers play a critical role in attention and predictive performance. Such observation has motivated the development of LLM-specific quantization methods (Dettmers et al., 2022;Xiao et al., 2023;Lin et al., 2024;Ashkboos et al., 2023) to handle outliers more effectively.",
            "score": 0.5072122305984246,
            "section_title": "Related Work",
            "char_start_offset": 22888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 123,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 194,
                    "end": 211,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 213,
                    "end": 229,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 817,
                    "end": 835,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1314,
                    "end": 1330,
                    "matchedPaperCorpusId": "259138847"
                },
                {
                    "start": 1330,
                    "end": 1348,
                    "matchedPaperCorpusId": "268955234"
                },
                {
                    "start": 1609,
                    "end": 1632,
                    "matchedPaperCorpusId": "258509304"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "249192002",
            "title": "Machine Learning for Microcontroller-Class Hardware: A Review",
            "text": "Exploiting sparsity and reducing bitwidth of models depend on three key factors. \n\n1) Large Sparse Versus Small Dense: Large-sparse models (compressed and vanilla models) are known to outperform small-dense models (uncompressed and lightweight models) for a broad range of network architectures [48] in terms of compression ratio for comparable accuracy. This is evident from the speech commands and MNIST-10 examples in Table XIX. LSTM-Prune and LSTM-KP outperform Fast-GRNN and Bonsai, providing on average 12\u00d7 model size reduction with only 2.3% accuracy loss. Moreover, on average, all uncompressed and vanilla models provided a 13.5\u00d7 reduction in model size when pruned compared to 2.1\u00d7 for lightweight models (FastGRNN). Therefore, sparsification is useful when working with vanilla models rather than lightweight ML blocks. \n\n2) Pruning and Quantization Gains: Unstructured pruning and posttraining quantization offer performance gain in different dimensions. Generally, both pruning and quantization are applied jointly [49]. \n\n1) Flash Savings: Pruning is more aggressive in reducing the model size than quantization [49]. In Table XIX, on average, pruning provides 13.6\u00d7 compression factor compared to 3.9\u00d7 compress factor provided by quantization. Pruning combined with quantization provides a 16\u00d7 reduction in model size on average. 2) SRAM Savings: Pruning is less likely to reduce working memory footprint than quantization. After pruning, the microcontroller still has to perform multiplication in the original floating-point bitwidth, whereas, in quantization, the bitwidth of the multiplication decreases. 3) Intuitively, pruning is less likely to reduce the inference latency compared to integer quantization in microcontrollers. The gains from the loss of redundant weights are lower than the gains from the integer matrix multiplication. Moreover, unstructured pruning can add processing and execution time overhead [85]. 4) Accuracy Loss: Pruning often causes higher accuracy loss than quantization.",
            "score": 0.5071460494085618,
            "section_title": "B. Compression Versus No Compression",
            "char_start_offset": 82763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 83,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "27494814"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 1126,
                    "end": 1130,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 1936,
                    "end": 1940,
                    "matchedPaperCorpusId": "220302470"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.215576171875
        },
        {
            "corpus_id": "256827874",
            "title": "Task-specific Compression for Multi-task Language Models using Attribution-based Pruning",
            "text": "One of the ways to categorize network pruning is to compare structured pruning to unstructured pruning. For structured pruning (Li et al., 2016;Hu et al., 2016;Wen et al., 2016), groups of weight con-nections are removed from a network together, such as entire channels or filters in CNN-based networks and layers or attention heads in transformer-based networks. For unstructured pruning (Han et al., 2015a,b), weight connections are removed from a network individually. However, unstructured pruning methods produce large sparse weight matrices which are computationally inefficient unless equipped with a specifically designed hardware. In this paper, we utilize the structured pruning method to propose a compression method that enables efficient weight matrix multiplication computation.",
            "score": 0.5068762782207589,
            "section_title": "Network Pruning",
            "char_start_offset": 6284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 792
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 177,
                    "matchedPaperCorpusId": "2056019"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "267547755",
            "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications",
            "text": "Our approach to attribution aligns closely with techniques used in neural network pruning. Our SNIP method (Lee et al., 2019) resembles more closely with unstructured pruning techniques, which are designed to establish criteria based on weight magnitude, activations, or network gradients for removing individual weights from a network (Han et al., 2016;Molchanov et al., 2017;Frankle & Carbin, 2018;Chen et al., 2020;Sanh et al., 2020;Zhao et al., 2020;Cao et al., 2021;Guo et al., 2021). These unstructured pruning methods have been adapted for use in large language models, as seen in Wanda (Sun et al., 2024), SparseGPT (Frantar & Alistarh, 2023). \n\nBroadly speaking, the low-rank compression techniques are akin to structured pruning approaches, with a focus on identifying important structured subnetworks. In computer vision settings, it is common to remove channels or filters (Li et al., 2017;Molchanov et al., 2017;Wen et al., 2016;He et al., 2017;Luo et al., 2017;Liu et al., 2017) from convolutional neural networks. Structured pruning of language models involves removing heads, dimensions, or ranks (Michel et al., 2019;Wang et al., 2020;Lagunas et al., 2021;Xia et al., 2022;Ma et al., 2023;Zhang et al., 2023b;a;Chen et al., 2023;Xia et al., 2024;Ashkboos et al., 2024). \n\nWhile pruning is commonly employed for model compression to decrease model sizes, our work adopts similar techniques to identify critical regions responsible for safety.",
            "score": 0.5062070071993449,
            "section_title": "A.4. Pruning",
            "char_start_offset": 41710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1458
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 400,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 436,
                    "end": 454,
                    "matchedPaperCorpusId": "216553665"
                },
                {
                    "start": 454,
                    "end": 471,
                    "matchedPaperCorpusId": "233181831"
                },
                {
                    "start": 471,
                    "end": 488,
                    "matchedPaperCorpusId": "229152766"
                },
                {
                    "start": 624,
                    "end": 650,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 942,
                    "end": 958,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 958,
                    "end": 975,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 975,
                    "end": 992,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 1134,
                    "end": 1152,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 1152,
                    "end": 1173,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 1173,
                    "end": 1190,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1206,
                    "end": 1226,
                    "matchedPaperCorpusId": "266435293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.253662109375
        },
        {
            "corpus_id": "258823276",
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
            "text": "Compression of Language Model. Language models [9,29,25] have gained much attention and increase the need to reduce the size of parameters and reduce the latency [23,46]. To compress the language model, previous works can be divided into several categories: network pruning [21,61,30,15], knowledge distillation [44,45,38], quantization [63,1,66] and other techniques, like early exit [60] or dynamic token reduction [64]. We focus on the pruning of the language models, especially structural pruning [26]. Structural pruning removes the entire filter from the neural network, which is more hardware friendly. There are several ways to remove the structure, such as l1-dependent pruning [16,67], first-order importance estimation [18], hessian-based estimation [21,52] or the optimal brain surgeon [24,21]. As for the pruning unit in structural pruning, some works adopt the entire layer [10] as the minimal unit, and others take the multi-head attention [50] or the feed-forward layers [18,34] as the basic structure to prune. CoFi [59] studies the pruning unit in different granularity. \n\nEfficient and Low Resource Compression. With the growing size of models, there is an increasing demand for efficient LLM compression and compression is independent of the original training data. \n\nAs for the efficient compression, [22] accelerate the post-training by defining the reconstruction error as a linear least squares problem. [13,12] propose the layer-wise optimal brain surgeon. As for the constraint of availability of the training corpus, data-free pruning [43,65] come up with several strategies to prune the model by measuring neurons' similarity. Besides, [32,31,40] proposes methods that distill the model without reliance on the training corpus of the model. However, those methods are too time-consuming, involving synthesizing samples by backpropagating the pre-trained language models.",
            "score": 0.5058776230990102,
            "section_title": "Related Work",
            "char_start_offset": 5248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "233297003"
                },
                {
                    "start": 281,
                    "end": 284,
                    "matchedPaperCorpusId": "236477807"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "221995575"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 343,
                    "end": 346,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "216552850"
                },
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "235097557"
                },
                {
                    "start": 687,
                    "end": 691,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 730,
                    "end": 734,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "155089879"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 1566,
                    "end": 1569,
                    "matchedPaperCorpusId": "238259421"
                },
                {
                    "start": 1668,
                    "end": 1671,
                    "matchedPaperCorpusId": "222290473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41845703125
        },
        {
            "corpus_id": "271903658",
            "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
            "text": "Pruning In early pruning methods like magnitude-based tuning, scalability is achieved but often at the cost of reduced effectiveness in large language models (LLMs) (Hagiwara, 1994;Han et al., 2015;Li et al., 2017;Frantar & Alistarh, 2023;van der Ouderaa et al., 2023). To improve  performance while managing computational demands, frameworks such as Optimal Brain Damage (LeCun et al., 1989) and Surgeon (Hassibi et al., 1993;Yu et al., 2022;van der Ouderaa et al., 2023) incorporate second-order loss information, necessitating substantial resources for Hessian calculations. Recent adaptations like WoodFisher (Singh & Alistarh, 2020), Kronecker factorization (Wang et al., 2019a;van der Ouderaa et al., 2023), and layer-wise compression (Dong et al., 2017;Frantar & Alistarh, 2022) aim to streamline these intensive methods. Concurrently, learnable parameters for pruning in vision and language models have been investigated (Liu et al., 2017;Huang & Wang, 2018;Xia et al., 2022), although these techniques generally demand significant computational resources for intensive backward propagation. Other approaches, such as feature-mimic-based methods (An et al., 2024;Ji et al., 2024), have not matched the performance of gradient-based methods like LLM Surgeon (van der Ouderaa et al., 2023). Alternatives like SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2024), and ZeroPruner (Dong et al., 2024), exploring unstructured and semi-structured pruning, offer scalability but often compromise runtime speed. Additional research has utilized layer importance scores for layer pruning and sparsity distribution, as demonstrated by ShortGPT (Men et al., 2024), OWL (Yin et al., 2023), LaCo (Yang et al., 2024), and others (Chen et al., 2024).",
            "score": 0.5054312753231165,
            "section_title": "RELATED WORKS",
            "char_start_offset": 5358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1752
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 181,
                    "matchedPaperCorpusId": "25970113"
                },
                {
                    "start": 181,
                    "end": 198,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 372,
                    "end": 392,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 405,
                    "end": 427,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 427,
                    "end": 443,
                    "matchedPaperCorpusId": "247318543"
                },
                {
                    "start": 663,
                    "end": 683,
                    "matchedPaperCorpusId": "155089879"
                },
                {
                    "start": 741,
                    "end": 760,
                    "matchedPaperCorpusId": "5750817"
                },
                {
                    "start": 929,
                    "end": 947,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 947,
                    "end": 966,
                    "matchedPaperCorpusId": "575794"
                },
                {
                    "start": 1154,
                    "end": 1171,
                    "matchedPaperCorpusId": "266362404"
                },
                {
                    "start": 1359,
                    "end": 1377,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.403564453125
        },
        {
            "corpus_id": "263830468",
            "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
            "text": "The emergence of Large Language Models (LLMs) (Zhao et al., 2023;Chang et al., 2023;Brown et al., 2020) has revolutionized natural language processing tasks with remarkable success. However, their massive model size leads to the high inference costs. For example, GPT-3, with its 175B parameters (350GB in half-precision), requires a minimum of five A100 GPUs for inference. Consequently, LLM compression research has become pivotal in mitigating these high inference costs. \n\nWhile existing LLM compression efforts focus on quantization (Liu et al., 2023;Xiao et al., 2023;Frantar et al., 2022;Yao et al., 2022), which reduces the bit number of model representations, the exploration of LLM pruning has remained limited. This is particularly true for structured pruning, which can directly cut inference costs on standard hardware but often is more challenging than unstructured pruning, as it strictly removes coherent groups of model parameters. A primary reason for the limited exploration on LLM pruning is that the success of various LLM families, such as GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022b), PALM (Chowdhery et al., 2022b), BLOOM (Scao et al., 2022), LLaMA (Touvron et al., 2023a), and LLaMA 2 (Touvron et al., 2023b) have demonstrated that increasing model size leads to enhanced capabilities. In contrast, the act of structured pruning, which reduces the model size, contradicts this trend and has been observed in existing attempt (Ma et al., 2023) to easily cause performance decline after pruning. \n\nIn this work, we explore the potential of structurally pruning non-essential parameters from LLMs as much as possible, while preserving their remarkable performance across various tasks. We begin by revisiting the existing pruning approaches.",
            "score": 0.5049958678141853,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1772
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 103,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 556,
                    "end": 574,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 595,
                    "end": 612,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 1068,
                    "end": 1088,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.759765625
        },
        {
            "corpus_id": "264904333",
            "title": "The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models",
            "text": "These difficulties suggest that there is a need for a more fine-grained understanding of the effects of compression schemes, comparing a variety of model families, compression techniques, and specialized measurements. \n\nWe address these challenges, specifically focusing on the preservation of parametric knowledge, i.e., knowledge acquired during pretraining, that is stored in model weights. This is particularly crucial for tasks involving reasoning and for specialized applications. Concretely, we examine the impact of different compression schemes on parametric knowledge across multiple model families (ENCODER, ENCODER-DECODER and DE-CODER) where we apply pruning and quantization approaches and analyze the performance of such techniques on downstream reasoning tasks. To the best of our knowledge, this work represents one of the first large-scale investigations in this direction. Among the crucial observations resulting from this study include: \n\n\u2022 Pruning all modules together has the most significant impact on parametric knowledge, compared to pruning specific modules, \u2022 At pruning levels of >50%, the parametric knowledge of all the models declines rapidly, \u2022 Quantizing attention modules has less impact on performance compared to quantizing feedforward networks for all the models, \u2022 Across all models, structured pruning at the final layer has detrimental effects compared to unstructured pruning.",
            "score": 0.5049736773044361,
            "section_title": "Introduction",
            "char_start_offset": 1885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 220,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1418
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "251979721",
            "title": "Efficient Methods for Natural Language Processing: A Survey",
            "text": "Proposed by LeCun et al. (1989), pruning removes irrelevant weights from a neural network to reduce computation, and furthermore, decreases memory capacity and bandwidth requirements. Pruning can be applied at different stages of the NLP pipeline (Fig. 2). For instance, Gordon et al. (2020) found that up to \u223c40% of BERT can be pruned at pretraining without affecting its performance. Others proposed pruning methods that work as regularizers and can be applied to pre-training and finetuning (Louizos et al., 2018;Wang et al., 2020b). Finally, works investigated pruning during finetuning (Han et al., 2015;Sanh et al., 2020) or dynamically during inference (Fan et al., 2020). \n\nPruning was initially introduced at the individual weight level (unstructured pruning), but more recent approaches prune larger components of the network (structured pruning). Examples of the latter include removing attention heads (Voita et al., 2019;Michel et al., 2019), weak attention values (Ji et al., 2021;Qu et al., 2022), and even entire hidden layers (Dong et al., 2017;Sajjad et al., 2023). In particular, Xia et al. (2022) found that pruning all these components yields more accurate and efficient models. When comparing the two pruning approaches, unstructured pruning is often found to better preserve a model's performance (Gale et al., 2019;Ahia et al., 2021), but existing hardware often cannot exploit the resulting sparsity. In contrast, structured pruning methods often lead to a higher improvement in terms of inference speed (Hoefler et al., 2021).",
            "score": 0.504734539489836,
            "section_title": "Pruning",
            "char_start_offset": 26934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1552
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 31,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 271,
                    "end": 291,
                    "matchedPaperCorpusId": "211171709"
                },
                {
                    "start": 494,
                    "end": 516,
                    "matchedPaperCorpusId": "30535508"
                },
                {
                    "start": 516,
                    "end": 535,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 591,
                    "end": 609,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 609,
                    "end": 627,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 660,
                    "end": 678,
                    "matchedPaperCorpusId": "202750230"
                },
                {
                    "start": 914,
                    "end": 934,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 934,
                    "end": 954,
                    "matchedPaperCorpusId": "226283739"
                },
                {
                    "start": 978,
                    "end": 995,
                    "matchedPaperCorpusId": "235293987"
                },
                {
                    "start": 995,
                    "end": 1011,
                    "matchedPaperCorpusId": "247026660"
                },
                {
                    "start": 1043,
                    "end": 1062,
                    "matchedPaperCorpusId": "5750817"
                },
                {
                    "start": 1062,
                    "end": 1082,
                    "matchedPaperCorpusId": "251005814"
                },
                {
                    "start": 1099,
                    "end": 1116,
                    "matchedPaperCorpusId": "247922354"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "269605957",
            "title": "COPAL: Continual Pruning in Large Language Generative Models",
            "text": "Pre-trained LLMs have shown generizability across various NLP tasks.Despite their advancements, finetuning of LLMs face challenges in computational efficiency, requiring optimization strategies like pruning for practical deployment.\n\nPruning.Pruning plays an important role in optimizing neural network architectures, especially in large models.\n\nTypes of Pruning: Structured Pruning involves removing entire structural elements such as neurons, filters, or layers.Key contributions in this area include network slimming Liu et al. (2017), channel pruning (He et al., 2017), and optimizing network architectures with minimal performance trade-offs (Luo et al., 2017;Yu et al., 2018).Structured pruning is particularly relevant for simplifying large language models and enhancing their efficiency.Whereas, unstructured pruning focuses on the selective removal of individual weights.It aims to eliminate less critical connections within the network.Pioneering techniques like \"Optimal Brain Damage\" (LeCun et al., 1989) and \"Deep Compression\" (Han et al., 2015) have significantly contributed to reducing neural network size.The \"Lottery Ticket Hypothesis\" (Frankle & Carbin, 2018) suggests the existence of smaller, effective sub-networks, which is a critical concept for large model optimization.\n\nStages of Pruning: Pruning can be applied before training, during training, or post-training.Each stage offers unique advantages and challenges.For instance, SNIP (Lee et al., 2018) and GraSP (Wang et al., 2020) focus on identifying critical connections before training starts.On the other hand, dynamic sparse training (Liu et al., 2020), soft filter pruning (He et al., 2018)",
            "score": 0.5047126980126585,
            "section_title": "Prior Works",
            "char_start_offset": 5187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 68,
                    "end": 232
                },
                {
                    "start": 234,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 345
                },
                {
                    "start": 347,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 683
                },
                {
                    "start": 683,
                    "end": 796
                },
                {
                    "start": 796,
                    "end": 881
                },
                {
                    "start": 881,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1296
                },
                {
                    "start": 1298,
                    "end": 1391
                },
                {
                    "start": 1391,
                    "end": 1442
                },
                {
                    "start": 1442,
                    "end": 1575
                },
                {
                    "start": 1575,
                    "end": 1675
                }
            ],
            "ref_mentions": [
                {
                    "start": 521,
                    "end": 538,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 556,
                    "end": 573,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 648,
                    "end": 666,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 666,
                    "end": 682,
                    "matchedPaperCorpusId": "4142619"
                },
                {
                    "start": 997,
                    "end": 1017,
                    "matchedPaperCorpusId": "7785881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50390625
        },
        {
            "corpus_id": "250294426",
            "title": "Data-Aware Adaptive Pruning Model Compression Algorithm Based on a Group Attention Mechanism and Reinforcement Learning",
            "text": "To avoid the disadvantages of unstructured sparse models, structured pruning has gradually become a hot topic of research. The structured pruning method attempts to produce structured sparseness in the tensor of parameters; that is, pruning takes the tensor as the basic unit. According to the different pruning granularities, the structured pruning algorithm can be divided into connection-level pruning, vector-level pruning, channel-level pruning, and filter-level pruning [5]. Yao et al. [6] discussed the impact of pruning with different granularities, realized different pruning algorithms from the connection level to the filter level, and analyzed the storage capacity and hardware computing efficiency of the model at different granularities. They found that coarsegrained pruning algorithms (such as the filter-level algorithm) can save more storage space, obtain a greater compression rate and greatly help in improving hardware efficiency. This paper also focuses on coarse-grained pruning algorithms. This is because the damage to the original network structure can be reduced by this method, and the pruned model perfectly fits the existing deep learning library. Moreover, since the network structure does not undergo major changes, the pruned model can also be combined with other compression algorithms to further reduce the number of calculations. \n\nThe main difficulty of the coarse-grained pruning algorithm arises from two aspects: the importance of the convolution kernel and the problem of model accuracy decline after pruning. The evaluation of the importance of convolution kernels is a very important part of the pruning task. After the evaluation of each convolution kernel is completed, the convolution kernels with lower importance are selected and cut in a certain proportion and order [7]. The basic idea of the current research is to first provide a large model, estimate the importance of the parameters or parameter sets in the model, and then use the model as a reference for pruning. Li et al. [8] proposed a filter pruning algorithm. They use the sum of the absolute value of each filter as a measure of the importance of the filter.",
            "score": 0.5046099076710553,
            "section_title": "II. RELATED WORKS",
            "char_start_offset": 3538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 479,
                    "matchedPaperCorpusId": "195657904"
                },
                {
                    "start": 492,
                    "end": 495,
                    "matchedPaperCorpusId": "53155421"
                },
                {
                    "start": 1816,
                    "end": 1819,
                    "matchedPaperCorpusId": "181393129"
                },
                {
                    "start": 2030,
                    "end": 2033,
                    "matchedPaperCorpusId": "14089312"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59716796875
        },
        {
            "corpus_id": "251476392",
            "title": "Heuristic Method for Minimizing Model Size of CNN by Combining Multiple Pruning Techniques",
            "text": "On the other hand, the unstructured pruning compresses the CNN models by removing the individual unimportant weights. \n\nIn this paper, we propose a heuristic method that combines the structured and unstructured pruning methods. In our method, we control the compression ratios of the original structured and unstructured pruning methods by repeating these methods under a target accuracy. Through combining these two pruning methods, we will explore the effective pruned model with a higher compression ratio than the single usage of them, respectively, and simultaneously achieve higher accuracy for the inference. \n\nThe main contributions of this paper are summarized as follows: \n\n1. \n\nWe have found a method for model compression that uses the structured pruning and unstructured pruning jointly. We also allow the CNN models to meet and maintain the target accuracy given by their applications. This will achieve the best reduction ratio regarding the parameters of the CNN models.",
            "score": 0.5040816601861201,
            "section_title": "Introduction",
            "char_start_offset": 2192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 120,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 986
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32177734375
        },
        {
            "corpus_id": "260918044",
            "title": "Reconstructed SqueezeNext with C-CBAM for offline handwritten Chinese character recognition",
            "text": "The most straightforward method of model compression is to reduce the number of parameters, that is the number of convolutional filters. In addition to adjusting the number of convolutional filters per layer directly, network pruning can also be done to reduce the number of convolutional filters. Network pruning can be divided into structured pruning and unstructured pruning. Structured pruning refers to the removal of the complete convolutional filter. The structured pruning process is shown in Fig. 4. Unstructured pruning refers to fine-grained pruning, where some weights are removed from the convolutional filter, but not the entire filter. Unstructured pruning is highly flexible and can form sparse networks without sacrificing accuracy. However, pruning the weights can make the matrix too sparse, leading to redundant memory allocations that affect the operational efficiency of the network, and when deployed with the hardware side, it relies on algorithm libraries such as cuSPARSE, making it difficult to accelerate parallel When the values of the elements in the matrix change significantly, the fluctuations in the L1 norm of the matrix are usually larger and the fluctuations in the L2 norm are smaller. So the L1 and L2 norm of each matrix will be weighted and then summed. When the L1 norm is too large, the matrix is too sparse or the element values are too small, the model may be under-fitted and the sparse weights are not conducive to subsequent model compression means; when the L1 norm is too small, the feature extraction performance of the weights is reduced and useful information cannot be filtered out. When the L2 norm is too large, it pulls the weights to a small value, resulting in an underfitting of the model; when it is too small, it loses the effect of smoothing the curve. Therefore, \u03b1 and \u03b2 need to be set within a reasonable range to ensure the validity of the evaluation formula.",
            "score": 0.5038294735468289,
            "section_title": "Pruning",
            "char_start_offset": 13146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1925
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14501953125
        },
        {
            "corpus_id": "251476392",
            "title": "Heuristic Method for Minimizing Model Size of CNN by Combining Multiple Pruning Techniques",
            "text": "We have developed an algorithm that achieves a better compression ratio than the individual usage of the structured and unstructured pruning methods under a target accuracy.",
            "score": 0.5032214161935442,
            "section_title": "2.",
            "char_start_offset": 3185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1649169921875
        },
        {
            "corpus_id": "267713932",
            "title": "Distilled Gradual Pruning With Pruned Fine-Tuning",
            "text": "Whatever the pruning method be, it usually involves careful fine-tuning [20] to maximize its performances. However, such constraints are expected to decrease the efficiency of pruning. Unstructured pruning, on the other hand, produces sparse matrices that are difficult to accelerate [21], even if some recent works withdraw this statement [22], [23]. In this context, different strategies have been proposed throughout the years for unstructured pruning in several application areas. The optimal brain damage algorithm [24] and magnitude-based pruning algorithm [25] are two popular unstructured pruning techniques. Other popular methods include Taylor expansion pruning [26], which prunes based on the loss function's second-order Taylor approximation, and random pruning [25], which prunes randomly to improve computation times. However, a simple pruning of the weights may lead to a drop in performance. To this extent, weight recovery between training cycles [27], [28] and fine-tuning the pruned model through additional training has shown to be an effective approach to mitigate this issue [29]. \n\nAs a popular approach for model compression, knowledge distillation has received significant attention in recent years [30], [31]. The basic idea behind this technique is to train a smaller model, referred to as the student model, and to mimic the behavior of a larger model, referred to as the teacher model. The student model is trained by minimizing the difference between its predictions and the predictions of the teacher model, which is often a pretrained NN. Self-distillation refers to a knowledge distillation approach where a NN is distilled into a smaller, more compact version of itself [32]. \n\nThe research direction goes toward more complex pruning and distillation strategies, but often with a large computational cost; Srinivas et al. [28] tried to introduce a cyclical pruning and weight recovery schedule, but significantly increasing the complexity of the algorithm at a price of a slight classification improvement. \n\nWe present a novel unstructured pruning algorithm that seamlessly integrates knowledge distillation techniques to achieve significant model compression without compromising its accuracy.",
            "score": 0.5030088041934918,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1102
                },
                {
                    "start": 1105,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "231699188"
                },
                {
                    "start": 284,
                    "end": 288,
                    "matchedPaperCorpusId": "210064442"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "208097594"
                },
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "234331559"
                },
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "10969179"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "10969179"
                },
                {
                    "start": 970,
                    "end": 974,
                    "matchedPaperCorpusId": "246485603"
                },
                {
                    "start": 1097,
                    "end": 1101,
                    "matchedPaperCorpusId": "248693442"
                },
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "233582109"
                },
                {
                    "start": 1704,
                    "end": 1708,
                    "matchedPaperCorpusId": "232302458"
                },
                {
                    "start": 1856,
                    "end": 1860,
                    "matchedPaperCorpusId": "246485603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49658203125
        },
        {
            "corpus_id": "247446572",
            "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models",
            "text": "Pre-trained Transformer models (Vaswani et al., 2017;Devlin et al., 2019) have become a standard building block for many natural language processing (NLP) tasks, providing robust language representations which can be specialized on \"downstream\" classification and generation tasks. Despite their success, these models have large parameter counts, which limits their usability. Techniques for reducing these parameter counts and the corresponding computational overheads have become vital, especially given the recent breakneck pace of model growth (Radford et al., 2019;MTN).\n\nSeveral compression approaches are known for large language models (LLMs). One example is Knowledge Distillation (KD) (Hinton et al., 2015), which led to smaller models like DistillBERT (Sanh et al., 2019), MobileBERT (Sun et al., 2020), and TinyBERT (Jiao et al., 2020) which approximate the original model with minor accuracy changes. Other work has leveraged lower-precision representations to produce quantized models such as Q8BERT (Zafrir et al., 2019), Ternary-BERT (Zhang et al., 2020), and Q-BERT (Shen et al., 2020). An orthogonal approach, which is our primary focus, has been to apply fine-grained methods such as unstructured pruning to produce model families such as SparseBERT (Xu et al., 2021) and PruneBERT , which compress by removing individual weights. Figure 1 provides a comparative overview of state-of-the-art results for unstructured pruning on a standard model (BERT-base).\n\nWe investigate improved methods for unstructured and semi-structured (block) pruning approaches at BERT scale, by leveraging the second-order (curvature) approach pioneered by the Optimal Brain Surgeon framework (LeCun et al., 1989). We put our results in the context of a compound compression framework, which combines unstructured pruning with structured pruning and quantization, showing that these methods can be complementary. To realize these compression gains in practice, we tailor the resulting compressed models to execute efficiently on a sparsity-aware CPUbased runtime engine (NeuralMagic, 2021), leading to order",
            "score": 0.5029186773945135,
            "section_title": "Introduction",
            "char_start_offset": 120,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 795,
                    "end": 813,
                    "matchedPaperCorpusId": "215238853"
                },
                {
                    "start": 1083,
                    "end": 1101,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 1269,
                    "end": 1286,
                    "matchedPaperCorpusId": "233297003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67333984375
        },
        {
            "corpus_id": "277634221",
            "title": "Mosaic: Composite Projection Pruning for Resource-efficient LLMs",
            "text": "Pruning removes an individual or a group of parameters from the model using a ranking algorithm [12]. There are two categories of pruning: Unstructured pruning refers to setting parameter values to zero [12]; quality is maintained while model size is unaffected. Structured pruning refers to removing data structures containing parameters such as attention heads [13]; this reduces model size and inference latency but at the cost of model quality. In short, existing pruning methods do not adequately balance runtime performance and LLM quality. \n\nOur work, Mosaic, is positioned to address these shortcomings. Existing pruning methods focus on coarse-grained pruning at the global and layer level of the LLM (further discussed in Section II). They prune every LLM component uniformly. This results in removing parts of the model that are critical to quality. Mosaic introduces novel fine-grained pruning of LLM projections. We leverage non-uniform pruning and apply it to different components of the LLM to selectively retain critical model parts. In addition, Mosaic synergistically combines unstructured and structured pruning in LLMs for the first time to create 'composite projection pruning'. This pruning approach can produce compressed and resource-efficient LLMs that fit in limited memory and provide fast inference while having comparable quality to the foundation LLM. The models produced by Mosaic can be deployed on any hardware platform without requiring specific hardware/software accelerators. \n\nOur research contributions are as follows: \n\n(1) Mosaic, a novel system for compressing foundation LLMs for hardware-limited environments. Mosaic is 7.19\u00d7 faster in producing compressed models than existing approaches. \n\n(2) LLM projection pruning, a new method that maintains arXiv:2504.06323v1 [cs. \n\nLG] 8 Apr 2025 quality at higher compression levels. The method determines a projection outlier distribution to prune projections nonuniformly. Projection pruning is a performance-efficient extension of fine-grained LLM pruning explored in prior work. \n\nMosaic models produced by projection pruning achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models from uniform pruning.",
            "score": 0.5015991855010252,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1959,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2068
                },
                {
                    "start": 2071,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 100,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 203,
                    "end": 207,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 363,
                    "end": 367,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "253383996",
            "title": "A Framework for Designing Efficient Deep Learning-Based Genomic Basecallers",
            "text": "We observe established automatic-speech recognition (ASR) models being directly applied to basecalling without optimizing it for basecalling. Such an approach leads to large and unoptimized basecaller architectures. We evaluate the effect of using two popular model compression techniques on the Bonito _ CTC basecaller: (1) Pruning, and (2) Quantization. \n\n2.1.1. Effect of Pruning. We show the effect of pruning Bonito _ CTC on the validation accuracy and model size in Figure 2(a) and Figure 2(b), respectively. Pruning is a model compression technique where we discard network connections that are unimportant to network performance without affecting the inference accuracy [59][60][61][62]. We use unstructured element pruning and structured channel pruning with different degrees of sparsity. Unstructured or element pruning is a fine-grain way of pruning individual weights in a neural network without applying any pruning constraints. While in structured pruning, we remove a larger set of weights while maintaining a dense structure of the model [66,67]. We make three major observations. First, pruning up to 85% of the Bonito _ CTC model weights using unstructured pruning reduces the model size by 6.67\u00d7 while maintaining the same accuracy as the baseline, unpruned Bonito _ CTC model. Unstructured pruning leads to the highest model compression [68] at the cost of having sparse weights structure that is unsuitable for acceleration on any hardware platform. While pruning 30-40% of the Bonito _ CTC model filters, using structured pruning reduces the model size by 1.46-1.66\u00d7 while maintaining the same accuracy of the baseline, unpruned Bonito _ CTC model. Such a high pruning ratio shows that most of the weights are redundant and do not contribute to the actual accuracy.",
            "score": 0.5008236696233643,
            "section_title": "Analyzing the State-of-the-Art Basecaller",
            "char_start_offset": 12130,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1788
                }
            ],
            "ref_mentions": [
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "matchedPaperCorpusId": "15074600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50439453125
        },
        {
            "corpus_id": "256662263",
            "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
            "text": "Our results contribute to the line of work on efficient language models. Thus, it should help reduce the energy and monetary cost of inference over such models, and allow them to be used without access to powerful hardware. While this is a mainly positive outcome, it also reduces the cost of employing these models for detrimental purposes, such as spam generation. Thus, this significant cost reduction for inference should also be seen as further motivation for methods to ensure safe usage of these models, such as watermarking or alignment. As any academic study, our work is not without its limitations. All of our benchmarks are focused on English-language datasets and therefore our results do not provide insights into compression effects for low-data languages. Unfortunately, this limitation is inherent to all of the existing works on compression due to the lack of standardized benchmarks. Given that our structured pruning approach relies on a small sample of calibration data to perform pruning decisions, we hypothesize that our approach should be able to provide satisfying results in the low-data setup as well. At the moment we do not have data to support these claims, but we see it as an opportunity for future work.",
            "score": 0.5006590460558846,
            "section_title": "J Broader Impact and Limitations",
            "char_start_offset": 44092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360595703125
        },
        {
            "corpus_id": "257901132",
            "title": "oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes",
            "text": "While many methods to improve model efficiency exist, the same goal generally underpins them: given an original model \u03b8 with an accuracy of acc(\u03b8) and an inference cost of c(\u03b8) minimize the inference cost. While the methods used for compression can be highly optimized and specialized, they can commonly be used together to deliver massive improvements in inference speeds with minimal losses in accuracy.\n\nTransformer Based Language Models such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) provide contextual language representations built on the Transformer architecture (Vaswani et al., 2017) which can be specialized and adapted for specific tasks and domains (Lee et al., 2020). Using these models, it becomes relatively easy to excel at a broad range of natural languages processing tasks such as Question Answering, Text Classification, and sentiment analysis. Unstructured Pruning is a compression approach that removes individual weights or groups of weights in a model by applying a mask or setting the weight values to 0. This compression approach has been broadly studied in computer vision (Han et al., 2015), and many methods can remove 70% or more of model weights with little to no loss in accuracy. Models pruned can be 20x smaller in terms of pure model size and, when paired with a sparsity-aware inference engine such as DeepSparse (Magic, 2023), provide 3-5x speedups in inference throughput. Focused on language models, recent work has shown that it is possible to prune models during pre-training with little to no loss in accuracy (Sanh et al., 2020) (Kurti\u0107 et al., 2022  As applied to language models, the approach has been used to improve the performance of structurally pruned language models resulting in models like DistilBERT (Sanh et al., 2019b) and TinyBERT (Jiao et al., 2020). Quantization reduces the precision for the model weights and activations to lower the computational requirements of model execution. While researchers have explored reducing representation to binary representations (Pouransari and Tuzel, 2020), current hardware limits inference speedups to 8 or 4-bit representations. Quantization can be applied after the model is trained in a one-shot fashion, but this can",
            "score": 0.5005938964980005,
            "section_title": "Background and Related work",
            "char_start_offset": 4454,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5498046875
        },
        {
            "corpus_id": "271162137",
            "title": "Characterizing Prompt Compression Methods for Long Context Inference",
            "text": "For the token pruning methods in Section 4.1, the reranker selects 25% more chunks than originally and then applied a token pruning rate of 20% to achieve each compression ratio.\n\nHere, we perform a study where the reranker selects 2\u00d7 more chunks and an aggressive token pruning rate of 50% is applied.As shown in Figure 6, such aggressive token pruning leads to accuracy degradation.After observing the pruned context, we hypothesize that this is because aggressive token pruning leads to unstructured text that does not respect grammatical constructs, making it difficult for the downstream model to correctly reason over it.",
            "score": 0.5005550985813675,
            "section_title": "AGGRESSIVE TOKEN PRUNING",
            "char_start_offset": 26460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 180,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 627
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08465576171875
        },
        {
            "corpus_id": "270870579",
            "title": "Joint Pruning and Channel-Wise Mixed-Precision Quantization for Efficient Deep Neural Networks",
            "text": "Orthogonal to quantization, another key optimization for DNN compression and inference speed-up is pruning [21]. While quantization works at the operand representation level, pruning involves reducing the size of DNNs by selectively removing a subset of their parameters. Pruning techniques can be classified based on the granularity of the subsets to be removed and on the selection scheme employed to decide which subsets to eliminate. \n\nAccording to the pruning granularity, two main approaches can be identified, i.e., unstructured and structured. Unstructured pruning considers the removal of single weights. This approach leads to the largest theoretical reduction of parameters and operations without accuracy loss, reaching values as high as 90% [22]. However, it leads to sparsely connected networks that cannot be easily accelerated in hardware, especially on general-purpose platforms, without adding a significant overhead [21], [23]. In fact, sparse computations make it complex to achieve high utilization on parallel hardware and damage the regularity and locality of memory access patterns, resulting in worse cache behaviour [23]. \n\nStructured pruning refers to those approaches that remove weights with some regularity, e.g., in blocks, or that eliminate entire neurons, channels or filters. While blockwise and block-balanced pruning still require hardware support to be effectively accelerated [21], channel-/filterwise pruning leads to compressed architectures where the associative and distributive properties of linear algebra can be used to transform them into smaller dense structures [21], thus solving the main drawback of the unstructured approaches. The deployment of the obtained networks is straightforward and does not incur any overhead, nor needs special hardware/software support. The main disadvantage is represented by the worse achievable trade-off between task performance degradation and compression [22]. \n\nFor both unstructured and structured pruning techniques, different selection schemes have been proposed in literature. Removing weights with the lowest absolute magnitude is a simple yet effective strategy [22], [24]. It works for both individual weights and weight groups [24], and it can be applied without the need for additional data.",
            "score": 0.5004457719805829,
            "section_title": "Pruning",
            "char_start_offset": 8896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1945
                },
                {
                    "start": 1948,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 111,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 754,
                    "end": 758,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 935,
                    "end": 939,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 941,
                    "end": 945,
                    "matchedPaperCorpusId": "4497748"
                },
                {
                    "start": 1142,
                    "end": 1146,
                    "matchedPaperCorpusId": "4497748"
                },
                {
                    "start": 1414,
                    "end": 1418,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 1610,
                    "end": 1614,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 1940,
                    "end": 1944,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 2154,
                    "end": 2158,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 2160,
                    "end": 2164,
                    "matchedPaperCorpusId": "14089312"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65771484375
        },
        {
            "corpus_id": "267751193",
            "title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers",
            "text": "Pruning, a prevalent technique for model compression method, seeks to discard non-essential parameters within language models, thereby reducing computational and storage costs while maintaining performance [13], [31], [35]. Pruning methods can be broadly categorized into unstructured pruning, which removes individual weights, and structured pruning, which removes higher-granularity structures such as neurons, attention heads, or entire layers. Layer-wise pruning, a specific form of structured pruning, has been explored in several studies. Fan et al. (2019) [5] introduced LayerDrop, a structured dropout method that randomly drops layers during training to improve robustness to layer reduction at inference time. Additionally, several studies have investigated the disparities in representations across different layers, highlighting the significance of updating the last few layers for language models [12], [14], [18]. Building on this foundation, Peer et al. (2022) [22] and Sajjad et al. (2023) [24] aimed to identify and remove an optimal subset of layers directly from the pretrained models for use in downstream tasks. These works focus on pruning layers to reduce model size while preserving taskspecific performance within the standard fine-tuning paradigm. However, they do not explore the interaction between layer pruning and prompt-based fine-tuning, which is the focus of our work. \n\nRecent studies have explored the application of pruning techniques to large language models, such as Sheared LLMs [34] and SparseGPT [6], which aims to sparsify or prune LLMs for efficient inference. These methods often involve complex optimization strategies or require additional training steps [1], [4], [11]. In contrast, our approach is a simple top-layer pruning strategy that directly removes layers from LLMs and evaluates their performance in prompt-based fine-tuning scenarios. Unlike prior work, we quantitatively investigate the impact of layer pruning on LLMs in few-shot learning settings, demonstrating that even drastic reductions in layers can maintain or improve performance.",
            "score": 0.5003881413429174,
            "section_title": "A. Model Pruning",
            "char_start_offset": 3976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "258865530"
                },
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 545,
                    "end": 562,
                    "matchedPaperCorpusId": "202750230"
                },
                {
                    "start": 563,
                    "end": 566,
                    "matchedPaperCorpusId": "202750230"
                },
                {
                    "start": 910,
                    "end": 914,
                    "matchedPaperCorpusId": "201645145"
                },
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "247771234"
                },
                {
                    "start": 985,
                    "end": 1005,
                    "matchedPaperCorpusId": "251005814"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "251005814"
                },
                {
                    "start": 1519,
                    "end": 1523,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 1538,
                    "end": 1541,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "268032253",
            "title": "LLM Inference Unveiled: Survey and Roofline Model Insights",
            "text": "Pruning [LeCun et al., 1989, Liang et al., 2021], which concentrates on identifying and eliminating model parameters that are deemed unnecessary or redundant, is another popular technique for compressing LLMs. In the context of LLMs, the parameters often account for a considerable portion of the model size and computational demand. By carefully pruning these parameters, it's possible to streamline the model, making it more efficient without significantly compromising its performance. Pruning methods can be broadly classified into two categories: unstructured pruning and structured pruning, and we describe the research progress of each category in turn below.",
            "score": 0.5002538875021805,
            "section_title": "Pruning",
            "char_start_offset": 35572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 666
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 27,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 27,
                    "end": 48,
                    "matchedPaperCorpusId": "231699188"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.269775390625
        },
        {
            "corpus_id": "272835782",
            "title": "Advances in Pruning and Quantization for Natural Language Processing",
            "text": "With ongoing advancements in natural language processing (NLP) and deep learning methods, the demand for computational and memory resources has considerably increased, which signifies the determination of efficient and compact models in resource-constrained environments. A comprehensive overview of the most recent advancements in pruning and quantization methods for deep neural networks is provided in this paper. Numerous cutting-edge techniques that harness the complementary advantages of pruning and quantization have been analyzed, highlighting their effectiveness in reducing model size, enhancing computational efficiency, and minimizing memory usage. These techniques include Quantization and Sparsity Aware Fine Tuning, Compression Learning by In-Parallel Pruning-Quantization, GroupReduce, Quantization-Pruned Attention, Structured Pruning, Normalized Linear Quantization (Prune and NLQ), Quantization and Pruning for Sentiment Analysis, an Automatic mixed-precision Quantization approach for BERT compression (AQ-BERT), Mixed Precision Quantization, Unstructured Pruning, and Quantization, and Magnitude Pruning. The datasets utilized, models employed, and outcomes achieved are taken into account within this research. The utilization of pruning and quantization techniques across diverse deep-learning tasks, NLP, and sentiment analysis are also discussed. Moreover, issues such as compatibility with hardware configurations, optimization complexities, accuracy degradation, and other constraints have been analysed. Several challenges and limitations of weight or unit pruning that are utilized for optimizing memory and quantization techniques to enhance precision are explored. The in-depth analysis of these state-of-the-art techniques and experiments provides a broad understanding. Furthermore, strategies to effectively reduce the computational and memory demands of neural networks without compromising their performance are also analysed.",
            "score": 0.5000608298518688,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5419921875
        },
        {
            "corpus_id": "277452419",
            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
            "text": "Following the experiments on unstructured pruning in Section A.2, we also conducted experiments on Semistructured pruning using similar methods. The pruning sparsity was set to 50%, and we specifically employed the 4:8 and 2:4 configurations, which represent pruning four out of eight weights, or two out of four weights, respectively. When comparing the results of unstructured pruning and Semi-structured pruning, we observed that Semi-structured pruning using the same method resulted in a higher accuracy loss than unstructured pruning (Fig A5). Based on these findings, unstructured pruning is preferred under the same conditions due to its better performance. Detailed experimental data for semi-structured pruning at 50% sparsity is provided in Table A2. Additionally, we performed ablation experiments using the Calibration dataset, with alignment results on the PTB dataset shown in Table A3. -80%).This image demonstrates how the WikiText2-PPL (Perplexity) of LLaMA2-7b changes as the pruning compression rate increases from 10% to 80%. It should be noted that methods using the \"PTB\" calibration dataset (indicated with \"(ptb)\" after the method name) are included as part of an ablation study. As the pruning rate rises, many methods experience significant performance loss, and some even result in model collapse at very high compression rates. Fig. A4: WikiText2-PPL Trend with Unstructured Pruning Methods at Compression Rates Below 60% for LLaMA2-7b. This figure contrasts various unstructured pruning methods, with Wanda and SparseGPT in red and blue, respectively. As shown, at compression rates below 60%, these two methods exhibit less performance degradation compared to most other approaches. It should be noted that methods using the \"PTB\" calibration dataset (indicated with \"(ptb)\" after the method name) are included as part of an ablation study. ADMM, Wanda and SparseGPT offer more stable and robust performance compared to other pruning methods.",
            "score": 0.5000464921273793,
            "section_title": "A.3 Comparison of Unstructured Pruning and Semi-structured Pruning Methods",
            "char_start_offset": 64047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1973
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4453125
        },
        {
            "corpus_id": "276421918",
            "title": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs",
            "text": "Model pruning is an effective approach to achieving sparse LLMs while maintaining model functionality. Pruning methods can be categorized into two main types: unstructured and structured pruning. Unstructured pruning operates at the weight level, allowing for arbitrary weight removal (Lee et al., 2018). In large language models, pruned weights are set to zero (Frantar and Alistarh, 2023;Sun et al., 2023). However, this method requires specialized hardware and software support for acceleration (Han et al., 2015;Wen et al., 2016;Filters'Importance, 2016;Tang et al., 2021). Structured pruning takes a coarser-grained approach by removing complete structural units such as convolution kernels, channels, attention heads, or entire layers (You et al., 2019;Ashkboos et al., 2024;Liu et al., 2021;Ma et al., 2023;Men et al.). Its main advantage is the ability to directly produce regular, narrow model architectures that can achieve acceleration without specialized sparse computation libraries (Luo et al., 2017;Liu et al., 2021;Filters'Importance, 2016;Nonnenmacher et al., 2021). However, both approaches face a fundamental limitation: achieving efficiency through permanent parameter removal may discard valuable knowledge and lose the ability to adapt computation based on input complexity. In recent years, there has been growing interest in exploring sparse computation in large language models. Mixture-of-Experts (MoE) represents a pioneering approach that demonstrates how sparse activation can effectively balance model capacity and computational efficiency. In MoE architectures, only a subset of FFN modules (experts) are activated for each input token (Fedus et al., 2022;Lepikhin et al., 2021;Huang et al., 2024). This foundational idea of conditional computation has inspired various innovations in expert activation strategies. Some works explore heterogeneous expert architectures (Sun et al., 2024) or introduce zero-computation experts (Jin et al., 2024) to further optimize computational efficiency.",
            "score": 0.49999232531974835,
            "section_title": "Related Work",
            "char_start_offset": 2804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 533,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 558,
                    "end": 576,
                    "matchedPaperCorpusId": "232170293"
                },
                {
                    "start": 741,
                    "end": 759,
                    "matchedPaperCorpusId": "202660914"
                },
                {
                    "start": 781,
                    "end": 798,
                    "matchedPaperCorpusId": "235825363"
                },
                {
                    "start": 996,
                    "end": 1014,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 1014,
                    "end": 1031,
                    "matchedPaperCorpusId": "235825363"
                },
                {
                    "start": 1667,
                    "end": 1687,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1687,
                    "end": 1709,
                    "matchedPaperCorpusId": "220265858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66796875
        },
        {
            "corpus_id": "277275922",
            "title": "Efficient self-attention with smart pruning for sustainable large language models",
            "text": "Structure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility 25 . Furthermore, this method can achieve a different level of sparsity than unstructured pruning, limiting its ability to compress models efficiently without retraining or redesigning the model architecture 26 . On the other hand, unstructured pruning operates at the level of individual weights in a neural network, meaning it can remove any specific weight, regardless of its position 27 . This allows for more fine-grained control over which weights to prune. This can target only the least essential weights, leading to a more refined reduction in model size while retaining critical model capacity 28 . It can achieve higher sparsity levels without compromising model performance because it is not constrained by the rigid structures to which structured pruning must adhere. \n\nDespite preserving model performance, unstructured pruning often results in sparse weight matrices, which may not fully utilize hardware efficiency, as specialized hardware (such as GPUs) is typically optimized for dense matrix operations. To address the limitations of unstructured pruning in efficiently utilizing hardware resources, we propose a novel approach that combines Identical Row Compression, Diagonal Weight Compression for self-attention mechanisms, and linear weight freezing for the feed-forward (FF) and embedding layers. By integrating these methods, we create a model that retains performance and enhances computational efficiency. The combination of IRC and DWC allows for a reduction in the number of active parameters while maintaining a structure that can be efficiently processed on hardware optimized for dense operations. Meanwhile, linear weight freezing stabilizes the training process and reduces computational overhead in the feed-forward and embedding layers. Overall, this multi-faceted approach effectively addresses the challenges associated with unstructured pruning while enhancing hardware efficiency and model performance.",
            "score": 0.49961313701303994,
            "section_title": "Related work",
            "char_start_offset": 6960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 148,
                    "matchedPaperCorpusId": "204009154"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "275920675",
            "title": "Hardware-Aware DNN Compression for Homogeneous Edge Devices",
            "text": "DNN compression has been widely studied to deploy complex models on resource-constrained devices. Common methods include pruning [9,16], quantization [22,32], knowledge distillation [24,43,54], and low-rank decomposition [38,44,48]. Pruning is particularly effective, as it reduces model size and computational costs while preserving accuracy. Pruning techniques fall into unstructured and structured categories. Unstructured pruning [3,11,23,34] removes individual weights, creating sparse matrices that may limit hardware efficiency [30]. Structured pruning [10,18,51], by contrast, removes filters or neurons, resulting in models with regular structures that are more hardwarefriendly [8]. This approach enhances hardware acceleration compatibility and is often more feasible for deploying DNNs on limited-resource devices.",
            "score": 0.4991390080120882,
            "section_title": "DNN Pruning",
            "char_start_offset": 6059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 826
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "260887757"
                },
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "257255597"
                },
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "256669234"
                },
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "261661742"
                },
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "257570575"
                },
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 189,
                    "end": 192,
                    "matchedPaperCorpusId": "251643827"
                },
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "213149700"
                },
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "4948235"
                },
                {
                    "start": 434,
                    "end": 437,
                    "matchedPaperCorpusId": "252872848"
                },
                {
                    "start": 437,
                    "end": 440,
                    "matchedPaperCorpusId": "5750817"
                },
                {
                    "start": 440,
                    "end": 443,
                    "matchedPaperCorpusId": "51608228"
                },
                {
                    "start": 443,
                    "end": 446,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "222066961"
                },
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "102350555"
                },
                {
                    "start": 564,
                    "end": 567,
                    "matchedPaperCorpusId": "102350938"
                },
                {
                    "start": 567,
                    "end": 570,
                    "matchedPaperCorpusId": "202660914"
                },
                {
                    "start": 688,
                    "end": 691,
                    "matchedPaperCorpusId": "259831763"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68017578125
        },
        {
            "corpus_id": "272366594",
            "title": "Designing Large Foundation Models for Efficient Training and Inference: A Survey",
            "text": "In addition to quantization and knowledge distillation, pruning is another efficient technique for reducing computational costs in LLMs. While quantization focuses on hardware optimization and knowledge distillation on knowledge transfer in neural networks, pruning concentrates on the model's structural efficiency. Specifically, pruning in machine learning involves removing less important connections in the neural network while retaining the important ones, making the model more memory-efficient and faster. \n\nPruning methods can be broadly categorized based on their granularity into unstructured pruning and structured pruning. Structured pruning removes entire neurons or larger components of the neural network. In contrast, unstructured pruning refers to the removal of individual connections, as seen in the conventional pruning methods studied by Han et al. (2015). Additionally, semi-structured pruning like N:M sparsity (Mishra et al., 2021), is generally considered a specialized form of unstructured pruning. In general, structured pruning usually yields better inference speed improvements compared to unstructured pruning, due to its hardware-friendly design. \n\nPruning methods for LLMs typically remove weights based either on (1) the loss function or (2) the layer output. \n\n(1) Loss-based. The weights are usually removed by computing metric such as gradients with respect to the training or validation loss. In some cases, pruning is performed directly through optimization techniques that minimize the model's loss after weight removal. \n\n(2) Layer output-based. Alternatively, other pruning methods for LLMs aim to find the following masks in a layer-wise pattern. The pruning quality is usually measured by the \u2113 2 -norm between the output, for given specific inputs X \u2113 , of the uncompressed layer and that of the compressed one. More formally, consider a layer \u2113 with weight matrix W \u2113 \u2208 R p . The objective is to find the binary mask M \u2113 \u2208 [0, 1] p and possibly an updated weight matrix \u0174\u2113 so that the following objective is minimized, \n\nHere, \u2299 denotes element-wise multiplication.",
            "score": 0.4991067094540347,
            "section_title": "Pruning",
            "char_start_offset": 18098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1177
                },
                {
                    "start": 1180,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2063
                },
                {
                    "start": 2066,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 859,
                    "end": 876,
                    "matchedPaperCorpusId": "2238772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58740234375
        },
        {
            "corpus_id": "266573164",
            "title": "The LLM Surgeon",
            "text": "Recent advancements in language modeling (Vaswani et al., 2017) allow fitting large language models (LLMs) with millions or even billions of parameters (such as OPT (Zhang et al., 2022) and Llama 2 (Touvron et al., 2023)) on big text corpora achieving high performance. Unfortunately, the size of these LLMs often makes it hard to deploy them within practical constraints. Cloudbased deployment can get very expensive for larger models, and efficient devices such as phones are frequently limited in the memory size to host a model. \n\nA body of literature extending back to the late 1980s, e.g., Optimal Brain Damage (OBD, LeCun et al. (1989)) and Optimal Brain Surgeon (OBS, Hassibi & Stork (1992)), phrases pruning as a constraint optimization problem to reduce a model's footprint and runtime requirements. The Hessian required for this approach grows with the square of the number of parameters, and can only be computed in practice for unrealistically small networks. To overcome this issue, Eigendamage (Wang et al., 2019) introduces a Kronecker factorization of a blockwise-diagonal approximation of the Hessian. Recent works, like Optimal Brain Compression (Frantar & Alistarh, 2022), SparseGPT (Frantar & Alistarh, 2023), demonstrate practical post-training pruning of LLMs, but only consider a loss curvature of a pruned layer's squared output reconstruction error, ignoring gradients that relate local removal costs to the target loss. As a result, their approximation to the target loss landscape is inaccurate, leading to a significant performance degradation for pruned LLMs. Further, these methods do not readily extend to structured pruning. This work introduces LLM Surgeon, a general framework for unstructured, semi-structured and structured pruning of LLMs. At paper submission, we deemed this the first method to successfully perform structured pruning of LLMs. Concurrent work by Ashkboos et al. (2024) also considers structured pruning of LLMs but ignores gradient information, resulting in lower final performance.",
            "score": 0.4990825185648171,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 676,
                    "end": 698,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 1009,
                    "end": 1028,
                    "matchedPaperCorpusId": "155089879"
                },
                {
                    "start": 1203,
                    "end": 1229,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "247922354",
            "title": "Structured Pruning Learns Compact and Accurate Models",
            "text": "Structured pruning has been widely explored in computer vision, where channel pruning (He et al., 2017;Luo et al., 2017;Liu et al., 2017Liu et al., , 2019cMolchanov et al., 2019;Guo et al., 2020) is a standard approach for convolution neural networks. The techniques can be adapted to Transformerbased models as introduced in \u00a72.3. Unstructured pruning is another major research direction, especially gaining popularity in the theory of Lottery Ticket Hypothesis (Frankle and Carbin, 2019;Zhou et al., 2019;Renda et al., 2020;Chen et al., 2020a). Unstructured pruning produces models with high sparsities (Sanh et al., 2020;Xu et al., 2021;Huang et al., 2021) yet hardly bring actual inference speedups. Developing computing platform for efficient sparse tensor operations is an active research area. DeepSparse 15 is CPU inference engine that leverages unstructured sparsity for speedup. Huang et al. (2021) measure the real inference speedup induced by unstructured pruning on Moffett AI's latest hardware platform ANTOM. We do not directly compare to these methods because the evaluation environments are different. While all the aforementioned methods produce task-specific models through pruning, several works explore upstream pruning where they prune a large pre-trained model with the masked 15 https://github.com/neuralmagic/deepsparse language modeling task. Chen et al. (2020a) show a 70%-sparsity model retains the MLM accuracy produced by iterative magnitude pruning. Zafrir et al. (2021) show the potential advantage of upstream unstructured pruning against downstream pruning. We consider applying CoFi for upstream pruning as a promising future direction to produce task-agnostic models with flexible structures.\n\nBesides pruning, many other techniques have been explored to gain inference speedups for Transformer models, including distillation as introduced in \u00a72.2, quantization (Shen et al., 2020;Fan et al., 2021), dynamic inference acceleration  and matrix decomposition (",
            "score": 0.4985089120241963,
            "section_title": "Related Work",
            "char_start_offset": 22949,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 103,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 103,
                    "end": 120,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 155,
                    "end": 178,
                    "matchedPaperCorpusId": "195657904"
                },
                {
                    "start": 178,
                    "end": 194,
                    "matchedPaperCorpusId": "218537870"
                },
                {
                    "start": 463,
                    "end": 489,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 489,
                    "end": 507,
                    "matchedPaperCorpusId": "145047837"
                },
                {
                    "start": 507,
                    "end": 526,
                    "matchedPaperCorpusId": "212415013"
                },
                {
                    "start": 605,
                    "end": 624,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 624,
                    "end": 640,
                    "matchedPaperCorpusId": "233297003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.433837890625
        },
        {
            "corpus_id": "276107452",
            "title": "Progressive Binarization with Semi-Structured Pruning for LLMs",
            "text": "It also maintains hardware-friendliness while achieving performance levels similar to unstructured pruning. This demonstrates that semi-structured pruning strikes a good trade-off between hardware efficiency and model performance. \n\nAblation for Metric in Fine-Stage Search. The Table 3e presents the performance of different pruning metrics in the fine-stage search. We compare several metrics, including random selection, magnitude-based selection, Wanda, SI, and our Hessian-based metric. The experimental results demonstrate that our metric significantly outperforms both random selection and magnitude-based pruning, achieving superior performance compared to Wanda and SI, thereby highlighting the effectiveness of our approach. \n\nAblation for Number of Split Points. We use a grouping strategy to quantize non-salient weights, where a split point p is used to group the non-salient weights. Table 3f shows the impact of different numbers of split points on performance. From the table, we can see that as the number of split points increases, the model performance improves. However, an increase in the number of split points also leads to higher computational and storage demands. As a result, we choose the number of split points as 2, which strikes a balance between performance and resource requirements. This choice ensures a fair comparison across methods.",
            "score": 0.498094522229273,
            "section_title": "Ablation Study",
            "char_start_offset": 23592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1369
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "266755678",
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "text": "Model pruning involves removing redundant portions from the parameter matrices of large models. It is divided into unstructured pruning and structured pruning. Unstructured pruning involves removing individual connections or weights in a neural network without adhering to any specific structural pattern. In structured pruning, specific structural patterns or units within a neural network are pruned or removed. Gordon et al. [179] compared the effects of unstructured and structured pruning on the BERT model. They found that the effectiveness of unstructured pruning significantly decreases as the pruning ratio increases, while in structured pruning, 30-40% of the weights can be discarded without affecting BERT's universality. Different structures in the model can be structurally pruned. Michel et al. [180] pruned attention heads and found that ablating one head often positively impacts the performance of WMT and BERT. They proposed a gradient-based metric for evaluating the importance of attention heads to enhance pruning effectiveness. Fan et al. [179] performed layer pruning by extending dropout from weights to layers. During training, they randomly dropped layers and achieved good inference results by selecting sub-networks with any desired depth during testing.",
            "score": 0.49756931884988764,
            "section_title": "Model Pruning",
            "char_start_offset": 68369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1283
                }
            ],
            "ref_mentions": [
                {
                    "start": 810,
                    "end": 815,
                    "matchedPaperCorpusId": "166227946"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2509765625
        },
        {
            "corpus_id": "249063170",
            "title": "Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models",
            "text": "We reiterate that we are not proposing that our strategies or models be adopted off the shelf as is. This is especially true because our work does not include rigorous analysis of our compressed models' properties and behaviors outside of task accuracy, resilience to compression, and transferability to other tasks. Detailed study of properties such as long-tail performance, robustness to data distribution shifts, and fairness in performance across demographic groups, for example, which have important real-world implications, is outside of the scope of our current work. However, preliminary evaluations on certain model behaviors and properties suggests that many of our models which achieve high end-task performance are vulnerable to simple perturbations in data and lack basic desirable linguistic capabilities (although not necessarily more so than is \"typical\" of language models (Ribeiro et al., 2020;Xu et al., 2021). Blalock et al., 2020;Sanh et al., 2020;Lagunas et al., 2021), quantization, where model weights, activations, gradients, and/or add-multiply accumulators are reduced in precision from 32-bit floating point representations to floating or fixed-point representations as low as one or two bits (Gray and Neuhoff, 1998;Vanhoucke et al., 2011;Gholami et al., 2021), and knowledge distillation where a smaller model is trained to replicate the predictions, and often intermediate embedded representations, of a larger model (Bucilu\u01ce et al., 2006;Hinton et al., 2014;Sanh et al., 2019;Jiao et al., 2020). Unstructured pruning can achieve some of the highest sparsity levels using various criteria and schedules for determining which parameters to prune (Frankle and Carbin, 2019;Chen et al., 2020;Sanh et al., 2020;Guo et al., 2021), though sparsity patterns resulting from unstructured pruning often do not result in latency reduction on modern accelerator hardware.",
            "score": 0.4973669031611517,
            "section_title": "Ethics Statement",
            "char_start_offset": 30407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 891,
                    "end": 913,
                    "matchedPaperCorpusId": "218551201"
                },
                {
                    "start": 913,
                    "end": 929,
                    "matchedPaperCorpusId": "237433629"
                },
                {
                    "start": 931,
                    "end": 952,
                    "matchedPaperCorpusId": "212628335"
                },
                {
                    "start": 952,
                    "end": 970,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 970,
                    "end": 991,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 1222,
                    "end": 1246,
                    "matchedPaperCorpusId": "119156912"
                },
                {
                    "start": 1246,
                    "end": 1269,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 1449,
                    "end": 1471,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1471,
                    "end": 1491,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1509,
                    "end": 1527,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2646484375
        },
        {
            "corpus_id": "277197048",
            "title": "A survey of model compression techniques: past, present, and future",
            "text": "Model pruning and compression methods involve the removal of non-essential components from over-parameterized deep models. Pruning techniques are primarily categorized into weight pruning and unit pruning, depending on the level of pruning applied. Weight pruning, also referred to as unstructured pruning, compresses the network by eliminating insignificant neural connections, while unit pruning, also known as structured pruning, reduces the model size by removing neurons or convolutional filters. The central concept of neural network pruning is to eliminate parts deemed unimportant, redundant, or unnecessary based on their significance, thus simplifying the model without causing significant degradation in performance. As shown in Figure 4, unstructured weight pruning removes unnecessary, low-weight connections between layers in the The structure of a neural network after pruning. \n\nneural network, while structured unit pruning eliminates all weight connections associated with specific neurons.",
            "score": 0.49707249013866994,
            "section_title": "Pruning methods",
            "char_start_offset": 42960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 1008
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285888671875
        },
        {
            "corpus_id": "256615905",
            "title": "Ten Lessons We Have Learned in the New \"Sparseland\": A Short Handbook for Sparse Neural Network Researchers",
            "text": "What is the difference between unstructured and structured weight pruning? What is the difference between weight pruning and activation pruning? \n\nUnstructured pruning, as the most elementary pruning form, eliminates the least important weights based on the selected criteria, regardless of where they are. Unstructured pruning leads to irregular sparse patterns, which typically only see limited speedups on commodity hardware such as GPUs. On the other hand, structured pruning involves the selective removal of an entire group of weights. The definition of 'group', which makes those amenable to hardware speedup, could refer to weight blocks (Gray et al. 2017;Ding et al. 2017), neurons (Jiang et al. 2018), filters/channels (Li et al. 2016) Activation pruning has lots of overlap with structured weight pruning in terms of their outcomes: completely removing certain intermediate feature outputs. For example, structural pruning by neurons in MLP layers, channels/filters in convolutional layers, or heads in self-attention layers naturally yield activation pruning. However, one shall notice that those are still two separate streams of research and their overlap is only partial because: (1) on one hand, some structured weight pruning methods do not lead to any reduction of output dimension. For example, applying N:M sparsity to convolutional layers will change neither the output channel number nor the feature map size; (2) On the other hand, some activation pruning methods do not come from removing weight: a few methods sparsify activations after computing them fully (with full unpruned weights) (Ardakani et al. 2017;Chakrabarti & Moseley 2019;Jiang et al. 2022b), and that often happens when saving memory, instead of computation or latency, is the main focus. \n\n3.3. Is structured pruning just channel pruning? Is channel pruning the only \"practical meaningful\" sparse structure on hardware?",
            "score": 0.4969660036515384,
            "section_title": "3.2.",
            "char_start_offset": 8176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 144
                },
                {
                    "start": 147,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 664,
                    "end": 681,
                    "matchedPaperCorpusId": "2084885"
                },
                {
                    "start": 691,
                    "end": 710,
                    "matchedPaperCorpusId": "51608226"
                },
                {
                    "start": 729,
                    "end": 745,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 1612,
                    "end": 1634,
                    "matchedPaperCorpusId": "3794142"
                },
                {
                    "start": 1634,
                    "end": 1661,
                    "matchedPaperCorpusId": "59158824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265625
        },
        {
            "corpus_id": "260900101",
            "title": "A Survey on Model Compression for Large Language Models",
            "text": "Pruning (LeCun et al., 1989) is a powerful technique to reduce the size or complexity of a model by removing redundant components. Pruning can be divided into Unstructured Pruning, Semi-Structured Pruning, and Structured Pruning. Structured pruning removes entire components like neurons, attention heads, or layers based on specific rules while preserving the overall network structure. On the other hand, unstructured pruning prunes individual parameters, resulting in an irregular sparse structure. Semi-structured pruning is a method that lies between structured pruning and unstructured pruning, capable of achieving fine-grained pruning and structural regularization simultaneously. It prunes partial parameters based on specific patterns rather than entire channels, filters, or neurons, making it a fine-grained form of structured pruning. Table 2 shows the performance of many representative LLM pruning methods.",
            "score": 0.49688444431219214,
            "section_title": "Pruning",
            "char_start_offset": 16983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 921
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 27,
                    "matchedPaperCorpusId": "7785881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "272550518",
            "title": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning",
            "text": "and \u03d5 < 1 is pruning ratio. While this was acceptable in an early MoE work with few experts, it does not scale to recent trends in MoEs with large n (Bai et al., 2023;Dai et al., 2024;Snowflake, 2024), or even infinity (He, 2024). \n\nOur distinction is drastically reducing the complexity of expert pruning to O(1), without compromising the performance. The main intuition is leveraging a latent structure between experts, based on behavior similarity, such that the greedy decision of whether to prune closely captures the joint pruning effect. \n\nOur contributions can be summarized as follows: \n\n\u2022 We propose STUN, the first counterintuitive solution combining structured and unstructured pruning to break the pruning ratio limits of unstructured pruning, to the best of our knowledge. \n\n\u2022 We show that a well-designed expert pruning would retain the performance after the pruning, and the pruned network remains robust to unstructured pruning. \n\n\u2022 To materialize, we design O(1) expert-level pruning method, which outperforms the pre- (Lu et al., 2024). \n\n\u2022 Our structured pruning, when followed by unstructured pruning, jointly optimizes pruning to achieve a state-of-the-art compression ratio. For Snowflake Arctic, a 480B-sized MoE with 128 experts, STUN needs only 1 H100 and two hours, achieving no loss up to 40% of sparsity, even for generative tasks such as GSM8K, where state-of-the-art unstructured pruning methods fail. \n\n\u2022 Code will be publicly available. \n\n2 Related Work",
            "score": 0.4967994447692081,
            "section_title": "Introduction",
            "char_start_offset": 3666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1055
                },
                {
                    "start": 1058,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1432
                },
                {
                    "start": 1435,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1486
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 184,
                    "matchedPaperCorpusId": "266933338"
                },
                {
                    "start": 1037,
                    "end": 1054,
                    "matchedPaperCorpusId": "263829692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.350830078125
        },
        {
            "corpus_id": "273345395",
            "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
            "text": "The advent of large language models (LLMs) such as GPT-4 (OpenAI et al., 2024), Gemini (Gemini et al., 2024), and Llama 3 (Dubey et al., 2024) has revolutionized natural language processing (NLP), driving significant advancements across various tasks through extensive pre-training on textual data. These models, enhanced by supervised fine-tuning (SFT), demonstrate impressive instruction-following abilities (Ouyang et al., 2022;Touvron et al., 2023a), but come with high compute costs for both training and inference (Kaplan et al., 2020;Hoffmann et al., 2022). To address diverse deployment requirements across varying model scales, sizes, and compute budgets, compressing models for efficient inference is essential, particularly given the significant time, data, and resource constraints associated with training multiple multi-billion parameter models from scratch. \n\nMost model compression techniques can be grouped into four main categories: knowledge distillation (KD) (Hinton et al., 2015), factorization (Hu et al., 2022), pruning (Le-Cun et al., 1989), and quantization (Han et al., 2015). In our work, we focus on pruning, though we aim for our method to inspire further developments across these other compression methods. Structured pruning, which selectively removes less critical components of a neural network, has emerged as a promising method for improving LLM efficiency (Ma et al., 2023). This method has gained attention for its ability to reduce memory and compute requirements, making inference more efficient. Recent works have shown that LLMs exhibit significant redundancy, particularly in the middle layers, where removing these layers has a minimal impact on overall model quality (Men et al., 2024;Gromov et al., 2024). The residual stream of the Transformer (Vaswani et al., 2017) architecture is only slightly modified by the output of non-essential layers, enabling the removal of these layers without drastically harming model quality. \n\nDespite its potential advantages, depth-wise structured pruning presents inherent challenges.",
            "score": 0.4965184080365143,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1971
                },
                {
                    "start": 1974,
                    "end": 2067
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.564453125
        },
        {
            "corpus_id": "248693442",
            "title": "Revisiting Random Channel Pruning for Neural Network Compression",
            "text": "Pruning. \n\nIn the this section, we provide the justification of the statement in the main paper \"the performance of the channel pruned network is upper bounded by the original network\". \n\nIn the paper \"The Lottery Ticket Hypothesis\", the authors showed that some pruned networks could learn faster while reaching higher test accuracy and generalizing better than the original one [12]. Yet, the conclusion is derived for unstructured pruning. The problems of unstructured pruning and structured pruning are quite different. Unstructured pruning removes single connections in a CNN and results in irregular kernels. And it is possible that the number of kernels in the resultant sparse network is the same as the original network. The capacity of a network could be         fully utilized by the sparse network. This is why unstructured pruning could easily lead to an extremely pruned network without accuracy drop while for structured pruning researchers struggle with the trade-off between accuracy drop and compression ratio. Without expanding the search space (i.e. changing the position of pooling layers [41], widening the network [34,58,60]), it is very difficult to find a pruned network with better performance. Thus, we can safely conclude that the performance of channel pruned networks is upper bounded by the original networks.",
            "score": 0.4963292323773379,
            "section_title": "B. Upper Bounded Performance of Channel",
            "char_start_offset": 34850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 11,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1340
                }
            ],
            "ref_mentions": [
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "85498737"
                },
                {
                    "start": 1137,
                    "end": 1141,
                    "matchedPaperCorpusId": "235185353"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2197265625
        },
        {
            "corpus_id": "271064490",
            "title": "Composable Interventions for Language Models",
            "text": "We use four state-of-the-art compression methods including two pruning methods: \n\n\u2022 SparseGPT (Frantar & Alistarh, 2023): an efficient one-shot pruning method tailored for large models. It converts the pruning process into solving large-scale sparse regression problems using an approximate solver. This approach enables rapid pruning on a single GPU with minimal accuracy loss, achieving 50-60% on large models. \n\n\u2022 Wanda (Sun et al., 2023): is another popular method for pruning large language models that relies on a pruning metric that combines a weight's magnitude with the norm of its corresponding input activations, determined from a small calibration dataset. The method focuses on selectively pruning weights within individual outputs of linear layers, aiming for high sparsity levels without modifying unpruned weights. Wanda is computationally efficient, executable in a single forward pass. \n\nand two quantization methods: \n\n\u2022 GPTQ (Frantar et al., 2023): an algorithm designed for efficient weight quantization in large-scale models. It revises the weight quantization approach by quantizing weights in a fixed order rather than a greedy order, which shows minimal performance difference, especially in larger models. GPTQ introduced a novel method where each weight is quantized column-by-column, reducing computational complexity. \n\n\u2022 AWQ (Lin et al., 2023): is based on the premise that not all weights are equally critical for model performance, and it identifies a small fraction of salient weights whose quantization significantly impacts model accuracy. This identification is done by analyzing activation distributions rather than weight distributions, under the rationale that weights linked to larger activation magnitudes are more crucial.",
            "score": 0.4958986243443343,
            "section_title": "C.3.2 COMPRESSOR DETAILS",
            "char_start_offset": 35862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 82,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1764
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.389892578125
        },
        {
            "corpus_id": "270285715",
            "title": "Decay Pruning Method: Smooth Pruning With a Self-Rectifying Procedure",
            "text": "Deep Neural Networks (DNNs) have been widely used for various applications, such as image classification [22; 40], object segmentation [33; 35], and object detection [6; 43]. However, the increasing size and complexity of DNNs often result in substantial computational and memory requirements, posing challenges for deployment on resource-constrained platforms, such as mobile or embedded devices. Consequently, developing efficient methods to reduce the computational complexity and storage demands of large models, while minimizing performance degradation, has become essential. \n\nNetwork pruning is one of the most popular methods in model compression. Specifically, current network pruning methods are categorized into unstructured and structured pruning [5]. Unstructured pruning [11; 24] focuses on eliminating individual weights from a network to create fine-grained sparsity. Although these approaches achieve an excellent balance between model size reduction and accuracy retention, they often require specific hardware support for acceleration, which is impractical for general-purpose computing environments. Conversely, structured pruning [23; 18; 29] avoids these hardware dependencies by eliminating redundant network structures, thus introducing a more manageable and hardware-compatible form of sparsity. As a result, structured pruning has become popular and is extensively utilized.",
            "score": 0.4958687431106881,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1400
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60693359375
        },
        {
            "corpus_id": "256188839",
            "title": "A 1.6-mW Sparse Deep Learning Accelerator for Speech Separation",
            "text": "To enable low power and real-time execution of speech separation at the edge devices, model compression is first applied to reduce model size and complexity. This paper adopts mixedlevel model pruning that uses structured sensitivity pruning and unstructured pruning to reduce the size of the model and the complexity of the computation. The pruned model is further quantized from the 32-bit floating-point format to the shifted 8-bit floating-point format with little performance degradation. In addition, to avoid computing with zero values in the dilated and transposed convolutions, we apply a 1-D decomposition scheme to these convolutions to remove the computations with zero weights and zero activations, respectively. \n\nC. Pruning 1) Structured sensitive pruning: Fig. 3 shows the initial results of the structured pruning by shrinking the layer and channel numbers. This figure shows that the layer pruning in the encoder and decoder networks is more sensitive than that in the separator network due to its regression computation nature. In contrast, the number of layers and channels in the separator network can tolerate a higher pruning ratio due to its classification nature. Thus, we apply the channel pruning in the encoder and decoder and both layer and channel pruning in the separator network, which is denoted as structured sensitive pruning. \n\nThe proposed structured sensitivity pruning flow is described below. To simplify the pruning process, we adopt the predefined structured pruning technique [20] that directly shrinks the model structure to the target size and trains from scratch. In this paper, based on the network sensitivity, we gradually shrink the channel width of the encoder and decoder, and remove the layer and shrink channel width of the separator until the target size and accuracy constraints are met. The pruned network is also the input of the unstructured pruning algorithm. \n\n2) Unstructured pruning: After the structured sensitivity pruning, we further reduce the weight size by the unstructured pruning. The unstructured pruning flow follows the approach in [13] and is described below.",
            "score": 0.495846197053626,
            "section_title": "B. Overview of the proposed approach",
            "char_start_offset": 7160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1919
                },
                {
                    "start": 1922,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 1519,
                    "end": 1523,
                    "matchedPaperCorpusId": "52978527"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.296142578125
        },
        {
            "corpus_id": "268732932",
            "title": "Structurally Prune Anything: Any Architecture, Any Framework, Any Time",
            "text": "The increasing complexity and scale of deep learning models He et al. (2015); Simonyan & Zisserman (2015); Dosovitskiy et al. (2020) have sparked significant research interest in compression methods.Compression methods, like pruning, aim to reduce model size and computational cost in order to increase inference speed, save energy, and enable deployment on computationally limited devices.In particular, pruning methods mostly fall into two main categories: unstructured pruning which involves setting specific parameters to zero while maintaining the overall network structure LeCun et al. (1989); Hassibi & Stork (1992); Dong et al. (2017); Han et al. (2015); Lee et al. (2019); Frantar et al. (2022); Xiao et al. (2019), and structured pruning which involves removing entire channels Li et al. (2016); He et al. (2018bHe et al. ( , 2017)); Lin et al. (2020); Liu et al. (2017a); Rachwan et al. (2022).While structured pruning advantageously results in direct computational and memory reduction, it is Figure 1: SPA overview.The source model can be chosen freely from different frameworks with different structures, either trained or not.A computational graph is built to store the dependency information between operators and data.The pruning procedure consists of four steps: coupling channels, grouping channels & importance estimation, and pruning.After pruning, the pruned model can be converted to other frameworks for further usage.considered a more complex undertaking.Specifically, structured pruning methods often come with three main challenges.\n\nChallenge 1: The first major challenge consists of the difficulty of applying different structured pruning methods to various model architectures.Indeed, structured pruning entails managing the interdependencies between coupled channels in different layers to modify the model structure without breaking the model connectivity (e.g.see residual connection in Fig. 5).Hence, when dealing with coupled channels, most of the existing approaches heavily rely on case-by-case analysis of different model architectures.",
            "score": 0.49582571687696264,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1028
                },
                {
                    "start": 1028,
                    "end": 1141
                },
                {
                    "start": 1141,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1355
                },
                {
                    "start": 1355,
                    "end": 1442
                },
                {
                    "start": 1442,
                    "end": 1480
                },
                {
                    "start": 1480,
                    "end": 1559
                },
                {
                    "start": 1561,
                    "end": 1707
                },
                {
                    "start": 1707,
                    "end": 1893
                },
                {
                    "start": 1893,
                    "end": 1928
                },
                {
                    "start": 1928,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 76,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 78,
                    "end": 105,
                    "matchedPaperCorpusId": "14124313"
                },
                {
                    "start": 579,
                    "end": 598,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 600,
                    "end": 622,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 624,
                    "end": 642,
                    "matchedPaperCorpusId": "5750817"
                },
                {
                    "start": 644,
                    "end": 661,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 682,
                    "end": 703,
                    "matchedPaperCorpusId": "251765570"
                },
                {
                    "start": 705,
                    "end": 723,
                    "matchedPaperCorpusId": "202787073"
                },
                {
                    "start": 806,
                    "end": 822,
                    "matchedPaperCorpusId": "102350938"
                },
                {
                    "start": 822,
                    "end": 842,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 844,
                    "end": 861,
                    "matchedPaperCorpusId": "211258761"
                },
                {
                    "start": 863,
                    "end": 881,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 883,
                    "end": 904,
                    "matchedPaperCorpusId": "249890244"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.316162109375
        },
        {
            "corpus_id": "273501976",
            "title": "Pruning Foundation Models for High Accuracy without Retraining",
            "text": "Despite the superior performance, it is challenging to deploy foundation models or large language models (LLMs) due to their massive parameters and computations. While pruning is a promising technique to reduce model size and accelerate the inference, the traditional pruning techniques can hardly be applied for LLMs as they need to finetune the model on the full dataset with multiple epochs consuming massive data and hardware resources. To deal with this problem, post-training pruning methods are proposed to prune LLMs in one-shot without retraining. However, their accuracy after pruning may suffer from certain performance degradation due to the lack of retraining with massive data. To address this issue, in this paper, we first formulate the post-training problem for layer-wise LLM compression to simultaneously prune multiple weights in LLMs. Next, we provide an optimal solution for this problem and design our post-training pruning algorithm for both unstructured and semi-structured sparsity. Our extensive experiments demonstrate the superior performance of the proposed methods in comparison to SOTA baselines across various LLM families including transformer-based LLMs and Mamba-based LLMs. Code link: https://github.com/piuzha/APT",
            "score": 0.49581850876534284,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.389892578125
        },
        {
            "corpus_id": "261395559",
            "title": "SP\u00b3: Enhancing Structured Pruning via PCA Projection",
            "text": "Distillation. Knowledge distillation (Hinton et al., 2015;Hou et al., 2020;Jiao et al., 2020;Sanh et al., 2019) is a model compression approach that trans-fers knowledge from a larger teacher model to a smaller student model. In contrast to the distillation methods, our approach uses less computational cost while achieving the same performance. \n\nPruning. Existing pruning methods can be broadly divided into two categories: unstructured and structured. Unstructured pruning (Gale et al., 2019;Frankle and Carbin, 2018;Kurtic et al., 2022;Louizos et al., 2018;Sanh et al., 2020) aims to remove unimportant scalar values from the model's parameters. In contrast, structured pruning (Lin et al., 2020;Lagunas et al., 2021;Wang et al., 2020;Xia et al., 2022) are proposed to remove weight blocks in PLMs, including the entire layer (Fan et al., 2019;Sajjad et al., 2023Sajjad et al., , 2020)), attention heads (Michel et al., 2019;Voita et al., 2019), andfilters (McCarley et al., 2019;Prasanna et al., 2020). Unlike the previous structured pruning, we focus on exploring how to compress the hidden dimensions of the model efficiently. \n\nLow-Rank Factorization (LRF). LRF methods compress the PLMs by decomposing the weight matrices (Hua et al., 2022;Xiao et al., 2023) based on low-rank property. In contrast, we propose PCA Projection based on the low-rank property of the features. For more discussions refer to Appendix I. \n\nRe-parameterization (Re-p). Re-p methods have been proposed to improve existing model compression methods (Ding et al., 2021;Lin et al., 2021;Wu et al., 2023b). Its core idea is to represent the compressed model based on the original model.",
            "score": 0.4957259050034718,
            "section_title": "Related Work",
            "char_start_offset": 20602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1668
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 75,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 75,
                    "end": 93,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 521,
                    "end": 541,
                    "matchedPaperCorpusId": "247446572"
                },
                {
                    "start": 541,
                    "end": 562,
                    "matchedPaperCorpusId": "30535508"
                },
                {
                    "start": 562,
                    "end": 580,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 683,
                    "end": 701,
                    "matchedPaperCorpusId": "222134166"
                },
                {
                    "start": 701,
                    "end": 722,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 722,
                    "end": 740,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 740,
                    "end": 757,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 831,
                    "end": 849,
                    "matchedPaperCorpusId": "202750230"
                },
                {
                    "start": 849,
                    "end": 868,
                    "matchedPaperCorpusId": "251005814"
                },
                {
                    "start": 930,
                    "end": 954,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 1232,
                    "end": 1250,
                    "matchedPaperCorpusId": "253581300"
                },
                {
                    "start": 1250,
                    "end": 1268,
                    "matchedPaperCorpusId": "258959444"
                },
                {
                    "start": 1534,
                    "end": 1553,
                    "matchedPaperCorpusId": "237108363"
                },
                {
                    "start": 1553,
                    "end": 1570,
                    "matchedPaperCorpusId": "221818663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33447265625
        },
        {
            "corpus_id": "246706034",
            "title": "Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets",
            "text": "Pruning. Network pruning is a technique that aims at eliminating the unnecessary model parameters (Blalock et al., 2020), which can effectively shrink models for the deployment on resource-constrained devices (LeCun et al., 1990;Hanson & Pratt, 1988). Pruning algorithms are roughly categorized into two groups: (1) unstructured pruning (LeCun et al., 1990;Han et al., 2015a;b;Ren et al., 2018;Zhang et al., 2018) with irregular sparse patterns; (2) structural pruning (He et al., 2017;Liu et al., 2017;Li et al., 2016;Hu et al., 2016;Wen et al., 2016;Hong et al., 2018) with structural sparse patterns such as layer-wise, channel-wise, block-wise, column-wise, etc.. Within the group of unstructured pruning methods, Han et al. (2015a;b) remove insignificant connections of models in the post-training stage, with respect to certain heuristics like weight/gradient magnitudes; during training sparsification is also another popular trend for pruning by leveraging 0 regularization (Louizos et al., 2017) or alternating direction method of multipliers (ADMM) (Ren et al., 2018;Zhang et al., 2018). Recently, several pruning-atinitialization methods (Wang et al., 2020;Lee et al., 2019b;Tanaka et al., 2020) are proposed to identify critical unstructured connections for gradient-flow preserving, without any training. Although the unstructured sparse model has superior performance, it usually suffers from poor data locality and low parallelism (He et al., 2017;Mao et al., 2017;Wen et al., 2016), which make it hard to be accelerated in real-world hardware platforms. \n\nOn the contrary, structural pruning is more hardwarefriendly at the cost of notable accuracy loss when the compression ratio increases.",
            "score": 0.49570521624491926,
            "section_title": "Related Work",
            "char_start_offset": 5839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1707
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 229,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 229,
                    "end": 250,
                    "matchedPaperCorpusId": "9344018"
                },
                {
                    "start": 337,
                    "end": 357,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 394,
                    "end": 413,
                    "matchedPaperCorpusId": "4752389"
                },
                {
                    "start": 469,
                    "end": 486,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 486,
                    "end": 503,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 535,
                    "end": 552,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 1077,
                    "end": 1096,
                    "matchedPaperCorpusId": "4752389"
                },
                {
                    "start": 1149,
                    "end": 1168,
                    "matchedPaperCorpusId": "211146532"
                },
                {
                    "start": 1168,
                    "end": 1186,
                    "matchedPaperCorpusId": "52920837"
                },
                {
                    "start": 1186,
                    "end": 1206,
                    "matchedPaperCorpusId": "219558821"
                },
                {
                    "start": 1446,
                    "end": 1463,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 1480,
                    "end": 1497,
                    "matchedPaperCorpusId": "2056019"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "259858812",
            "title": "Structured Pruning for Efficient Generative Pre-trained Language Models",
            "text": "Large-scale generative pretrained language models (PLMs) (Radford and Narasimhan, 2018;Brown et al., 2020;Lewis et al., 2020;Raffel et al., 2020) show remarkable performance on various tasks. However, their increasing sizes also lead to expensive memory and computation, hindering their deployment in real applications. \n\nRecent attempts (Tao et al., 2022;Frantar et al., 2022;Dettmers et al., 2022;Xiao et al., 2022;Wang et al., 2021) propose to compress generative PLMs models by quantization. However, hardware-dependent low-bit kernels need to be specially developed for real inference speedup. Com-pared to quantization, structured pruning methods prune parts of the model structures without requiring designing extra operators to achieve inference speedup and run-time memory saving. Recently, Anonymous (2023) show that the feed-forward networks (FFNs) of GPT models can be pruned to smaller widths, and Li et al. (2022) compress the BART models by combining layer pruning and model quantization for a higher compression rate. However, these models consider only limited components for pruning, i.e., the FFNs or Transformer layers, which can be restrictive for various deployment requirements. \n\nIn this work, we propose a new structured pruning framework named SIMPLE (Sparsity-Induced Mask learning for Pruning generative pre-trained Language modEls), which offers a wider range of compressible components. Aside from the attention heads and the width of FFNs commonly used for structured pruning of discriminative PLMs, we also propose to prune the hidden dimension, to further push the trade-off between performance and model size. It is motivated by the observation that persistent outliers exist in the hidden dimension of both decoder-only and encoder-decoder generative PLMs. The observation implies that the hidden dimension may be slimmed sharply with a slight performance drop.",
            "score": 0.4956540846134376,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 106,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 106,
                    "end": 125,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 125,
                    "end": 145,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 377,
                    "end": 399,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 800,
                    "end": 816,
                    "matchedPaperCorpusId": "256662734"
                },
                {
                    "start": 911,
                    "end": 927,
                    "matchedPaperCorpusId": "247596810"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77880859375
        },
        {
            "corpus_id": "271217883",
            "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models",
            "text": "Structured/Unstructured/Semi-structured LLM pruning.\n\nThe pruning methods for LLMs can still be generally cat-egorized as unstructured ([Sun et al., 2024;Frantar et al., 2022]), semi-structured ( [Frantar and Alistarh, 2023]), and structured ( [Ma et al., 2023;Wang et al., 2020b]) pruning methods, similar to the categorization for pruning small and mid-size neural networks.Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models.However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware.In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]).Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup.In this paper, we focus on structured pruning for LLMs.\n\nPruning criteria for LLMs.Neural network pruning methods search for an optimal subnetwork by removing unimportant weights.As one of the most popular criterion factors, gradients have already been demonstrated effective in constructing scoring functions for pruning small or medium-size networks [Liu et al., 2021;Fu et al., 2022;Wang et al., 2020a;Yu et al., 2022;Molchanov et al., 2019;Kwon et al., 2022].However, calculating gradients using backpropagation is highly resource-intensive for GPU memory, making it challenging to implement for LLMs, where meeting such high memory demands is difficult.",
            "score": 0.49564616656112404,
            "section_title": "Related Work",
            "char_start_offset": 5188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 54,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1146
                },
                {
                    "start": 1146,
                    "end": 1201
                },
                {
                    "start": 1203,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1325
                },
                {
                    "start": 1325,
                    "end": 1609
                },
                {
                    "start": 1609,
                    "end": 1804
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 175,
                    "matchedPaperCorpusId": "249282255"
                },
                {
                    "start": 196,
                    "end": 224,
                    "matchedPaperCorpusId": "249282255"
                },
                {
                    "start": 244,
                    "end": 261,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 261,
                    "end": 280,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 952,
                    "end": 972,
                    "matchedPaperCorpusId": "247318543"
                },
                {
                    "start": 972,
                    "end": 999,
                    "matchedPaperCorpusId": "249282255"
                },
                {
                    "start": 1056,
                    "end": 1084,
                    "matchedPaperCorpusId": "249282255"
                },
                {
                    "start": 1498,
                    "end": 1516,
                    "matchedPaperCorpusId": "235825363"
                },
                {
                    "start": 1516,
                    "end": 1532,
                    "matchedPaperCorpusId": "249282255"
                },
                {
                    "start": 1532,
                    "end": 1551,
                    "matchedPaperCorpusId": "5358338"
                },
                {
                    "start": 1551,
                    "end": 1567,
                    "matchedPaperCorpusId": "247318543"
                },
                {
                    "start": 1567,
                    "end": 1590,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 1590,
                    "end": 1608,
                    "matchedPaperCorpusId": "247446572"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7734375
        },
        {
            "corpus_id": "269318104",
            "title": "Edge Federated Optimization for Heterogeneous Data",
            "text": "Structured pruning is a technique employed to prune structural elements of a model, such as neurons or convolutional kernels within a neural network. This pruning method typically adheres to the structure and topology of the model to ensure that the connectivity and hierarchical structure of the network remain unchanged. The structured pruning used in this work involves setting the weights of certain neurons to zero, thereby disabling the associated connections. On the other hand, unstructured pruning involves pruning individual weights within the model, regardless of their location or structure. In this scenario, pruning is typically achieved by setting weights with smaller values to zero. This may result in some weights within the model being entirely eliminated, thereby reducing the model's size. Unstructured pruning is often used to further reduce the model's storage requirements and computational costs, but it may require sophisticated techniques to minimize its impact on model performance. In this work, we utilized the sparse optimization framework provided by DessiLB [14] to transform the original unstructured architecture into a structured one and make it iteratively adapt to our model. \n\nFigure 2a shows the initial network. The unstructured (weight) pruning (Figure 2b) and structured (filter) pruning (Figure 2c) approaches are techniques for reducing the size of deep learning models, improving efficiency, and mitigating overfitting. Structured pruning cuts out full filters and kernel rows, leading to fewer intermediate feature maps, unlike just pruning connections [49]. The primary distinction between them lies in their focus on pruning different types of elements within the model (Figure 3). In [10], it is mentioned that conventional training is applied to specific parameters (weak nodes), while non-specific parameters (strong nodes) are not subjected to training. Similar to iterative structured pruning, pruning is applied to strong nodes while iteratively training weak nodes. \n\nStructured pruning is a technique employed to prune structural elements of a model, such as neurons or convolutional kernels within a neural network. This pruning method typically adheres to the structure and topology of the model to ensure that the connectivity and hierarchical structure of the network remain unchanged.",
            "score": 0.49469531967389024,
            "section_title": "Pruning",
            "char_start_offset": 25095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1213
                },
                {
                    "start": 1216,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2346
                }
            ],
            "ref_mentions": [
                {
                    "start": 1091,
                    "end": 1095,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "3586416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.232177734375
        },
        {
            "corpus_id": "268692262",
            "title": "The Need for Speed: Pruning Transformers with One Recipe",
            "text": "Performance on NLP Benchmarks In Tab. 9 we investigate the OPTIN Framework for compressing language models on the GLUE dataset using BERT BASE .Measuring performance and throughput speeds, we show a relatively low average decline in baseline accuracy (\u2264 2%) at a 40% FLOPS compression rate.Similarly, we benchmark our performance with a leading one-shot SoTA method: Post-Training-Pruning-Framework (PTF) (Kwon et al., 2022) at the same compression rate and show superior performance.In particular, we compare with the mask search results from PTF, as subsequent phases in their method could be stacked on other post-training pruning methods (refer to Appendix A.7 for exetended comparisons).We demonstrate robustness over various compression ratios in Fig. 2 where we benchmark OPTIN against pipelines that incorporate re-training, including CoFi (Xia et al., 2022), DynaBert (Hou et al., 2020), SLIP (Lin et al., 2020b), EBERT (Liu et al., 2021b) et al., 2019) benchmark (refer to A.7 for additional results).The relative FLOP constraint is set to 60% for a fair comparison.\n\nFigure 2: Natural Language FLOPs vs Accuracy.We directly benchmark the OPTIN Framework against leading state-of-the-art methods in natural language model compression.Due to the numerous different baselines reported in each work, we plot the relative performance drops for each method.\n\nOn the right, we compare the performance gap with latency showing that with an average drop of \u2264 1.75% we can achieve a 1.25\u00d7 speedup in throughput purely from static model size reduction.",
            "score": 0.4944305960731642,
            "section_title": "LANGUAGE EXPERIMENTS",
            "char_start_offset": 18496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 290
                },
                {
                    "start": 290,
                    "end": 484
                },
                {
                    "start": 484,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 1011
                },
                {
                    "start": 1011,
                    "end": 1076
                },
                {
                    "start": 1078,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1244
                },
                {
                    "start": 1244,
                    "end": 1362
                },
                {
                    "start": 1364,
                    "end": 1552
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 424,
                    "matchedPaperCorpusId": "248266822"
                },
                {
                    "start": 877,
                    "end": 895,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 902,
                    "end": 921,
                    "matchedPaperCorpusId": "222134166"
                },
                {
                    "start": 929,
                    "end": 948,
                    "matchedPaperCorpusId": "236477807"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11968994140625
        },
        {
            "corpus_id": "271051500",
            "title": "Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional Activations",
            "text": "The evaluation results of pruned models on heldout benchmarks are listed in Table 3  models.We interpret that TransAct better preserved the world knowledge of the original LLM, which is much harder than preserving language modeling and commonsense reasoning capabilities.At 80% compression, TransAct-1.3B achieves 78.0% performance of LLaMA2-7B on average, addressing the effectiveness of TransAct at highly compressed settings.Whereas LLM-Pruner fails at most few-shot tasks.Thereby, we address the inapplicability of structured pruning with the Taylor expansion-based metric.LLMs are fundamentally pre-trained on a large corpus to obtain world knowledge.However, the Taylor expansion-based metric, which guides the pruning by minimizing the approximated language modeling loss on a small calibration set, fails to preserve knowledge and degrade the pruned LLM.Amplifying the calibration set by a significant order of magnitude is an intuitive solution.However, the computation of Jacobian and Hessian matrices of LLM weights on a large calibration set is enormous.\n\nNotably, the reproduced LLM-Pruner-2.6Bwith iterative pruning reaches 83.6% performance of the uncompressed LLaMA2-7B.Whereas in its original paper, the performance at 50% compression ratio can barely reach 78% of the original model (Ma et al., 2023b).The results strengthen the necessity of iterative pruning at LLM structured pruning.Specifically, iterative pruning is gradual and conservative at each step, lessening the approximation error of pruning metrics.",
            "score": 0.4941895684630491,
            "section_title": "Performance Metrics",
            "char_start_offset": 16923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 92,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 428
                },
                {
                    "start": 428,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1066
                },
                {
                    "start": 1068,
                    "end": 1107
                },
                {
                    "start": 1107,
                    "end": 1186
                },
                {
                    "start": 1186,
                    "end": 1320
                },
                {
                    "start": 1320,
                    "end": 1404
                },
                {
                    "start": 1404,
                    "end": 1531
                }
            ],
            "ref_mentions": [
                {
                    "start": 1301,
                    "end": 1319,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.403076171875
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "SparseGPT (Frantar & Alistarh, 2023) is the first post-training quantization method that performs unstructured pruning using an approximated Hessian matrix.Wanda (Sun et al., 2024) further simplifies the Hessian matrix by using just the weight and l 2 norm of activation.GBLM-Pruner (Das et al., 2023) further introduces the gradient to boost the performance.Plug-and-play (Zhang et al., 2024b) The paper presents a plug-and-play post-training pruning method for large language models (LLMs) that introduces two innovative components: Relative Importance and Activations (RIA), a new pruning metric, and Channel Permutation, a technique to maximize the preservation of important weights under N:M sparsity constraints.The proposed method, named plug-and-play, outperforms existing pruning techniques and achieves practical speed-up on specific hardware without the need for additional fine-tuning or retraining.PERP (Zimmer et al., 2023) uses Low-rank adaptation to mitigate the expense of the retraining process in the original prune-retrain paradigm.NutePrune (Li et al., 2024b) combines structure pruning with progressive knowledge distillation by utilizing the unpruned model as a teacher and the pruned model as a student.OWL (Yin et al., 2024) proposed outliers metric to re-assign the sparsity of different layers.BESA (Xu et al., 2024) proposes to use parameter-efficient sparsity learning to learn the sparsity ratio in a differentiable manner.GRAIN (Yang et al., 2022) utilizes gradient information to prune intra-attention structures, Find the optimal proxy that can measure the vit-based architectures\n\nFind the optimal metric that can better rank candidate bitwidth configurations\n\nFind the optimal symbolic pruning metric that can measure the importance of different weights Table 9. Impact of Pruning Metrics on GBLM-Pruner Performance.",
            "score": 0.4941767769549106,
            "section_title": "A. Related Work",
            "char_start_offset": 35644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1052
                },
                {
                    "start": 1052,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1321
                },
                {
                    "start": 1321,
                    "end": 1453
                },
                {
                    "start": 1453,
                    "end": 1613
                },
                {
                    "start": 1615,
                    "end": 1693
                },
                {
                    "start": 1695,
                    "end": 1851
                }
            ],
            "ref_mentions": [
                {
                    "start": 1231,
                    "end": 1249,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 1326,
                    "end": 1343,
                    "matchedPaperCorpusId": "268032346"
                },
                {
                    "start": 1459,
                    "end": 1478,
                    "matchedPaperCorpusId": "254685796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47509765625
        },
        {
            "corpus_id": "272367189",
            "title": "Hyper-Compression: Model Compression via Hyperfunction",
            "text": "Model compression Zhu et al. (2023) plays a crucial role in the widespread deployment of deep learning models in a myriad of different settings. There are mainly four different classes of techniques including pruning, quantization, low-rank decomposition, and knowledge distillation. As Table 1 shows, we highlight that hyper-compression is a fundamentally different method that performs the low-dimensional transformation for the target data. Such an essential difference can open a lot of doors for research and translation opportunities. We summarize model compression techniques below, accompanied by the recent advances in compressing large language models. Due to the limit of pages, we can only cover a few representative works. Moreover, because our proposed hyper-compression here focuses on post-training compression, and knowledge distillation is to a large extent based on training, we do not review articles on knowledge distillation here. Pruning is a method to achieve sparsity in neural networks, which involves removing unimportant synapses, neurons, layers, and even blocks from a neural network. Therefore, a good amount of the pruning research is dedicated to design different evaluation metrics to find which part of a network is unimportant. Slight pruning can also lead to better generalization in addition to compression, while heavy pruning often needs meticulous retraining to avoid high performance loss. Pruning is divided into the unstructured and structured. Unstructured pruning usually results in an irregular model composition. To truly harvest gains in inference time and parameter saving, users need specialized storage and computation schemes. Unstructured pruning targets individual parameters, without the need of considering the internal structures of a model. SparseGPT Frantar and Alistarh (2023) turned the pruning problem into a set of extremely large-scale instances of sparse regression, thereby avoiding the inversion of each matrix. Thus, SparseGPT could compress models of 10-100 billion parameters in just a few hours. Wanda Sun et al. (2023) simultaneously considers weights and activations in pruning. This technique was motivated by an observation in Dettmers et al. ( 2022) that a small subset of hidden features are substantially large in magnitude.",
            "score": 0.49375083903356903,
            "section_title": "Model Compression",
            "char_start_offset": 6516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 2074,
                    "end": 2091,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "261705612",
            "title": "Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity",
            "text": "Pruning methods can be usually categorized according to their granularity [23] or to their importance policy. In terms of granularity, pruning can usually operate with unstructured or structured sparsity patterns. Unstructured pruning involves removing single connections in the network based on their importance [20,43]. Targeting individual weights offers flexibility in achieving high accuracy but may lead to challenges in efficient inference due to irregular memory access patterns. A custom runtime with specialized sparse kernels is often necessary to achieve speedup in case of unstructured sparsity (e.g., DeepSparse [27]). Conversely, structured pruning [35,45] involves the removal of entire channels or filters from the network, which can pose challenges during model training due to its more substantial impact on accuracy. However, pruning at this level of granularity can significantly enhance model efficiency in many existing runtimes, resulting in notable reductions in storage requirements and accelerated inference latency. \n\nPruning policies encompass various schemes and criteria for efficient model compression. Magnitude-based criteria rely on the absolute weight values to identify less important parameters [20,40], while first-order methods leverage gradients for importance ranking [7,44]. Some approaches involve one-time pruning followed by retraining [21], while others adopt iterative pruning techniques [34]. Recent research has explored the efficacy of various pruning methods, offering valuable insights to enhance model compression techniques [54]. Notably, DepGraph [12] introduced a novel method for general structural pruning of arbitrary architectures, efficiently removing coupled parameters for model acceleration. The results demonstrate its superior performance compared to many other techniques.",
            "score": 0.49369123811044247,
            "section_title": "Pruning",
            "char_start_offset": 6352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1840
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 78,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 317,
                    "end": 320,
                    "matchedPaperCorpusId": "18201582"
                },
                {
                    "start": 1237,
                    "end": 1240,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 1313,
                    "end": 1316,
                    "matchedPaperCorpusId": "195657904"
                },
                {
                    "start": 1382,
                    "end": 1386,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1603,
                    "end": 1607,
                    "matchedPaperCorpusId": "256390345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "271162137",
            "title": "Characterizing Prompt Compression Methods for Long Context Inference",
            "text": "There are three token pruning methods: LongLLMLingua (orange), reranker + LongLLMLingua (purple), reranker + token pruning (green).We observe that LongLLMLingua and reranker + LongLLMLingua typically exhibit the worst behavior across datasets.In Section B.1, we perform a sweep over LongLLMLingua hyper-parameters but do not see any significant improvement.Reranker + token pruning generally trails slightly behind the plain reranker method.We hypothesize that the lackluster performance of token pruning is due to the disruption of grammar and sentence comprehension caused by unstructured pruning.However, we notice that reranker + token pruning outperforms the reranker model for GovReport and QMSum on Mixtral 8x7B at higher compression rates.Similarly, on GPT-3.5-Turbo,reranker + token pruning is competitive with the plain reranker on summarization datasets at high compression rates.Nonetheless, the performance of the reranker + token pruning method trails the reranker on questionanswering tasks.In general, token pruning methods appear better suited for aggregation-style tasks that require pieces of knowledge from all segments of the initial context.Furthermore, rather than using out-of-the-box language models, practitioners may see better results by training language models specifically for token pruning (Jung & Kim, 2023;Pan et al., 2024).",
            "score": 0.4931025681663955,
            "section_title": "TOKEN PRUNING",
            "char_start_offset": 24018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 599
                },
                {
                    "start": 599,
                    "end": 747
                },
                {
                    "start": 747,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1006
                },
                {
                    "start": 1006,
                    "end": 1163
                },
                {
                    "start": 1163,
                    "end": 1358
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28125
        },
        {
            "corpus_id": "260912511",
            "title": "CoMFLP: Correlation Measure based Fast Search on ASR Layer Pruning",
            "text": "Neural network based ASR models with deep layers have shown superior performance [1][2][3]. This is mainly due to the recent advances in the transformer architecture [4][5][6] and the effectiveness of scaling laws [7]. However, large and deep models are difficult, if not impossible, to be deployed on resourceconstrained devices at the edge [8][9][10]. \n\nModel compression [11][12][13][14][15][16][17] was extensively studied for realizing large models on small-footprint devices without significant performance degradation. One of the predominant approaches is pruning [15][16][17]. Depending on whether the pruned parameters are grouped according to inherent structures (e.g., channel, head, and layer), there are unstructured [18] and structured pruning [19][20][21][22] approaches. Unstructured pruning treats each individual weight as the unit of pruning, often resulting in a sparse model that is not suitable for speedup on current hardware [23]. Structured pruning can achieve great acceleration, especially when dropping many layers [22,24]. \n\nIt is noted that the transformer stacked with a large number of layers, e.g., DeepNet with 1,000 layers [25], demonstrates significant performance. However, it comes at the cost of higher computational resources and memory consumption. It is hypothesized that some of the many layers might be redundant and have little contribution to the overall system per-formance [26]. This motivates the present study to inspect the redundancy among layers and perform layer-level structured pruning, i.e., layer pruning (LP), for simplifying deep models. \n\nLet L be the total number of layers. The core of LP is a pruning proposal that stipulates N of the L layers to be removed. The optimal pruning proposal can be found by enumerating all possible combinations of N layers and selecting the best one. For ASR systems, performance metrics like word error rate (WER) are used for determining the best proposal.",
            "score": 0.49256037938729336,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1953
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 84,
                    "matchedPaperCorpusId": "240419899"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "237941095"
                },
                {
                    "start": 87,
                    "end": 90,
                    "matchedPaperCorpusId": "245218698"
                },
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "218674528"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "231715684"
                },
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "211062209"
                },
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "3009483"
                },
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "195583987"
                },
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "212628335"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "3009483"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "195583987"
                },
                {
                    "start": 579,
                    "end": 583,
                    "matchedPaperCorpusId": "212628335"
                },
                {
                    "start": 730,
                    "end": 734,
                    "matchedPaperCorpusId": "220768628"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 770,
                    "end": 774,
                    "matchedPaperCorpusId": "202750230"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 1043,
                    "end": 1047,
                    "matchedPaperCorpusId": "202750230"
                },
                {
                    "start": 1047,
                    "end": 1050,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1421,
                    "end": 1425,
                    "matchedPaperCorpusId": "235458289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53515625
        },
        {
            "corpus_id": "278129815",
            "title": "Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning",
            "text": "Existing research primarily focuses on model quantization and pruning techniques. Quantization methods have advanced rapidly, with two primary approaches: quantization-aware training (QAT) and post-training quantization (PTQ). QAT optimizes quantization strategies during model retraining to improve compression performance [34]. Classic methods such as Q-BERTQ [35] and HAWQ [36] have achieved remarkable compression results. However, retraining large language models (LLMs) for QAT requires significant computational resources and time, limiting its practical deployment. \n\nAlthough PTQ eliminates the need for retraining, it has its own limitations. For instance, Liu et al. [37] proposed a mixed-precision quantization scheme that integrates ranking loss and nuclear norm-based methods. Lin et al. [38] introduced the Power-of-Two Factor (PTF) and Log-Int-Softmax (LIS) methods, while Ding et al. [39] developed the APQ-ViT framework. Despite their innovations, these methods often require additional training or involve complex quantization matching processes, making deployment challenging. \n\nPruning techniques represent another key approach to model compression, divided into unstructured [40,41] and structured pruning [42,43,44]. Unstructured pruning removes individual weights, while structured pruning eliminates entire model components, preserving performance more effectively but demanding significant computational resources to determine optimal pruning paths [45]. Additionally, model importance and redundancy vary across different tasks and scenarios, resulting in poor adaptability and limiting the ability to optimize models effectively based on task-specific requirements.",
            "score": 0.49223382381393166,
            "section_title": "Efficient ViT",
            "char_start_offset": 4984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1693
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "261106954"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 678,
                    "end": 682,
                    "matchedPaperCorpusId": "235658553"
                },
                {
                    "start": 901,
                    "end": 905,
                    "matchedPaperCorpusId": "252783098"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.685546875
        },
        {
            "corpus_id": "277634221",
            "title": "Mosaic: Composite Projection Pruning for Resource-efficient LLMs",
            "text": "Deploying large language models (LLMs) on hardwarelimited (compute and memory) resources remains a challenge [12], [22], [21]. Model pruning [12], [13] is one method that compresses a large foundation model to create a small language model (SLM). However, existing pruning methods negatively impact model quality [13], [48] or rely on vendor-specific hardware and software [12], [44]. This is because existing methods prune LLMs using coarse-grained approaches, such as uniform pruning, that inherently remove critical parameters since all layers of the LLM are pruned uniformly. Consequently, the SLMs produced are unusable when a large number of model parameters are removed [13] or require specialized software and hardware to demonstrate any performance gain when fewer parameters are removed [12], [22], [21]. \n\nThis article explores a new paradigm for pruning LLMs, referred to as projection pruning. Projections are the smallest LLM components within a layer that capture intrinsic learning properties during training. Uniform pruning removes from every projection equally, thereby removing parameters critical to model quality and under-pruning less important projections. While existing methods identify that specific layers should be pruned non-uniformly [22], this article investigates the optimization of pruning targets for every projection to maximize model quality. To this end, a novel projection-based pruning system for LLMs, Mosaic, is developed to reduce the resource requirements for producing SLMs while improving model accuracy compared to other pruning methods. Mosaic proposes composite projection pruning, which combines unstructured pruning with structured pruning to produce high-quality SLMs that can be deployed on a range of hardware platforms. Mosaic produces faster models than existing methods. Mosaic models have lower perplexity and higher accuracy than coarse-grained pruning. They achieve faster inference and have a lower GPU memory use than structure pruning. We provide the exact source as the same LLM can come in different formats, such as serialized Python objects (.bin) or Safetensors (.safetensors) depending on the Transformers library version used.",
            "score": 0.4922043606626772,
            "section_title": "VII. CONCLUSION",
            "char_start_offset": 48159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 113,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 115,
                    "end": 119,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 121,
                    "end": 125,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 313,
                    "end": 317,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1265,
                    "end": 1269,
                    "matchedPaperCorpusId": "263829692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74462890625
        },
        {
            "corpus_id": "267069084",
            "title": "APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference",
            "text": "Model compression methods like quantization and pruning boost inference efficiency. Quantization aims to reduce LMs' memory consumption via converting parameters to low-bit data types (Frantar et al., 2023;Dettmers et al., 2022;Lin et al., 2023). However, despite reducing LM's memory consumption, the speedup benefits of quantization require specific framework support, which limits their adaptability. \n\nPruning (LeCun et al., 1989;Han et al., 2016;Frankle & Carbin, 2019;Xu et al., 2021) aims to discard unimportant parameters in LMs for inference efficiency. Unstructured pruning (Sanh et al., 2020) prunes sparse parameters in LMs, which requires dedicated hardware support for efficiency improvements. Meanwhile, structured pruning (Lagunas et al., 2021;Xia et al., 2022) prunes consistent blocks in transformer layers (MHA heads, FFN neurons, and model dimensions) for ubiquitous inference efficiency gains. Such pruning often uses knowledge distillation (Hinton et al., 2015), which causes more training costs. Post-training pruning (Kwon et al., 2022;Frantar & Alistarh, 2023) aims to prune fine-tuned models with limited extra costs but requires initialization from fully fine-tuned models. Moreover, taskagnostic pruning (Sun et al., 2023;Ma et al., 2023) cannot achieve on-par performance with task-specific pruning.",
            "score": 0.4921470354761295,
            "section_title": "Model Compression",
            "char_start_offset": 6051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1328
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 228,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 434,
                    "end": 451,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 451,
                    "end": 474,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 474,
                    "end": 490,
                    "matchedPaperCorpusId": "233297003"
                },
                {
                    "start": 584,
                    "end": 603,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 738,
                    "end": 760,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 760,
                    "end": 777,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 962,
                    "end": 983,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1041,
                    "end": 1060,
                    "matchedPaperCorpusId": "248266822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "263671908",
            "title": "Neural Language Model Pruning for Automatic Speech Recognition",
            "text": "Recent literature has shown that increasing the size of neural network language models (NNLMs) can improve accuracy for several natural language processing tasks [1,2,3]. Employing large models on applications with memory and complexity constraints can be challenging. For such cases, the default strategy is to estimate small footprint models that meet the given constraints. However, better accuracy levels can be obtained by starting from a large model and shrinking it to the target size [4,5,6]. Popular model size reduction strategies include, among others, knowledge distillation [7], weight quantization [8], low rank layer factorization [9] or model pruning [10,11]. \n\nModel pruning has long been investigated as a way to effectively compress large models [12]. Defining which parameters to remove is one of the aspects to consider. One of the first criteria proposed is to remove low-magnitude parameters, following the intuition that parameter relevance correlates well with their magnitude [13,14]. Other works take the data distribution into account to define the parameter importance, making such methods suitable for joint pruning and task-adaptation [12,15]. \n\nAnother aspect to consider is the pruning method. Unstructured pruning, or sparsification, works by removing a certain number of neuron connections presumed to be redundant. Such process yields regularization effects similar to dropout [16]. Instead of removing individual connections, structured pruning enforces the presence of block sparsity patterns, which can improve memory consumption and latency [17]. Low-rank approximation methods [18], on the other hand, rely on factorizing the layer projections into the multiplication of two or more smaller matrices. Then, the inner channels that carry less information to reconstruct the original matrices are removed. \n\nThe pruning scheduling is another aspect to take into account. Usually, the straightforward choice is to remove parameters in a single step (hereafter one-shot). More recently, [10] showed that inducing sparsity incrementally while training can let the model recover from pruning losses, yielding better classification rates.",
            "score": 0.49206965937676944,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 167,
                    "end": 169,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 492,
                    "end": 495,
                    "matchedPaperCorpusId": "3036949"
                },
                {
                    "start": 495,
                    "end": 497,
                    "matchedPaperCorpusId": "209202715"
                },
                {
                    "start": 497,
                    "end": 499,
                    "matchedPaperCorpusId": "231855472"
                },
                {
                    "start": 587,
                    "end": 590,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 646,
                    "end": 649,
                    "matchedPaperCorpusId": "297464"
                },
                {
                    "start": 667,
                    "end": 671,
                    "matchedPaperCorpusId": "27494814"
                },
                {
                    "start": 671,
                    "end": 674,
                    "matchedPaperCorpusId": "231699188"
                },
                {
                    "start": 765,
                    "end": 769,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "215416219"
                },
                {
                    "start": 1006,
                    "end": 1009,
                    "matchedPaperCorpusId": "211171709"
                },
                {
                    "start": 1166,
                    "end": 1170,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1170,
                    "end": 1173,
                    "matchedPaperCorpusId": "17651092"
                },
                {
                    "start": 1413,
                    "end": 1417,
                    "matchedPaperCorpusId": "6844431"
                },
                {
                    "start": 1581,
                    "end": 1585,
                    "matchedPaperCorpusId": "236477807"
                },
                {
                    "start": 1618,
                    "end": 1622,
                    "matchedPaperCorpusId": "10163399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "252846445",
            "title": "GMP*: Well-Tuned Gradual Magnitude Pruning Can Outperform Most BERT-Pruning Methods",
            "text": "The massive recent growth of the computational cost of accurate deep learning models, in particular large language models (LLMs), has motivated the development of several advanced model compression techniques (Hoefler et al., 2021;Gholami et al., 2021), encompassing unstructured and structured pruning, quantization, and knowledge distillation. In this paper, we focus on the unstructured pruning, for which we follow the standard pipeline. Such models are first pre-trained on a large upstream corpus of unlabelled text. Then, they are fine-tuned in a supervised manner on a smaller downstream task, such as question-answering or text classification. In the context of compression, this pipeline led to two paradigms: 1) upstream pruning, followed by fine-tuning of the remaining weights on a downstream task, and 2) downstream pruning, pruning and fine-tuning directly on the downstream task. \n\nA tempting baseline approach in most settings is gradual magnitude pruning (GMP) (Hagiwara, 1994;Zhu and Gupta, 2017), that is, periodically removing the smallest fraction of weights during training, possibly interspersed with fine-tuning steps designed to recover accuracy. GMP has been shown to be an extremely strong baseline in the context of computer vision (Gale et al., 2019;Hoefler et al., 2021). However, the literature on pruning LLMs, and in particular BERT models (Sanh et al., 2020;Chen et al., 2020;Zafrir et al., 2021), clearly suggests that GMP does not perform well. \n\nContribution. In this paper, we re-examine this conclusion and investigate whether GMP can be a competitive baseline, once carefully tuned. Specifically, we show that a well tuned variant which we call GMP , can produce highly accurate and sparse language models in both upstream and downstream pruning regimes, matching or even outperforming more complex methods. We explore effects of the crucial parameters for gradual pruning, and provide simple and intuitive guidelines on how to integrate them in a principled manner. \n\nOur results are summarized in Figure 1, which presents performance of state-of-the-art unstructured pruning techniques on two benchmarks.",
            "score": 0.491693427110656,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2007
                },
                {
                    "start": 2010,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 979,
                    "end": 995,
                    "matchedPaperCorpusId": "25970113"
                },
                {
                    "start": 1261,
                    "end": 1280,
                    "matchedPaperCorpusId": "67855585"
                },
                {
                    "start": 1374,
                    "end": 1393,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 1393,
                    "end": 1411,
                    "matchedPaperCorpusId": "220768628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "263830468",
            "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
            "text": "SparseGPT is the first unstructured pruning approach specifically developed to be fast enough for pruning LLMs within a few hours. Wanda applies magnitude pruning by weights and activations, which further improves the pruning speed than SparseGPT. Both can be extended for semi-structured pruning (i.e., the N:M sparsity (Pool & Yu, 2021;Hubara et al., 2021)). However, in practice, it is more challenging to translate the theoretically achieved sparsity in unstructured or semistructured pruning to practical computation and storage savings on current GPU hardware (Frantar & Alistarh, 2023). LLM-Pruner (Ma et al., 2023) is the first attempt to structurally prune LLMs, offering the benefit of reducing both model computation and memory usage while keeping the overall LLM structure intact. It uses one-shot pruning based on first-order and approximated Hessian information and requires fine-tuning using LoRA to recover pruned model weights. \n\nDespite its fast speed, one-shot pruning has limitations. First, it depends heavily on pre-defined weight importance metrics for pruning decisions, and thus adopts a uniform-sparsity ratio across all layers without considering the different redundancy at each layer. Second, error recovery for remaining model parameters is limited compared to training-based pruning, potentially affecting the final performance. Our Compresso addresses all these limitations. \n\nPrompting. Prompting has emerged as a new paradigm for adapting pre-trained LLMs to new tasks by augmenting the model input with task-specific hints. Notable methods include template-based prompting (Schick & Sch\u00fctze, 2021), instruction-based prompting (Wei et al., 2021;Sanh et al., 2022) , and Chain-of-Thought prompting (Wei et al., 2022). Despite its demonstrated success across a spectrum of NLP tasks (Chung et al., 2022;Goyal et al., 2022;Wei et al., 2022;Chowdhery et al., 2022a), the application of prompting for pruning LLMs remains unexplored in the literature.",
            "score": 0.4916608855820961,
            "section_title": "RELATED WORKS",
            "char_start_offset": 5425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 338,
                    "matchedPaperCorpusId": "245002847"
                },
                {
                    "start": 338,
                    "end": 358,
                    "matchedPaperCorpusId": "231934142"
                },
                {
                    "start": 1608,
                    "end": 1632,
                    "matchedPaperCorpusId": "210838924"
                },
                {
                    "start": 1680,
                    "end": 1698,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 1732,
                    "end": 1750,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "270067694",
            "title": "Exploring Activation Patterns of Parameters in Language Models",
            "text": "The crux of model pruning lies in identifying the crucial parameters within the network.From the perspective of model pruning, we can derive insights into the significant role scoring of parameters.\n\nModel pruning techniques [19,15,14] for LLMs can be broadly categorized into two types [39]: structured pruning [10,36,30] and unstructured pruning [27,21].Structured pruning aims to reduce the hidden state size by removing entire rows or columns from the weight matrix, which can lead to actual acceleration and pruning benefits.However, this method often results in a significant loss of performance.Unstructured pruning, on the other hand, involves eliminating individual connections, i.e., specific elements within the weight matrix.This approach can maintain model performance even at high pruning ratios but does not inherently lead to computational speedup unless a substantial proportion of connections is pruned within specific regions.\n\nRegardless of the type of pruning, both methods focus on identifying which components of the network have the least impact on the output.Many studies have utilized Taylor expansion to define the rank of weights in terms of their influence on the network's structure, thereby guiding the pruning process by removing weights with minimal impact.\n\nIn this work, we draw upon the Taylor expansion to define the degree to which internal weights are activated, thereby investigating the underlying mechanisms within the model.",
            "score": 0.49137408897591006,
            "section_title": "Model Pruning",
            "char_start_offset": 7473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 88,
                    "end": 198
                },
                {
                    "start": 200,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 945
                },
                {
                    "start": 947,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1290
                },
                {
                    "start": 1292,
                    "end": 1467
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 229,
                    "end": 232,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 312,
                    "end": 316,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3525390625
        },
        {
            "corpus_id": "268875867",
            "title": "Rethinking Pruning for Vision-Language Models: Strategies for Effective Sparsity and Performance Restoration",
            "text": "Based on Figure 2, we found: (1) VLMs would collapse when the language models are under high sparsity ratios (i.e., s l > 70%), whereas sparsity imposed on visual models has a comparatively lower impact on performance; (2) When constrained by the summation of sparsity (s v + s l ), pruning the modalityspecific models with equal sparsity ratios leads to optimal performance.For the second question, we initially prune VLMs with different unstructured sparsity ratios using the following strategies: pruning language models and visual models with the same sparsity ratios (\"V + L\"), pruning visual models only (\"Vision\"), and pruning language models only (\"Language\").According to Figure 3a, when sparsity ratios exceed 50%, all settings experience a significant performance drop, although VLMs pruned by a single modality model maintain relatively high performance.\n\nSimilarly, in Figure 3b when employing structured N:M sparsity (andYukun Ma et al., 2021;Zhang et al., 2022) (i.e., in each contiguous block of M values, N values must be zero), all models encounter significant performance degradation and even collapse (2:4 for pruning both vision models and language models).This situation prompts us to reflect on how to restore the pruning-caused performance degradation for VLMs.",
            "score": 0.49102597289292027,
            "section_title": "Preliminary Study",
            "char_start_offset": 6752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 375
                },
                {
                    "start": 375,
                    "end": 668
                },
                {
                    "start": 668,
                    "end": 866
                },
                {
                    "start": 868,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1285
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 957,
                    "matchedPaperCorpusId": "231847094"
                },
                {
                    "start": 957,
                    "end": 976,
                    "matchedPaperCorpusId": "249642429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23828125
        },
        {
            "paperId": "d0656bdf977f2a82cf918ecd6a8cdd068f91a44b",
            "corpusId": 277626866,
            "title": "Thanos: A Block-wise Pruning Algorithm for Efficient Large Language Model Compression",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 53,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.05346, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268766339",
                    "name": "Ivan Ilin"
                },
                {
                    "authorId": "2268766087",
                    "name": "Peter Richt\u00e1rik"
                }
            ],
            "abstract": "This paper presents Thanos, a novel weight-pruning algorithm designed to reduce the memory footprint and enhance the computational efficiency of large language models (LLMs) by removing redundant weights while maintaining accuracy. Thanos introduces a block-wise pruning strategy with adaptive masks that dynamically adjust to weight importance, enabling flexible sparsity patterns and structured formats, such as $n:m$ sparsity, optimized for hardware acceleration. Experimental evaluations demonstrate that Thanos achieves state-of-the-art performance in structured pruning and outperforms existing methods in unstructured pruning. By providing an efficient and adaptable approach to model compression, Thanos offers a practical solution for deploying large models in resource-constrained environments.",
            "corpus_id": "277626866",
            "text": "This paper presents Thanos, a novel weight-pruning algorithm designed to reduce the memory footprint and enhance the computational efficiency of large language models (LLMs) by removing redundant weights while maintaining accuracy. Thanos introduces a block-wise pruning strategy with adaptive masks that dynamically adjust to weight importance, enabling flexible sparsity patterns and structured formats, such as $n:m$ sparsity, optimized for hardware acceleration. Experimental evaluations demonstrate that Thanos achieves state-of-the-art performance in structured pruning and outperforms existing methods in unstructured pruning. By providing an efficient and adaptable approach to model compression, Thanos offers a practical solution for deploying large models in resource-constrained environments.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7861328125
        },
        {
            "paperId": "6e151f9966e732aab92a48d5e4a0c0b130b79d1b",
            "corpusId": 267256758,
            "title": "VLCP: A High-Performance FPGA-based CNN Accelerator with Vector-level Cluster Pruning",
            "venue": "IEEE/ACM International Symposium on Nanoscale Architectures",
            "year": 2023,
            "referenceCount": 14,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3611315.3633271",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3611315.3633271?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3611315.3633271, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281332303",
                    "name": "Shuo Ran"
                },
                {
                    "authorId": "2152566135",
                    "name": "Bi Wu"
                },
                {
                    "authorId": "2268689492",
                    "name": "Ke Chen"
                },
                {
                    "authorId": "2257377778",
                    "name": "Weiqiang Liu"
                }
            ],
            "abstract": "Convolutional neural networks (CNNs) are widely used in computer vision, natural language processing, and other application scenarios. But deploying CNNs at the edge is challenging due to their large number of parameters. Pruning is a solution that can effectively reduce the number of parameters and off-chip memory accesses. However, high sparsity unstructured pruning is not hardware-friendly, while structured pruning has low compression efficiency. As a result, vector-level pruning, with a coarser granularity, is a promising alternative that balances pruning performance and hardware-friendliness. In this paper, a hardware-oriented vector-level pruning strategy is proposed based on the CNN vector distribution properties. By expanding the dynamic range of vector groups, more important weights can be preserved without sacrificing accuracy. When applied to the VGG-16 and ResNet-18 models on the ImageNet dataset, the proposed strategy achieved 10.93 \u00d7 and 10.17 \u00d7 compression ratios in convolutional layers with a 66% reduction in computation and an acceptable drop in top-1 accuracy. Furthermore, the proposed pruning scheme achieves a remarkable performance of 188 FPS on the VCU118 evaluation board, demonstrating its compatibility with hardware. Compared to the state-of-the-art, the proposed strategy reaches 69% performance improvement and up to 2.8 \u00d7 higher LUT efficiency.",
            "corpus_id": "267256758",
            "text": "Convolutional neural networks (CNNs) are widely used in computer vision, natural language processing, and other application scenarios. But deploying CNNs at the edge is challenging due to their large number of parameters. Pruning is a solution that can effectively reduce the number of parameters and off-chip memory accesses. However, high sparsity unstructured pruning is not hardware-friendly, while structured pruning has low compression efficiency. As a result, vector-level pruning, with a coarser granularity, is a promising alternative that balances pruning performance and hardware-friendliness. In this paper, a hardware-oriented vector-level pruning strategy is proposed based on the CNN vector distribution properties. By expanding the dynamic range of vector groups, more important weights can be preserved without sacrificing accuracy. When applied to the VGG-16 and ResNet-18 models on the ImageNet dataset, the proposed strategy achieved 10.93 \u00d7 and 10.17 \u00d7 compression ratios in convolutional layers with a 66% reduction in computation and an acceptable drop in top-1 accuracy. Furthermore, the proposed pruning scheme achieves a remarkable performance of 188 FPS on the VCU118 evaluation board, demonstrating its compatibility with hardware. Compared to the state-of-the-art, the proposed strategy reaches 69% performance improvement and up to 2.8 \u00d7 higher LUT efficiency.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.475830078125
        },
        {
            "paperId": "f6a5ada263c886c5f4c672e3f43b9ce2f407ca02",
            "corpusId": 279323158,
            "title": "HAPE: Hardware-Aware LLM Pruning For Efficient On-Device Inference Optimization",
            "venue": "ACM Transactions on Design Automation of Electronic Systems",
            "year": 2025,
            "referenceCount": 7,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3744244?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3744244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2197681604",
                    "name": "Wenqian Zhao"
                },
                {
                    "authorId": "2241630124",
                    "name": "Lancheng Zou"
                },
                {
                    "authorId": "2243988812",
                    "name": "Zixiao Wang"
                },
                {
                    "authorId": "2244166898",
                    "name": "Xufeng Yao"
                },
                {
                    "authorId": "2330763304",
                    "name": "Bei Yu"
                }
            ],
            "abstract": "Over the past few years, large language models (LLMs) have demonstrated remarkable performance and versatility across a variety of complex tasks. However, their deployment has been challenged by their substantial model size and computational requirements. Pruning is a effective approach to make the model parameters sparse, thereby acquire inference acceleration. While not everyone requires training or fine-tuning large models, the diverse range of applications necessitates the deployment of LLMs on different devices. Model pruning and compression have emerged as areas of deep research interest to address these challenges. In consideration of versatility and practicality, we have designed a hardware-aware pruning process for general-purpose hardware/edge devices to enable efficient deployment and inference of LLMs. Instead of considering sparse ratio alone, we are motivated to design a pruning framework that incorporates genuine inference speed-up sensitivity from each pruning structure. Moreover, our framework breaks the layer-by-layer pruning setting and fuse several layers into one pruning stage to allow cross-layer optimization. Apart from that, we holds pragmatism by conducting compilation optimization during pruning. This step is critical because most sparsity patterns barely show distinct speed acceleration with corresponding dataflow and memory optimization. Our process operates within a post-training framework, obviating the need for additional training and thereby reducing resource requirements, while ensuring diverse inference speed and accuracy requirements on hardware.",
            "corpus_id": "279323158",
            "text": "Over the past few years, large language models (LLMs) have demonstrated remarkable performance and versatility across a variety of complex tasks. However, their deployment has been challenged by their substantial model size and computational requirements. Pruning is a effective approach to make the model parameters sparse, thereby acquire inference acceleration. While not everyone requires training or fine-tuning large models, the diverse range of applications necessitates the deployment of LLMs on different devices. Model pruning and compression have emerged as areas of deep research interest to address these challenges. In consideration of versatility and practicality, we have designed a hardware-aware pruning process for general-purpose hardware/edge devices to enable efficient deployment and inference of LLMs. Instead of considering sparse ratio alone, we are motivated to design a pruning framework that incorporates genuine inference speed-up sensitivity from each pruning structure. Moreover, our framework breaks the layer-by-layer pruning setting and fuse several layers into one pruning stage to allow cross-layer optimization. Apart from that, we holds pragmatism by conducting compilation optimization during pruning. This step is critical because most sparsity patterns barely show distinct speed acceleration with corresponding dataflow and memory optimization. Our process operates within a post-training framework, obviating the need for additional training and thereby reducing resource requirements, while ensuring diverse inference speed and accuracy requirements on hardware.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.51123046875
        },
        {
            "paperId": "a68a75270b529732b6a28ebca8f32ff1b36229d5",
            "corpusId": 276961144,
            "title": "T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 41,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301331844",
                    "name": "Guanchen Li"
                },
                {
                    "authorId": "2279769996",
                    "name": "Yixing Xu"
                },
                {
                    "authorId": "2307589652",
                    "name": "Zeping Li"
                },
                {
                    "authorId": "2279399536",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "2346265083",
                    "name": "Xuanwu Yin"
                },
                {
                    "authorId": "2297014912",
                    "name": "Dong Li"
                },
                {
                    "authorId": "2271751612",
                    "name": "E. Barsoum"
                }
            ],
            "abstract": "Structural pruning enhances hardware-agnostic inference efficiency for large language models (LLMs) but often struggles to maintain performance. Local pruning performs efficient layer-by-layer compression but ignores global topology. Global pruning has the potential to find the optimal solution although resource-intensive. However, existing methods tend to rank structural saliency uniformly, ignoring inter-structure dependencies and failing to achieve end-to-end optimization. To address these limitations, we propose T\\'yr-the-Pruner, an efficient end-to-end search-based global structural pruning framework. This framework constructs a supernet by repeatedly applying local pruning across a range of sparsity ratios to each layer in an LLM, with the core goal of determining the optimal sparsity distribution under a target overall sparsity ratio. Concretely, we introduce an effective local pruning and an expectation error accumulation approach to improve supernet construction. Furthermore, we employ an iterative prune-and-search strategy with coarse-to-fine sparsity granularity to ensure efficient search convergence. Experimental results show that T\\'yr-the-Pruner achieves state-of-the-art structural pruning, retaining 97% of the dense model's performance while removing a challenging 50% of Llama-3.1-70B's parameters.",
            "corpus_id": "276961144",
            "text": "Structural pruning enhances hardware-agnostic inference efficiency for large language models (LLMs) but often struggles to maintain performance. Local pruning performs efficient layer-by-layer compression but ignores global topology. Global pruning has the potential to find the optimal solution although resource-intensive. However, existing methods tend to rank structural saliency uniformly, ignoring inter-structure dependencies and failing to achieve end-to-end optimization. To address these limitations, we propose T\\'yr-the-Pruner, an efficient end-to-end search-based global structural pruning framework. This framework constructs a supernet by repeatedly applying local pruning across a range of sparsity ratios to each layer in an LLM, with the core goal of determining the optimal sparsity distribution under a target overall sparsity ratio. Concretely, we introduce an effective local pruning and an expectation error accumulation approach to improve supernet construction. Furthermore, we employ an iterative prune-and-search strategy with coarse-to-fine sparsity granularity to ensure efficient search convergence. Experimental results show that T\\'yr-the-Pruner achieves state-of-the-art structural pruning, retaining 97% of the dense model's performance while removing a challenging 50% of Llama-3.1-70B's parameters.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5908203125
        },
        {
            "paperId": "1575a13482945a766ac5b5369b4d7118af8f2d42",
            "corpusId": 272896856,
            "title": "Removing Neurons From Deep Neural Networks Trained With Tabular Data",
            "venue": "IEEE Open Journal of the Computer Society",
            "year": 2024,
            "referenceCount": 42,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/ojcs.2024.3467182",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/OJCS.2024.3467182?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/OJCS.2024.3467182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2217498641",
                    "name": "Antti Klemetti"
                },
                {
                    "authorId": "2269743280",
                    "name": "Mikko Raatikainen"
                },
                {
                    "authorId": "121350276",
                    "name": "Juhani Kivim\u00e4ki"
                },
                {
                    "authorId": "1387874184",
                    "name": "Lalli Myllyaho"
                },
                {
                    "authorId": "2273027470",
                    "name": "Jukka K. Nurminen"
                }
            ],
            "abstract": "Deep neural networks bear substantial cloud computational loads and often surpass client devices' capabilities. Research has concentrated on reducing the inference burden of convolutional neural networks processing images. Unstructured pruning, which leads to sparse matrices requiring specialized hardware, has been extensively studied. However, neural networks trained with tabular data and structured pruning, which produces dense matrices handled by standard hardware, are less explored. We compare two approaches: 1) Removing neurons followed by training from scratch, and 2) Structured pruning followed by fine-tuning through additional training over a limited number of epochs. We evaluate these approaches using three models of varying sizes (1.5, 9.2, and 118.7 million parameters) from Kaggle-winning neural networks trained with tabular data. Approach 1 consistently outperformed Approach 2 in predictive performance. The models from Approach 1 had 52%, 8%, and 12% fewer parameters than the original models, with latency reductions of 18%, 5%, and 5%, respectively. Approach 2 required at least one epoch of fine-tuning for recovering predictive performance, with further fine-tuning offering diminishing returns. Approach 1 yields lighter models for retraining in the presence of concept drift and avoids shifting computational load from inference to training, which is inherent in Approach 2. However, Approach 2 can be used to pinpoint the layers that have the least impact on the model's predictive performance when neurons are removed. We found that the feed-forward component of the transformer architecture used in large language models is a promising target for neuron removal.",
            "corpus_id": "272896856",
            "text": "Deep neural networks bear substantial cloud computational loads and often surpass client devices' capabilities. Research has concentrated on reducing the inference burden of convolutional neural networks processing images. Unstructured pruning, which leads to sparse matrices requiring specialized hardware, has been extensively studied. However, neural networks trained with tabular data and structured pruning, which produces dense matrices handled by standard hardware, are less explored. We compare two approaches: 1) Removing neurons followed by training from scratch, and 2) Structured pruning followed by fine-tuning through additional training over a limited number of epochs. We evaluate these approaches using three models of varying sizes (1.5, 9.2, and 118.7 million parameters) from Kaggle-winning neural networks trained with tabular data. Approach 1 consistently outperformed Approach 2 in predictive performance. The models from Approach 1 had 52%, 8%, and 12% fewer parameters than the original models, with latency reductions of 18%, 5%, and 5%, respectively. Approach 2 required at least one epoch of fine-tuning for recovering predictive performance, with further fine-tuning offering diminishing returns. Approach 1 yields lighter models for retraining in the presence of concept drift and avoids shifting computational load from inference to training, which is inherent in Approach 2. However, Approach 2 can be used to pinpoint the layers that have the least impact on the model's predictive performance when neurons are removed. We found that the feed-forward component of the transformer architecture used in large language models is a promising target for neuron removal.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.483642578125
        }
    ],
    "quotes": {
        "cost": 0.26449200000000006,
        "quotes": [
            {
                "idx": 0,
                "key": "[246276158 | Zhao et al. | 2022 | Citations: 0]",
                "snippets": "Unstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al., 2018)), e.g, (Hill et al., 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al., 2018).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[52048008 | He et al. | 2018 | Citations: 1349]": "Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4\\(\\times \\) FLOPs reduction, we achieved 2.7% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53\\(\\times \\) on the GPU (Titan Xp) and 1.95\\(\\times \\) on an Android phone (Google Pixel 1), with negligible loss of accuracy."
                },
                "metadata": [
                    {
                        "section_title": "Unstructured vs. Structured Pruning",
                        "pdf_hash": "",
                        "start": 696,
                        "end": 1969,
                        "sentence_offsets": [
                            {
                                "start": 696,
                                "end": 806
                            },
                            {
                                "start": 807,
                                "end": 1011
                            },
                            {
                                "start": 1012,
                                "end": 1419
                            },
                            {
                                "start": 1422,
                                "end": 1604
                            },
                            {
                                "start": 1605,
                                "end": 1799
                            },
                            {
                                "start": 1800,
                                "end": 1969
                            }
                        ],
                        "ref_mentions": [
                            "52048008",
                            "13876417",
                            "52048008"
                        ],
                        "quote": "Unstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al., 2018)), e.g, (Hill et al., 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al., 2018)."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[247794014 | Yang et al. | 2022 | Citations: 12]",
                "snippets": "In the unstructured pruning, each model parameter is individually removed if it reaches some criteria based on the magnitude or importance score (Han et al., 2015;(Zhu et al., 2017). The unstructured pruning results in sparse matrices and allows for significant model compression, but the inference speed can hardly be improved without specialized devices. While in the structured pruning, rows or columns of the parameters are removed from the weight matrices (McCarley, 2019;(Michel et al., 2019)(Voita et al., 2019)(Lagunas et al., 2021)(Hou et al., 2020). Thus, the resulting model speeds up on the common CPU and GPU devices.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[162183964 | Voita et al. | 2019 | Citations: 1147]": "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.",
                    "[166227946 | Michel et al. | 2019 | Citations: 1068]": "Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention \"head\" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.",
                    "[215415863 | Hou et al. | 2020 | Citations: 322]": "The pre-trained language models like BERT and RoBERTa, though powerful in many natural language processing tasks, are both computational and memory expensive. To alleviate this problem, one approach is to compress them for specific tasks before deployment. However, recent works on BERT compression usually reduce the large BERT model to a fixed smaller size, and can not fully satisfy the requirements of different edge devices with various hardware performances. In this paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT), which can run at adaptive width and depth. The training process of DynaBERT includes first training a width-adaptive BERT and then allows both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. Comprehensive experiments under various efficiency constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its largest size has comparable performance as BERT (or RoBERTa), while at smaller widths and depths consistently outperforms existing BERT compression methods.",
                    "[237485472 | Lagunas et al. | 2021 | Citations: 223]": "Pre-training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up inference. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine-tuning. We find that this approach learns to prune out full components of the underlying model, such as attention heads. Experiments consider classification and generation tasks, yielding among other results a pruned model that is a 2.4x faster, 74% smaller BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models in speed and pruned models in size.",
                    "[27494814 | Zhu et al. | 2017 | Citations: 1281]": "Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 609,
                        "end": 1237,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "27494814",
                            "166227946",
                            "162183964",
                            "237485472",
                            "215415863"
                        ],
                        "quote": "In the unstructured pruning, each model parameter is individually removed if it reaches some criteria based on the magnitude or importance score (Han et al., 2015;(Zhu et al., 2017). The unstructured pruning results in sparse matrices and allows for significant model compression, but the inference speed can hardly be improved without specialized devices. While in the structured pruning, rows or columns of the parameters are removed from the weight matrices (McCarley, 2019;(Michel et al., 2019)(Voita et al., 2019)(Lagunas et al., 2021)(Hou et al., 2020). Thus, the resulting model speeds up on the common CPU and GPU devices."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[256662263 | Kurtic et al. | 2023 | Citations: 26]",
                "snippets": "The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning (Hoefler et al., 2021), quantization [3], and distillation (Gou et al., 2020). In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.",
                    "[231740691 | Hoefler et al. | 2021 | Citations: 725]": "The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1060,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 198
                            },
                            {
                                "start": 199,
                                "end": 349
                            },
                            {
                                "start": 350,
                                "end": 528
                            },
                            {
                                "start": 529,
                                "end": 795
                            },
                            {
                                "start": 796,
                                "end": 867
                            },
                            {
                                "start": 868,
                                "end": 1060
                            }
                        ],
                        "ref_mentions": [
                            "231740691",
                            "219559263"
                        ],
                        "quote": "The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning (Hoefler et al., 2021), quantization [3], and distillation (Gou et al., 2020). In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[256662734 | Santacroce et al. | 2023 | Citations: 34]",
                "snippets": "Unstructured pruning removes individual weights from the network based on some criteria, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2019)Xu et al., 2021;Zafrir et al., 2021;Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast services based on LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204009154 | Wang et al. | 2019 | Citations: 292]": "Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a novel, structured pruning approach based on low rank factorization and augmented Lagrangian L0 norm regularization. Our structured approach achieves significant inference speedups while matching or outperforming our unstructured pruning baseline at various sparsity levels. We apply our method to state of the art models on the enwiki8 dataset and obtain a 1.19 perplexity score with just 5M parameters, vastly outperforming a model of the same size trained from scratch. We also demonstrate that our method can be applied to language model fine-tuning by pruning the BERT model on several downstream classification benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 731,
                        "end": 1478,
                        "sentence_offsets": [
                            {
                                "start": 731,
                                "end": 906
                            },
                            {
                                "start": 907,
                                "end": 1085
                            },
                            {
                                "start": 1086,
                                "end": 1293
                            },
                            {
                                "start": 1294,
                                "end": 1478
                            }
                        ],
                        "ref_mentions": [
                            "204009154"
                        ],
                        "quote": "Unstructured pruning removes individual weights from the network based on some criteria, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2019)Xu et al., 2021;Zafrir et al., 2021;Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast services based on LLMs."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[256827874 | Yang et al. | 2022 | Citations: 8]",
                "snippets": "For structured pruning (Li et al., 2016;Hu et al., 2016;Wen et al., 2016), groups of weight connections are removed from a network together, such as entire channels or filters in CNN-based networks and layers or attention heads in transformer-based networks. For unstructured pruning (Han et al., 2015a,b), weight connections are removed from a network individually. However, unstructured pruning methods produce large sparse weight matrices which are computationally inefficient unless equipped with a specifically designed hardware.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[2056019 | Wen et al. | 2016 | Citations: 2341]": "High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around ~1%. Open source code is in this https URL"
                },
                "metadata": [
                    {
                        "section_title": "Network Pruning",
                        "pdf_hash": "",
                        "start": 87,
                        "end": 621,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 103
                            },
                            {
                                "start": 104,
                                "end": 363
                            },
                            {
                                "start": 364,
                                "end": 471
                            },
                            {
                                "start": 472,
                                "end": 639
                            }
                        ],
                        "ref_mentions": [
                            "2056019"
                        ],
                        "quote": "For structured pruning (Li et al., 2016;Hu et al., 2016;Wen et al., 2016), groups of weight connections are removed from a network together, such as entire channels or filters in CNN-based networks and layers or attention heads in transformer-based networks. For unstructured pruning (Han et al., 2015a,b), weight connections are removed from a network individually. However, unstructured pruning methods produce large sparse weight matrices which are computationally inefficient unless equipped with a specifically designed hardware."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[266362404 | An et al. | 2023 | Citations: 61]",
                "snippets": "Structured pruning (He and Xiao 2023), which prunes entire rows or columns of weights, offers a promising solution to the deployment challenges of LLMs. Unlike unstructured pruning, structured pruning reduces both parameters and inference time without relying on specific hardware, making it more widely applicable (Anwar et al., 2015). For effective structured pruning, it's crucial to have a metric that captures the collective significance of an entire row or column. However, current unstructured pruning techniques for LLMs, as seen in methods like (Sun et al. 2023;Frantar and Alistarh 2023), primarily focus on the importance of individual elements of each row in isolation. This absence of structured metrics that evaluate entire rows or columns makes them less suitable for structured pruning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7333079 | Anwar et al. | 2015 | Citations: 748]": "Real-time application of deep learning algorithms is often hindered by high computational complexity and frequent memory accesses. Network pruning is a promising technique to solve this problem. However, pruning usually results in irregular network connections that not only demand extra representation efforts but also do not fit well on parallel computation. We introduce structured sparsity at various scales for convolutional neural networks: feature map-wise, kernel-wise, and intra-kernel strided sparsity. This structured sparsity is very advantageous for direct computational resource savings on embedded computers, in parallel computing environments, and in hardware-based systems. To decide the importance of network connections and paths, the proposed method uses a particle filtering approach. The importance weight of each particle is assigned by assessing the misclassification rate with a corresponding connectivity pattern. The pruned network is retrained to compensate for the losses due to pruning. While implementing convolutions as matrix products, we particularly show that intra-kernel strided sparsity with a simple constraint can significantly reduce the size of the kernel and feature map tensors. The proposed work shows that when pruning granularities are applied in combination, we can prune the CIFAR-10 network by more than 70% with less than a 1% loss in accuracy."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 881,
                        "end": 1692,
                        "sentence_offsets": [
                            {
                                "start": 881,
                                "end": 1033
                            },
                            {
                                "start": 1034,
                                "end": 1226
                            },
                            {
                                "start": 1227,
                                "end": 1360
                            },
                            {
                                "start": 1361,
                                "end": 1571
                            },
                            {
                                "start": 1572,
                                "end": 1692
                            }
                        ],
                        "ref_mentions": [
                            "7333079"
                        ],
                        "quote": "Structured pruning (He and Xiao 2023), which prunes entire rows or columns of weights, offers a promising solution to the deployment challenges of LLMs. Unlike unstructured pruning, structured pruning reduces both parameters and inference time without relying on specific hardware, making it more widely applicable (Anwar et al., 2015). For effective structured pruning, it's crucial to have a metric that captures the collective significance of an entire row or column. However, current unstructured pruning techniques for LLMs, as seen in methods like (Sun et al. 2023;Frantar and Alistarh 2023), primarily focus on the importance of individual elements of each row in isolation. This absence of structured metrics that evaluate entire rows or columns makes them less suitable for structured pruning."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[267103825 | Gong et al. | 2024 | Citations: 0]",
                "snippets": "By analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67].\n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[202558809 | Liu et al. | 2019 | Citations: 186]": "Structured weight pruning is a representative model compression technique of DNNs to reduce the storage and computation requirements and accelerate inference. An automatic hyperparameter determination process is necessary due to the large number of flexible hyperparameters. This work proposes AutoCompress, an automatic structured pruning framework with the following key performance improvements: (i) effectively incorporate the combination of structured pruning schemes in the automatic process; (ii) adopt the state-of-art ADMM-based structured weight pruning as the core algorithm, and propose an innovative additional purification step for further weight reduction without accuracy loss; and (iii) develop effective heuristic search method enhanced by experience-based guided search, replacing the prior deep reinforcement learning technique which has underlying incompatibility with the target pruning problem. Extensive experiments on CIFAR-10 and ImageNet datasets demonstrate that AutoCompress is the key to achieve ultra-high pruning rates on the number of weights and FLOPs that cannot be achieved before. As an example, AutoCompress outperforms the prior work on automatic model compression by up to 33\u00d7 in pruning rate (120\u00d7 reduction in the actual parameter count) under the same accuracy. Significant inference speedup has been observed from the AutoCompress framework on actual measurements on smartphone. We release models of this work at anonymous link: http://bit.ly/2VZ63dS.",
                    "[249205107 | Hanson et al. | 2022 | Citations: 22]": "Performance and efficiency of running modern Deep Neural Networks (DNNs) are heavily bounded by data movement. To mitigate the data movement bottlenecks, recent DNN inference accelerator designs widely adopt aggressive compression techniques and sparse-skipping mechanisms. These mechanisms avoid transferring or computing with zero-valued weights or activations to save time and energy. However, such sparse-skipping logic involves large input buffers and irregular data access patterns, thus precluding many energy-efficient data reuse opportunities and dataflows. In this work, we propose Cascading Structured Pruning (CSP), a technique that preserves significantly more data reuse opportunities for higher energy efficiency while maintaining comparable performance relative to recent sparse architectures such as SparTen. CSP includes the following two components: At algorithm level, CSP-A induces a predictable sparsity pattern that allows for low-overhead compression of weight data and sequential access to both activation and weight data. At architecture level, CSP-H leverages CSP-A's induced sparsity pattern with a novel dataflow to access unique activation data only once, thus removing the demand for large input buffers. Each CSP-H processing element (PE) employs a novel accumulation buffer design and a counter-based sparse-skipping mechanism to support the dataflow with minimum controller overhead. We verify our approach on several representative models. Our simulated results show that CSP achieves on average 15\u00d7 energy efficiency improvement over SparTen with comparable or superior speedup under most evaluations.",
                    "[256390345 | Fang et al. | 2023 | Citations: 270]": "Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and fully automatic method, Dependency Graph (DepGraph), to explicitly model the dependency between layers and comprehensively group coupled parameters for pruning. In this work, we extensively evaluate our method on several architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and Vision transformer for images, GAT for graph, DGCNN for 3D point cloud, alongside LSTM for language, and demonstrate that, even with a simple norm-based criterion, the proposed method consistently yields gratifying performances.",
                    "[5423629 | Xiong et al. | 2012 | Citations: 75]": "The authors are particularly aware that green cloud computing (GCC) is a broad range and a hot field. The distinction between \u2018consumer of\u2019 and \u2018provider of\u2019 cloud-based energy resources may very important in creating a world-wide ecosystem of GCC. A user simply submits its service request to the cloud service provider with the connection of Internet or wired/wireless networks. The result of the requested service is delivered back to the user in time, whereas the information storage and process, interoperating protocols, service composition, communications and distributed computing, are all smoothly interactive by the networks. In this study, this is a survey on GCC schemes based on networks. The concept and history of Green computing were introduced first, and then focus on the challenge and requirement of cloud computing. Cloud computing needs to become green, which means provisioning cloud service while considering energy consumption under a set of energy consumption criteria and it is called GCC. Furthermore, the recent work done in GCC based on networks, including microprocessors, task scheduling algorithms, virtualisation technology, cooling systems, networks and disk storage were introduced. After that, the works on GCC from their research group was presented in Georgia State University. Finally, the conclusion and some future works were given."
                },
                "metadata": [
                    {
                        "section_title": "Analysis and discussion",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2502,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 145
                            },
                            {
                                "start": 146,
                                "end": 332
                            },
                            {
                                "start": 333,
                                "end": 476
                            },
                            {
                                "start": 477,
                                "end": 537
                            },
                            {
                                "start": 538,
                                "end": 689
                            },
                            {
                                "start": 690,
                                "end": 845
                            },
                            {
                                "start": 846,
                                "end": 999
                            },
                            {
                                "start": 1000,
                                "end": 1186
                            },
                            {
                                "start": 1187,
                                "end": 1263
                            },
                            {
                                "start": 1264,
                                "end": 1419
                            },
                            {
                                "start": 1420,
                                "end": 1559
                            },
                            {
                                "start": 1562,
                                "end": 1724
                            },
                            {
                                "start": 1725,
                                "end": 1877
                            },
                            {
                                "start": 1878,
                                "end": 1944
                            },
                            {
                                "start": 1945,
                                "end": 2053
                            },
                            {
                                "start": 2054,
                                "end": 2142
                            },
                            {
                                "start": 2143,
                                "end": 2254
                            },
                            {
                                "start": 2255,
                                "end": 2410
                            },
                            {
                                "start": 2411,
                                "end": 2503
                            }
                        ],
                        "ref_mentions": [
                            "202558809",
                            "15663507",
                            "5423629",
                            "256390345",
                            "249205107"
                        ],
                        "quote": "By analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67].\n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[267312283 | Park et al. | 2024 | Citations: 16]",
                "snippets": "Unstructured pruning algorithms (Kurtic et al., 2022)122,(Xu et al., 2021)[162] have demonstrated high accuracy in compressing language models achieving up to 97% compression rate with a small accuracy degradation. However, their drawback lies in the acceleration difficulty due to the resulting sparse pruning patterns. Therefore, researchers have focused on extending the findings in unstructured pruning algorithms to structured pruning algorithms to achieve acceleration.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245131431 | Xu et al. | 2021 | Citations: 26]": "Pre-trained Language Models (PLMs) have achieved great success in various Natural Language Processing (NLP) tasks under the pre-training and fine-tuning paradigm. \n With large quantities of parameters, PLMs are computation-intensive and resource-hungry. Hence, model pruning has been introduced to compress large-scale PLMs. \n However, most prior approaches only consider task-specific knowledge towards downstream tasks, but ignore the essential task-agnostic knowledge during pruning, which may cause catastrophic forgetting problem and lead to poor generalization ability. \n To maintain both task-agnostic and task-specific knowledge in our pruned model, we propose ContrAstive Pruning (CAP) under the paradigm of pre-training and fine-tuning. \n It is designed as a general framework, compatible with both structured and unstructured pruning. \n Unified in contrastive learn- ing, CAP enables the pruned model to learn from the pre-trained model for task-agnostic knowledge, and fine-tuned model for task-specific knowledge. \n Besides, to better retain the performance of the pruned model, the snapshots (i.e., the intermediate models at each pruning iteration) also serve as effective supervisions for pruning. \n Our extensive experiments show that adopting CAP consistently yields significant improvements, especially in extremely high sparsity scenarios. \n With only 3% model parameters reserved (i.e., 97% sparsity), CAP successfully achieves 99.2% and 96.3% of the original BERT performance in QQP and MNLI tasks. \n In addition, our probing experiments demonstrate that the model pruned by CAP tends to achieve better generalization ability.",
                    "[247446572 | Kurtic et al. | 2022 | Citations: 126]": "In this paper, we consider the problem of sparsifying BERT models, which are a key building block for natural language processing, in order to reduce their storage and computational cost. We introduce the Optimal BERT Surgeon (oBERT), an efficient and accurate pruning method based on approximate second-order information, which we show to yield state-of-the-art results in both stages of language tasks: pre-training and fine-tuning. Specifically, oBERT extends existing work on second-order pruning by allowing for pruning weight blocks, and is the first such method that is applicable at BERT scale. Second, we investigate compounding compression approaches to obtain highly compressed but accurate models for deployment on edge devices. These models significantly push boundaries of the current state-of-the-art sparse BERT models with respect to all metrics: model size, inference speed and task accuracy. For example, relative to the dense BERT-base, we obtain 10x model size compression with < 1% accuracy drop, 10x CPU-inference speedup with < 2% accuracy drop, and 29x CPU-inference speedup with < 7.5% accuracy drop. Our code, fully integrated with Transformers and SparseML, is available at https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT."
                },
                "metadata": [
                    {
                        "section_title": "Pruning Granularity: Unstructured vs. Structured",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 444,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 183
                            },
                            {
                                "start": 184,
                                "end": 289
                            },
                            {
                                "start": 290,
                                "end": 444
                            }
                        ],
                        "ref_mentions": [
                            "247446572",
                            "245131431"
                        ],
                        "quote": "Unstructured pruning algorithms (Kurtic et al., 2022)122,(Xu et al., 2021)[162] have demonstrated high accuracy in compressing language models achieving up to 97% compression rate with a small accuracy degradation. However, their drawback lies in the acceleration difficulty due to the resulting sparse pruning patterns. Therefore, researchers have focused on extending the findings in unstructured pruning algorithms to structured pruning algorithms to achieve acceleration."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[267412232 | Wang et al. | 2024 | Citations: 45]",
                "snippets": "In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters.\n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters.\n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[267751193 | Yuan et al. | 2024 | Citations: 3]",
                "snippets": "Pruning, a prevalent technique for model compression method, seeks to discard non-essential parameters within language models, thereby reducing computational and storage costs while maintaining performance (LeCun et al., 1989), (Wang et al., 2023), (Xia et al., 2022). Pruning methods can be broadly categorized into unstructured pruning, which removes individual weights, and structured pruning, which removes higher-granularity structures such as neurons, attention heads, or entire layers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247922354 | Xia et al. | 2022 | Citations: 187]": "The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches.",
                    "[258865530 | Wang et al. | 2023 | Citations: 8]": "Recently, various intermediate layer distillation (ILD) objectives have been shown to improve compression of BERT models via Knowledge Distillation (KD). However, a comprehensive evaluation of the objectives in both task-specific and task-agnostic settings is lacking. To the best of our knowledge, this is the first work comprehensively evaluating distillation objectives in both settings. We show that attention transfer gives the best performance overall. We also study the impact of layer choice when initializing the student from the teacher layers, finding a significant impact on the performance in task-specific distillation. For vanilla KD and hidden states transfer, initialisation with lower layers of the teacher gives a considerable improvement over higher layers, especially on the task of QNLI (up to an absolute percentage change of 17.8 in accuracy). Attention transfer behaves consistently under different initialisation settings. We release our code as an efficient transformer-based model distillation framework for further studies."
                },
                "metadata": [
                    {
                        "section_title": "A. Model Pruning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 447,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 223
                            },
                            {
                                "start": 224,
                                "end": 447
                            }
                        ],
                        "ref_mentions": [
                            "7785881",
                            "258865530",
                            "247922354"
                        ],
                        "quote": "Pruning, a prevalent technique for model compression method, seeks to discard non-essential parameters within language models, thereby reducing computational and storage costs while maintaining performance (LeCun et al., 1989), (Wang et al., 2023), (Xia et al., 2022). Pruning methods can be broadly categorized into unstructured pruning, which removes individual weights, and structured pruning, which removes higher-granularity structures such as neurons, attention heads, or entire layers."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[268364208 | Wang et al. | 2024 | Citations: 63]",
                "snippets": "In general, these methods can be grouped into four categories: unstructured pruning, structured pruning, quantization, and low-rank approximation. Specifically, unstructured pruning methods (Frantar et al., 2023) set the individual weights of an LLM to zero without changing its shape. However, irregular sparsification of unstructured pruning is difficult to achieve the desired speedup or memory saving. Unlike unstructured pruning, structured pruning methods (Ma et al., 2023;(Ashkboos et al., 2024)Zhong et al., 2024) remove entire channels or other structured components from LLMs, making them easier to implement on hardware.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
                    "[267301573 | Ashkboos et al. | 2024 | Citations: 184]": "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression"
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 308,
                        "end": 944,
                        "sentence_offsets": [
                            {
                                "start": 308,
                                "end": 454
                            },
                            {
                                "start": 455,
                                "end": 599
                            },
                            {
                                "start": 600,
                                "end": 719
                            },
                            {
                                "start": 720,
                                "end": 944
                            }
                        ],
                        "ref_mentions": [
                            "255372747",
                            "267301573"
                        ],
                        "quote": "In general, these methods can be grouped into four categories: unstructured pruning, structured pruning, quantization, and low-rank approximation. Specifically, unstructured pruning methods (Frantar et al., 2023) set the individual weights of an LLM to zero without changing its shape. However, irregular sparsification of unstructured pruning is difficult to achieve the desired speedup or memory saving. Unlike unstructured pruning, structured pruning methods (Ma et al., 2023;(Ashkboos et al., 2024)Zhong et al., 2024) remove entire channels or other structured components from LLMs, making them easier to implement on hardware."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[269588232 | Klein et al. | 2024 | Citations: 2]",
                "snippets": "Unstructured pruning (Blalock et al., 2020) computes a score for each weight in the network, such as the weight's magnitude, and removes weights with scores below a predetermined threshold. This approach often achieves high pruning rates with minimal performance degradation, but it also leads to sparse weight matrices, which are not well-supported by commonly used machine learning frameworks. Structured pruning (Michel et al., 2019;Sajjad et al., 2022) removes larger components of the network, such as layers or heads. Although it typically does not achieve the same pruning rates as unstructured pruning, it only prunes entire columns/rows of the weight matrix, making it compatible with popular deep learning frameworks and hardware.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Unstructured pruning (Blalock et al., 2020) computes a score for each weight in the network, such as the weight's magnitude, and removes weights with scores below a predetermined threshold. This approach often achieves high pruning rates with minimal performance degradation, but it also leads to sparse weight matrices, which are not well-supported by commonly used machine learning frameworks. Structured pruning (Michel et al., 2019;Sajjad et al., 2022) removes larger components of the network, such as layers or heads. Although it typically does not achieve the same pruning rates as unstructured pruning, it only prunes entire columns/rows of the weight matrix, making it compatible with popular deep learning frameworks and hardware.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[270621063 | Guo et al. | 2024 | Citations: 1]",
                "snippets": "Neural network pruning in LLM can be broadly categorized into two groups: structured pruning (Ma et al., 2023)Zhang et al., 2023) and unstructured pruning (Frantar & Alistarh, 2023;(Sun et al., 2023). (Ma et al., 2023) proposes a dependency detection algorithm to detect and prune non-critical grouped structures followed by LoRA fine-tuning. Although structured pruning can usually achieve better hardware efficiency, the accuracy drops a lot even at a low compression rate. Unstructured pruning can yield a higher compression rate and achieve acceleration on Nvidia's GPUs by employing a hardware-friendly N:M sparsity pattern.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",
                    "[259203115 | Sun et al. | 2023 | Citations: 439]": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 13,
                        "end": 640,
                        "sentence_offsets": [
                            {
                                "start": 13,
                                "end": 212
                            },
                            {
                                "start": 213,
                                "end": 353
                            },
                            {
                                "start": 354,
                                "end": 486
                            },
                            {
                                "start": 487,
                                "end": 640
                            }
                        ],
                        "ref_mentions": [
                            "258823276",
                            "259203115",
                            "258823276"
                        ],
                        "quote": "Neural network pruning in LLM can be broadly categorized into two groups: structured pruning (Ma et al., 2023)Zhang et al., 2023) and unstructured pruning (Frantar & Alistarh, 2023;(Sun et al., 2023). (Ma et al., 2023) proposes a dependency detection algorithm to detect and prune non-critical grouped structures followed by LoRA fine-tuning. Although structured pruning can usually achieve better hardware efficiency, the accuracy drops a lot even at a low compression rate. Unstructured pruning can yield a higher compression rate and achieve acceleration on Nvidia's GPUs by employing a hardware-friendly N:M sparsity pattern."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[271217883 | Cheng et al. | 2024 | Citations: 3]",
                "snippets": "Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models. However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware. In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]). Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247318543 | Yu et al. | 2022 | Citations: 49]": "Neural networks tend to achieve better accuracy with training if they are larger -- even if the resulting models are overparameterized. Nevertheless, carefully removing such excess parameters before, during, or after training may also produce models with similar or even improved accuracy. In many cases, that can be curiously achieved by heuristics as simple as removing a percentage of the weights with the smallest absolute value -- even though magnitude is not a perfect proxy for weight relevance. With the premise that obtaining significantly better performance from pruning depends on accounting for the combined effect of removing multiple weights, we revisit one of the classic approaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose a tractable heuristic for solving the combinatorial extension of OBS, in which we select weights for simultaneous removal, as well as a systematic update of the remaining weights. Our selection method outperforms other methods under high sparsity, and the weight update is advantageous even when combined with the other methods.",
                    "[249282255 | Fu et al. | 2022 | Citations: 19]": "Efficient deep neural network (DNN) models equipped with compact operators (e.g., depthwise convolutions) have shown great potential in reducing DNNs' theoretical complexity (e.g., the total number of weights/operations) while maintaining a decent model accuracy. However, existing efficient DNNs are still limited in fulfilling their promise in boosting real-hardware efficiency, due to their commonly adopted compact operators' low hardware utilization. In this work, we open up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy. Interestingly, we observe that while some DNN layers' activation functions help DNNs' training optimization and achievable accuracy, they can be properly removed after training without compromising the model accuracy. Inspired by this observation, we propose a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient DNNs that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real-hardware efficiency. Excitingly, our DepthShrinker framework delivers hardware-friendly compact networks that outperform both state-of-the-art efficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and 1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method MetaPruning. Our codes are available at: https://github.com/facebookresearch/DepthShrinker."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 271,
                        "end": 1044,
                        "sentence_offsets": [
                            {
                                "start": 54,
                                "end": 376
                            },
                            {
                                "start": 376,
                                "end": 552
                            },
                            {
                                "start": 552,
                                "end": 711
                            },
                            {
                                "start": 711,
                                "end": 1001
                            },
                            {
                                "start": 1001,
                                "end": 1146
                            }
                        ],
                        "ref_mentions": [
                            "247318543",
                            "249282255"
                        ],
                        "quote": "Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models. However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware. In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]). Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[271270022 | Wang et al. | 2024 | Citations: 2]",
                "snippets": "Unstructured pruning [19,20,21] performs at the individual weight level, which brings about larger sparsity but fails to accelerate the model and reduce the storage cost without requiring additional hardware support. By contrast, structured pruning [22,23]24] removes a group of weights, such as an entire channel, head, layer, and so on, therefore providing a more hardware-friendly solution, enhancing the lower inference latency and memory demands, so we focus on structured pruning in this paper.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                },
                "metadata": [
                    {
                        "section_title": "Network Pruning for Language Models",
                        "pdf_hash": "",
                        "start": 231,
                        "end": 731,
                        "sentence_offsets": [
                            {
                                "start": 191,
                                "end": 286
                            },
                            {
                                "start": 286,
                                "end": 502
                            },
                            {
                                "start": 502,
                                "end": 785
                            }
                        ],
                        "ref_mentions": [
                            "255372747"
                        ],
                        "quote": "Unstructured pruning [19,20,21] performs at the individual weight level, which brings about larger sparsity but fails to accelerate the model and reduce the storage cost without requiring additional hardware support. By contrast, structured pruning [22,23]24] removes a group of weights, such as an entire channel, head, layer, and so on, therefore providing a more hardware-friendly solution, enhancing the lower inference latency and memory demands, so we focus on structured pruning in this paper."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[271744772 | Zhao et al. | 2024 | Citations: 1]",
                "snippets": "For structured pruning, SliceGPT (Ashkboos et al., 2024) and Eigenpruning (Vergara-Browne et al., 2024) utilize singular value decompositions to prune singular values of weight matrices and reduce model dimensions. ZipLM (Kurtic et al., 2023) adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Our proposed FISTAPruner focuses on unstructured and semi-structured pruning, and thus is orthogonal to these structured pruning methods, enabling further model compression. For unstructured and semi-structured pruning, SparseGPT (Frantar et al., 2023) and ISC (Shao et al., 2023) leverage the OBS framework to calculate saliency for each entry using the inverse Hessian of the loss metric, based on which pruning masks are generated and weights updated. Wanda (Sun et al., 2023) implements a heuristic approach, removing weights based on the product of their magnitudes and activations without compensation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256662263 | Kurtic et al. | 2023 | Citations: 26]": "The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and pruning techniques, making it a cost-effective approach for generating an entire family of smaller, faster, and highly accurate models, guaranteed to meet the desired inference specifications. In particular, ZipLM outperforms all prior BERT-base distillation and pruning techniques, such as CoFi, MiniLM, and TinyBERT. Moreover, it matches the performance of the heavily optimized MobileBERT model, obtained via extensive architecture search, by simply pruning the baseline BERT-large model. When compressing GPT2, ZipLM outperforms DistilGPT2 while being 60% smaller and 30% faster. Our code is available at: https://github.com/IST-DASLab/ZipLM.",
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
                    "[264146174 | Shao et al. | 2023 | Citations: 21]": "Various Large Language Models (LLMs) from the Generative Pretrained Transformer (GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs."
                },
                "metadata": [
                    {
                        "section_title": "Background and Related Work",
                        "pdf_hash": "",
                        "start": 227,
                        "end": 1191,
                        "sentence_offsets": [
                            {
                                "start": 227,
                                "end": 441
                            },
                            {
                                "start": 442,
                                "end": 576
                            },
                            {
                                "start": 577,
                                "end": 750
                            },
                            {
                                "start": 751,
                                "end": 1037
                            },
                            {
                                "start": 1038,
                                "end": 1191
                            }
                        ],
                        "ref_mentions": [
                            "256662263",
                            "255372747",
                            "264146174"
                        ],
                        "quote": "For structured pruning, SliceGPT (Ashkboos et al., 2024) and Eigenpruning (Vergara-Browne et al., 2024) utilize singular value decompositions to prune singular values of weight matrices and reduce model dimensions. ZipLM (Kurtic et al., 2023) adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Our proposed FISTAPruner focuses on unstructured and semi-structured pruning, and thus is orthogonal to these structured pruning methods, enabling further model compression. For unstructured and semi-structured pruning, SparseGPT (Frantar et al., 2023) and ISC (Shao et al., 2023) leverage the OBS framework to calculate saliency for each entry using the inverse Hessian of the loss metric, based on which pruning masks are generated and weights updated. Wanda (Sun et al., 2023) implements a heuristic approach, removing weights based on the product of their magnitudes and activations without compensation."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[271909421 | Li et al. | 2024 | Citations: 1]",
                "snippets": "Pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,(Liu et al., 2023). Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]...Table 7 indicates that the pruned models can achieve 1.19x \u223c 1.87x speedup compared to their dense counterparts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[260815690 | Liu et al. | 2023 | Citations: 218]": "Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference. We validate that DejaVu can reduce the inference latency of OPT-175B by over 2X compared to the state-of-the-art FasterTransformer, and over 6X compared to the widely used Hugging Face implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu."
                },
                "metadata": [
                    {
                        "section_title": "Efficiency Analysis",
                        "pdf_hash": "",
                        "start": 722,
                        "end": 1147,
                        "sentence_offsets": [
                            {
                                "start": 716,
                                "end": 808
                            },
                            {
                                "start": 809,
                                "end": 955
                            },
                            {
                                "start": 956,
                                "end": 1148
                            }
                        ],
                        "ref_mentions": [
                            "260815690"
                        ],
                        "quote": "Pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,(Liu et al., 2023). Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]"
                    },
                    {
                        "section_title": "Efficiency Analysis",
                        "pdf_hash": "",
                        "start": 235,
                        "end": 348,
                        "sentence_offsets": [
                            {
                                "start": 235,
                                "end": 347
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Table 7 indicates that the pruned models can achieve 1.19x \u223c 1.87x speedup compared to their dense counterparts."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[272368391 | Xu et al. | 2024 | Citations: 35]",
                "snippets": "Structured Pruning: This approach removes entire subsets of parameters like layers, channels, or filters, which is beneficial for hardware optimization due to more regular memory access patterns and simplified computations. The 'LLM-Pruner' (Kaddour et al., 2023) employs structured pruning to eliminate non-essential groups based on gradient data, thus maintaining critical functionalities. It also facilitates performance recovery through techniques such as LoRA, allowing efficient restoration with minimal data. 2. Unstructured Pruning: Unlike structured pruning, unstructured pruning removes individual weights across the model, offering finer granularity and potentially higher compression rates (Li et al., 2023a). However, this method typically results in sparse matrices, which can be less compatible with traditional hardware architectures, compromising computational efficiency. It is most suitable where maximum compression is needed without constraints on structural preservation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Pruning",
                        "pdf_hash": "",
                        "start": 297,
                        "end": 1290,
                        "sentence_offsets": [
                            {
                                "start": 294,
                                "end": 520
                            },
                            {
                                "start": 521,
                                "end": 688
                            },
                            {
                                "start": 689,
                                "end": 812
                            },
                            {
                                "start": 813,
                                "end": 1018
                            },
                            {
                                "start": 1019,
                                "end": 1186
                            },
                            {
                                "start": 1187,
                                "end": 1290
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Structured Pruning: This approach removes entire subsets of parameters like layers, channels, or filters, which is beneficial for hardware optimization due to more regular memory access patterns and simplified computations. The 'LLM-Pruner' (Kaddour et al., 2023) employs structured pruning to eliminate non-essential groups based on gradient data, thus maintaining critical functionalities. It also facilitates performance recovery through techniques such as LoRA, allowing efficient restoration with minimal data. 2. Unstructured Pruning: Unlike structured pruning, unstructured pruning removes individual weights across the model, offering finer granularity and potentially higher compression rates (Li et al., 2023a). However, this method typically results in sparse matrices, which can be less compatible with traditional hardware architectures, compromising computational efficiency. It is most suitable where maximum compression is needed without constraints on structural preservation."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[272693912 | Lv et al. | 2024 | Citations: 2]",
                "snippets": "Currently, optimization methods for large models include pruning (structured pruning [17]-[19] and unstructured pruning [20], [21]), quantization [22]-[24], and distillation [25], [26]. This work focuses on structured pruning, making it more deployment-friendly and hardware-friendly.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"
                },
                "metadata": [
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 533,
                        "end": 817,
                        "sentence_offsets": [
                            {
                                "start": 306,
                                "end": 566
                            },
                            {
                                "start": 567,
                                "end": 684
                            },
                            {
                                "start": 687,
                                "end": 874
                            }
                        ],
                        "ref_mentions": [
                            "258823276"
                        ],
                        "quote": "Currently, optimization methods for large models include pruning (structured pruning [17]-[19] and unstructured pruning [20], [21]), quantization [22]-[24], and distillation [25], [26]. This work focuses on structured pruning, making it more deployment-friendly and hardware-friendly."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[272770793 | Wang et al. | 2024 | Citations: 4]",
                "snippets": "Existing LLM pruning work (Frantar and Alistarh, 2023;Sun et al., 2023;Xu et al., 2024a;Zhang et al., 2024b) focuses mainly on unstructured or semi-structured sparsity. However, these paradigms require specific hardware to achieve practical acceleration. \n\nIn contrast, structured pruning, which imposes structured sparsity by removing groups of consecutive parameters (Louizos et al., 2017;(Wang et al., 2019)(Xia et al., 2022), is more hardware-friendly on general devices.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204009154 | Wang et al. | 2019 | Citations: 292]": "Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a novel, structured pruning approach based on low rank factorization and augmented Lagrangian L0 norm regularization. Our structured approach achieves significant inference speedups while matching or outperforming our unstructured pruning baseline at various sparsity levels. We apply our method to state of the art models on the enwiki8 dataset and obtain a 1.19 perplexity score with just 5M parameters, vastly outperforming a model of the same size trained from scratch. We also demonstrate that our method can be applied to language model fine-tuning by pruning the BERT model on several downstream classification benchmarks.",
                    "[247922354 | Xia et al. | 2022 | Citations: 187]": "The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 498,
                        "end": 971,
                        "sentence_offsets": [
                            {
                                "start": 498,
                                "end": 666
                            },
                            {
                                "start": 667,
                                "end": 752
                            },
                            {
                                "start": 755,
                                "end": 971
                            }
                        ],
                        "ref_mentions": [
                            "204009154",
                            "247922354"
                        ],
                        "quote": "Existing LLM pruning work (Frantar and Alistarh, 2023;Sun et al., 2023;Xu et al., 2024a;Zhang et al., 2024b) focuses mainly on unstructured or semi-structured sparsity. However, these paradigms require specific hardware to achieve practical acceleration. \n\nIn contrast, structured pruning, which imposes structured sparsity by removing groups of consecutive parameters (Louizos et al., 2017;(Wang et al., 2019)(Xia et al., 2022), is more hardware-friendly on general devices."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[272910976 | Fang et al. | 2024 | Citations: 10]",
                "snippets": "According to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning (Ma et al., 2023)43,(Liu et al., 2023), Unstructured Pruning (Han et al., 2015)[15], and Semi-Structured Pruning (Frantar et al., 2023)38,29,32,(Pool et al., 2021). Structured pruning physically eliminates substructures like attention heads (Ma et al., 2023), embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters (Ma et al., 2023)43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy (Frantar et al., 2023)38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32](Pool et al., 2021)(Frantar et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[2238772 | Han et al. | 2015 | Citations: 6708]": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.",
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",
                    "[260815690 | Liu et al. | 2023 | Citations: 218]": "Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference. We validate that DejaVu can reduce the inference latency of OPT-175B by over 2X compared to the state-of-the-art FasterTransformer, and over 6X compared to the widely used Hugging Face implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu."
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 186,
                        "end": 1476,
                        "sentence_offsets": [
                            {
                                "start": 186,
                                "end": 391
                            },
                            {
                                "start": 392,
                                "end": 607
                            },
                            {
                                "start": 608,
                                "end": 761
                            },
                            {
                                "start": 762,
                                "end": 965
                            },
                            {
                                "start": 966,
                                "end": 1139
                            },
                            {
                                "start": 1140,
                                "end": 1476
                            }
                        ],
                        "ref_mentions": [
                            "258823276",
                            "260815690",
                            "2238772",
                            "255372747",
                            "245002847",
                            "258823276",
                            "258823276",
                            "255372747",
                            "245002847",
                            "255372747"
                        ],
                        "quote": "According to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning (Ma et al., 2023)43,(Liu et al., 2023), Unstructured Pruning (Han et al., 2015)[15], and Semi-Structured Pruning (Frantar et al., 2023)38,29,32,(Pool et al., 2021). Structured pruning physically eliminates substructures like attention heads (Ma et al., 2023), embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters (Ma et al., 2023)43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy (Frantar et al., 2023)38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32](Pool et al., 2021)(Frantar et al., 2023)."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[273963228 | Rostami et al. | 2024 | Citations: 1]",
                "snippets": "While unstructured pruning allows for the sparsification of specific weights, it often necessitates specialized hardware for speed improve- ments (Cheng et al., 2023). In contrast, structural pruning (Molchanov et al., 2016) focuses on removing entire filters, blocks, or layers, providing efficiency that is not dependent on specific devices (Cheng et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[260887757 | Cheng et al. | 2023 | Citations: 154]": "Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and to accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. More than three thousand pruning papers have been published from 2020 to 2024. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of eight pairs of contrast settings for pruning (e.g., unstructured/structured, one-shot/iterative, data-free/data-driven, initialized/pre-trained weights, etc.) and explore several emerging topics, including pruning for large language models, vision transformers, diffusion models, and large multimodal models, post-training pruning, and different levels of supervision for pruning to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. Finally, we provide some valuable recommendations on selecting pruning methods and prospect several promising research directions for neural network pruning. To facilitate future research on deep neural network pruning, we summarize broad pruning applications (e.g., adversarial robustness, natural language understanding, etc.) and build a curated collection of datasets, networks, and evaluations on different applications. We maintain a repository on https://github.com/hrcheng1066/awesome-pruning that serves as a comprehensive resource for neural network pruning papers and corresponding open-source codes. We will keep updating this repository to include the latest advancements in the field."
                },
                "metadata": [
                    {
                        "section_title": "Structural Pruning",
                        "pdf_hash": "",
                        "start": 156,
                        "end": 520,
                        "sentence_offsets": [
                            {
                                "start": 156,
                                "end": 323
                            },
                            {
                                "start": 324,
                                "end": 520
                            }
                        ],
                        "ref_mentions": [
                            "260887757",
                            "17240902",
                            "260887757"
                        ],
                        "quote": "While unstructured pruning allows for the sparsification of specific weights, it often necessitates specialized hardware for speed improve- ments (Cheng et al., 2023). In contrast, structural pruning (Molchanov et al., 2016) focuses on removing entire filters, blocks, or layers, providing efficiency that is not dependent on specific devices (Cheng et al., 2023)."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[274981759 | Sy et al. | 2024 | Citations: 0]",
                "snippets": "Structured Pruning removes entire groups of parameters, which results in a smaller and faster model (Xia et al., 2024;Ma et al., 2023). Ma et al. (2023) propose a new gradient-based criterion to eliminate substructures in LLMs, while Xia et al. (2024) use a joint loss combining a pruning mask loss with the language modeling loss. However, optimizing these criteria can be computationally intensive. For example, the pruning step of Sheared-LLaMA (Xia et al., 2023) is 5x expensive compared to standard LM training, according to the authors. In contrast, thanks to the local gradient updates, our approach is computationally efficient, allowing us to compress a 47B model within minutes on a single A100 GPU. Regarding unstructured pruning, these methods do not provide any gains in terms of memory or speedup, at least with current algorithmic implementations. Semi-structured pruning (e.g., 2:4 and 4:8) (Sun et al., 2024;Frantar and Alistarh, 2023;Liu et al., 2024) does not lead to memory gain but can speed up processing on kernels optimized for such matrix structures. On the other hand, our method, which directly shrinks matrices, saves memory across all hardware and leads to speed up, as fewer computations are performed.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 291,
                        "end": 1523,
                        "sentence_offsets": [
                            {
                                "start": 291,
                                "end": 426
                            },
                            {
                                "start": 427,
                                "end": 622
                            },
                            {
                                "start": 623,
                                "end": 691
                            },
                            {
                                "start": 692,
                                "end": 833
                            },
                            {
                                "start": 834,
                                "end": 1000
                            },
                            {
                                "start": 1001,
                                "end": 1153
                            },
                            {
                                "start": 1154,
                                "end": 1366
                            },
                            {
                                "start": 1367,
                                "end": 1523
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Structured Pruning removes entire groups of parameters, which results in a smaller and faster model (Xia et al., 2024;Ma et al., 2023). Ma et al. (2023) propose a new gradient-based criterion to eliminate substructures in LLMs, while Xia et al. (2024) use a joint loss combining a pruning mask loss with the language modeling loss. However, optimizing these criteria can be computationally intensive. For example, the pruning step of Sheared-LLaMA (Xia et al., 2023) is 5x expensive compared to standard LM training, according to the authors. In contrast, thanks to the local gradient updates, our approach is computationally efficient, allowing us to compress a 47B model within minutes on a single A100 GPU. Regarding unstructured pruning, these methods do not provide any gains in terms of memory or speedup, at least with current algorithmic implementations. Semi-structured pruning (e.g., 2:4 and 4:8) (Sun et al., 2024;Frantar and Alistarh, 2023;Liu et al., 2024) does not lead to memory gain but can speed up processing on kernels optimized for such matrix structures. On the other hand, our method, which directly shrinks matrices, saves memory across all hardware and leads to speed up, as fewer computations are performed."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[274992799 | Ling et al. | 2024 | Citations: 9]",
                "snippets": "In the domain of LLM pruning, recent studies have largely focused on unstructured (or semistructured) pruning [8], a method that shrinks models by selectively zeroing out weights considered non-critical. Despite its advancements, unstructured pruning falls short in substantially reducing parameter count, which is crucial for accelerating LLM inference as it is often bottlenecked on memory bandwidth and communication (Leviathan et al., 2022). To accelerate inference speed, unstructured pruning models are often paired with specialized frameworks or hardware solutions. Conversely, structured pruning (Anwar et al., 2015)[11] effectively decreases the model's parameter count by systematically eliminating columns or rows from weight matrices, enabling significant improvements in inference speed, and reduce deployment cost on conventional hardware. Yet, structured pruning often entails more pronounced compromises in model performance, which poses a greater challenge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[254096365 | Leviathan et al. | 2022 | Citations: 736]": "Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.",
                    "[7333079 | Anwar et al. | 2015 | Citations: 748]": "Real-time application of deep learning algorithms is often hindered by high computational complexity and frequent memory accesses. Network pruning is a promising technique to solve this problem. However, pruning usually results in irregular network connections that not only demand extra representation efforts but also do not fit well on parallel computation. We introduce structured sparsity at various scales for convolutional neural networks: feature map-wise, kernel-wise, and intra-kernel strided sparsity. This structured sparsity is very advantageous for direct computational resource savings on embedded computers, in parallel computing environments, and in hardware-based systems. To decide the importance of network connections and paths, the proposed method uses a particle filtering approach. The importance weight of each particle is assigned by assessing the misclassification rate with a corresponding connectivity pattern. The pruned network is retrained to compensate for the losses due to pruning. While implementing convolutions as matrix products, we particularly show that intra-kernel strided sparsity with a simple constraint can significantly reduce the size of the kernel and feature map tensors. The proposed work shows that when pruning granularities are applied in combination, we can prune the CIFAR-10 network by more than 70% with less than a 1% loss in accuracy."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1031,
                        "end": 1967,
                        "sentence_offsets": [
                            {
                                "start": 1031,
                                "end": 1234
                            },
                            {
                                "start": 1235,
                                "end": 1455
                            },
                            {
                                "start": 1456,
                                "end": 1582
                            },
                            {
                                "start": 1583,
                                "end": 1846
                            },
                            {
                                "start": 1847,
                                "end": 1967
                            }
                        ],
                        "ref_mentions": [
                            "254096365",
                            "7333079"
                        ],
                        "quote": "In the domain of LLM pruning, recent studies have largely focused on unstructured (or semistructured) pruning [8], a method that shrinks models by selectively zeroing out weights considered non-critical. Despite its advancements, unstructured pruning falls short in substantially reducing parameter count, which is crucial for accelerating LLM inference as it is often bottlenecked on memory bandwidth and communication (Leviathan et al., 2022). To accelerate inference speed, unstructured pruning models are often paired with specialized frameworks or hardware solutions. Conversely, structured pruning (Anwar et al., 2015)[11] effectively decreases the model's parameter count by systematically eliminating columns or rows from weight matrices, enabling significant improvements in inference speed, and reduce deployment cost on conventional hardware. Yet, structured pruning often entails more pronounced compromises in model performance, which poses a greater challenge."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[275550746 | Lee et al. | 2025 | Citations: 0]",
                "snippets": "Unlike unstructured pruning, which removes connections with individual weights [8], (Molchanov et al., 2016), (Frankle et al., 2018), structured pruning removes whole structured components, such as channels or layers from a neural network. Although structured pruning generally has a lower compression rate than unstructured pruning, it provides practical performance improvements on modern GPUs where computations are performed at the tensor level due to computational efficiency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[53388625 | Frankle et al. | 2018 | Citations: 3488]": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. \nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. \nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy."
                },
                "metadata": [
                    {
                        "section_title": "II. RELATED WORKS A. STRUCTURED PRUNING",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 442,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 200
                            },
                            {
                                "start": 201,
                                "end": 442
                            }
                        ],
                        "ref_mentions": [
                            "17240902",
                            "53388625"
                        ],
                        "quote": "Unlike unstructured pruning, which removes connections with individual weights [8], (Molchanov et al., 2016), (Frankle et al., 2018), structured pruning removes whole structured components, such as channels or layers from a neural network. Although structured pruning generally has a lower compression rate than unstructured pruning, it provides practical performance improvements on modern GPUs where computations are performed at the tensor level due to computational efficiency."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[275920740 | Sander et al. | 2025 | Citations: 1]",
                "snippets": "While structured pruning offers consistent inference acceleration across hardware, unstructured pruning often achieves higher compression ratios but requires specialized hardware or compiler optimizations to realize speedups.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Pruning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 225,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 225
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "While structured pruning offers consistent inference acceleration across hardware, unstructured pruning often achieves higher compression ratios but requires specialized hardware or compiler optimizations to realize speedups."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[275921475 | Sengupta et al. | 2025 | Citations: 4]",
                "snippets": "Unstructured pruning removes individual weights, as seen in SparseGPT (Frantar & Alistarh, 2023), which leverages Hessian matrix inversion to identify and eliminate less critical weights. However, unstructured pruning often requires hardware-specific optimizations and may not always result in significant computational gains (Yang et al., 2024;Wang et al., 2024b). In contrast, structured pruning removes entire channels or components, making it more compatible with various hardware setups. For example, LLM-Pruner (Ma et al., 2023) evaluates weight group importance and uses LoRA fine-tuning to recover lost accuracy. While structured pruning is more hardware-friendly, it can lead to greater accuracy loss at higher compression ratios.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 210,
                        "end": 950,
                        "sentence_offsets": [
                            {
                                "start": 210,
                                "end": 397
                            },
                            {
                                "start": 398,
                                "end": 575
                            },
                            {
                                "start": 576,
                                "end": 702
                            },
                            {
                                "start": 703,
                                "end": 831
                            },
                            {
                                "start": 832,
                                "end": 950
                            }
                        ],
                        "ref_mentions": [
                            "258823276"
                        ],
                        "quote": "Unstructured pruning removes individual weights, as seen in SparseGPT (Frantar & Alistarh, 2023), which leverages Hessian matrix inversion to identify and eliminate less critical weights. However, unstructured pruning often requires hardware-specific optimizations and may not always result in significant computational gains (Yang et al., 2024;Wang et al., 2024b). In contrast, structured pruning removes entire channels or components, making it more compatible with various hardware setups. For example, LLM-Pruner (Ma et al., 2023) evaluates weight group importance and uses LoRA fine-tuning to recover lost accuracy. While structured pruning is more hardware-friendly, it can lead to greater accuracy loss at higher compression ratios."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[275993741 | Feng et al. | 2025 | Citations: 1]",
                "snippets": "Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar et al., 2023)Sun et al., 2023) and structured pruning (Ma et al., 2023)An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1029,
                        "end": 1560,
                        "sentence_offsets": [
                            {
                                "start": 1029,
                                "end": 1224
                            },
                            {
                                "start": 1225,
                                "end": 1423
                            },
                            {
                                "start": 1424,
                                "end": 1560
                            }
                        ],
                        "ref_mentions": [
                            "255372747",
                            "258823276"
                        ],
                        "quote": "Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar et al., 2023)Sun et al., 2023) and structured pruning (Ma et al., 2023)An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[276259010 | Tang et al. | 2025 | Citations: 3]",
                "snippets": "Unlike unstructured pruning (Frantar & Alistarh, 2023), the model produced by structured pruning can be accelerated on mainstream hardware without any specific design for computation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 591,
                        "end": 774,
                        "sentence_offsets": [
                            {
                                "start": 591,
                                "end": 774
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unlike unstructured pruning (Frantar & Alistarh, 2023), the model produced by structured pruning can be accelerated on mainstream hardware without any specific design for computation."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[276482745 | Qin et al. | 2025 | Citations: 0]",
                "snippets": "Unstructured pruning compresses models by removing individual parameters, resulting in sparse weight matrices that consume less memory. However, without dedicated hardware support, the updated models do not achieve faster inference, thereby still imposing computational burdens during the inference process. Semi-structured pruning offers some speed improvements, but these are limited compared to those achieved by structured pruning. Structured pruning adopts a more modular approach to remove modules from models, typically targeting attention heads, embedding dimensions, FFN intermediate dimensions, experts in Mixture-of-Experts (MoE) networks, or layers. After structured pruning, the weight matrices of the models remain dense, and their reduced dimensions typically lead to greater inference acceleration. However, the coarser granularity of this pruning method makes it more challenging to preserve model capabilities after pruning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 942,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 135
                            },
                            {
                                "start": 136,
                                "end": 307
                            },
                            {
                                "start": 308,
                                "end": 435
                            },
                            {
                                "start": 436,
                                "end": 661
                            },
                            {
                                "start": 662,
                                "end": 814
                            },
                            {
                                "start": 815,
                                "end": 942
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unstructured pruning compresses models by removing individual parameters, resulting in sparse weight matrices that consume less memory. However, without dedicated hardware support, the updated models do not achieve faster inference, thereby still imposing computational burdens during the inference process. Semi-structured pruning offers some speed improvements, but these are limited compared to those achieved by structured pruning. Structured pruning adopts a more modular approach to remove modules from models, typically targeting attention heads, embedding dimensions, FFN intermediate dimensions, experts in Mixture-of-Experts (MoE) networks, or layers. After structured pruning, the weight matrices of the models remain dense, and their reduced dimensions typically lead to greater inference acceleration. However, the coarser granularity of this pruning method makes it more challenging to preserve model capabilities after pruning."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[276576138 | Islam et al. | 2025 | Citations: 0]",
                "snippets": "Structured pruning methods (Chen et al., 2021)(Chen et al., , 2023) ) remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235899080 | Chen et al. | 2021 | Citations: 105]": "Structured pruning is a commonly used technique in deploying deep neural networks (DNNs) onto resource-constrained devices. However, the existing pruning methods are usually heuristic, task-specified, and require an extra fine-tuning procedure. To overcome these limitations, we propose a framework that compresses DNNs into slimmer architectures with competitive performances and significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two keys: (i) we partition the parameters of DNNs into zero-invariant groups, enabling us to prune zero groups without affecting the output; and (ii) to promote zero groups, we then formulate a structured-sparsity optimization problem and propose a novel optimization algorithm, Half-Space Stochastic Projected Gradient (HSPG), to solve it, which outperforms the standard proximal methods on group sparsity exploration and maintains comparable convergence. To demonstrate the effectiveness of OTO, we train and compress full models simultaneously from scratch without fine-tuning for inference speedup and parameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10, ResNet50 for CIFAR10 and Bert for SQuAD and competitive result on ResNet50 for ImageNet. The source code is available at https://github.com/tianyic/only_train_once."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 246,
                        "end": 420,
                        "sentence_offsets": [
                            {
                                "start": 246,
                                "end": 420
                            }
                        ],
                        "ref_mentions": [
                            "235899080"
                        ],
                        "quote": "Structured pruning methods (Chen et al., 2021)(Chen et al., , 2023) ) remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[276902790 | Kong et al. | 2025 | Citations: 0]",
                "snippets": "Unstructured pruning. It performs pruning operations on weights or neurons instead of structured blocks. Although unstructured pruning typically results in higher compression ratios, the need for specific hardware support leads to the failure to achieve proper inference acceleration or storage reduction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model Quantization Model quantization compresses",
                        "pdf_hash": "",
                        "start": 1466,
                        "end": 1771,
                        "sentence_offsets": [
                            {
                                "start": 1464,
                                "end": 1487
                            },
                            {
                                "start": 1488,
                                "end": 1570
                            },
                            {
                                "start": 1571,
                                "end": 1771
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unstructured pruning. It performs pruning operations on weights or neurons instead of structured blocks. Although unstructured pruning typically results in higher compression ratios, the need for specific hardware support leads to the failure to achieve proper inference acceleration or storage reduction."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[277244079 | Lu et al. | 2025 | Citations: 1]",
                "snippets": "However, the irregular sparsification achieved through unstructured pruning often fails to achieve the desired speedup or memory savings. Structured pruning, in contrast, removes entire channels or components from LLMs, making it easier to implement on hardware but potentially leading to significant accuracy degradation, especially under high compression ratios as seen with LLM-Pruner (Ma et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 406,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 137
                            },
                            {
                                "start": 138,
                                "end": 406
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "However, the irregular sparsification achieved through unstructured pruning often fails to achieve the desired speedup or memory savings. Structured pruning, in contrast, removes entire channels or components from LLMs, making it easier to implement on hardware but potentially leading to significant accuracy degradation, especially under high compression ratios as seen with LLM-Pruner (Ma et al., 2023)."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[277275922 | Belhaouari et al. | 2025 | Citations: 1]",
                "snippets": "Structure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility. Furthermore, this method can achieve a different level of sparsity than unstructured pruning, limiting its ability to compress models efficiently without retraining or redesigning the model architecture. On the other hand, unstructured pruning operates at the level of individual weights in a neural network, meaning it can remove any specific weight, regardless of its position. This allows for more fine-grained control over which weights to prune. This can target only the least essential weights, leading to a more refined reduction in model size while retaining critical model capacity. It can achieve higher sparsity levels without compromising model performance because it is not constrained by the rigid structures to which structured pruning must adhere.\n\nDespite preserving model performance, unstructured pruning often results in sparse weight matrices, which may not fully utilize hardware efficiency, as specialized hardware (such as GPUs) is typically optimized for dense matrix operations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204009154 | Wang et al. | 2019 | Citations: 292]": "Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a novel, structured pruning approach based on low rank factorization and augmented Lagrangian L0 norm regularization. Our structured approach achieves significant inference speedups while matching or outperforming our unstructured pruning baseline at various sparsity levels. We apply our method to state of the art models on the enwiki8 dataset and obtain a 1.19 perplexity score with just 5M parameters, vastly outperforming a model of the same size trained from scratch. We also demonstrate that our method can be applied to language model fine-tuning by pruning the BERT model on several downstream classification benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1151,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 150
                            },
                            {
                                "start": 151,
                                "end": 358
                            },
                            {
                                "start": 359,
                                "end": 538
                            },
                            {
                                "start": 539,
                                "end": 609
                            },
                            {
                                "start": 610,
                                "end": 754
                            },
                            {
                                "start": 755,
                                "end": 926
                            },
                            {
                                "start": 929,
                                "end": 1168
                            }
                        ],
                        "ref_mentions": [
                            "204009154"
                        ],
                        "quote": "Structure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility. Furthermore, this method can achieve a different level of sparsity than unstructured pruning, limiting its ability to compress models efficiently without retraining or redesigning the model architecture. On the other hand, unstructured pruning operates at the level of individual weights in a neural network, meaning it can remove any specific weight, regardless of its position. This allows for more fine-grained control over which weights to prune. This can target only the least essential weights, leading to a more refined reduction in model size while retaining critical model capacity. It can achieve higher sparsity levels without compromising model performance because it is not constrained by the rigid structures to which structured pruning must adhere.\n\nDespite preserving model performance, unstructured pruning often results in sparse weight matrices, which may not fully utilize hardware efficiency, as specialized hardware (such as GPUs) is typically optimized for dense matrix operations."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[277452419 | Ma et al. | 2025 | Citations: 1]",
                "snippets": "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]. However, unstructured pruning often results in irregular sparse patterns in the weight matrix, necessitating specialized hardware accelerators (e.g., sparse matrix multiplication units) to efficiently handle sparse matrix computations and fully exploit the benefits of sparsity in terms of storage and computation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259203115 | Sun et al. | 2023 | Citations: 439]": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda."
                },
                "metadata": [
                    {
                        "section_title": "Unstructured Pruning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 925,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 130
                            },
                            {
                                "start": 131,
                                "end": 239
                            },
                            {
                                "start": 240,
                                "end": 610
                            },
                            {
                                "start": 611,
                                "end": 925
                            }
                        ],
                        "ref_mentions": [
                            "259203115"
                        ],
                        "quote": "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]. However, unstructured pruning often results in irregular sparse patterns in the weight matrix, necessitating specialized hardware accelerators (e.g., sparse matrix multiplication units) to efficiently handle sparse matrix computations and fully exploit the benefits of sparsity in terms of storage and computation."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[278033481 | Garg et al. | 2025 | Citations: 2]",
                "snippets": "Structured pruning streamlines LLM by eliminating entire architectural components-such as neurons, filter channels, attention heads, or even layers-instead of removing individual parameters (Ma et al., 2023)[198]. By selectively pruning structurally coherent groups of parameters, this approach yields a smaller yet regularly structured model-maintaining dense connectivity while reducing inference costs, for instance, LLM-Pruner (Ma et al., 2023). Unstructured pruning removes individual weights from LLM without considering any specific structure within the model (Frantar et al., 2023). Unstructured pruning removes less important weights based on a threshold, resulting in a sparse model where the architecture remains unchanged, as long as enough weights stay to maintain performance. Unstructured pruning can deliver higher accuracy for a given compression ratio by allowing more targeted weight removal. However, its irregular sparsity pattern leads to inefficient memory access and limited speed-ups on standard hardware. To fully leverage the compression benefits, specialized libraries or hardware are needed, and even then, acceleration gains are modest unless the model is highly sparse. Post unstructured pruning, significant retraining or fine-tuning is often required to recover lost accuracy that can be resource-intensive.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"
                },
                "metadata": [
                    {
                        "section_title": "Pruning",
                        "pdf_hash": "",
                        "start": 637,
                        "end": 1934,
                        "sentence_offsets": [
                            {
                                "start": 637,
                                "end": 837
                            },
                            {
                                "start": 838,
                                "end": 1061
                            },
                            {
                                "start": 1062,
                                "end": 1184
                            },
                            {
                                "start": 1185,
                                "end": 1384
                            },
                            {
                                "start": 1385,
                                "end": 1505
                            },
                            {
                                "start": 1506,
                                "end": 1624
                            },
                            {
                                "start": 1625,
                                "end": 1794
                            },
                            {
                                "start": 1795,
                                "end": 1934
                            }
                        ],
                        "ref_mentions": [
                            "258823276",
                            "258823276",
                            "255372747"
                        ],
                        "quote": "Structured pruning streamlines LLM by eliminating entire architectural components-such as neurons, filter channels, attention heads, or even layers-instead of removing individual parameters (Ma et al., 2023)[198]. By selectively pruning structurally coherent groups of parameters, this approach yields a smaller yet regularly structured model-maintaining dense connectivity while reducing inference costs, for instance, LLM-Pruner (Ma et al., 2023). Unstructured pruning removes individual weights from LLM without considering any specific structure within the model (Frantar et al., 2023). Unstructured pruning removes less important weights based on a threshold, resulting in a sparse model where the architecture remains unchanged, as long as enough weights stay to maintain performance. Unstructured pruning can deliver higher accuracy for a given compression ratio by allowing more targeted weight removal. However, its irregular sparsity pattern leads to inefficient memory access and limited speed-ups on standard hardware. To fully leverage the compression benefits, specialized libraries or hardware are needed, and even then, acceleration gains are modest unless the model is highly sparse. Post unstructured pruning, significant retraining or fine-tuning is often required to recover lost accuracy that can be resource-intensive."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[278208127 | Mugnaini et al. | 2025 | Citations: 1]",
                "snippets": "Within the field of LLMs, pruning techniques fall into three main categories: structured, semistructured and unstructured pruning.\n\nStructured pruning removes entire components -such as attention heads or layers -while preserving the overall network structure without introducing sparsity (i.e., without zeroing out a significant portion of the model's parameters) [6]. However, the removal of larger and potentially more critical components may result in performance degradation, typically requiring Parameter-Efficient Fine-Tuning (PEFT) techniques for performance recovery [14]. Due to the removal of complete components, structured pruning usually achieves inference acceleration and memory reduction without the need for specialized hardware or software [15].\n\nSemi-structured (a.k.a. structured N : M ) pruning promotes model sparsity by removing groups of consecutive parameters following a pruning mask [16]. Specifically, structured N : M sparsity requires that at least N out of every M consecutive weights be non-zero [17], [18]. While this is a promising technique, it requires specialized hardware to achieve practical speedup, making it less suitable for deployment on consumergrade GPUs [17].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Within the field of LLMs, pruning techniques fall into three main categories: structured, semistructured and unstructured pruning.\n\nStructured pruning removes entire components -such as attention heads or layers -while preserving the overall network structure without introducing sparsity (i.e., without zeroing out a significant portion of the model's parameters) [6]. However, the removal of larger and potentially more critical components may result in performance degradation, typically requiring Parameter-Efficient Fine-Tuning (PEFT) techniques for performance recovery [14]. Due to the removal of complete components, structured pruning usually achieves inference acceleration and memory reduction without the need for specialized hardware or software [15].\n\nSemi-structured (a.k.a. structured N : M ) pruning promotes model sparsity by removing groups of consecutive parameters following a pruning mask [16]. Specifically, structured N : M sparsity requires that at least N out of every M consecutive weights be non-zero [17], [18]. While this is a promising technique, it requires specialized hardware to achieve practical speedup, making it less suitable for deployment on consumergrade GPUs [17].",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[278310893 | Sengupta et al. | 2025 | Citations: 0]",
                "snippets": "Unstructured pruning (Sun et al., 2023) removes individual weights, producing sparse matrices that maintain performance but are less hardwareefficient. Semi-structured pruning (Frantar & Alistarh, 2023), such as the 2:4 sparsity pattern (Pool et al., 2021), introduces a hardware-friendly structured sparsity that accelerates computation. Structured pruning (Ashkboos et al., 2024;Yuan et al., 2023;Sengupta et al., 2025) takes a broader approach by removing entire components, such as Transformer layers (depth pruning) (Fan et al., 2019) or reducing embedding dimensions and attention heads (width pruning) (Zhu et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model Downscaling",
                        "pdf_hash": "",
                        "start": 137,
                        "end": 765,
                        "sentence_offsets": [
                            {
                                "start": 137,
                                "end": 288
                            },
                            {
                                "start": 289,
                                "end": 475
                            },
                            {
                                "start": 476,
                                "end": 765
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unstructured pruning (Sun et al., 2023) removes individual weights, producing sparse matrices that maintain performance but are less hardwareefficient. Semi-structured pruning (Frantar & Alistarh, 2023), such as the 2:4 sparsity pattern (Pool et al., 2021), introduces a hardware-friendly structured sparsity that accelerates computation. Structured pruning (Ashkboos et al., 2024;Yuan et al., 2023;Sengupta et al., 2025) takes a broader approach by removing entire components, such as Transformer layers (depth pruning) (Fan et al., 2019) or reducing embedding dimensions and attention heads (width pruning) (Zhu et al., 2021)."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[278327238 | Sun et al. | 2025 | Citations: 3]",
                "snippets": "Recent advances in pruning have progressed from unstructured sparsity (i.e., individual weight removal) to structured sparsity (i.e., eliminating entire neurons, heads or layers), thereby enabling hardware-efficient implementation [4; 8; 9; 10].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1240,
                        "end": 1485,
                        "sentence_offsets": [
                            {
                                "start": 1240,
                                "end": 1485
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent advances in pruning have progressed from unstructured sparsity (i.e., individual weight removal) to structured sparsity (i.e., eliminating entire neurons, heads or layers), thereby enabling hardware-efficient implementation [4; 8; 9; 10]."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[278338989 | Hu et al. | 2025 | Citations: 0]",
                "snippets": "While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware or sparse computations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware or sparse computations.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[278367624 | Liang et al. | 2025 | Citations: 0]",
                "snippets": "Pruning boosts model compactness at the architectural level by removing redundant parameters, ranging from individual weights (unstructured pruning (Shao et al., 2023)- (Xu et al., 2024)) to entire channels or layers (structured pruning (Ma et al., 2023)- (Li et al., 2023)). Although unstructured pruning can achieve significant compression ratios, the resulting irregular sparsity is not conducive to hardware implementation (Molchanov et al., 2016). In contrast, structured pruning is more compatible with hardware acceleration but often results in model accuracy degradation and limited sparsity (He et al., 2017). To balance model accuracy and hardware efficiency, N :M semi-structured pruning (Frantar et al., 2023), (Sun et al., 2023), where N out of every M elements are pruned, is commonly adopted in prevalent LLMs (Zeng et al., 2024), (Mishra et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[16167970 | Molchanov et al. | 2016 | Citations: 418]": "We propose a new framework for pruning convolutional kernels in neural networks to enable efficient inference, focusing on transfer learning where large and potentially unwieldy pretrained networks are adapted to specialized tasks. We interleave greedy criteria-based pruning with fine-tuning by backpropagation - a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on an efficient first-order Taylor expansion to approximate the absolute change in training cost induced by pruning a network component. After normalization, the proposed criterion scales appropriately across all layers of a deep CNN, eliminating the need for per-layer sensitivity analysis. The proposed criterion demonstrates superior performance compared to other criteria, such as the norm of kernel weights or average feature map activation.",
                    "[20157893 | He et al. | 2017 | Citations: 2529]": "In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks. Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5\u00d7 speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2\u00d7 speedup respectively, which is significant.",
                    "[233296249 | Mishra et al. | 2021 | Citations: 235]": "As neural network model sizes have dramatically increased, so has the interest in various techniques to reduce their parameter counts and accelerate their execution. An active area of research in this field is sparsity - encouraging zero values in parameters that can then be discarded from storage or computations. While most research focuses on high levels of sparsity, there are challenges in universally maintaining model accuracy as well as achieving significant speedups over modern matrix-math hardware. To make sparsity adoption practical, the NVIDIA Ampere GPU architecture introduces sparsity support in its matrix-math units, Tensor Cores. We present the design and behavior of Sparse Tensor Cores, which exploit a 2:4 (50%) sparsity pattern that leads to twice the math throughput of dense matrix units. We also describe a simple workflow for training networks that both satisfy 2:4 sparsity pattern requirements and maintain accuracy, verifying it on a wide range of common tasks and model architectures. This workflow makes it easy to prepare accurate models for efficient deployment on Sparse Tensor Cores.",
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",
                    "[259203115 | Sun et al. | 2023 | Citations: 439]": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
                    "[259203385 | Li et al. | 2023 | Citations: 76]": "Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compression methods.",
                    "[264146174 | Shao et al. | 2023 | Citations: 21]": "Various Large Language Models (LLMs) from the Generative Pretrained Transformer (GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.",
                    "[266844224 | Zeng et al. | 2024 | Citations: 63]": "Transformer-based Large Language Models (LLMs) have made a significant impact on various domains. However, LLMs' efficiency suffers from both heavy computation and memory overheads. Compression techniques like sparsification and quantization are commonly used to mitigate the gap between LLM's computation/memory overheads and hardware capacity. However, existing GPU and transformer-based accelerators cannot efficiently process compressed LLMs, due to the following unresolved challenges: low computational efficiency, underutilized memory bandwidth, and large compilation overheads. This paper proposes FlightLLM, enabling efficient LLMs inference with a complete mapping flow on FPGAs. In FlightLLM, we highlight an innovative solution that the computation and memory overhead of LLMs can be solved by utilizing FPGA-specific resources (e.g., DSP48 and heterogeneous memory hierarchy). We propose a configurable sparse DSP chain to support different sparsity patterns with high computation efficiency. Second, we propose an always-on-chip decode scheme to boost memory bandwidth with mixed-precision support. Finally, to make FlightLLM available for real-world LLMs, we propose a length adaptive compilation method to reduce the compilation overhead. Implemented on the Xilinx Alveo U280 FPGA, FlightLLM achieves 6.0\u00d7 higher energy efficiency and 1.8\u00d7 better cost efficiency against commercial GPUs (e.g., NVIDIA V100S) on modern LLMs (e.g., LLaMA2-7B) using vLLM and SmoothQuant under the batch size of one. FlightLLM beats NVIDIA A100 GPU with 1.2\u00d7 higher throughput using the latest Versal VHK158 FPGA.",
                    "[268032346 | Xu et al. | 2024 | Citations: 32]": "Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available at https://github.com/OpenGVLab/LLMPrune-BESA."
                },
                "metadata": [
                    {
                        "section_title": "A. Pruning for LLMs",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 717,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 221
                            },
                            {
                                "start": 222,
                                "end": 378
                            },
                            {
                                "start": 379,
                                "end": 531
                            },
                            {
                                "start": 532,
                                "end": 717
                            }
                        ],
                        "ref_mentions": [
                            "264146174",
                            "268032346",
                            "258823276",
                            "259203385",
                            "16167970",
                            "20157893",
                            "255372747",
                            "259203115",
                            "266844224",
                            "233296249"
                        ],
                        "quote": "Pruning boosts model compactness at the architectural level by removing redundant parameters, ranging from individual weights (unstructured pruning (Shao et al., 2023)- (Xu et al., 2024)) to entire channels or layers (structured pruning (Ma et al., 2023)- (Li et al., 2023)). Although unstructured pruning can achieve significant compression ratios, the resulting irregular sparsity is not conducive to hardware implementation (Molchanov et al., 2016). In contrast, structured pruning is more compatible with hardware acceleration but often results in model accuracy degradation and limited sparsity (He et al., 2017). To balance model accuracy and hardware efficiency, N :M semi-structured pruning (Frantar et al., 2023), (Sun et al., 2023), where N out of every M elements are pruned, is commonly adopted in prevalent LLMs (Zeng et al., 2024), (Mishra et al., 2021)."
                    }
                ]
            },
            {
                "idx": 41,
                "key": "[278501529 | Laborde et al. | 2025 | Citations: 0]",
                "snippets": "Unstructured pruning offers maximum theoretical compression by removing individual weights, but often results in irregular sparsity patterns that are challenging to accelerate on current hardware. Semi-structured approaches, like N:M sparsity patterns (Zhou et al., 2021), balance compression rates with hardware efficiency by enforcing regular pruning patterns, where N out of every M consecutive weights are pruned. Structured pruning (Ma et al., 2023) takes this further by removing entire structures, channels, or attention heads, with recent work showing that up to 50% of attention layers in large models can be removed while preserving performance [12].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231847094 | Zhou et al. | 2021 | Citations: 248]": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.",
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"
                },
                "metadata": [
                    {
                        "section_title": "A. Pruning",
                        "pdf_hash": "",
                        "start": 272,
                        "end": 904,
                        "sentence_offsets": [
                            {
                                "start": 272,
                                "end": 468
                            },
                            {
                                "start": 469,
                                "end": 674
                            },
                            {
                                "start": 675,
                                "end": 904
                            }
                        ],
                        "ref_mentions": [
                            "231847094",
                            "258823276"
                        ],
                        "quote": "Unstructured pruning offers maximum theoretical compression by removing individual weights, but often results in irregular sparsity patterns that are challenging to accelerate on current hardware. Semi-structured approaches, like N:M sparsity patterns (Zhou et al., 2021), balance compression rates with hardware efficiency by enforcing regular pruning patterns, where N out of every M consecutive weights are pruned. Structured pruning (Ma et al., 2023) takes this further by removing entire structures, channels, or attention heads, with recent work showing that up to 50% of attention layers in large models can be removed while preserving performance [12]."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.039888,
        "cot": "I need to organize these quotes into meaningful dimensions that directly address the query about differences between unstructured and structured pruning methods for LLMs in terms of compression ratio, performance impact, and hardware compatibility.\n\nFirst, I'll outline the key dimensions needed to comprehensively answer this query:\n\n1. Introduction/Background: This section will provide a basic overview of pruning in LLMs and introduce the two main approaches (unstructured and structured pruning). This will help establish context for readers. This should be a synthesis paragraph to provide a cohesive foundation.\n\n2. Compression Ratio Comparison: This dimension will focus on how unstructured and structured pruning methods differ in their ability to compress models. Based on the quotes, unstructured pruning generally achieves higher compression ratios than structured pruning. This should be a synthesis paragraph to properly compare the methods.\n\n3. Performance Impact: This dimension will address how each pruning method affects model accuracy and performance. The quotes suggest unstructured pruning generally preserves model accuracy better than structured pruning at equivalent compression rates. This should be a synthesis paragraph to clearly analyze the nuanced performance differences.\n\n4. Hardware Compatibility and Acceleration: This dimension will focus on how each pruning method works with existing hardware. This is a major differentiator, as structured pruning is more hardware-friendly while unstructured pruning often requires specialized hardware/software. This should be a synthesis paragraph to properly explain the technical differences.\n\n5. Semi-Structured Pruning: Several quotes mention \"semi-structured\" pruning as a middle ground between the two main approaches. This dimension will explain this hybrid approach and how it balances the trade-offs. This should be a synthesis paragraph to properly explain this important compromise approach.\n\n6. Recent Advancements: This dimension will highlight some of the most recent methods mentioned in the quotes for both structured and unstructured pruning. This should be a list format to clearly enumerate different recent techniques.\n\nNow I'll assign quotes to each dimension:",
        "plan": {
            "Introduction to Pruning Methods for LLMs (synthesis)": [
                2,
                9,
                18,
                35
            ],
            "Compression Ratio Comparison (synthesis)": [
                0,
                1,
                3,
                4,
                7,
                8,
                11,
                12,
                13,
                22,
                23,
                25,
                26,
                32,
                33,
                34,
                41
            ],
            "Performance Impact on Model Accuracy (synthesis)": [
                0,
                2,
                3,
                6,
                11,
                12,
                13,
                17,
                21,
                23,
                24,
                26,
                27,
                29,
                32,
                33,
                35,
                36
            ],
            "Hardware Compatibility and Acceleration (synthesis)": [
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                8,
                10,
                11,
                13,
                14,
                16,
                17,
                19,
                20,
                21,
                22,
                24,
                25,
                27,
                28,
                29,
                30,
                31,
                35,
                36,
                37,
                38,
                39,
                40
            ],
            "Semi-Structured Pruning as a Middle Ground (synthesis)": [
                12,
                13,
                15,
                16,
                20,
                22,
                36,
                37,
                39,
                40,
                41
            ],
            "Recent Methods and Techniques (list)": [
                5,
                6,
                10,
                15,
                17,
                19,
                20,
                22,
                26,
                27,
                29,
                32,
                37,
                39
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Pruning Methods for LLMs",
                "tldr": "Pruning methods for large language models fall into two main categories: structured pruning, which removes entire architectural components, and unstructured pruning, which eliminates individual weights. These approaches offer different trade-offs in terms of compression, performance, and hardware compatibility. (10 sources)",
                "text": "\nThe exponential growth in size of large language models (LLMs) has created significant challenges for their deployment in resource-constrained environments. Model compression techniques have emerged as essential solutions to this problem, with pruning being one of the primary approaches alongside quantization and distillation <Paper corpusId=\"256662263\" paperTitle=\"(Kurtic et al., 2023)\" isShortName></Paper> <Paper corpusId=\"231740691\" paperTitle=\"(Hoefler et al., 2021)\" isShortName></Paper> <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>.\n\nPruning methods for LLMs can be broadly categorized into two main approaches:\n\n1. **Structured pruning** removes entire architectural components such as neurons, attention heads, or even complete layers from the model <Paper corpusId=\"267751193\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247922354\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>. This approach maintains a regular, dense structure in the resulting model, which makes it more deployment-friendly and hardware-compatible <Paper corpusId=\"272693912\" paperTitle=\"(Lv et al., 2024)\" isShortName></Paper>. A key advantage of structured pruning is that the model can be physically reshaped to new dimensions, allowing computational savings to be leveraged on standard hardware without specialized support <Paper corpusId=\"256662263\" paperTitle=\"(Kurtic et al., 2023)\" isShortName></Paper>. However, models are typically highly sensitive to structured compression, often requiring gradual compression and retraining cycles to recover accuracy.\n\n2. **Unstructured pruning** targets individual weights within the model without considering any specific structural patterns <Paper corpusId=\"267751193\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258865530\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This approach can achieve higher accuracy for a given compression ratio by enabling more targeted weight removal <Paper corpusId=\"278033481\" paperTitle=\"(Garg et al., 2025)\" isShortName></Paper>. Recent work has shown that even massive GPT-family models can be pruned to at least 50% sparsity in one shot without retraining, with minimal accuracy loss <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. However, the resulting irregular sparsity patterns lead to inefficient memory access and limited speed-ups on standard hardware, often requiring specialized libraries or hardware to fully leverage the compression benefits <Paper corpusId=\"278033481\" paperTitle=\"(Garg et al., 2025)\" isShortName></Paper>.\n\nThe fundamental trade-off between these approaches lies in their practical applicability: structured pruning delivers more immediate hardware efficiency but may sacrifice more accuracy, while unstructured pruning can retain more accuracy but requires specialized hardware support to realize performance gains <Paper corpusId=\"278033481\" paperTitle=\"(Garg et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Kurtic et al., 2023)",
                        "snippets": [
                            "The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning (Hoefler et al., 2021), quantization [3], and distillation (Gou et al., 2020). In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy."
                        ],
                        "paper": {
                            "corpus_id": 256662263,
                            "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
                            "authors": [
                                {
                                    "authorId": "40992614",
                                    "name": "Eldar Kurtic"
                                },
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 26
                        },
                        "score": 0.90576171875
                    },
                    {
                        "id": "(Hoefler et al., 2021)",
                        "snippets": [
                            "The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field."
                        ],
                        "paper": {
                            "corpus_id": 231740691,
                            "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks",
                            "authors": [
                                {
                                    "authorId": "1713648",
                                    "name": "T. Hoefler"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                },
                                {
                                    "authorId": "1402921119",
                                    "name": "Tal Ben-Nun"
                                },
                                {
                                    "authorId": "2134146",
                                    "name": "Nikoli Dryden"
                                },
                                {
                                    "authorId": "3341722",
                                    "name": "Alexandra Peste"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of machine learning research",
                            "n_citations": 725
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gou et al., 2020)",
                        "snippets": [
                            "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                        ],
                        "paper": {
                            "corpus_id": 219559263,
                            "title": "Knowledge Distillation: A Survey",
                            "authors": [
                                {
                                    "authorId": "38978232",
                                    "name": "Jianping Gou"
                                },
                                {
                                    "authorId": "2425630",
                                    "name": "B. Yu"
                                },
                                {
                                    "authorId": "144555237",
                                    "name": "S. Maybank"
                                },
                                {
                                    "authorId": "143719920",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Journal of Computer Vision",
                            "n_citations": 2984
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yuan et al., 2024)",
                        "snippets": [
                            "Pruning, a prevalent technique for model compression method, seeks to discard non-essential parameters within language models, thereby reducing computational and storage costs while maintaining performance (LeCun et al., 1989), (Wang et al., 2023), (Xia et al., 2022). Pruning methods can be broadly categorized into unstructured pruning, which removes individual weights, and structured pruning, which removes higher-granularity structures such as neurons, attention heads, or entire layers."
                        ],
                        "paper": {
                            "corpus_id": 267751193,
                            "title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers",
                            "authors": [
                                {
                                    "authorId": "2201687496",
                                    "name": "Shuzhou Yuan"
                                },
                                {
                                    "authorId": "2197254657",
                                    "name": "Ercong Nie"
                                },
                                {
                                    "authorId": "2188764477",
                                    "name": "Bolei Ma"
                                },
                                {
                                    "authorId": "2281825175",
                                    "name": "Michael Farber"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.7646484375
                    },
                    {
                        "id": "(Xia et al., 2022)",
                        "snippets": [
                            "The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches."
                        ],
                        "paper": {
                            "corpus_id": 247922354,
                            "title": "Structured Pruning Learns Compact and Accurate Models",
                            "authors": [
                                {
                                    "authorId": "67284811",
                                    "name": "Mengzhou Xia"
                                },
                                {
                                    "authorId": "49164966",
                                    "name": "Zexuan Zhong"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 187
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lv et al., 2024)",
                        "snippets": [
                            "Currently, optimization methods for large models include pruning (structured pruning [17]-[19] and unstructured pruning [20], [21]), quantization [22]-[24], and distillation [25], [26]. This work focuses on structured pruning, making it more deployment-friendly and hardware-friendly."
                        ],
                        "paper": {
                            "corpus_id": 272693912,
                            "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2321454694",
                                    "name": "Bo Lv"
                                },
                                {
                                    "authorId": "2321495086",
                                    "name": "Quan Zhou"
                                },
                                {
                                    "authorId": "2216237674",
                                    "name": "Xuanang Ding"
                                },
                                {
                                    "authorId": "2321964263",
                                    "name": "Yan Wang"
                                },
                                {
                                    "authorId": "2321489070",
                                    "name": "Zeming Ma"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                            "n_citations": 2
                        },
                        "score": 0.76953125
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Recently, various intermediate layer distillation (ILD) objectives have been shown to improve compression of BERT models via Knowledge Distillation (KD). However, a comprehensive evaluation of the objectives in both task-specific and task-agnostic settings is lacking. To the best of our knowledge, this is the first work comprehensively evaluating distillation objectives in both settings. We show that attention transfer gives the best performance overall. We also study the impact of layer choice when initializing the student from the teacher layers, finding a significant impact on the performance in task-specific distillation. For vanilla KD and hidden states transfer, initialisation with lower layers of the teacher gives a considerable improvement over higher layers, especially on the task of QNLI (up to an absolute percentage change of 17.8 in accuracy). Attention transfer behaves consistently under different initialisation settings. We release our code as an efficient transformer-based model distillation framework for further studies."
                        ],
                        "paper": {
                            "corpus_id": 258865530,
                            "title": "How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives",
                            "authors": [
                                {
                                    "authorId": "2108254820",
                                    "name": "Xinpeng Wang"
                                },
                                {
                                    "authorId": "31832571",
                                    "name": "Leonie Weissweiler"
                                },
                                {
                                    "authorId": "2130001188",
                                    "name": "Hinrich Schutze"
                                },
                                {
                                    "authorId": "2065013761",
                                    "name": "Barbara Plank"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0
                    },
                    {
                        "id": "(Garg et al., 2025)",
                        "snippets": [
                            "Structured pruning streamlines LLM by eliminating entire architectural components-such as neurons, filter channels, attention heads, or even layers-instead of removing individual parameters (Ma et al., 2023)[198]. By selectively pruning structurally coherent groups of parameters, this approach yields a smaller yet regularly structured model-maintaining dense connectivity while reducing inference costs, for instance, LLM-Pruner (Ma et al., 2023). Unstructured pruning removes individual weights from LLM without considering any specific structure within the model (Frantar et al., 2023). Unstructured pruning removes less important weights based on a threshold, resulting in a sparse model where the architecture remains unchanged, as long as enough weights stay to maintain performance. Unstructured pruning can deliver higher accuracy for a given compression ratio by allowing more targeted weight removal. However, its irregular sparsity pattern leads to inefficient memory access and limited speed-ups on standard hardware. To fully leverage the compression benefits, specialized libraries or hardware are needed, and even then, acceleration gains are modest unless the model is highly sparse. Post unstructured pruning, significant retraining or fine-tuning is often required to recover lost accuracy that can be resource-intensive."
                        ],
                        "paper": {
                            "corpus_id": 278033481,
                            "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey",
                            "authors": [
                                {
                                    "authorId": "2258141722",
                                    "name": "Muskan Garg"
                                },
                                {
                                    "authorId": "2278330619",
                                    "name": "Shaina Raza"
                                },
                                {
                                    "authorId": "3023076",
                                    "name": "Shebuti Rayana"
                                },
                                {
                                    "authorId": "2278394763",
                                    "name": "Xingyi Liu"
                                },
                                {
                                    "authorId": "2267490593",
                                    "name": "Sunghwan Sohn"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.822265625
                    },
                    {
                        "id": "(Frantar et al., 2023)",
                        "snippets": [
                            "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                        ],
                        "paper": {
                            "corpus_id": 255372747,
                            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
                            "authors": [
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 734
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ma et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"
                        ],
                        "paper": {
                            "corpus_id": 258823276,
                            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "15532066",
                                    "name": "Xinyin Ma"
                                },
                                {
                                    "authorId": "150110431",
                                    "name": "Gongfan Fang"
                                },
                                {
                                    "authorId": "48631088",
                                    "name": "Xinchao Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 440
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Compression Ratio Comparison",
                "tldr": "Unstructured pruning achieves higher compression ratios with less performance degradation compared to structured pruning, but the resulting sparse matrices are difficult to accelerate without specialized hardware. Structured pruning yields lower compression rates but delivers immediate hardware efficiency gains. (11 sources)",
                "text": "\nUnstructured pruning consistently achieves higher compression ratios while maintaining model performance compared to structured pruning. This approach can reach extreme sparsity levels\u2014up to 60-70% on large language models like LLaMA-7B with minimal performance degradation <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper> and even 97% compression on some BERT models with small accuracy drops <Paper corpusId=\"245131431\" paperTitle=\"(Xu et al., 2021)\" isShortName></Paper>. The superior compression capability stems from unstructured pruning's ability to selectively remove only the least important individual weights while preserving critical ones <Paper corpusId=\"277275922\" paperTitle=\"(Belhaouari et al., 2025)\" isShortName></Paper>.\n\nIn contrast, structured pruning typically achieves lower compression ratios before significant performance degradation occurs. This approach faces inherent limitations because it removes entire components (neurons, attention heads, or layers), which may include both essential and non-essential weights <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper>. As a result, structured pruning tends to cause higher accuracy drops than unstructured pruning at equivalent compression levels <Paper corpusId=\"247794014\" paperTitle=\"(Yang et al., 2022)\" isShortName></Paper>.\n\nRecent research illustrates this gap in compression capabilities. For example, SparseGPT has demonstrated the ability to compress LLMs with billions of parameters by up to 60% with negligible performance impact using unstructured techniques <Paper corpusId=\"267412232\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Similarly, Wanda achieves 60% sparsity on LLaMA-7B with minimal performance degradation across multiple downstream tasks <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper> <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. These unstructured pruning methods consistently outperform structured approaches in terms of raw compression potential.\n\nThe fundamental challenge with unstructured pruning, however, is that the resulting irregular sparse matrices do not translate to practical efficiency gains without specialized hardware or software support <Paper corpusId=\"256662734\" paperTitle=\"(Santacroce et al., 2023)\" isShortName></Paper>. In fact, directly applying sparse kernels to unstructured pruned models can lead to significant slowdowns\u2014up to 60\u00d7 compared to dense kernels on GPUs <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper>. This represents the central trade-off in pruning approaches: unstructured pruning yields higher theoretical compression but requires specialized implementations to realize practical benefits, while structured pruning provides immediate efficiency gains but with more limited compression potential <Paper corpusId=\"275920740\" paperTitle=\"(Sander et al., 2025)\" isShortName></Paper>.\n\nSemi-structured approaches, like N:M sparsity patterns where N out of every M consecutive weights are pruned, attempt to strike a balance between the two extremes by enforcing regular pruning patterns that can be more efficiently implemented in hardware <Paper corpusId=\"278501529\" paperTitle=\"(Laborde et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231847094\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>. For example, 2:4 sparse networks can achieve 2\u00d7 speedup without performance drops on specialized GPUs like NVIDIA A100s <Paper corpusId=\"231847094\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda."
                        ],
                        "paper": {
                            "corpus_id": 259203115,
                            "title": "A Simple and Effective Pruning Approach for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2984183",
                                    "name": "Mingjie Sun"
                                },
                                {
                                    "authorId": "2109168016",
                                    "name": "Zhuang Liu"
                                },
                                {
                                    "authorId": "25901845",
                                    "name": "Anna Bair"
                                },
                                {
                                    "authorId": "145116464",
                                    "name": "J. Z. Kolter"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 439
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2021)",
                        "snippets": [
                            "Pre-trained Language Models (PLMs) have achieved great success in various Natural Language Processing (NLP) tasks under the pre-training and fine-tuning paradigm. \n With large quantities of parameters, PLMs are computation-intensive and resource-hungry. Hence, model pruning has been introduced to compress large-scale PLMs. \n However, most prior approaches only consider task-specific knowledge towards downstream tasks, but ignore the essential task-agnostic knowledge during pruning, which may cause catastrophic forgetting problem and lead to poor generalization ability. \n To maintain both task-agnostic and task-specific knowledge in our pruned model, we propose ContrAstive Pruning (CAP) under the paradigm of pre-training and fine-tuning. \n It is designed as a general framework, compatible with both structured and unstructured pruning. \n Unified in contrastive learn- ing, CAP enables the pruned model to learn from the pre-trained model for task-agnostic knowledge, and fine-tuned model for task-specific knowledge. \n Besides, to better retain the performance of the pruned model, the snapshots (i.e., the intermediate models at each pruning iteration) also serve as effective supervisions for pruning. \n Our extensive experiments show that adopting CAP consistently yields significant improvements, especially in extremely high sparsity scenarios. \n With only 3% model parameters reserved (i.e., 97% sparsity), CAP successfully achieves 99.2% and 96.3% of the original BERT performance in QQP and MNLI tasks. \n In addition, our probing experiments demonstrate that the model pruned by CAP tends to achieve better generalization ability."
                        ],
                        "paper": {
                            "corpus_id": 245131431,
                            "title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression",
                            "authors": [
                                {
                                    "authorId": "1748844142",
                                    "name": "Runxin Xu"
                                },
                                {
                                    "authorId": "2140495101",
                                    "name": "Fuli Luo"
                                },
                                {
                                    "authorId": "121899912",
                                    "name": "Chengyu Wang"
                                },
                                {
                                    "authorId": "7267809",
                                    "name": "Baobao Chang"
                                },
                                {
                                    "authorId": "2078113",
                                    "name": "Jun Huang"
                                },
                                {
                                    "authorId": "2410938",
                                    "name": "Songfang Huang"
                                },
                                {
                                    "authorId": "143857288",
                                    "name": "Fei Huang"
                                }
                            ],
                            "year": 2021,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 26
                        },
                        "score": 0
                    },
                    {
                        "id": "(Belhaouari et al., 2025)",
                        "snippets": [
                            "Structure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility. Furthermore, this method can achieve a different level of sparsity than unstructured pruning, limiting its ability to compress models efficiently without retraining or redesigning the model architecture. On the other hand, unstructured pruning operates at the level of individual weights in a neural network, meaning it can remove any specific weight, regardless of its position. This allows for more fine-grained control over which weights to prune. This can target only the least essential weights, leading to a more refined reduction in model size while retaining critical model capacity. It can achieve higher sparsity levels without compromising model performance because it is not constrained by the rigid structures to which structured pruning must adhere.\n\nDespite preserving model performance, unstructured pruning often results in sparse weight matrices, which may not fully utilize hardware efficiency, as specialized hardware (such as GPUs) is typically optimized for dense matrix operations."
                        ],
                        "paper": {
                            "corpus_id": 277275922,
                            "title": "Efficient self-attention with smart pruning for sustainable large language models",
                            "authors": [
                                {
                                    "authorId": "102804035",
                                    "name": "S. Belhaouari"
                                },
                                {
                                    "authorId": "2292003273",
                                    "name": "Insaf Kraidia"
                                }
                            ],
                            "year": 2025,
                            "venue": "Scientific Reports",
                            "n_citations": 1
                        },
                        "score": 0.96142578125
                    },
                    {
                        "id": "(Zhao et al., 2022)",
                        "snippets": [
                            "Unstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al., 2018)), e.g, (Hill et al., 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 246276158,
                            "title": "Iterative Activation-based Structured Pruning",
                            "authors": [
                                {
                                    "authorId": "1995855",
                                    "name": "Kaiqi Zhao"
                                },
                                {
                                    "authorId": "101682296",
                                    "name": "Animesh Jain"
                                },
                                {
                                    "authorId": "2152527896",
                                    "name": "Ming Zhao"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.89111328125
                    },
                    {
                        "id": "(Yang et al., 2022)",
                        "snippets": [
                            "In the unstructured pruning, each model parameter is individually removed if it reaches some criteria based on the magnitude or importance score (Han et al., 2015;(Zhu et al., 2017). The unstructured pruning results in sparse matrices and allows for significant model compression, but the inference speed can hardly be improved without specialized devices. While in the structured pruning, rows or columns of the parameters are removed from the weight matrices (McCarley, 2019;(Michel et al., 2019)(Voita et al., 2019)(Lagunas et al., 2021)(Hou et al., 2020). Thus, the resulting model speeds up on the common CPU and GPU devices."
                        ],
                        "paper": {
                            "corpus_id": 247794014,
                            "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models",
                            "authors": [
                                {
                                    "authorId": "48599077",
                                    "name": "Ziqing Yang"
                                },
                                {
                                    "authorId": "3043830",
                                    "name": "Yiming Cui"
                                },
                                {
                                    "authorId": "2156610145",
                                    "name": "Zhigang Chen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 12
                        },
                        "score": 0.83837890625
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters.\n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models."
                        ],
                        "paper": {
                            "corpus_id": 267412232,
                            "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2108048327",
                                    "name": "Xindi Wang"
                                },
                                {
                                    "authorId": "1904419",
                                    "name": "Mahsa Salmani"
                                },
                                {
                                    "authorId": "2282534833",
                                    "name": "Parsa Omidi"
                                },
                                {
                                    "authorId": "2283447900",
                                    "name": "Xiangyu Ren"
                                },
                                {
                                    "authorId": "2066076226",
                                    "name": "Mehdi Rezagholizadeh"
                                },
                                {
                                    "authorId": "50782111",
                                    "name": "A. Eshaghi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 45
                        },
                        "score": 0.83740234375
                    },
                    {
                        "id": "(Ma et al., 2025)",
                        "snippets": [
                            "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]. However, unstructured pruning often results in irregular sparse patterns in the weight matrix, necessitating specialized hardware accelerators (e.g., sparse matrix multiplication units) to efficiently handle sparse matrix computations and fully exploit the benefits of sparsity in terms of storage and computation."
                        ],
                        "paper": {
                            "corpus_id": 277452419,
                            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2352948034",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2274202084",
                                    "name": "Zuchao Li"
                                },
                                {
                                    "authorId": "2269488794",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2343636012",
                                    "name": "Gui-Song Xia"
                                },
                                {
                                    "authorId": "2306994733",
                                    "name": "Bo Du"
                                },
                                {
                                    "authorId": "2268745050",
                                    "name": "Liangpei Zhang"
                                },
                                {
                                    "authorId": "2275194788",
                                    "name": "Dacheng Tao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.94482421875
                    },
                    {
                        "id": "(Santacroce et al., 2023)",
                        "snippets": [
                            "Unstructured pruning removes individual weights from the network based on some criteria, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2019)Xu et al., 2021;Zafrir et al., 2021;Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast services based on LLMs."
                        ],
                        "paper": {
                            "corpus_id": 256662734,
                            "title": "What Matters In The Structured Pruning of Generative Language Models?",
                            "authors": [
                                {
                                    "authorId": "1413038175",
                                    "name": "Michael Santacroce"
                                },
                                {
                                    "authorId": "2051054583",
                                    "name": "Zixin Wen"
                                },
                                {
                                    "authorId": "1752875",
                                    "name": "Yelong Shen"
                                },
                                {
                                    "authorId": "152244300",
                                    "name": "Yuan-Fang Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 34
                        },
                        "score": 0.77490234375
                    },
                    {
                        "id": "(Sander et al., 2025)",
                        "snippets": [
                            "While structured pruning offers consistent inference acceleration across hardware, unstructured pruning often achieves higher compression ratios but requires specialized hardware or compiler optimizations to realize speedups."
                        ],
                        "paper": {
                            "corpus_id": 275920740,
                            "title": "On Accelerating Edge AI: Optimizing Resource-Constrained Environments",
                            "authors": [
                                {
                                    "authorId": "2342530769",
                                    "name": "Jacob Sander"
                                },
                                {
                                    "authorId": "2342484279",
                                    "name": "Achraf Cohen"
                                },
                                {
                                    "authorId": "69387233",
                                    "name": "Venkateswara Dasari"
                                },
                                {
                                    "authorId": "1712010",
                                    "name": "K. Venable"
                                },
                                {
                                    "authorId": "3116427",
                                    "name": "B. Jalaeian"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.80126953125
                    },
                    {
                        "id": "(Laborde et al., 2025)",
                        "snippets": [
                            "Unstructured pruning offers maximum theoretical compression by removing individual weights, but often results in irregular sparsity patterns that are challenging to accelerate on current hardware. Semi-structured approaches, like N:M sparsity patterns (Zhou et al., 2021), balance compression rates with hardware efficiency by enforcing regular pruning patterns, where N out of every M consecutive weights are pruned. Structured pruning (Ma et al., 2023) takes this further by removing entire structures, channels, or attention heads, with recent work showing that up to 50% of attention layers in large models can be removed while preserving performance [12]."
                        ],
                        "paper": {
                            "corpus_id": 278501529,
                            "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?",
                            "authors": [
                                {
                                    "authorId": "2360373404",
                                    "name": "Stanislas Laborde"
                                },
                                {
                                    "authorId": "2360359994",
                                    "name": "Martin Cousseau"
                                },
                                {
                                    "authorId": "40605834",
                                    "name": "Antoun Yaacoub"
                                },
                                {
                                    "authorId": "2266474578",
                                    "name": "Lionel Prevost"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.89892578125
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity."
                        ],
                        "paper": {
                            "corpus_id": 231847094,
                            "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch",
                            "authors": [
                                {
                                    "authorId": "9548994",
                                    "name": "Aojun Zhou"
                                },
                                {
                                    "authorId": "2289831903",
                                    "name": "Yukun Ma"
                                },
                                {
                                    "authorId": "24925751",
                                    "name": "Junnan Zhu"
                                },
                                {
                                    "authorId": "2124809722",
                                    "name": "Jianbo Liu"
                                },
                                {
                                    "authorId": "1490508571",
                                    "name": "Zhijie Zhang"
                                },
                                {
                                    "authorId": "50492964",
                                    "name": "Kun Yuan"
                                },
                                {
                                    "authorId": "8397576",
                                    "name": "Wenxiu Sun"
                                },
                                {
                                    "authorId": "47893312",
                                    "name": "Hongsheng Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 248
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Impact on Model Accuracy",
                "tldr": "Unstructured pruning preserves model accuracy better than structured pruning at equivalent compression rates, but structured pruning's hardware benefits come at the cost of greater performance degradation. Recent methods like LLM-Pruner attempt to mitigate accuracy loss in structured pruning through targeted component removal and lightweight fine-tuning. (18 sources)",
                "text": "\nThe impact of pruning methods on model accuracy reveals a clear trade-off between compression and performance. Unstructured pruning consistently outperforms structured pruning in maintaining model accuracy at equivalent compression ratios <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"267103825\" paperTitle=\"(Gong et al., 2024)\" isShortName></Paper>. This advantage stems from unstructured pruning's ability to selectively remove only the least important individual weights while preserving critical ones, offering a more fine-grained approach to model compression <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"277275922\" paperTitle=\"(Belhaouari et al., 2025)\" isShortName></Paper>.\n\nStructured pruning, while beneficial for hardware efficiency, typically causes more substantial accuracy degradation because it removes entire architectural components (neurons, attention heads, layers) that may contain both essential and non-essential weights <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"267103825\" paperTitle=\"(Gong et al., 2024)\" isShortName></Paper>. Models are generally highly sensitive to structured compression, often requiring gradual compression and retraining cycles to recover accuracy <Paper corpusId=\"256662263\" paperTitle=\"(Kurtic et al., 2023)\" isShortName></Paper>. The coarser granularity of structured pruning makes it more challenging to preserve model capabilities, especially at higher compression ratios <Paper corpusId=\"276482745\" paperTitle=\"(Qin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"277244079\" paperTitle=\"(Lu et al., 2025)\" isShortName></Paper>.\n\nRecent approaches have attempted to mitigate the performance impact of structured pruning. For example, LLM-Pruner employs structured pruning to remove non-critical grouped structures based on gradient information, followed by LoRA fine-tuning to recover performance <Paper corpusId=\"270621063\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>. This method can restore model capabilities with minimal data (just 50K samples) and short fine-tuning periods (approximately 3 hours) <Paper corpusId=\"275921475\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>.\n\nUnstructured pruning methods have shown impressive results in preserving model accuracy even at high sparsity levels. Recent techniques like SparseGPT can prune large language models to at least 50% sparsity in one shot without retraining, while maintaining minimal accuracy loss <Paper corpusId=\"278033481\" paperTitle=\"(Garg et al., 2025)\" isShortName></Paper> <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. Similarly, Wanda achieves 60% sparsity on LLaMA-7B with negligible performance degradation across multiple downstream tasks <Paper corpusId=\"275993741\" paperTitle=\"(Feng et al., 2025)\" isShortName></Paper> <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n\nThe performance impact also varies by pruning implementation. Semi-structured approaches like N:M sparsity patterns (where N out of every M consecutive weights are retained) offer a middle ground, balancing performance preservation with some degree of hardware compatibility <Paper corpusId=\"271217883\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper>. These patterns enable more efficient implementation in hardware while maintaining better accuracy than fully structured methods <Paper corpusId=\"278208127\" paperTitle=\"(Mugnaini et al., 2025)\" isShortName></Paper>.\n\nThe fundamental challenge remains that while unstructured pruning preserves accuracy better, the resulting irregular sparse matrices require specialized hardware or software to translate into computational efficiency <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"269588232\" paperTitle=\"(Klein et al., 2024)\" isShortName></Paper>. Structured pruning, despite its greater impact on accuracy, provides immediate hardware efficiency gains and deployment benefits without requiring specialized support <Paper corpusId=\"273963228\" paperTitle=\"(Rostami et al., 2024)\" isShortName></Paper> <Paper corpusId=\"260887757\" paperTitle=\"(Cheng et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhao et al., 2022)",
                        "snippets": [
                            "Unstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al., 2018)), e.g, (Hill et al., 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 246276158,
                            "title": "Iterative Activation-based Structured Pruning",
                            "authors": [
                                {
                                    "authorId": "1995855",
                                    "name": "Kaiqi Zhao"
                                },
                                {
                                    "authorId": "101682296",
                                    "name": "Animesh Jain"
                                },
                                {
                                    "authorId": "2152527896",
                                    "name": "Ming Zhao"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.89111328125
                    },
                    {
                        "id": "(Gong et al., 2024)",
                        "snippets": [
                            "By analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67].\n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation."
                        ],
                        "paper": {
                            "corpus_id": 267103825,
                            "title": "A Review of Neural Network Lightweighting Techniques",
                            "authors": [
                                {
                                    "authorId": "2280410840",
                                    "name": "Ziyi Gong"
                                },
                                {
                                    "authorId": "2280726529",
                                    "name": "Huifu Zhang"
                                },
                                {
                                    "authorId": "2280414619",
                                    "name": "Hao Yang"
                                },
                                {
                                    "authorId": "2280959623",
                                    "name": "Fangjun Liu"
                                },
                                {
                                    "authorId": "2280411835",
                                    "name": "Fan Luo"
                                }
                            ],
                            "year": 2024,
                            "venue": "Innovation &amp; Technology Advances",
                            "n_citations": 0
                        },
                        "score": 0.8623046875
                    },
                    {
                        "id": "(Belhaouari et al., 2025)",
                        "snippets": [
                            "Structure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility. Furthermore, this method can achieve a different level of sparsity than unstructured pruning, limiting its ability to compress models efficiently without retraining or redesigning the model architecture. On the other hand, unstructured pruning operates at the level of individual weights in a neural network, meaning it can remove any specific weight, regardless of its position. This allows for more fine-grained control over which weights to prune. This can target only the least essential weights, leading to a more refined reduction in model size while retaining critical model capacity. It can achieve higher sparsity levels without compromising model performance because it is not constrained by the rigid structures to which structured pruning must adhere.\n\nDespite preserving model performance, unstructured pruning often results in sparse weight matrices, which may not fully utilize hardware efficiency, as specialized hardware (such as GPUs) is typically optimized for dense matrix operations."
                        ],
                        "paper": {
                            "corpus_id": 277275922,
                            "title": "Efficient self-attention with smart pruning for sustainable large language models",
                            "authors": [
                                {
                                    "authorId": "102804035",
                                    "name": "S. Belhaouari"
                                },
                                {
                                    "authorId": "2292003273",
                                    "name": "Insaf Kraidia"
                                }
                            ],
                            "year": 2025,
                            "venue": "Scientific Reports",
                            "n_citations": 1
                        },
                        "score": 0.96142578125
                    },
                    {
                        "id": "(Kurtic et al., 2023)",
                        "snippets": [
                            "The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning (Hoefler et al., 2021), quantization [3], and distillation (Gou et al., 2020). In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy."
                        ],
                        "paper": {
                            "corpus_id": 256662263,
                            "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
                            "authors": [
                                {
                                    "authorId": "40992614",
                                    "name": "Eldar Kurtic"
                                },
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 26
                        },
                        "score": 0.90576171875
                    },
                    {
                        "id": "(Qin et al., 2025)",
                        "snippets": [
                            "Unstructured pruning compresses models by removing individual parameters, resulting in sparse weight matrices that consume less memory. However, without dedicated hardware support, the updated models do not achieve faster inference, thereby still imposing computational burdens during the inference process. Semi-structured pruning offers some speed improvements, but these are limited compared to those achieved by structured pruning. Structured pruning adopts a more modular approach to remove modules from models, typically targeting attention heads, embedding dimensions, FFN intermediate dimensions, experts in Mixture-of-Experts (MoE) networks, or layers. After structured pruning, the weight matrices of the models remain dense, and their reduced dimensions typically lead to greater inference acceleration. However, the coarser granularity of this pruning method makes it more challenging to preserve model capabilities after pruning."
                        ],
                        "paper": {
                            "corpus_id": 276482745,
                            "title": "MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures",
                            "authors": [
                                {
                                    "authorId": "2290611525",
                                    "name": "Jiayu Qin"
                                },
                                {
                                    "authorId": "2326256572",
                                    "name": "Jianchao Tan"
                                },
                                {
                                    "authorId": "2326248013",
                                    "name": "Kefeng Zhang"
                                },
                                {
                                    "authorId": "2326248599",
                                    "name": "Xunliang Cai"
                                },
                                {
                                    "authorId": "2338695871",
                                    "name": "Wei Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.92919921875
                    },
                    {
                        "id": "(Lu et al., 2025)",
                        "snippets": [
                            "However, the irregular sparsification achieved through unstructured pruning often fails to achieve the desired speedup or memory savings. Structured pruning, in contrast, removes entire channels or components from LLMs, making it easier to implement on hardware but potentially leading to significant accuracy degradation, especially under high compression ratios as seen with LLM-Pruner (Ma et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 277244079,
                            "title": "Large Language Model Compression via the Nested Activation-Aware Decomposition",
                            "authors": [
                                {
                                    "authorId": "2316872781",
                                    "name": "Jun Lu"
                                },
                                {
                                    "authorId": "2351716503",
                                    "name": "Tianyi Xu"
                                },
                                {
                                    "authorId": "2316639822",
                                    "name": "Bill Ding"
                                },
                                {
                                    "authorId": "2316673285",
                                    "name": "David Li"
                                },
                                {
                                    "authorId": "2316671224",
                                    "name": "Yu Kang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.89013671875
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Neural network pruning in LLM can be broadly categorized into two groups: structured pruning (Ma et al., 2023)Zhang et al., 2023) and unstructured pruning (Frantar & Alistarh, 2023;(Sun et al., 2023). (Ma et al., 2023) proposes a dependency detection algorithm to detect and prune non-critical grouped structures followed by LoRA fine-tuning. Although structured pruning can usually achieve better hardware efficiency, the accuracy drops a lot even at a low compression rate. Unstructured pruning can yield a higher compression rate and achieve acceleration on Nvidia's GPUs by employing a hardware-friendly N:M sparsity pattern."
                        ],
                        "paper": {
                            "corpus_id": 270621063,
                            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2300138950",
                                    "name": "Zhiyu Guo"
                                },
                                {
                                    "authorId": "2300756",
                                    "name": "Hidetaka Kamigaito"
                                },
                                {
                                    "authorId": "2299941873",
                                    "name": "Taro Wanatnabe"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 1
                        },
                        "score": 0.76953125
                    },
                    {
                        "id": "(Ma et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"
                        ],
                        "paper": {
                            "corpus_id": 258823276,
                            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "15532066",
                                    "name": "Xinyin Ma"
                                },
                                {
                                    "authorId": "150110431",
                                    "name": "Gongfan Fang"
                                },
                                {
                                    "authorId": "48631088",
                                    "name": "Xinchao Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 440
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sengupta et al., 2025)",
                        "snippets": [
                            "Unstructured pruning removes individual weights, as seen in SparseGPT (Frantar & Alistarh, 2023), which leverages Hessian matrix inversion to identify and eliminate less critical weights. However, unstructured pruning often requires hardware-specific optimizations and may not always result in significant computational gains (Yang et al., 2024;Wang et al., 2024b). In contrast, structured pruning removes entire channels or components, making it more compatible with various hardware setups. For example, LLM-Pruner (Ma et al., 2023) evaluates weight group importance and uses LoRA fine-tuning to recover lost accuracy. While structured pruning is more hardware-friendly, it can lead to greater accuracy loss at higher compression ratios."
                        ],
                        "paper": {
                            "corpus_id": 275921475,
                            "title": "You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning",
                            "authors": [
                                {
                                    "authorId": "34920835",
                                    "name": "Ayan Sengupta"
                                },
                                {
                                    "authorId": "2261280847",
                                    "name": "Siddhant Chaudhary"
                                },
                                {
                                    "authorId": "2249914540",
                                    "name": "Tanmoy Chakraborty"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 4
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Garg et al., 2025)",
                        "snippets": [
                            "Structured pruning streamlines LLM by eliminating entire architectural components-such as neurons, filter channels, attention heads, or even layers-instead of removing individual parameters (Ma et al., 2023)[198]. By selectively pruning structurally coherent groups of parameters, this approach yields a smaller yet regularly structured model-maintaining dense connectivity while reducing inference costs, for instance, LLM-Pruner (Ma et al., 2023). Unstructured pruning removes individual weights from LLM without considering any specific structure within the model (Frantar et al., 2023). Unstructured pruning removes less important weights based on a threshold, resulting in a sparse model where the architecture remains unchanged, as long as enough weights stay to maintain performance. Unstructured pruning can deliver higher accuracy for a given compression ratio by allowing more targeted weight removal. However, its irregular sparsity pattern leads to inefficient memory access and limited speed-ups on standard hardware. To fully leverage the compression benefits, specialized libraries or hardware are needed, and even then, acceleration gains are modest unless the model is highly sparse. Post unstructured pruning, significant retraining or fine-tuning is often required to recover lost accuracy that can be resource-intensive."
                        ],
                        "paper": {
                            "corpus_id": 278033481,
                            "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey",
                            "authors": [
                                {
                                    "authorId": "2258141722",
                                    "name": "Muskan Garg"
                                },
                                {
                                    "authorId": "2278330619",
                                    "name": "Shaina Raza"
                                },
                                {
                                    "authorId": "3023076",
                                    "name": "Shebuti Rayana"
                                },
                                {
                                    "authorId": "2278394763",
                                    "name": "Xingyi Liu"
                                },
                                {
                                    "authorId": "2267490593",
                                    "name": "Sunghwan Sohn"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.822265625
                    },
                    {
                        "id": "(Frantar et al., 2023)",
                        "snippets": [
                            "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                        ],
                        "paper": {
                            "corpus_id": 255372747,
                            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
                            "authors": [
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 734
                        },
                        "score": 0
                    },
                    {
                        "id": "(Feng et al., 2025)",
                        "snippets": [
                            "Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar et al., 2023)Sun et al., 2023) and structured pruning (Ma et al., 2023)An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns."
                        ],
                        "paper": {
                            "corpus_id": 275993741,
                            "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2332540527",
                                    "name": "Mingkuan Feng"
                                },
                                {
                                    "authorId": "2141911656",
                                    "name": "Jinyang Wu"
                                },
                                {
                                    "authorId": "2298428469",
                                    "name": "Shuai Zhang"
                                },
                                {
                                    "authorId": "2221575807",
                                    "name": "Pengpeng Shao"
                                },
                                {
                                    "authorId": "2300370837",
                                    "name": "Ruihan Jin"
                                },
                                {
                                    "authorId": "1718662",
                                    "name": "Zhengqi Wen"
                                },
                                {
                                    "authorId": "2298423822",
                                    "name": "Jianhua Tao"
                                },
                                {
                                    "authorId": "1471057495",
                                    "name": "Feihu Che"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8154296875
                    },
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda."
                        ],
                        "paper": {
                            "corpus_id": 259203115,
                            "title": "A Simple and Effective Pruning Approach for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2984183",
                                    "name": "Mingjie Sun"
                                },
                                {
                                    "authorId": "2109168016",
                                    "name": "Zhuang Liu"
                                },
                                {
                                    "authorId": "25901845",
                                    "name": "Anna Bair"
                                },
                                {
                                    "authorId": "145116464",
                                    "name": "J. Z. Kolter"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 439
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cheng et al., 2024)",
                        "snippets": [
                            "Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models. However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware. In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]). Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup."
                        ],
                        "paper": {
                            "corpus_id": 271217883,
                            "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2230189096",
                                    "name": "Hongrong Cheng"
                                },
                                {
                                    "authorId": "2311635513",
                                    "name": "Miao Zhang"
                                },
                                {
                                    "authorId": "2262197686",
                                    "name": "J. Q. Shi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.7734375
                    },
                    {
                        "id": "(Mugnaini et al., 2025)",
                        "snippets": [
                            "Within the field of LLMs, pruning techniques fall into three main categories: structured, semistructured and unstructured pruning.\n\nStructured pruning removes entire components -such as attention heads or layers -while preserving the overall network structure without introducing sparsity (i.e., without zeroing out a significant portion of the model's parameters) [6]. However, the removal of larger and potentially more critical components may result in performance degradation, typically requiring Parameter-Efficient Fine-Tuning (PEFT) techniques for performance recovery [14]. Due to the removal of complete components, structured pruning usually achieves inference acceleration and memory reduction without the need for specialized hardware or software [15].\n\nSemi-structured (a.k.a. structured N : M ) pruning promotes model sparsity by removing groups of consecutive parameters following a pruning mask [16]. Specifically, structured N : M sparsity requires that at least N out of every M consecutive weights be non-zero [17], [18]. While this is a promising technique, it requires specialized hardware to achieve practical speedup, making it less suitable for deployment on consumergrade GPUs [17]."
                        ],
                        "paper": {
                            "corpus_id": 278208127,
                            "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning",
                            "authors": [
                                {
                                    "authorId": "2331613426",
                                    "name": "Leandro Giusti Mugnaini"
                                },
                                {
                                    "authorId": "2303405382",
                                    "name": "B. Yamamoto"
                                },
                                {
                                    "authorId": "2358264111",
                                    "name": "Lucas Lauton de Alcantara"
                                },
                                {
                                    "authorId": "2358264489",
                                    "name": "Victor Zacarias"
                                },
                                {
                                    "authorId": "2358263602",
                                    "name": "Edson Bollis"
                                },
                                {
                                    "authorId": "2344249909",
                                    "name": "Lucas F. A. O. Pellicer"
                                },
                                {
                                    "authorId": "2303454322",
                                    "name": "A. H. R. Costa"
                                },
                                {
                                    "authorId": "2303404119",
                                    "name": "Artur Jordao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7861328125
                    },
                    {
                        "id": "(Klein et al., 2024)",
                        "snippets": [
                            "Unstructured pruning (Blalock et al., 2020) computes a score for each weight in the network, such as the weight's magnitude, and removes weights with scores below a predetermined threshold. This approach often achieves high pruning rates with minimal performance degradation, but it also leads to sparse weight matrices, which are not well-supported by commonly used machine learning frameworks. Structured pruning (Michel et al., 2019;Sajjad et al., 2022) removes larger components of the network, such as layers or heads. Although it typically does not achieve the same pruning rates as unstructured pruning, it only prunes entire columns/rows of the weight matrix, making it compatible with popular deep learning frameworks and hardware."
                        ],
                        "paper": {
                            "corpus_id": 269588232,
                            "title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search",
                            "authors": [
                                {
                                    "authorId": "2238461878",
                                    "name": "Aaron Klein"
                                },
                                {
                                    "authorId": "2238481381",
                                    "name": "Jacek Golebiowski"
                                },
                                {
                                    "authorId": "2238532717",
                                    "name": "Xingchen Ma"
                                },
                                {
                                    "authorId": "2299943113",
                                    "name": "Valerio Perrone"
                                },
                                {
                                    "authorId": "2262457089",
                                    "name": "C\u00e9dric Archambeau"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 2
                        },
                        "score": 0.82666015625
                    },
                    {
                        "id": "(Rostami et al., 2024)",
                        "snippets": [
                            "While unstructured pruning allows for the sparsification of specific weights, it often necessitates specialized hardware for speed improve- ments (Cheng et al., 2023). In contrast, structural pruning (Molchanov et al., 2016) focuses on removing entire filters, blocks, or layers, providing efficiency that is not dependent on specific devices (Cheng et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 273963228,
                            "title": "CULL-MT: Compression Using Language and Layer pruning for Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2279336018",
                                    "name": "Pedram Rostami"
                                },
                                {
                                    "authorId": "1702695",
                                    "name": "M. Dousti"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.77880859375
                    },
                    {
                        "id": "(Cheng et al., 2023)",
                        "snippets": [
                            "Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and to accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. More than three thousand pruning papers have been published from 2020 to 2024. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of eight pairs of contrast settings for pruning (e.g., unstructured/structured, one-shot/iterative, data-free/data-driven, initialized/pre-trained weights, etc.) and explore several emerging topics, including pruning for large language models, vision transformers, diffusion models, and large multimodal models, post-training pruning, and different levels of supervision for pruning to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. Finally, we provide some valuable recommendations on selecting pruning methods and prospect several promising research directions for neural network pruning. To facilitate future research on deep neural network pruning, we summarize broad pruning applications (e.g., adversarial robustness, natural language understanding, etc.) and build a curated collection of datasets, networks, and evaluations on different applications. We maintain a repository on https://github.com/hrcheng1066/awesome-pruning that serves as a comprehensive resource for neural network pruning papers and corresponding open-source codes. We will keep updating this repository to include the latest advancements in the field."
                        ],
                        "paper": {
                            "corpus_id": 260887757,
                            "title": "A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations",
                            "authors": [
                                {
                                    "authorId": "2230189096",
                                    "name": "Hongrong Cheng"
                                },
                                {
                                    "authorId": "2211872272",
                                    "name": "Miao Zhang"
                                },
                                {
                                    "authorId": "3177281",
                                    "name": "Javen Qinfeng Shi"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 154
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Hardware Compatibility and Acceleration",
                "tldr": "Structured pruning offers immediate hardware efficiency gains on standard devices by physically reducing model dimensions, while unstructured pruning achieves higher compression but requires specialized hardware support to translate sparsity into actual speedups. Semi-structured approaches like N:M sparsity patterns attempt to balance compression benefits with hardware compatibility. (17 sources)",
                "text": "\nThe hardware compatibility of pruning methods represents a critical trade-off between theoretical compression and practical acceleration benefits. Structured pruning has a distinct advantage in hardware compatibility, as it removes entire architectural components (channels, heads, or layers), allowing models to be physically reshaped to smaller dimensions <Paper corpusId=\"256662263\" paperTitle=\"(Kurtic et al., 2023)\" isShortName></Paper>. This approach enables immediate computational efficiency gains on standard hardware without requiring specialized support <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"52048008\" paperTitle=\"(He et al., 2018)\" isShortName></Paper>. The resulting dense matrices with reduced dimensions maintain regular memory access patterns that work efficiently with existing deep learning frameworks and hardware architectures <Paper corpusId=\"271217883\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper>.\n\nIn contrast, unstructured pruning, while achieving higher compression ratios, faces significant hardware implementation challenges. The irregular sparsity patterns resulting from individual weight removal lead to inefficient memory access patterns that standard hardware cannot exploit <Paper corpusId=\"267103825\" paperTitle=\"(Gong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"272368391\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. Several studies have demonstrated that directly applying sparse kernels to unstructured pruned models can lead to significant performance degradation\u2014up to 60\u00d7 slowdown on GPUs compared to dense kernels <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"27494814\" paperTitle=\"(Zhu et al., 2017)\" isShortName></Paper>. Without specialized hardware or software support, the theoretical compression benefits of unstructured pruning do not translate into practical efficiency gains <Paper corpusId=\"267412232\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nRecent hardware developments have attempted to address this gap. Semi-structured pruning approaches, such as N:M sparsity patterns where N out of every M consecutive weights are pruned, offer a middle ground <Paper corpusId=\"271217883\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper>. These patterns create more regular sparsity that can be more efficiently implemented in hardware <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. For example, 2:4 sparse networks can achieve 2\u00d7 speedup on specialized GPUs like NVIDIA A100s that support this specific pattern <Paper corpusId=\"233296249\" paperTitle=\"(Mishra et al., 2021)\" isShortName></Paper>. However, these semi-structured approaches still require specialized hardware support and may not deliver the same level of acceleration as structured pruning at equivalent sparsity levels <Paper corpusId=\"278338989\" paperTitle=\"(Hu et al., 2025)\" isShortName></Paper>.\n\nThe hardware compatibility gap has significant implications for deployment scenarios. For resource-constrained devices or general deployment where specialized hardware is unavailable, structured pruning remains the most practical approach despite its lower compression potential <Paper corpusId=\"256662734\" paperTitle=\"(Santacroce et al., 2023)\" isShortName></Paper>. As noted by multiple researchers, structured pruning is particularly advantageous for deployment on resource-constrained devices or for providing fast services based on LLMs <Paper corpusId=\"275993741\" paperTitle=\"(Feng et al., 2025)\" isShortName></Paper> <Paper corpusId=\"278367624\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>.\n\nEmpirical measurements confirm these theoretical distinctions. Several studies report concrete speedups from structured pruning\u2014ranging from 1.19\u00d7 to 1.87\u00d7 faster inference compared to dense models on standard hardware <Paper corpusId=\"271909421\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. In contrast, unstructured pruning methods like SparseGPT, while achieving up to 60% parameter reduction, require specialized support to translate this into computational gains <Paper corpusId=\"267412232\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. For deployment scenarios requiring immediate efficiency gains without specialized hardware, structured pruning remains the preferred approach <Paper corpusId=\"276259010\" paperTitle=\"(Tang et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Kurtic et al., 2023)",
                        "snippets": [
                            "The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning (Hoefler et al., 2021), quantization [3], and distillation (Gou et al., 2020). In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy."
                        ],
                        "paper": {
                            "corpus_id": 256662263,
                            "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
                            "authors": [
                                {
                                    "authorId": "40992614",
                                    "name": "Eldar Kurtic"
                                },
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 26
                        },
                        "score": 0.90576171875
                    },
                    {
                        "id": "(Zhao et al., 2022)",
                        "snippets": [
                            "Unstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al., 2018)), e.g, (Hill et al., 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 246276158,
                            "title": "Iterative Activation-based Structured Pruning",
                            "authors": [
                                {
                                    "authorId": "1995855",
                                    "name": "Kaiqi Zhao"
                                },
                                {
                                    "authorId": "101682296",
                                    "name": "Animesh Jain"
                                },
                                {
                                    "authorId": "2152527896",
                                    "name": "Ming Zhao"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.89111328125
                    },
                    {
                        "id": "(He et al., 2018)",
                        "snippets": [
                            "Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4\\(\\times \\) FLOPs reduction, we achieved 2.7% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53\\(\\times \\) on the GPU (Titan Xp) and 1.95\\(\\times \\) on an Android phone (Google Pixel 1), with negligible loss of accuracy."
                        ],
                        "paper": {
                            "corpus_id": 52048008,
                            "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
                            "authors": [
                                {
                                    "authorId": "39838894",
                                    "name": "Yihui He"
                                },
                                {
                                    "authorId": "46698300",
                                    "name": "Ji Lin"
                                },
                                {
                                    "authorId": "47781592",
                                    "name": "Zhijian Liu"
                                },
                                {
                                    "authorId": "35446689",
                                    "name": "Hanrui Wang"
                                },
                                {
                                    "authorId": "2040091191",
                                    "name": "Li-Jia Li"
                                },
                                {
                                    "authorId": "143840275",
                                    "name": "Song Han"
                                }
                            ],
                            "year": 2018,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 1349
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cheng et al., 2024)",
                        "snippets": [
                            "Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models. However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware. In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]). Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup."
                        ],
                        "paper": {
                            "corpus_id": 271217883,
                            "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2230189096",
                                    "name": "Hongrong Cheng"
                                },
                                {
                                    "authorId": "2311635513",
                                    "name": "Miao Zhang"
                                },
                                {
                                    "authorId": "2262197686",
                                    "name": "J. Q. Shi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.7734375
                    },
                    {
                        "id": "(Gong et al., 2024)",
                        "snippets": [
                            "By analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67].\n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation."
                        ],
                        "paper": {
                            "corpus_id": 267103825,
                            "title": "A Review of Neural Network Lightweighting Techniques",
                            "authors": [
                                {
                                    "authorId": "2280410840",
                                    "name": "Ziyi Gong"
                                },
                                {
                                    "authorId": "2280726529",
                                    "name": "Huifu Zhang"
                                },
                                {
                                    "authorId": "2280414619",
                                    "name": "Hao Yang"
                                },
                                {
                                    "authorId": "2280959623",
                                    "name": "Fangjun Liu"
                                },
                                {
                                    "authorId": "2280411835",
                                    "name": "Fan Luo"
                                }
                            ],
                            "year": 2024,
                            "venue": "Innovation &amp; Technology Advances",
                            "n_citations": 0
                        },
                        "score": 0.8623046875
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Structured Pruning: This approach removes entire subsets of parameters like layers, channels, or filters, which is beneficial for hardware optimization due to more regular memory access patterns and simplified computations. The 'LLM-Pruner' (Kaddour et al., 2023) employs structured pruning to eliminate non-essential groups based on gradient data, thus maintaining critical functionalities. It also facilitates performance recovery through techniques such as LoRA, allowing efficient restoration with minimal data. 2. Unstructured Pruning: Unlike structured pruning, unstructured pruning removes individual weights across the model, offering finer granularity and potentially higher compression rates (Li et al., 2023a). However, this method typically results in sparse matrices, which can be less compatible with traditional hardware architectures, compromising computational efficiency. It is most suitable where maximum compression is needed without constraints on structural preservation."
                        ],
                        "paper": {
                            "corpus_id": 272368391,
                            "title": "On-Device Language Models: A Comprehensive Review",
                            "authors": [
                                {
                                    "authorId": "2316519813",
                                    "name": "Jiajun Xu"
                                },
                                {
                                    "authorId": "2294674012",
                                    "name": "Zhiyuan Li"
                                },
                                {
                                    "authorId": "2294845809",
                                    "name": "Wei Chen"
                                },
                                {
                                    "authorId": "2316514278",
                                    "name": "Qun Wang"
                                },
                                {
                                    "authorId": "2319809164",
                                    "name": "Xin Gao"
                                },
                                {
                                    "authorId": "2364055424",
                                    "name": "Qi Cai"
                                },
                                {
                                    "authorId": "2319410023",
                                    "name": "Ziyuan Ling"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 35
                        },
                        "score": 0.85986328125
                    },
                    {
                        "id": "(Zhu et al., 2017)",
                        "snippets": [
                            "Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy."
                        ],
                        "paper": {
                            "corpus_id": 27494814,
                            "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression",
                            "authors": [
                                {
                                    "authorId": "2152183723",
                                    "name": "Michael Zhu"
                                },
                                {
                                    "authorId": "2116011472",
                                    "name": "Suyog Gupta"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1281
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters.\n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models."
                        ],
                        "paper": {
                            "corpus_id": 267412232,
                            "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2108048327",
                                    "name": "Xindi Wang"
                                },
                                {
                                    "authorId": "1904419",
                                    "name": "Mahsa Salmani"
                                },
                                {
                                    "authorId": "2282534833",
                                    "name": "Parsa Omidi"
                                },
                                {
                                    "authorId": "2283447900",
                                    "name": "Xiangyu Ren"
                                },
                                {
                                    "authorId": "2066076226",
                                    "name": "Mehdi Rezagholizadeh"
                                },
                                {
                                    "authorId": "50782111",
                                    "name": "A. Eshaghi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 45
                        },
                        "score": 0.83740234375
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "According to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning (Ma et al., 2023)43,(Liu et al., 2023), Unstructured Pruning (Han et al., 2015)[15], and Semi-Structured Pruning (Frantar et al., 2023)38,29,32,(Pool et al., 2021). Structured pruning physically eliminates substructures like attention heads (Ma et al., 2023), embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters (Ma et al., 2023)43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy (Frantar et al., 2023)38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32](Pool et al., 2021)(Frantar et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 272910976,
                            "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "150110431",
                                    "name": "Gongfan Fang"
                                },
                                {
                                    "authorId": "1989015",
                                    "name": "Hongxu Yin"
                                },
                                {
                                    "authorId": "31225166",
                                    "name": "Saurav Muralidharan"
                                },
                                {
                                    "authorId": "2273650910",
                                    "name": "Greg Heinrich"
                                },
                                {
                                    "authorId": "2322991313",
                                    "name": "Jeff Pool"
                                },
                                {
                                    "authorId": "2273651410",
                                    "name": "Jan Kautz"
                                },
                                {
                                    "authorId": "2824500",
                                    "name": "Pavlo Molchanov"
                                },
                                {
                                    "authorId": "2322993154",
                                    "name": "Xinchao Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 10
                        },
                        "score": 0.8720703125
                    },
                    {
                        "id": "(Mishra et al., 2021)",
                        "snippets": [
                            "As neural network model sizes have dramatically increased, so has the interest in various techniques to reduce their parameter counts and accelerate their execution. An active area of research in this field is sparsity - encouraging zero values in parameters that can then be discarded from storage or computations. While most research focuses on high levels of sparsity, there are challenges in universally maintaining model accuracy as well as achieving significant speedups over modern matrix-math hardware. To make sparsity adoption practical, the NVIDIA Ampere GPU architecture introduces sparsity support in its matrix-math units, Tensor Cores. We present the design and behavior of Sparse Tensor Cores, which exploit a 2:4 (50%) sparsity pattern that leads to twice the math throughput of dense matrix units. We also describe a simple workflow for training networks that both satisfy 2:4 sparsity pattern requirements and maintain accuracy, verifying it on a wide range of common tasks and model architectures. This workflow makes it easy to prepare accurate models for efficient deployment on Sparse Tensor Cores."
                        ],
                        "paper": {
                            "corpus_id": 233296249,
                            "title": "Accelerating Sparse Deep Neural Networks",
                            "authors": [
                                {
                                    "authorId": "35769149",
                                    "name": "Asit K. Mishra"
                                },
                                {
                                    "authorId": "2060797517",
                                    "name": "J. Latorre"
                                },
                                {
                                    "authorId": "47325862",
                                    "name": "Jeff Pool"
                                },
                                {
                                    "authorId": "33749574",
                                    "name": "Darko Stosic"
                                },
                                {
                                    "authorId": "2737605",
                                    "name": "D. Stosic"
                                },
                                {
                                    "authorId": "145595812",
                                    "name": "Ganesh Venkatesh"
                                },
                                {
                                    "authorId": "2116145799",
                                    "name": "Chong Yu"
                                },
                                {
                                    "authorId": "1802359",
                                    "name": "P. Micikevicius"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 235
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hu et al., 2025)",
                        "snippets": [
                            "While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware or sparse computations."
                        ],
                        "paper": {
                            "corpus_id": 278338989,
                            "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods",
                            "authors": [
                                {
                                    "authorId": "2283405371",
                                    "name": "Hanyu Hu"
                                },
                                {
                                    "authorId": "2283433779",
                                    "name": "Xiaoming Yuan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Santacroce et al., 2023)",
                        "snippets": [
                            "Unstructured pruning removes individual weights from the network based on some criteria, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2019)Xu et al., 2021;Zafrir et al., 2021;Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast services based on LLMs."
                        ],
                        "paper": {
                            "corpus_id": 256662734,
                            "title": "What Matters In The Structured Pruning of Generative Language Models?",
                            "authors": [
                                {
                                    "authorId": "1413038175",
                                    "name": "Michael Santacroce"
                                },
                                {
                                    "authorId": "2051054583",
                                    "name": "Zixin Wen"
                                },
                                {
                                    "authorId": "1752875",
                                    "name": "Yelong Shen"
                                },
                                {
                                    "authorId": "152244300",
                                    "name": "Yuan-Fang Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 34
                        },
                        "score": 0.77490234375
                    },
                    {
                        "id": "(Feng et al., 2025)",
                        "snippets": [
                            "Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar et al., 2023)Sun et al., 2023) and structured pruning (Ma et al., 2023)An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns."
                        ],
                        "paper": {
                            "corpus_id": 275993741,
                            "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2332540527",
                                    "name": "Mingkuan Feng"
                                },
                                {
                                    "authorId": "2141911656",
                                    "name": "Jinyang Wu"
                                },
                                {
                                    "authorId": "2298428469",
                                    "name": "Shuai Zhang"
                                },
                                {
                                    "authorId": "2221575807",
                                    "name": "Pengpeng Shao"
                                },
                                {
                                    "authorId": "2300370837",
                                    "name": "Ruihan Jin"
                                },
                                {
                                    "authorId": "1718662",
                                    "name": "Zhengqi Wen"
                                },
                                {
                                    "authorId": "2298423822",
                                    "name": "Jianhua Tao"
                                },
                                {
                                    "authorId": "1471057495",
                                    "name": "Feihu Che"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8154296875
                    },
                    {
                        "id": "(Liang et al., 2025)",
                        "snippets": [
                            "Pruning boosts model compactness at the architectural level by removing redundant parameters, ranging from individual weights (unstructured pruning (Shao et al., 2023)- (Xu et al., 2024)) to entire channels or layers (structured pruning (Ma et al., 2023)- (Li et al., 2023)). Although unstructured pruning can achieve significant compression ratios, the resulting irregular sparsity is not conducive to hardware implementation (Molchanov et al., 2016). In contrast, structured pruning is more compatible with hardware acceleration but often results in model accuracy degradation and limited sparsity (He et al., 2017). To balance model accuracy and hardware efficiency, N :M semi-structured pruning (Frantar et al., 2023), (Sun et al., 2023), where N out of every M elements are pruned, is commonly adopted in prevalent LLMs (Zeng et al., 2024), (Mishra et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 278367624,
                            "title": "AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design",
                            "authors": [
                                {
                                    "authorId": "2325996847",
                                    "name": "Yanbiao Liang"
                                },
                                {
                                    "authorId": "30984015",
                                    "name": "Huihong Shi"
                                },
                                {
                                    "authorId": "2146641923",
                                    "name": "Haikuo Shao"
                                },
                                {
                                    "authorId": "2275509567",
                                    "name": "Zhongfeng Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.939453125
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,(Liu et al., 2023). Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]",
                            "Table 7 indicates that the pruned models can achieve 1.19x \u223c 1.87x speedup compared to their dense counterparts."
                        ],
                        "paper": {
                            "corpus_id": 271909421,
                            "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
                            "authors": [
                                {
                                    "authorId": "2301331844",
                                    "name": "Guanchen Li"
                                },
                                {
                                    "authorId": "2270847262",
                                    "name": "Xiandong Zhao"
                                },
                                {
                                    "authorId": "2316517251",
                                    "name": "Lian Liu"
                                },
                                {
                                    "authorId": "2307589652",
                                    "name": "Zeping Li"
                                },
                                {
                                    "authorId": "2279335698",
                                    "name": "Dong Li"
                                },
                                {
                                    "authorId": "2279539118",
                                    "name": "Lu Tian"
                                },
                                {
                                    "authorId": "2316522396",
                                    "name": "Jie He"
                                },
                                {
                                    "authorId": "2316484957",
                                    "name": "Ashish Sirasao"
                                },
                                {
                                    "authorId": "2271751612",
                                    "name": "E. Barsoum"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.77734375
                    },
                    {
                        "id": "(Frantar et al., 2023)",
                        "snippets": [
                            "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                        ],
                        "paper": {
                            "corpus_id": 255372747,
                            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
                            "authors": [
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 734
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tang et al., 2025)",
                        "snippets": [
                            "Unlike unstructured pruning (Frantar & Alistarh, 2023), the model produced by structured pruning can be accelerated on mainstream hardware without any specific design for computation."
                        ],
                        "paper": {
                            "corpus_id": 276259010,
                            "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2331680259",
                                    "name": "Shengkun Tang"
                                },
                                {
                                    "authorId": "2326834165",
                                    "name": "Oliver Sieberling"
                                },
                                {
                                    "authorId": "40992614",
                                    "name": "Eldar Kurtic"
                                },
                                {
                                    "authorId": "2344981603",
                                    "name": "Zhiqiang Shen"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.779296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Semi-Structured Pruning as a Middle Ground",
                "tldr": "Semi-structured pruning approaches like N:M sparsity patterns attempt to balance the compression benefits of unstructured pruning with the hardware compatibility of structured pruning. These methods enforce regular sparsity patterns where N out of every M consecutive weights are pruned, enabling acceleration on specialized hardware while maintaining better accuracy than fully structured approaches. (13 sources)",
                "text": "\nSemi-structured pruning has emerged as a compromise between the high compression ratios of unstructured pruning and the immediate hardware efficiency of structured pruning. This approach implements specific regular patterns of sparsity, most commonly N:M patterns where N out of every M consecutive weights are pruned <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278367624\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. The 2:4 sparsity pattern, where 2 out of every 4 consecutive weights are retained, has gained particular prominence due to hardware support in NVIDIA's Ampere GPU architecture <Paper corpusId=\"233296249\" paperTitle=\"(Mishra et al., 2021)\" isShortName></Paper> <Paper corpusId=\"278367624\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>.\n\nThe key advantage of semi-structured pruning is that it creates more regular sparsity patterns that can be efficiently implemented in specialized hardware <Paper corpusId=\"271217883\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper>. For instance, 2:4 sparse networks can achieve up to 2\u00d7 speedup on NVIDIA A100 GPUs that are specifically designed to support these patterns <Paper corpusId=\"271217883\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"231847094\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>. This approach provides a balance between compression benefits and hardware compatibility that neither fully structured nor unstructured methods can achieve alone <Paper corpusId=\"271744772\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nIn terms of performance preservation, semi-structured pruning maintains better accuracy than fully structured methods while still enabling some degree of hardware acceleration <Paper corpusId=\"270621063\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. Methods like SparseGPT have demonstrated that semi-structured pruning patterns (2:4 and 4:8) can be effectively applied to large language models with minimal performance degradation <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. This approach harmonizes \"the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity\" <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.\n\nHowever, semi-structured pruning still has limitations. It requires specialized hardware support to realize practical speedups, making it less suitable for deployment on consumer-grade GPUs or general hardware <Paper corpusId=\"278208127\" paperTitle=\"(Mugnaini et al., 2025)\" isShortName></Paper>. Some empirical studies suggest that semi-structured pruning may underperform structured methods in inference acceleration at equivalent sparsity levels <Paper corpusId=\"278338989\" paperTitle=\"(Hu et al., 2025)\" isShortName></Paper>. Additionally, while semi-structured approaches can achieve computational speedups, they typically do not reduce the model's memory footprint as structured pruning does <Paper corpusId=\"274981759\" paperTitle=\"(Sy et al., 2024)\" isShortName></Paper>.\n\nThe choice between pruning approaches ultimately depends on the specific deployment scenario and hardware availability. Semi-structured pruning represents a valuable middle ground for cases where specialized hardware support is available and both compression ratio and performance preservation are priorities <Paper corpusId=\"278501529\" paperTitle=\"(Laborde et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271909421\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "According to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning (Ma et al., 2023)43,(Liu et al., 2023), Unstructured Pruning (Han et al., 2015)[15], and Semi-Structured Pruning (Frantar et al., 2023)38,29,32,(Pool et al., 2021). Structured pruning physically eliminates substructures like attention heads (Ma et al., 2023), embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters (Ma et al., 2023)43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy (Frantar et al., 2023)38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32](Pool et al., 2021)(Frantar et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 272910976,
                            "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "150110431",
                                    "name": "Gongfan Fang"
                                },
                                {
                                    "authorId": "1989015",
                                    "name": "Hongxu Yin"
                                },
                                {
                                    "authorId": "31225166",
                                    "name": "Saurav Muralidharan"
                                },
                                {
                                    "authorId": "2273650910",
                                    "name": "Greg Heinrich"
                                },
                                {
                                    "authorId": "2322991313",
                                    "name": "Jeff Pool"
                                },
                                {
                                    "authorId": "2273651410",
                                    "name": "Jan Kautz"
                                },
                                {
                                    "authorId": "2824500",
                                    "name": "Pavlo Molchanov"
                                },
                                {
                                    "authorId": "2322993154",
                                    "name": "Xinchao Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 10
                        },
                        "score": 0.8720703125
                    },
                    {
                        "id": "(Liang et al., 2025)",
                        "snippets": [
                            "Pruning boosts model compactness at the architectural level by removing redundant parameters, ranging from individual weights (unstructured pruning (Shao et al., 2023)- (Xu et al., 2024)) to entire channels or layers (structured pruning (Ma et al., 2023)- (Li et al., 2023)). Although unstructured pruning can achieve significant compression ratios, the resulting irregular sparsity is not conducive to hardware implementation (Molchanov et al., 2016). In contrast, structured pruning is more compatible with hardware acceleration but often results in model accuracy degradation and limited sparsity (He et al., 2017). To balance model accuracy and hardware efficiency, N :M semi-structured pruning (Frantar et al., 2023), (Sun et al., 2023), where N out of every M elements are pruned, is commonly adopted in prevalent LLMs (Zeng et al., 2024), (Mishra et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 278367624,
                            "title": "AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design",
                            "authors": [
                                {
                                    "authorId": "2325996847",
                                    "name": "Yanbiao Liang"
                                },
                                {
                                    "authorId": "30984015",
                                    "name": "Huihong Shi"
                                },
                                {
                                    "authorId": "2146641923",
                                    "name": "Haikuo Shao"
                                },
                                {
                                    "authorId": "2275509567",
                                    "name": "Zhongfeng Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.939453125
                    },
                    {
                        "id": "(Frantar et al., 2023)",
                        "snippets": [
                            "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                        ],
                        "paper": {
                            "corpus_id": 255372747,
                            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
                            "authors": [
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 734
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mishra et al., 2021)",
                        "snippets": [
                            "As neural network model sizes have dramatically increased, so has the interest in various techniques to reduce their parameter counts and accelerate their execution. An active area of research in this field is sparsity - encouraging zero values in parameters that can then be discarded from storage or computations. While most research focuses on high levels of sparsity, there are challenges in universally maintaining model accuracy as well as achieving significant speedups over modern matrix-math hardware. To make sparsity adoption practical, the NVIDIA Ampere GPU architecture introduces sparsity support in its matrix-math units, Tensor Cores. We present the design and behavior of Sparse Tensor Cores, which exploit a 2:4 (50%) sparsity pattern that leads to twice the math throughput of dense matrix units. We also describe a simple workflow for training networks that both satisfy 2:4 sparsity pattern requirements and maintain accuracy, verifying it on a wide range of common tasks and model architectures. This workflow makes it easy to prepare accurate models for efficient deployment on Sparse Tensor Cores."
                        ],
                        "paper": {
                            "corpus_id": 233296249,
                            "title": "Accelerating Sparse Deep Neural Networks",
                            "authors": [
                                {
                                    "authorId": "35769149",
                                    "name": "Asit K. Mishra"
                                },
                                {
                                    "authorId": "2060797517",
                                    "name": "J. Latorre"
                                },
                                {
                                    "authorId": "47325862",
                                    "name": "Jeff Pool"
                                },
                                {
                                    "authorId": "33749574",
                                    "name": "Darko Stosic"
                                },
                                {
                                    "authorId": "2737605",
                                    "name": "D. Stosic"
                                },
                                {
                                    "authorId": "145595812",
                                    "name": "Ganesh Venkatesh"
                                },
                                {
                                    "authorId": "2116145799",
                                    "name": "Chong Yu"
                                },
                                {
                                    "authorId": "1802359",
                                    "name": "P. Micikevicius"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 235
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cheng et al., 2024)",
                        "snippets": [
                            "Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models. However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware. In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]). Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup."
                        ],
                        "paper": {
                            "corpus_id": 271217883,
                            "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2230189096",
                                    "name": "Hongrong Cheng"
                                },
                                {
                                    "authorId": "2311635513",
                                    "name": "Miao Zhang"
                                },
                                {
                                    "authorId": "2262197686",
                                    "name": "J. Q. Shi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.7734375
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity."
                        ],
                        "paper": {
                            "corpus_id": 231847094,
                            "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch",
                            "authors": [
                                {
                                    "authorId": "9548994",
                                    "name": "Aojun Zhou"
                                },
                                {
                                    "authorId": "2289831903",
                                    "name": "Yukun Ma"
                                },
                                {
                                    "authorId": "24925751",
                                    "name": "Junnan Zhu"
                                },
                                {
                                    "authorId": "2124809722",
                                    "name": "Jianbo Liu"
                                },
                                {
                                    "authorId": "1490508571",
                                    "name": "Zhijie Zhang"
                                },
                                {
                                    "authorId": "50492964",
                                    "name": "Kun Yuan"
                                },
                                {
                                    "authorId": "8397576",
                                    "name": "Wenxiu Sun"
                                },
                                {
                                    "authorId": "47893312",
                                    "name": "Hongsheng Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 248
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "For structured pruning, SliceGPT (Ashkboos et al., 2024) and Eigenpruning (Vergara-Browne et al., 2024) utilize singular value decompositions to prune singular values of weight matrices and reduce model dimensions. ZipLM (Kurtic et al., 2023) adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Our proposed FISTAPruner focuses on unstructured and semi-structured pruning, and thus is orthogonal to these structured pruning methods, enabling further model compression. For unstructured and semi-structured pruning, SparseGPT (Frantar et al., 2023) and ISC (Shao et al., 2023) leverage the OBS framework to calculate saliency for each entry using the inverse Hessian of the loss metric, based on which pruning masks are generated and weights updated. Wanda (Sun et al., 2023) implements a heuristic approach, removing weights based on the product of their magnitudes and activations without compensation."
                        ],
                        "paper": {
                            "corpus_id": 271744772,
                            "title": "A Convex-optimization-based Layer-wise Post-training Pruner for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2114873379",
                                    "name": "Pengxiang Zhao"
                                },
                                {
                                    "authorId": "2283405371",
                                    "name": "Hanyu Hu"
                                },
                                {
                                    "authorId": "2293474362",
                                    "name": "Ping Li"
                                },
                                {
                                    "authorId": "2293396844",
                                    "name": "Yi Zheng"
                                },
                                {
                                    "authorId": "2293231668",
                                    "name": "Zhefeng Wang"
                                },
                                {
                                    "authorId": "2283433779",
                                    "name": "Xiaoming Yuan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.76611328125
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Neural network pruning in LLM can be broadly categorized into two groups: structured pruning (Ma et al., 2023)Zhang et al., 2023) and unstructured pruning (Frantar & Alistarh, 2023;(Sun et al., 2023). (Ma et al., 2023) proposes a dependency detection algorithm to detect and prune non-critical grouped structures followed by LoRA fine-tuning. Although structured pruning can usually achieve better hardware efficiency, the accuracy drops a lot even at a low compression rate. Unstructured pruning can yield a higher compression rate and achieve acceleration on Nvidia's GPUs by employing a hardware-friendly N:M sparsity pattern."
                        ],
                        "paper": {
                            "corpus_id": 270621063,
                            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2300138950",
                                    "name": "Zhiyu Guo"
                                },
                                {
                                    "authorId": "2300756",
                                    "name": "Hidetaka Kamigaito"
                                },
                                {
                                    "authorId": "2299941873",
                                    "name": "Taro Wanatnabe"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 1
                        },
                        "score": 0.76953125
                    },
                    {
                        "id": "(Mugnaini et al., 2025)",
                        "snippets": [
                            "Within the field of LLMs, pruning techniques fall into three main categories: structured, semistructured and unstructured pruning.\n\nStructured pruning removes entire components -such as attention heads or layers -while preserving the overall network structure without introducing sparsity (i.e., without zeroing out a significant portion of the model's parameters) [6]. However, the removal of larger and potentially more critical components may result in performance degradation, typically requiring Parameter-Efficient Fine-Tuning (PEFT) techniques for performance recovery [14]. Due to the removal of complete components, structured pruning usually achieves inference acceleration and memory reduction without the need for specialized hardware or software [15].\n\nSemi-structured (a.k.a. structured N : M ) pruning promotes model sparsity by removing groups of consecutive parameters following a pruning mask [16]. Specifically, structured N : M sparsity requires that at least N out of every M consecutive weights be non-zero [17], [18]. While this is a promising technique, it requires specialized hardware to achieve practical speedup, making it less suitable for deployment on consumergrade GPUs [17]."
                        ],
                        "paper": {
                            "corpus_id": 278208127,
                            "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning",
                            "authors": [
                                {
                                    "authorId": "2331613426",
                                    "name": "Leandro Giusti Mugnaini"
                                },
                                {
                                    "authorId": "2303405382",
                                    "name": "B. Yamamoto"
                                },
                                {
                                    "authorId": "2358264111",
                                    "name": "Lucas Lauton de Alcantara"
                                },
                                {
                                    "authorId": "2358264489",
                                    "name": "Victor Zacarias"
                                },
                                {
                                    "authorId": "2358263602",
                                    "name": "Edson Bollis"
                                },
                                {
                                    "authorId": "2344249909",
                                    "name": "Lucas F. A. O. Pellicer"
                                },
                                {
                                    "authorId": "2303454322",
                                    "name": "A. H. R. Costa"
                                },
                                {
                                    "authorId": "2303404119",
                                    "name": "Artur Jordao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7861328125
                    },
                    {
                        "id": "(Hu et al., 2025)",
                        "snippets": [
                            "While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware or sparse computations."
                        ],
                        "paper": {
                            "corpus_id": 278338989,
                            "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods",
                            "authors": [
                                {
                                    "authorId": "2283405371",
                                    "name": "Hanyu Hu"
                                },
                                {
                                    "authorId": "2283433779",
                                    "name": "Xiaoming Yuan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Sy et al., 2024)",
                        "snippets": [
                            "Structured Pruning removes entire groups of parameters, which results in a smaller and faster model (Xia et al., 2024;Ma et al., 2023). Ma et al. (2023) propose a new gradient-based criterion to eliminate substructures in LLMs, while Xia et al. (2024) use a joint loss combining a pruning mask loss with the language modeling loss. However, optimizing these criteria can be computationally intensive. For example, the pruning step of Sheared-LLaMA (Xia et al., 2023) is 5x expensive compared to standard LM training, according to the authors. In contrast, thanks to the local gradient updates, our approach is computationally efficient, allowing us to compress a 47B model within minutes on a single A100 GPU. Regarding unstructured pruning, these methods do not provide any gains in terms of memory or speedup, at least with current algorithmic implementations. Semi-structured pruning (e.g., 2:4 and 4:8) (Sun et al., 2024;Frantar and Alistarh, 2023;Liu et al., 2024) does not lead to memory gain but can speed up processing on kernels optimized for such matrix structures. On the other hand, our method, which directly shrinks matrices, saves memory across all hardware and leads to speed up, as fewer computations are performed."
                        ],
                        "paper": {
                            "corpus_id": 274981759,
                            "title": "Lillama: Large Language Models Compression via Low-Rank Feature Distillation",
                            "authors": [
                                {
                                    "authorId": "2336865596",
                                    "name": "Yaya Sy"
                                },
                                {
                                    "authorId": "2257152061",
                                    "name": "Christophe Cerisara"
                                },
                                {
                                    "authorId": "1696945",
                                    "name": "I. Illina"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.80615234375
                    },
                    {
                        "id": "(Laborde et al., 2025)",
                        "snippets": [
                            "Unstructured pruning offers maximum theoretical compression by removing individual weights, but often results in irregular sparsity patterns that are challenging to accelerate on current hardware. Semi-structured approaches, like N:M sparsity patterns (Zhou et al., 2021), balance compression rates with hardware efficiency by enforcing regular pruning patterns, where N out of every M consecutive weights are pruned. Structured pruning (Ma et al., 2023) takes this further by removing entire structures, channels, or attention heads, with recent work showing that up to 50% of attention layers in large models can be removed while preserving performance [12]."
                        ],
                        "paper": {
                            "corpus_id": 278501529,
                            "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?",
                            "authors": [
                                {
                                    "authorId": "2360373404",
                                    "name": "Stanislas Laborde"
                                },
                                {
                                    "authorId": "2360359994",
                                    "name": "Martin Cousseau"
                                },
                                {
                                    "authorId": "40605834",
                                    "name": "Antoun Yaacoub"
                                },
                                {
                                    "authorId": "2266474578",
                                    "name": "Lionel Prevost"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.89892578125
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,(Liu et al., 2023). Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]",
                            "Table 7 indicates that the pruned models can achieve 1.19x \u223c 1.87x speedup compared to their dense counterparts."
                        ],
                        "paper": {
                            "corpus_id": 271909421,
                            "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
                            "authors": [
                                {
                                    "authorId": "2301331844",
                                    "name": "Guanchen Li"
                                },
                                {
                                    "authorId": "2270847262",
                                    "name": "Xiandong Zhao"
                                },
                                {
                                    "authorId": "2316517251",
                                    "name": "Lian Liu"
                                },
                                {
                                    "authorId": "2307589652",
                                    "name": "Zeping Li"
                                },
                                {
                                    "authorId": "2279335698",
                                    "name": "Dong Li"
                                },
                                {
                                    "authorId": "2279539118",
                                    "name": "Lu Tian"
                                },
                                {
                                    "authorId": "2316522396",
                                    "name": "Jie He"
                                },
                                {
                                    "authorId": "2316484957",
                                    "name": "Ashish Sirasao"
                                },
                                {
                                    "authorId": "2271751612",
                                    "name": "E. Barsoum"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.77734375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Methods and Techniques",
                "tldr": "Recent pruning methods include both structured approaches like LLM-Pruner and SliceGPT that remove entire components for hardware efficiency, and unstructured approaches like SparseGPT and Wanda that target individual weights for higher compression. Semi-structured methods like N:M sparsity patterns attempt to balance the benefits of both approaches. (14 sources)",
                "text": "\n## Structured Pruning Methods\n- **LLM-Pruner**: Employs gradient-based structured pruning to selectively remove non-critical coupled structures while preserving the model's core functionality. Performance recovery is achieved through LoRA fine-tuning in just 3 hours with only 50K samples. <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper> <Paper corpusId=\"272368391\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>\n\n- **SliceGPT**: Implements a post-training structured pruning scheme that replaces each weight matrix with a smaller dense matrix, reducing the embedding dimension. Can remove up to 25% of parameters while maintaining 99% zero-shot performance for LLAMA2-70B and OPT-66B models. <Paper corpusId=\"267301573\" paperTitle=\"(Ashkboos et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al._1, 2025)\" isShortName></Paper>\n\n- **ZipLM**: Adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Achieves state-of-the-art accuracy-vs-speedup trade-offs while matching desired target runtime speedups in any inference environment. <Paper corpusId=\"256662263\" paperTitle=\"(Kurtic et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271744772\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>\n\n- **CoFi**: A task-specific structured pruning method that jointly prunes coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules. Delivers highly parallelizable subnetworks without resorting to unlabeled data. <Paper corpusId=\"247922354\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper> <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>\n\n## Unstructured Pruning Methods\n- **SparseGPT**: Demonstrates that large-scale GPT-family models can be pruned to at least 50% sparsity in one shot without retraining, with minimal accuracy loss. Can be executed on models as large as OPT-175B and BLOOM-176B in under 4.5 hours and is compatible with semi-structured patterns and weight quantization. <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper> <Paper corpusId=\"267103825\" paperTitle=\"(Gong et al., 2024)\" isShortName></Paper>\n\n- **Wanda**: Implements a heuristic approach that removes weights based on the product of their magnitudes and activations without compensation. Achieves 60% sparsity on LLaMA-7B with minimal performance degradation across multiple downstream tasks. <Paper corpusId=\"271744772\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278338989\" paperTitle=\"(Hu et al., 2025)\" isShortName></Paper>\n\n- **ISC (Improved Sparse Compression)**: Leverages the OBS framework to calculate saliency for each weight using the inverse Hessian of the loss metric. This approach generates pruning masks and updates weights to maintain performance at high sparsity levels. <Paper corpusId=\"264146174\" paperTitle=\"(Shao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271744772\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>\n\n## Semi-Structured Pruning Methods\n- **N:M Sparsity Patterns**: Semi-structured approaches that enforce regular sparsity patterns where N out of every M consecutive weights are retained. The 2:4 sparsity pattern is particularly notable for its hardware support in NVIDIA's Ampere architecture, enabling up to 2\u00d7 speedup on A100 GPUs. <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al._1, 2025)\" isShortName></Paper>\n\n- **FISTA-Pruner**: Focuses on unstructured and semi-structured pruning, enabling further model compression and is orthogonal to structured pruning methods. Compatible with different sparsity patterns to balance accuracy and hardware efficiency. <Paper corpusId=\"271744772\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>\n\nThe research trend shows increasing interest in developing pruning methods that balance theoretical compression benefits with practical hardware acceleration. While unstructured pruning techniques like SparseGPT continue to achieve impressive compression ratios, structured approaches like LLM-Pruner are gaining popularity due to their immediate hardware efficiency benefits without requiring specialized support <Paper corpusId=\"266362404\" paperTitle=\"(An et al., 2023)\" isShortName></Paper> <Paper corpusId=\"275993741\" paperTitle=\"(Feng et al., 2025)\" isShortName></Paper>. Semi-structured approaches serve as a middle ground, offering more regular sparsity patterns that can be efficiently implemented in specialized hardware <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ma et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"
                        ],
                        "paper": {
                            "corpus_id": 258823276,
                            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "15532066",
                                    "name": "Xinyin Ma"
                                },
                                {
                                    "authorId": "150110431",
                                    "name": "Gongfan Fang"
                                },
                                {
                                    "authorId": "48631088",
                                    "name": "Xinchao Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 440
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Structured Pruning: This approach removes entire subsets of parameters like layers, channels, or filters, which is beneficial for hardware optimization due to more regular memory access patterns and simplified computations. The 'LLM-Pruner' (Kaddour et al., 2023) employs structured pruning to eliminate non-essential groups based on gradient data, thus maintaining critical functionalities. It also facilitates performance recovery through techniques such as LoRA, allowing efficient restoration with minimal data. 2. Unstructured Pruning: Unlike structured pruning, unstructured pruning removes individual weights across the model, offering finer granularity and potentially higher compression rates (Li et al., 2023a). However, this method typically results in sparse matrices, which can be less compatible with traditional hardware architectures, compromising computational efficiency. It is most suitable where maximum compression is needed without constraints on structural preservation."
                        ],
                        "paper": {
                            "corpus_id": 272368391,
                            "title": "On-Device Language Models: A Comprehensive Review",
                            "authors": [
                                {
                                    "authorId": "2316519813",
                                    "name": "Jiajun Xu"
                                },
                                {
                                    "authorId": "2294674012",
                                    "name": "Zhiyuan Li"
                                },
                                {
                                    "authorId": "2294845809",
                                    "name": "Wei Chen"
                                },
                                {
                                    "authorId": "2316514278",
                                    "name": "Qun Wang"
                                },
                                {
                                    "authorId": "2319809164",
                                    "name": "Xin Gao"
                                },
                                {
                                    "authorId": "2364055424",
                                    "name": "Qi Cai"
                                },
                                {
                                    "authorId": "2319410023",
                                    "name": "Ziyuan Ling"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 35
                        },
                        "score": 0.85986328125
                    },
                    {
                        "id": "(Ashkboos et al., 2024)",
                        "snippets": [
                            "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression"
                        ],
                        "paper": {
                            "corpus_id": 267301573,
                            "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
                            "authors": [
                                {
                                    "authorId": "9543395",
                                    "name": "Saleh Ashkboos"
                                },
                                {
                                    "authorId": "2008063761",
                                    "name": "Maximilian L. Croci"
                                },
                                {
                                    "authorId": "2281641743",
                                    "name": "Marcelo Gennari do Nascimento"
                                },
                                {
                                    "authorId": "2258547286",
                                    "name": "Torsten Hoefler"
                                },
                                {
                                    "authorId": "2266803418",
                                    "name": "James Hensman"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 184
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sengupta et al._1, 2025)",
                        "snippets": [
                            "Unstructured pruning (Sun et al., 2023) removes individual weights, producing sparse matrices that maintain performance but are less hardwareefficient. Semi-structured pruning (Frantar & Alistarh, 2023), such as the 2:4 sparsity pattern (Pool et al., 2021), introduces a hardware-friendly structured sparsity that accelerates computation. Structured pruning (Ashkboos et al., 2024;Yuan et al., 2023;Sengupta et al., 2025) takes a broader approach by removing entire components, such as Transformer layers (depth pruning) (Fan et al., 2019) or reducing embedding dimensions and attention heads (width pruning) (Zhu et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 278310893,
                            "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
                            "authors": [
                                {
                                    "authorId": "34920835",
                                    "name": "Ayan Sengupta"
                                },
                                {
                                    "authorId": "2345922770",
                                    "name": "Yash Goel"
                                },
                                {
                                    "authorId": "2249914540",
                                    "name": "Tanmoy Chakraborty"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.78271484375
                    },
                    {
                        "id": "(Kurtic et al., 2023)",
                        "snippets": [
                            "The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning (Hoefler et al., 2021), quantization [3], and distillation (Gou et al., 2020). In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy."
                        ],
                        "paper": {
                            "corpus_id": 256662263,
                            "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
                            "authors": [
                                {
                                    "authorId": "40992614",
                                    "name": "Eldar Kurtic"
                                },
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 26
                        },
                        "score": 0.90576171875
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "For structured pruning, SliceGPT (Ashkboos et al., 2024) and Eigenpruning (Vergara-Browne et al., 2024) utilize singular value decompositions to prune singular values of weight matrices and reduce model dimensions. ZipLM (Kurtic et al., 2023) adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Our proposed FISTAPruner focuses on unstructured and semi-structured pruning, and thus is orthogonal to these structured pruning methods, enabling further model compression. For unstructured and semi-structured pruning, SparseGPT (Frantar et al., 2023) and ISC (Shao et al., 2023) leverage the OBS framework to calculate saliency for each entry using the inverse Hessian of the loss metric, based on which pruning masks are generated and weights updated. Wanda (Sun et al., 2023) implements a heuristic approach, removing weights based on the product of their magnitudes and activations without compensation."
                        ],
                        "paper": {
                            "corpus_id": 271744772,
                            "title": "A Convex-optimization-based Layer-wise Post-training Pruner for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2114873379",
                                    "name": "Pengxiang Zhao"
                                },
                                {
                                    "authorId": "2283405371",
                                    "name": "Hanyu Hu"
                                },
                                {
                                    "authorId": "2293474362",
                                    "name": "Ping Li"
                                },
                                {
                                    "authorId": "2293396844",
                                    "name": "Yi Zheng"
                                },
                                {
                                    "authorId": "2293231668",
                                    "name": "Zhefeng Wang"
                                },
                                {
                                    "authorId": "2283433779",
                                    "name": "Xiaoming Yuan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.76611328125
                    },
                    {
                        "id": "(Xia et al., 2022)",
                        "snippets": [
                            "The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches."
                        ],
                        "paper": {
                            "corpus_id": 247922354,
                            "title": "Structured Pruning Learns Compact and Accurate Models",
                            "authors": [
                                {
                                    "authorId": "67284811",
                                    "name": "Mengzhou Xia"
                                },
                                {
                                    "authorId": "49164966",
                                    "name": "Zexuan Zhong"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 187
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "According to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning (Ma et al., 2023)43,(Liu et al., 2023), Unstructured Pruning (Han et al., 2015)[15], and Semi-Structured Pruning (Frantar et al., 2023)38,29,32,(Pool et al., 2021). Structured pruning physically eliminates substructures like attention heads (Ma et al., 2023), embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters (Ma et al., 2023)43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy (Frantar et al., 2023)38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32](Pool et al., 2021)(Frantar et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 272910976,
                            "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "150110431",
                                    "name": "Gongfan Fang"
                                },
                                {
                                    "authorId": "1989015",
                                    "name": "Hongxu Yin"
                                },
                                {
                                    "authorId": "31225166",
                                    "name": "Saurav Muralidharan"
                                },
                                {
                                    "authorId": "2273650910",
                                    "name": "Greg Heinrich"
                                },
                                {
                                    "authorId": "2322991313",
                                    "name": "Jeff Pool"
                                },
                                {
                                    "authorId": "2273651410",
                                    "name": "Jan Kautz"
                                },
                                {
                                    "authorId": "2824500",
                                    "name": "Pavlo Molchanov"
                                },
                                {
                                    "authorId": "2322993154",
                                    "name": "Xinchao Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 10
                        },
                        "score": 0.8720703125
                    },
                    {
                        "id": "(Frantar et al., 2023)",
                        "snippets": [
                            "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                        ],
                        "paper": {
                            "corpus_id": 255372747,
                            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
                            "authors": [
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 734
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gong et al., 2024)",
                        "snippets": [
                            "By analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67].\n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation."
                        ],
                        "paper": {
                            "corpus_id": 267103825,
                            "title": "A Review of Neural Network Lightweighting Techniques",
                            "authors": [
                                {
                                    "authorId": "2280410840",
                                    "name": "Ziyi Gong"
                                },
                                {
                                    "authorId": "2280726529",
                                    "name": "Huifu Zhang"
                                },
                                {
                                    "authorId": "2280414619",
                                    "name": "Hao Yang"
                                },
                                {
                                    "authorId": "2280959623",
                                    "name": "Fangjun Liu"
                                },
                                {
                                    "authorId": "2280411835",
                                    "name": "Fan Luo"
                                }
                            ],
                            "year": 2024,
                            "venue": "Innovation &amp; Technology Advances",
                            "n_citations": 0
                        },
                        "score": 0.8623046875
                    },
                    {
                        "id": "(Hu et al., 2025)",
                        "snippets": [
                            "While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware or sparse computations."
                        ],
                        "paper": {
                            "corpus_id": 278338989,
                            "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods",
                            "authors": [
                                {
                                    "authorId": "2283405371",
                                    "name": "Hanyu Hu"
                                },
                                {
                                    "authorId": "2283433779",
                                    "name": "Xiaoming Yuan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Shao et al., 2023)",
                        "snippets": [
                            "Various Large Language Models (LLMs) from the Generative Pretrained Transformer (GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs."
                        ],
                        "paper": {
                            "corpus_id": 264146174,
                            "title": "One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2216418068",
                                    "name": "Hang Shao"
                                },
                                {
                                    "authorId": "2168549481",
                                    "name": "Bei Liu"
                                },
                                {
                                    "authorId": "2259050251",
                                    "name": "Yanmin Qian"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                            "n_citations": 21
                        },
                        "score": 0
                    },
                    {
                        "id": "(An et al., 2023)",
                        "snippets": [
                            "Structured pruning (He and Xiao 2023), which prunes entire rows or columns of weights, offers a promising solution to the deployment challenges of LLMs. Unlike unstructured pruning, structured pruning reduces both parameters and inference time without relying on specific hardware, making it more widely applicable (Anwar et al., 2015). For effective structured pruning, it's crucial to have a metric that captures the collective significance of an entire row or column. However, current unstructured pruning techniques for LLMs, as seen in methods like (Sun et al. 2023;Frantar and Alistarh 2023), primarily focus on the importance of individual elements of each row in isolation. This absence of structured metrics that evaluate entire rows or columns makes them less suitable for structured pruning."
                        ],
                        "paper": {
                            "corpus_id": 266362404,
                            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2167834971",
                                    "name": "Yongqi An"
                                },
                                {
                                    "authorId": "2118489444",
                                    "name": "Xu Zhao"
                                },
                                {
                                    "authorId": "40418746",
                                    "name": "Tao Yu"
                                },
                                {
                                    "authorId": "2113727378",
                                    "name": "Ming Tang"
                                },
                                {
                                    "authorId": "2241943585",
                                    "name": "Jinqiao Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 61
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Feng et al., 2025)",
                        "snippets": [
                            "Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar et al., 2023)Sun et al., 2023) and structured pruning (Ma et al., 2023)An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns."
                        ],
                        "paper": {
                            "corpus_id": 275993741,
                            "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2332540527",
                                    "name": "Mingkuan Feng"
                                },
                                {
                                    "authorId": "2141911656",
                                    "name": "Jinyang Wu"
                                },
                                {
                                    "authorId": "2298428469",
                                    "name": "Shuai Zhang"
                                },
                                {
                                    "authorId": "2221575807",
                                    "name": "Pengpeng Shao"
                                },
                                {
                                    "authorId": "2300370837",
                                    "name": "Ruihan Jin"
                                },
                                {
                                    "authorId": "1718662",
                                    "name": "Zhengqi Wen"
                                },
                                {
                                    "authorId": "2298423822",
                                    "name": "Jianhua Tao"
                                },
                                {
                                    "authorId": "1471057495",
                                    "name": "Feihu Che"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8154296875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.319575
    }
}