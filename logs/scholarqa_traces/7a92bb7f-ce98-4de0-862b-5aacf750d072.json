{
    "query": "Can you provide references on graph-based approaches in multi-document summarization, including both classical extractive methods (like LexRank) and recent neural models that incorporate graph representations?",
    "user_id": "lib_user",
    "task_id": "7a92bb7f-ce98-4de0-862b-5aacf750d072",
    "timestamp": "2025-06-24T00:04:03.365415",
    "n_retrieval": 256,
    "n_retrieved": 265,
    "n_candidates": 43,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.707268,
    "decomposed_query": {
        "rewritten_query": "Graph-based approaches in multi-document summarization, including both classical extractive methods like LexRank and recent neural models that incorporate graph representations.",
        "keyword_query": "graph-based multi-document summarization LexRank extractive neural models graph representations",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.00999,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Entity-Aware Abstractive Multi-Document Summarization",
            "venue": "Findings",
            "year": 2021,
            "reference_count": 57,
            "citation_count": 28,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-acl.30.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.findings-acl.30, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Hao Zhou"
                },
                {
                    "authorId": "2053308860",
                    "name": "Weidong Ren"
                },
                {
                    "authorId": "150112803",
                    "name": "Gongshen Liu"
                },
                {
                    "authorId": "153253583",
                    "name": "Bo Su"
                },
                {
                    "authorId": "143844110",
                    "name": "Wei Lu"
                }
            ],
            "abstract": ",",
            "corpus_id": 236478143,
            "sentences": [
                {
                    "corpus_id": "236478143",
                    "title": "Entity-Aware Abstractive Multi-Document Summarization",
                    "text": "Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan and Radev, 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Wan (2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. Fan et al. (2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. Huang et al. (2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. Li et al. (2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, Wang et al. (2020a) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization. Our work is partly similar to theirs, but we construct heterogeneous graphs composed of text unit nodes and entity nodes for abstractive multi-document summarization.",
                    "score": 0.7897238584291848,
                    "section_title": "Graph-based Document Summarization",
                    "char_start_offset": 5732,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 73
                        },
                        {
                            "start": 74,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 474
                        },
                        {
                            "start": 475,
                            "end": 669
                        },
                        {
                            "start": 672,
                            "end": 813
                        },
                        {
                            "start": 814,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1165
                        },
                        {
                            "start": 1166,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1624
                        },
                        {
                            "start": 1625,
                            "end": 1791
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 183,
                            "end": 206,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 333,
                            "end": 343,
                            "matchedPaperCorpusId": "17446655"
                        },
                        {
                            "start": 723,
                            "end": 740,
                            "matchedPaperCorpusId": "26698484"
                        },
                        {
                            "start": 814,
                            "end": 836,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 988,
                            "end": 1005,
                            "matchedPaperCorpusId": "202785778"
                        },
                        {
                            "start": 1166,
                            "end": 1185,
                            "matchedPaperCorpusId": "218487279"
                        },
                        {
                            "start": 1340,
                            "end": 1356,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1471,
                            "end": 1490,
                            "matchedPaperCorpusId": "216552978"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: Entity-Aware Abstractive Multi-Document Summarization\n# Venue: Findings\n# Authors: Hao Zhou, Weidong Ren, Gongshen Liu, Bo Su, Wei Lu\n## Abstract\n,\n## Graph-based Document Summarization\nGraph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan and Radev, 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Wan (2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. Fan et al. (2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. Huang et al. (2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. Li et al. (2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, Wang et al. (2020a) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization. Our work is partly similar to theirs, but we construct heterogeneous graphs composed of text unit nodes and entity nodes for abstractive multi-document summarization.",
            "reference_string": "[236478143 | Zhou et al. | 2021 | Citations: 28]"
        },
        {
            "title": "Analysis of GraphSum\u2019s Attention Weights to Improve the Explainability of Multi-Document Summarization",
            "venue": "International Conference on Information Integration and Web-based Applications & Services",
            "year": 2021,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2105.11908",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.11908, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1938939",
                    "name": "M. Hickmann"
                },
                {
                    "authorId": "2105164214",
                    "name": "Fabian Wurzberger"
                },
                {
                    "authorId": "2105165105",
                    "name": "Megi Hoxhalli"
                },
                {
                    "authorId": "2105164598",
                    "name": "Arne Lochner"
                },
                {
                    "authorId": "2025314313",
                    "name": "Jessica T\u00f6llich"
                },
                {
                    "authorId": "1753135",
                    "name": "A. Scherp"
                }
            ],
            "abstract": "Modern multi-document summarization (MDS) methods are based on transformer architectures. They generate state of the art summaries, but lack explainability. To overcome this, we analyze the attention weights of a graph-based MDS such as GraphSum. We compare GraphSum\u2019s performance utilizing different textual units, i. e., sentences versus paragraphs, on two MDS benchmark datasets, namely WikiSum and MultiNews. Our experiments show that paragraph-level representations provide the best summarization performance. Subsequently, we focus on analyzing the paragraph-level attention weights of GraphSum\u2019s multi-heads and decoding layers. Furthermore, we examine source origin information via text similarity over a ROUGE-based reference metric. We observe a high correlation between the attention weights and our reference metric, especially on the later decoding layers. Finally, we investigate if the generated summaries follow a pattern of positional bias. Our results show that there is a high correlation between the position of paragraphs in the input documents and the content provided for the generated summary.",
            "corpus_id": 235187219,
            "sentences": [
                {
                    "corpus_id": "235187219",
                    "title": "Analysis of GraphSum\u2019s Attention Weights to Improve the Explainability of Multi-Document Summarization",
                    "text": "Multi-document Summarization (MDS) refers to the task of providing a concise representation of multiple documents with overlapping textual content [14]. Besides transformer-based models for summarization [16,17], approaches on graph neural networks and specifically using knowledge graphs have gained popularity [8,12,20,21] while recent models combine graph neural networks with a transformer architecture [8,12].\n\nGenerally, we distinguish extractive and abstractive MDS. In extractive MDS, a model is trained to select the most relevant input sentences or paragraphs in order to create an optimal summary with regard to salience and coherence [21]. In comparison, abstractive MDS is able to generate new texts which are not present in the input documents, which improve the summary quality [8]. For extractive MDS, a graph-based representation can help to detect salient vertices, so that they are included in the generated summary to cover all important information. In abstractive MDS, salient vertices can guide the summarization process via attention mechanisms of the transformer architecture. To this end, the text from multiple documents is represented as independent, so-called textual units. The graph's vertices represent the textual units such as paragraphs and sentences, while the edges model a semantic relationship between the units like a text-based cosine similarity. Different textual units have been used for graph-based text summarization in the past, as discourse relations [8,20], sentences [3,21], or paragraphs [12].\n\nRecently, Li et al. [12] proposed GraphSum, a state of the art graph-based MDS, which utilizes a graph structure based on paragraphs as textual units to improve the transformer architecture and guide the summary generation process [12]. Paragraphs are assumed to divide text into contiguous topics. Leveraging interparagraph relations can provide the model additional information for detecting contextual relations between topics. The GraphSum model showed substantial improvements compared to strong MDS baselines such as LEAD-3 and LexRank [5]. The authors of Graph-Sum [12] considered paragraph-level textual units as vertices for their graph structure. Also Huang et al. [8] used paragraph-level representations for the graph-based MDS task",
                    "score": 0.7342863415464693,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 147,
                            "end": 151,
                            "matchedPaperCorpusId": "8448956"
                        },
                        {
                            "start": 321,
                            "end": 324,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 646,
                            "end": 650,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 1516,
                            "end": 1519,
                            "matchedPaperCorpusId": "337730"
                        },
                        {
                            "start": 1519,
                            "end": 1522,
                            "matchedPaperCorpusId": "6532096"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96337890625
                }
            ],
            "relevance_judgement": 0.96337890625,
            "relevance_judgment_input_expanded": "# Title: Analysis of GraphSum\u2019s Attention Weights to Improve the Explainability of Multi-Document Summarization\n# Venue: International Conference on Information Integration and Web-based Applications & Services\n# Authors: M. Hickmann, Fabian Wurzberger, Megi Hoxhalli, Arne Lochner, Jessica T\u00f6llich, A. Scherp\n## Abstract\nModern multi-document summarization (MDS) methods are based on transformer architectures. They generate state of the art summaries, but lack explainability. To overcome this, we analyze the attention weights of a graph-based MDS such as GraphSum. We compare GraphSum\u2019s performance utilizing different textual units, i. e., sentences versus paragraphs, on two MDS benchmark datasets, namely WikiSum and MultiNews. Our experiments show that paragraph-level representations provide the best summarization performance. Subsequently, we focus on analyzing the paragraph-level attention weights of GraphSum\u2019s multi-heads and decoding layers. Furthermore, we examine source origin information via text similarity over a ROUGE-based reference metric. We observe a high correlation between the attention weights and our reference metric, especially on the later decoding layers. Finally, we investigate if the generated summaries follow a pattern of positional bias. Our results show that there is a high correlation between the position of paragraphs in the input documents and the content provided for the generated summary.\n## INTRODUCTION\nMulti-document Summarization (MDS) refers to the task of providing a concise representation of multiple documents with overlapping textual content [14]. Besides transformer-based models for summarization [16,17], approaches on graph neural networks and specifically using knowledge graphs have gained popularity [8,12,20,21] while recent models combine graph neural networks with a transformer architecture [8,12].\n\nGenerally, we distinguish extractive and abstractive MDS. In extractive MDS, a model is trained to select the most relevant input sentences or paragraphs in order to create an optimal summary with regard to salience and coherence [21]. In comparison, abstractive MDS is able to generate new texts which are not present in the input documents, which improve the summary quality [8]. For extractive MDS, a graph-based representation can help to detect salient vertices, so that they are included in the generated summary to cover all important information. In abstractive MDS, salient vertices can guide the summarization process via attention mechanisms of the transformer architecture. To this end, the text from multiple documents is represented as independent, so-called textual units. The graph's vertices represent the textual units such as paragraphs and sentences, while the edges model a semantic relationship between the units like a text-based cosine similarity. Different textual units have been used for graph-based text summarization in the past, as discourse relations [8,20], sentences [3,21], or paragraphs [12].\n\nRecently, Li et al. [12] proposed GraphSum, a state of the art graph-based MDS, which utilizes a graph structure based on paragraphs as textual units to improve the transformer architecture and guide the summary generation process [12]. Paragraphs are assumed to divide text into contiguous topics. Leveraging interparagraph relations can provide the model additional information for detecting contextual relations between topics. The GraphSum model showed substantial improvements compared to strong MDS baselines such as LEAD-3 and LexRank [5]. The authors of Graph-Sum [12] considered paragraph-level textual units as vertices for their graph structure. Also Huang et al. [8] used paragraph-level representations for the graph-based MDS task",
            "reference_string": "[235187219 | Hickmann et al. | 2021 | Citations: 1]"
        },
        {
            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 63,
            "citation_count": 136,
            "influential_citation_count": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.555.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.10043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48624966",
                    "name": "Wei Li"
                },
                {
                    "authorId": "2107521158",
                    "name": "Xinyan Xiao"
                },
                {
                    "authorId": null,
                    "name": "Jiachen Liu"
                },
                {
                    "authorId": "40354707",
                    "name": "Hua Wu"
                },
                {
                    "authorId": "144270731",
                    "name": "Haifeng Wang"
                },
                {
                    "authorId": "2117218629",
                    "name": "Junping Du"
                }
            ],
            "abstract": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
            "corpus_id": 218718706,
            "sentences": [
                {
                    "corpus_id": "218718706",
                    "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                    "text": "Most previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been 1 Codes and results are in: https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2020-GraphSum further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences.",
                    "score": 0.5781379954910373,
                    "section_title": "Graph-based MDS",
                    "char_start_offset": 3840,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 292
                        },
                        {
                            "start": 293,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 661
                        },
                        {
                            "start": 662,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 983
                        },
                        {
                            "start": 984,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1448
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 293,
                            "end": 315,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 413,
                            "end": 438,
                            "matchedPaperCorpusId": "577937"
                        },
                        {
                            "start": 520,
                            "end": 530,
                            "matchedPaperCorpusId": "17446655"
                        },
                        {
                            "start": 945,
                            "end": 965,
                            "matchedPaperCorpusId": "58509157"
                        },
                        {
                            "start": 965,
                            "end": 982,
                            "matchedPaperCorpusId": "14383376"
                        },
                        {
                            "start": 999,
                            "end": 1021,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 1282,
                            "end": 1299,
                            "matchedPaperCorpusId": "199466313"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                },
                {
                    "corpus_id": "218718706",
                    "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                    "text": "Multi-document summarization (MDS) brings great challenges to the widely used sequence-tosequence (Seq2Seq) neural architecture as it requires effective representation of multiple input documents and content organization of long summaries. For MDS, different documents may contain the same content, include additional information, and present complementary or contradictory information (Radev, 2000). So different from single document summarization (SDS), cross-document links are very important in extracting salient information, detecting redundancy and generating overall coherent summaries for MDS. Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models. However, they are not well studied by most abstractive approaches, especially the end-to-end neural approaches. Few work has studied the effectiveness of explicit graph representations on neural abstractive MDS. \n\nIn this paper, we develop a neural abstractive MDS model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries. Our model augments the end-toend neural architecture with the ability to incorporate well-established graphs into both the document representation and summary generation processes. Specifically, a graph-informed attention mechanism is developed to incorporate graphs into the document encoding process, which enables our model to capture richer cross-document relations. Furthermore, graphs are utilized to guide the summary generation process via a hierarchical graph attention mechanism, which takes advantage of the explicit graph structure to help organize the summary content. Benefiting from the graph modeling, our model can extract salient information from long documents and generate coherent summaries more effectively. We experiment with three types of graph representations, including similarity graph, topic graph and discourse graph, which all significantly improve the MDS performance.",
                    "score": 0.7743836116186534,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 400
                        },
                        {
                            "start": 401,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 780
                        },
                        {
                            "start": 781,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1208
                        },
                        {
                            "start": 1209,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1420
                        },
                        {
                            "start": 1423,
                            "end": 1630
                        },
                        {
                            "start": 1631,
                            "end": 1811
                        },
                        {
                            "start": 1812,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2212
                        },
                        {
                            "start": 2213,
                            "end": 2360
                        },
                        {
                            "start": 2361,
                            "end": 2531
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 386,
                            "end": 399,
                            "matchedPaperCorpusId": "10103200"
                        },
                        {
                            "start": 1036,
                            "end": 1059,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1109,
                            "end": 1135,
                            "matchedPaperCorpusId": "337730"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                },
                {
                    "corpus_id": "218718706",
                    "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                    "text": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "score": 0.5642473185802919,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8916015625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: Leveraging Graph to Improve Abstractive Multi-Document Summarization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, Junping Du\n## Abstract\nGraphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.\n## Introduction\nMulti-document summarization (MDS) brings great challenges to the widely used sequence-tosequence (Seq2Seq) neural architecture as it requires effective representation of multiple input documents and content organization of long summaries. For MDS, different documents may contain the same content, include additional information, and present complementary or contradictory information (Radev, 2000). So different from single document summarization (SDS), cross-document links are very important in extracting salient information, detecting redundancy and generating overall coherent summaries for MDS. Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models. However, they are not well studied by most abstractive approaches, especially the end-to-end neural approaches. Few work has studied the effectiveness of explicit graph representations on neural abstractive MDS. \n\nIn this paper, we develop a neural abstractive MDS model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries. Our model augments the end-toend neural architecture with the ability to incorporate well-established graphs into both the document representation and summary generation processes. Specifically, a graph-informed attention mechanism is developed to incorporate graphs into the document encoding process, which enables our model to capture richer cross-document relations. Furthermore, graphs are utilized to guide the summary generation process via a hierarchical graph attention mechanism, which takes advantage of the explicit graph structure to help organize the summary content. Benefiting from the graph modeling, our model can extract salient information from long documents and generate coherent summaries more effectively. We experiment with three types of graph representations, including similarity graph, topic graph and discourse graph, which all significantly improve the MDS performance.\n\n## Graph-based MDS\nMost previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been 1 Codes and results are in: https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2020-GraphSum further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences.",
            "reference_string": "[218718706 | Li et al. | 2020 | Citations: 136]"
        },
        {
            "title": "Two-phase Multi-document Event Summarization on Core Event Graphs",
            "venue": "Journal of Artificial Intelligence Research",
            "year": 2022,
            "reference_count": 54,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/13267/26820",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1613/jair.1.13267?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1613/jair.1.13267, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48354529",
                    "name": "Zengjian Chen"
                },
                {
                    "authorId": "2116315442",
                    "name": "Jin Xu"
                },
                {
                    "authorId": "145865588",
                    "name": "M. Liao"
                },
                {
                    "authorId": "2138967543",
                    "name": "Tong Xue"
                },
                {
                    "authorId": "2190820519",
                    "name": "Kun He"
                }
            ],
            "abstract": "Succinct event description based on multiple documents is critical to news systems as well as search engines. Different from existing summarization or event tasks, Multi-document Event Summarization (MES) aims at the query-level event sequence generation, which has extra constraints on event expression and conciseness. Identifying and summarizing the key event from a set of related articles is a challenging task that has not been sufficiently studied, mainly because online articles exhibit characteristics of redundancy and sparsity, and a perfect event summarization needs high level information fusion among diverse sentences and articles. To address these challenges, we propose a two-phase framework for the MES task, that first performs event semantic graph construction and dominant event detection via graph-sequence matching, then summarizes the extracted key event by an event-aware pointer generator. For experiments in the new task, we construct two large-scale real-world datasets for training and assessment. Extensive evaluations show that the proposed framework significantly outperforms the related baseline methods, with the most dominant event of the articles effectively identified and correctly summarized.",
            "corpus_id": 250374734,
            "sentences": [
                {
                    "corpus_id": "250374734",
                    "title": "Two-phase Multi-document Event Summarization on Core Event Graphs",
                    "text": "Existing multi-document summarization methods mainly focus on sentence-level summarization and can be categorized into extractive and abstractive methods. Most extractive methods are operated over graph-based representations of sentences or passages with edge weights computed by tf-idf (Erkan et al., 2004), discourse relations (Christensen et al., 2013) or sentence embeddings (Yasunaga et al., 2017b) and then a specific algorithm is further adopted for ranking text units for inclusion in the final summary. More recently, some extractive summarization works also utilize graph convolutional networks for salient sentences estimation (Kipf et al., 2016) and sentence ordering (Yin et al., 2019). Abstractive models, especially neural abstractive ones, have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Lewis et al., 2019). However, the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward due to the lack of sufficient training data and the computational challenge of processing multiple documents. Intuitively, graph-based extractive methods are suitable to identify relationship of different sentences or documents and extract salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting. Hence, abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020). Our model, which also combines a graph-based extractive module and neural abstractive sequence-to-sequence architecture, is a higher level summarization and focuses on the core event summarization from multiple documents, which is more challenging.",
                    "score": 0.7811344608117196,
                    "section_title": "Multi-document Summarization",
                    "char_start_offset": 5936,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 879
                        },
                        {
                            "start": 880,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1357
                        },
                        {
                            "start": 1358,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1710
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 287,
                            "end": 307,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 821,
                            "end": 839,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 839,
                            "end": 859,
                            "matchedPaperCorpusId": "21850704"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95556640625
                }
            ],
            "relevance_judgement": 0.95556640625,
            "relevance_judgment_input_expanded": "# Title: Two-phase Multi-document Event Summarization on Core Event Graphs\n# Venue: Journal of Artificial Intelligence Research\n# Authors: Zengjian Chen, Jin Xu, M. Liao, Tong Xue, Kun He\n## Abstract\nSuccinct event description based on multiple documents is critical to news systems as well as search engines. Different from existing summarization or event tasks, Multi-document Event Summarization (MES) aims at the query-level event sequence generation, which has extra constraints on event expression and conciseness. Identifying and summarizing the key event from a set of related articles is a challenging task that has not been sufficiently studied, mainly because online articles exhibit characteristics of redundancy and sparsity, and a perfect event summarization needs high level information fusion among diverse sentences and articles. To address these challenges, we propose a two-phase framework for the MES task, that first performs event semantic graph construction and dominant event detection via graph-sequence matching, then summarizes the extracted key event by an event-aware pointer generator. For experiments in the new task, we construct two large-scale real-world datasets for training and assessment. Extensive evaluations show that the proposed framework significantly outperforms the related baseline methods, with the most dominant event of the articles effectively identified and correctly summarized.\n## Multi-document Summarization\nExisting multi-document summarization methods mainly focus on sentence-level summarization and can be categorized into extractive and abstractive methods. Most extractive methods are operated over graph-based representations of sentences or passages with edge weights computed by tf-idf (Erkan et al., 2004), discourse relations (Christensen et al., 2013) or sentence embeddings (Yasunaga et al., 2017b) and then a specific algorithm is further adopted for ranking text units for inclusion in the final summary. More recently, some extractive summarization works also utilize graph convolutional networks for salient sentences estimation (Kipf et al., 2016) and sentence ordering (Yin et al., 2019). Abstractive models, especially neural abstractive ones, have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Lewis et al., 2019). However, the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward due to the lack of sufficient training data and the computational challenge of processing multiple documents. Intuitively, graph-based extractive methods are suitable to identify relationship of different sentences or documents and extract salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting. Hence, abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020). Our model, which also combines a graph-based extractive module and neural abstractive sequence-to-sequence architecture, is a higher level summarization and focuses on the core event summarization from multiple documents, which is more challenging.",
            "reference_string": "[250374734 | Chen et al. | 2022 | Citations: 2]"
        },
        {
            "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 53,
            "citation_count": 22,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.emnlp-main.333.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.12645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108267116",
                    "name": "Moye Chen"
                },
                {
                    "authorId": "48624966",
                    "name": "Wei Li"
                },
                {
                    "authorId": null,
                    "name": "Jiachen Liu"
                },
                {
                    "authorId": "2107521158",
                    "name": "Xinyan Xiao"
                },
                {
                    "authorId": "40354707",
                    "name": "Hua Wu"
                },
                {
                    "authorId": "144270731",
                    "name": "Haifeng Wang"
                }
            ],
            "abstract": "Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary, which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the coherence and conciseness of the whole summary. In this paper, we propose a novel MDS framework (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub-graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly outputs an integrate summary in the form of sub-graph which is more informative and coherent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architecture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks.",
            "corpus_id": 239768911,
            "sentences": [
                {
                    "corpus_id": "239768911",
                    "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
                    "text": "The graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. Erkan and Radev (2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2020) both use discourse graphs to generate concise and informative summaries. Li et al. (2016) and Li and Zhuge (2019) propose to utilize event relation graph to represent documents for MDS. However, most existing graph-based summarization methods only consider the graph structure of source document. They neglect that summary is also a graph and its graph structure can reflect the quality of a summary. For example, in a similarity graph, if selected sentences are lexical similar, the summary is probably redundant. And in a discourse graph, if selected sentences have strong discourse connections, the summary tend to be coherent.\n\nWe argue that the graph structure of summary is equally important as the source document. Document graph helps to extract salient sentences, while summary graph helps to evluate the quality of summary. Based on this thought, we propose a novel MDS framework SgSum which transforms summarization into the problem of sub-graph selection. SgSum captures relation of sentences both in whole graph structure (source documents) and sub-graph structures (candidate summaries). Moreover, in our framework, summary is viewed as a whole unit in the form of sub-graph. Thus, SgSum can generate more coherent and informative results than traditional sentence-level extractors. Figure 1 shows the overview of our framework. Firstly, source documents are transformed into a relation graph by well-known graph construction methods such as similarity graph and discourse graph. Sentences are the basic information units and represented as nodes in the graph. And relations between sentences are represented as edges. For example, a similarity graph can be built based on cosine similarities between tf-idf representations of sentences. Let G denotes a graph representation matrix of the input documents, where G[i] [j] indicates the tf-idf weights between sentence S i and S j .",
                    "score": 0.6100012653846988,
                    "section_title": "Summarization as Sub-graph Selection",
                    "char_start_offset": 4318,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 175,
                            "end": 197,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 299,
                            "end": 325,
                            "matchedPaperCorpusId": "337730"
                        },
                        {
                            "start": 340,
                            "end": 357,
                            "matchedPaperCorpusId": "219036690"
                        },
                        {
                            "start": 431,
                            "end": 447,
                            "matchedPaperCorpusId": "6788641"
                        },
                        {
                            "start": 452,
                            "end": 471,
                            "matchedPaperCorpusId": "196170254"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9423828125
                },
                {
                    "corpus_id": "239768911",
                    "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
                    "text": "Most previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Christensen et al. (2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, Yasunaga et al. (2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. Li et al. (2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. Wu et al. (2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries.",
                    "score": 0.6791372350565622,
                    "section_title": "Graph-based Summarization",
                    "char_start_offset": 25715,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 153,
                            "end": 175,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 324,
                            "end": 349,
                            "matchedPaperCorpusId": "337730"
                        },
                        {
                            "start": 515,
                            "end": 537,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 846,
                            "end": 862,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1079,
                            "end": 1095,
                            "matchedPaperCorpusId": "235187330"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.939453125
                }
            ],
            "relevance_judgement": 0.9423828125,
            "relevance_judgment_input_expanded": "# Title: SgSum:Transforming Multi-document Summarization into Sub-graph Selection\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Moye Chen, Wei Li, Jiachen Liu, Xinyan Xiao, Hua Wu, Haifeng Wang\n## Abstract\nMost of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary, which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the coherence and conciseness of the whole summary. In this paper, we propose a novel MDS framework (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub-graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly outputs an integrate summary in the form of sub-graph which is more informative and coherent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architecture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks.\n## Summarization as Sub-graph Selection\nThe graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. Erkan and Radev (2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2020) both use discourse graphs to generate concise and informative summaries. Li et al. (2016) and Li and Zhuge (2019) propose to utilize event relation graph to represent documents for MDS. However, most existing graph-based summarization methods only consider the graph structure of source document. They neglect that summary is also a graph and its graph structure can reflect the quality of a summary. For example, in a similarity graph, if selected sentences are lexical similar, the summary is probably redundant. And in a discourse graph, if selected sentences have strong discourse connections, the summary tend to be coherent.\n\nWe argue that the graph structure of summary is equally important as the source document. Document graph helps to extract salient sentences, while summary graph helps to evluate the quality of summary. Based on this thought, we propose a novel MDS framework SgSum which transforms summarization into the problem of sub-graph selection. SgSum captures relation of sentences both in whole graph structure (source documents) and sub-graph structures (candidate summaries). Moreover, in our framework, summary is viewed as a whole unit in the form of sub-graph. Thus, SgSum can generate more coherent and informative results than traditional sentence-level extractors. Figure 1 shows the overview of our framework. Firstly, source documents are transformed into a relation graph by well-known graph construction methods such as similarity graph and discourse graph. Sentences are the basic information units and represented as nodes in the graph. And relations between sentences are represented as edges. For example, a similarity graph can be built based on cosine similarities between tf-idf representations of sentences. Let G denotes a graph representation matrix of the input documents, where G[i] [j] indicates the tf-idf weights between sentence S i and S j .\n\n## Graph-based Summarization\nMost previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Christensen et al. (2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, Yasunaga et al. (2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. Li et al. (2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. Wu et al. (2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries.",
            "reference_string": "[239768911 | Chen et al. | 2021 | Citations: 22]"
        },
        {
            "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis",
            "venue": "Information Processing & Management",
            "year": 2019,
            "reference_count": 70,
            "citation_count": 101,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://jultika.oulu.fi/files/nbnfi-fe2020042322174.pdf",
                "status": "GREEN",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/J.IPM.2019.04.003?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/J.IPM.2019.04.003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47302320",
                    "name": "Muhidin A. Mohamed"
                },
                {
                    "authorId": "1714030",
                    "name": "M. Oussalah"
                }
            ],
            "abstract": null,
            "corpus_id": 145993667,
            "sentences": [
                {
                    "corpus_id": "145993667",
                    "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis",
                    "text": "In a more feature-based fashion, Zhou et al. [13] applied ESA to query-focussed text summarisation and integrated an ESA-based technique and traditional sentence features to score document sentences using machine learning algorithms. \n\nIn addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization [14,15,18,19,20,21,22,23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev [14] proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization [18,19,20]. In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. [19] and Wan [20] considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts [24,25,28,22]. Of this, Azadani et al. [22] and Plaza et al. [24] have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28].",
                    "score": 0.7540794343974762,
                    "section_title": "Related Works",
                    "char_start_offset": 7625,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 233
                        },
                        {
                            "start": 236,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 652
                        },
                        {
                            "start": 653,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 931
                        },
                        {
                            "start": 932,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1333
                        },
                        {
                            "start": 1334,
                            "end": 1492
                        },
                        {
                            "start": 1493,
                            "end": 1669
                        },
                        {
                            "start": 1670,
                            "end": 1821
                        },
                        {
                            "start": 1822,
                            "end": 1987
                        },
                        {
                            "start": 1988,
                            "end": 2073
                        },
                        {
                            "start": 2074,
                            "end": 2204
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 45,
                            "end": 49,
                            "matchedPaperCorpusId": "45403764"
                        },
                        {
                            "start": 388,
                            "end": 392,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 395,
                            "end": 398,
                            "matchedPaperCorpusId": "16493063"
                        },
                        {
                            "start": 398,
                            "end": 401,
                            "matchedPaperCorpusId": "35133176"
                        },
                        {
                            "start": 401,
                            "end": 404,
                            "matchedPaperCorpusId": "17224077"
                        },
                        {
                            "start": 407,
                            "end": 410,
                            "matchedPaperCorpusId": "49217427"
                        },
                        {
                            "start": 559,
                            "end": 563,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1072,
                            "end": 1076,
                            "matchedPaperCorpusId": "16493063"
                        },
                        {
                            "start": 1076,
                            "end": 1079,
                            "matchedPaperCorpusId": "35133176"
                        },
                        {
                            "start": 1079,
                            "end": 1082,
                            "matchedPaperCorpusId": "17224077"
                        },
                        {
                            "start": 1371,
                            "end": 1375,
                            "matchedPaperCorpusId": "35133176"
                        },
                        {
                            "start": 1384,
                            "end": 1388,
                            "matchedPaperCorpusId": "17224077"
                        },
                        {
                            "start": 1655,
                            "end": 1659,
                            "matchedPaperCorpusId": "5674504"
                        },
                        {
                            "start": 1662,
                            "end": 1665,
                            "matchedPaperCorpusId": "16459894"
                        },
                        {
                            "start": 1665,
                            "end": 1668,
                            "matchedPaperCorpusId": "49217427"
                        },
                        {
                            "start": 1694,
                            "end": 1698,
                            "matchedPaperCorpusId": "49217427"
                        },
                        {
                            "start": 1716,
                            "end": 1720,
                            "matchedPaperCorpusId": "5674504"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94140625
                },
                {
                    "corpus_id": "145993667",
                    "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis",
                    "text": "\u2022 Alguliyev et al. ( 2013) is an optimization-based multi-document summarization model. It uses sentence-to-document collection, summary-to-document collection and sentence-tosentence relations to select salient sentences and reduce redundancy in the summary. \n\n\u2022 Kumar et al. ( 2014) is a topic-focused multi-document summarization strategy based on cross document relations and fuzzy reasoning. The approach follows three phases; extracting news components in the documents using Wordnet thresasure, named entity recognition and Gazateer lists, establishing cross document relations to identify relevant sentences, and finally the application of fuzzy reasoning to assign final sentence scores. \n\n\u2022 Sankarasubramaniam et al. ( 2014) is a graph-based summarization system leveraged with Wikipedia concepts. The study maps document sentences to Wikipedia concepts for the construction of a bipartite sentence-concept graphs. The summarizer then ranks sentences based on the ranking of corresponding concept nodes. It is noteworthy that this is one of the closest studies to our work in terms of the implementation, e.g., the use of concept graphs and Wikpedia as external knowledge. \n\n\u2022 Oliveira et al. ( 2016) is a generic single and multi document summarizer based on eighteen of the most widely used sentence scoring techniques including TextRank, TF-IDF, graph-based similarity, named-entities, sentence centrality and word co-occurrence. The scoring methods are used to compute the sentence importance in a document/cluster. This comparative study found that the strategy of combining features can lead to improved results. \n\n\u2022 Ren et al. ( 2016) is a redundancy-aware summarization system which considers the importance of the sentences and the redundancy in the summary simultaneously instead of modelling them as two separate processes. Particularly, this approach first evaluates the importance of each sentence and then selects sentences to generate a summary based on both the importance scores and redundancy among sentences. \n\n\u2022 Sun & Zhuge (2018) is a SDS approach which extracts semantic link network from documents using graph representations of different language granularities (words, sentences, paragraphs and sections) as nodes and semantic links between the nodes.",
                    "score": 0.6047576449556373,
                    "section_title": "Comparison with Benchmark Methods and Related Works",
                    "char_start_offset": 42316,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 87
                        },
                        {
                            "start": 88,
                            "end": 259
                        },
                        {
                            "start": 262,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 696
                        },
                        {
                            "start": 699,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 1013
                        },
                        {
                            "start": 1014,
                            "end": 1182
                        },
                        {
                            "start": 1185,
                            "end": 1442
                        },
                        {
                            "start": 1443,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1628
                        },
                        {
                            "start": 1631,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 2037
                        },
                        {
                            "start": 2040,
                            "end": 2285
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87646484375
                }
            ],
            "relevance_judgement": 0.94140625,
            "relevance_judgment_input_expanded": "# Title: SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis\n# Venue: Information Processing & Management\n# Authors: Muhidin A. Mohamed, M. Oussalah\n## Abstract\nNone\n## Related Works\nIn a more feature-based fashion, Zhou et al. [13] applied ESA to query-focussed text summarisation and integrated an ESA-based technique and traditional sentence features to score document sentences using machine learning algorithms. \n\nIn addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization [14,15,18,19,20,21,22,23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev [14] proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization [18,19,20]. In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. [19] and Wan [20] considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts [24,25,28,22]. Of this, Azadani et al. [22] and Plaza et al. [24] have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28].\n\n## Comparison with Benchmark Methods and Related Works\n\u2022 Alguliyev et al. ( 2013) is an optimization-based multi-document summarization model. It uses sentence-to-document collection, summary-to-document collection and sentence-tosentence relations to select salient sentences and reduce redundancy in the summary. \n\n\u2022 Kumar et al. ( 2014) is a topic-focused multi-document summarization strategy based on cross document relations and fuzzy reasoning. The approach follows three phases; extracting news components in the documents using Wordnet thresasure, named entity recognition and Gazateer lists, establishing cross document relations to identify relevant sentences, and finally the application of fuzzy reasoning to assign final sentence scores. \n\n\u2022 Sankarasubramaniam et al. ( 2014) is a graph-based summarization system leveraged with Wikipedia concepts. The study maps document sentences to Wikipedia concepts for the construction of a bipartite sentence-concept graphs. The summarizer then ranks sentences based on the ranking of corresponding concept nodes. It is noteworthy that this is one of the closest studies to our work in terms of the implementation, e.g., the use of concept graphs and Wikpedia as external knowledge. \n\n\u2022 Oliveira et al. ( 2016) is a generic single and multi document summarizer based on eighteen of the most widely used sentence scoring techniques including TextRank, TF-IDF, graph-based similarity, named-entities, sentence centrality and word co-occurrence. The scoring methods are used to compute the sentence importance in a document/cluster. This comparative study found that the strategy of combining features can lead to improved results. \n\n\u2022 Ren et al. ( 2016) is a redundancy-aware summarization system which considers the importance of the sentences and the redundancy in the summary simultaneously instead of modelling them as two separate processes. Particularly, this approach first evaluates the importance of each sentence and then selects sentences to generate a summary based on both the importance scores and redundancy among sentences. \n\n\u2022 Sun & Zhuge (2018) is a SDS approach which extracts semantic link network from documents using graph representations of different language granularities (words, sentences, paragraphs and sections) as nodes and semantic links between the nodes.",
            "reference_string": "[145993667 | Mohamed et al. | 2019 | Citations: 101]"
        },
        {
            "title": "Improved Affinity Graph Based Multi-Document Summarization",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2006,
            "reference_count": 16,
            "citation_count": 109,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.5555/1614049.1614095",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N06-2046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145078589",
                    "name": "Xiaojun Wan"
                },
                {
                    "authorId": "1743923",
                    "name": "Jianwu Yang"
                }
            ],
            "abstract": "This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems.",
            "corpus_id": 5457260,
            "sentences": [
                {
                    "corpus_id": "5457260",
                    "title": "Improved Affinity Graph Based Multi-Document Summarization",
                    "text": "Automated multi-document summarization has drawn much attention in recent years. Multidocument summary is usually used to provide concise topic description about a cluster of documents and facilitate the users to browse the document cluster. A particular challenge for multi-document summarization is that the information stored in different documents inevitably overlaps with each other, and hence we need effective summarization methods to merge information stored in different documents, and if possible, contrast their differences. \n\nA variety of multi-document summarization methods have been developed recently. In this study, we focus on extractive summarization, which involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting t e sentences with highest scores. \n\nMEAD is an implementation of the centroid-based method (Radev et al., 2004) that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TF*IDF, etc. NeATS (Lin and Hovy, 2002) selects important content using entence position, term frequency, topic signature and term clustering, and then uses MMR (Goldstein et al., 1999) to remove redundancy. XDoX (Hardy et al., 1998) identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Harabagiu and Lacatusu (2005) investigate different topic representations and extraction methods. \n\nGraph-based methods have been proposed to rank sentences or passages. Websumm (Mani and Bloedorn, 2000) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization. \n\nIn this study, we extend the above graph-based works by proposing an integrated framework for considering both information richness and information novelty of a sentence based on sentence affinity graph.",
                    "score": 0.591998654942947,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 80
                        },
                        {
                            "start": 81,
                            "end": 241
                        },
                        {
                            "start": 242,
                            "end": 535
                        },
                        {
                            "start": 538,
                            "end": 617
                        },
                        {
                            "start": 618,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 821
                        },
                        {
                            "start": 824,
                            "end": 1221
                        },
                        {
                            "start": 1222,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1492
                        },
                        {
                            "start": 1495,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1756
                        },
                        {
                            "start": 1757,
                            "end": 1889
                        },
                        {
                            "start": 1890,
                            "end": 2033
                        },
                        {
                            "start": 2036,
                            "end": 2239
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 879,
                            "end": 898,
                            "matchedPaperCorpusId": "6354619"
                        },
                        {
                            "start": 1573,
                            "end": 1598,
                            "matchedPaperCorpusId": "9177142"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9384765625
                }
            ],
            "relevance_judgement": 0.9384765625,
            "relevance_judgment_input_expanded": "# Title: Improved Affinity Graph Based Multi-Document Summarization\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Xiaojun Wan, Jianwu Yang\n## Abstract\nThis paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems.\n## Introduction\nAutomated multi-document summarization has drawn much attention in recent years. Multidocument summary is usually used to provide concise topic description about a cluster of documents and facilitate the users to browse the document cluster. A particular challenge for multi-document summarization is that the information stored in different documents inevitably overlaps with each other, and hence we need effective summarization methods to merge information stored in different documents, and if possible, contrast their differences. \n\nA variety of multi-document summarization methods have been developed recently. In this study, we focus on extractive summarization, which involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting t e sentences with highest scores. \n\nMEAD is an implementation of the centroid-based method (Radev et al., 2004) that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TF*IDF, etc. NeATS (Lin and Hovy, 2002) selects important content using entence position, term frequency, topic signature and term clustering, and then uses MMR (Goldstein et al., 1999) to remove redundancy. XDoX (Hardy et al., 1998) identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Harabagiu and Lacatusu (2005) investigate different topic representations and extraction methods. \n\nGraph-based methods have been proposed to rank sentences or passages. Websumm (Mani and Bloedorn, 2000) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization. \n\nIn this study, we extend the above graph-based works by proposing an integrated framework for considering both information richness and information novelty of a sentence based on sentence affinity graph.",
            "reference_string": "[5457260 | Wan et al. | 2006 | Citations: 109]"
        },
        {
            "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2019,
            "reference_count": 44,
            "citation_count": 22,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D19-5404.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.12231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "26399699",
                    "name": "Diego Antognini"
                },
                {
                    "authorId": "1735128",
                    "name": "B. Faltings"
                }
            ],
            "abstract": "Linking facts across documents is a challenging task, as the language used to express the same information in a sentence can vary significantly, which complicates the task of multi-document summarization. Consequently, existing approaches heavily rely on hand-crafted features, which are domain-dependent and hard to craft, or additional annotated data, which is costly to gather. To overcome these limitations, we present a novel method, which makes use of two types of sentence embeddings: universal embeddings, which are trained on a large unrelated corpus, and domain-specific embeddings, which are learned during training. To this end, we develop SemSentSum, a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of summary, consisting of 665 bytes and 100 words. Unlike other state-of-the-art models, neither hand-crafted features nor additional annotated data are necessary, and the method is easily adaptable for other tasks. To our knowledge, we are the first to use multiple sentence embeddings for the task of multi-document summarization.",
            "corpus_id": 202889056,
            "sentences": [
                {
                    "corpus_id": "202889056",
                    "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
                    "text": "The idea of using multiple embeddings has been employed at the word level. Kiela et al. (2018) use an attention mechanism to combine the embeddings for each word for the task of natural language inference. Xu et al. (2018); Bollegala et al. (2015) concatenate the embeddings of each word into a vector before feeding a neural network for the tasks of aspect extraction and sentiment analysis. To our knowledge, we are the first to combine multiple types of sentence embeddings. \n\nExtractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988). \n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges.",
                    "score": 0.7665279306179295,
                    "section_title": "Related Work",
                    "char_start_offset": 23272,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 392
                        },
                        {
                            "start": 393,
                            "end": 477
                        },
                        {
                            "start": 480,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 614
                        },
                        {
                            "start": 615,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 873
                        },
                        {
                            "start": 874,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1042
                        },
                        {
                            "start": 1043,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1404
                        },
                        {
                            "start": 1407,
                            "end": 1772
                        },
                        {
                            "start": 1773,
                            "end": 1957
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 75,
                            "end": 94,
                            "matchedPaperCorpusId": "52166626"
                        },
                        {
                            "start": 206,
                            "end": 222,
                            "matchedPaperCorpusId": "44009215"
                        },
                        {
                            "start": 224,
                            "end": 247,
                            "matchedPaperCorpusId": "14116842"
                        },
                        {
                            "start": 615,
                            "end": 627,
                            "matchedPaperCorpusId": "10103200"
                        },
                        {
                            "start": 719,
                            "end": 741,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 945,
                            "end": 965,
                            "matchedPaperCorpusId": "5457260"
                        },
                        {
                            "start": 965,
                            "end": 989,
                            "matchedPaperCorpusId": "17064982"
                        },
                        {
                            "start": 991,
                            "end": 1010,
                            "matchedPaperCorpusId": "9849366"
                        },
                        {
                            "start": 1073,
                            "end": 1090,
                            "matchedPaperCorpusId": "11977708"
                        },
                        {
                            "start": 1209,
                            "end": 1234,
                            "matchedPaperCorpusId": "337730"
                        },
                        {
                            "start": 1378,
                            "end": 1403,
                            "matchedPaperCorpusId": "60514661"
                        },
                        {
                            "start": 1495,
                            "end": 1526,
                            "matchedPaperCorpusId": "4508623"
                        },
                        {
                            "start": 1665,
                            "end": 1682,
                            "matchedPaperCorpusId": "1207010"
                        },
                        {
                            "start": 1710,
                            "end": 1724,
                            "matchedPaperCorpusId": "5879376"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9345703125
                }
            ],
            "relevance_judgement": 0.9345703125,
            "relevance_judgment_input_expanded": "# Title: Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Diego Antognini, B. Faltings\n## Abstract\nLinking facts across documents is a challenging task, as the language used to express the same information in a sentence can vary significantly, which complicates the task of multi-document summarization. Consequently, existing approaches heavily rely on hand-crafted features, which are domain-dependent and hard to craft, or additional annotated data, which is costly to gather. To overcome these limitations, we present a novel method, which makes use of two types of sentence embeddings: universal embeddings, which are trained on a large unrelated corpus, and domain-specific embeddings, which are learned during training. To this end, we develop SemSentSum, a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of summary, consisting of 665 bytes and 100 words. Unlike other state-of-the-art models, neither hand-crafted features nor additional annotated data are necessary, and the method is easily adaptable for other tasks. To our knowledge, we are the first to use multiple sentence embeddings for the task of multi-document summarization.\n## Related Work\nThe idea of using multiple embeddings has been employed at the word level. Kiela et al. (2018) use an attention mechanism to combine the embeddings for each word for the task of natural language inference. Xu et al. (2018); Bollegala et al. (2015) concatenate the embeddings of each word into a vector before feeding a neural network for the tasks of aspect extraction and sentiment analysis. To our knowledge, we are the first to combine multiple types of sentence embeddings. \n\nExtractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988). \n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges.",
            "reference_string": "[202889056 | Antognini et al. | 2019 | Citations: 22]"
        },
        {
            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 32,
            "citation_count": 73,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-main.380.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.naacl-main.380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "10721120",
                    "name": "Ramakanth Pasunuru"
                },
                {
                    "authorId": "2940333",
                    "name": "Mengwen Liu"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "120209444",
                    "name": "Sujith Ravi"
                },
                {
                    "authorId": "40262269",
                    "name": "Markus Dreyer"
                }
            ],
            "abstract": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.",
            "corpus_id": 235097309,
            "sentences": [
                {
                    "corpus_id": "235097309",
                    "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                    "text": "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani and Bloedorn, 1997;Radev and McKeown, 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. Mani and Bloedorn (1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and McKeown (1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, Erkan and Radev (2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). Dasgupta et al. (2013) incorporated dependency graph features into their sentence relation graphs. Baralis et al. (2013) built graphs over sets of terms, rather than sentences. Li et al. (2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. Liu et al. (2015) and Liao et al. (2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com",
                    "score": 0.6949899016033594,
                    "section_title": "Related Work",
                    "char_start_offset": 4341,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 697
                        },
                        {
                            "start": 700,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1765
                        },
                        {
                            "start": 1768,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2011
                        },
                        {
                            "start": 2012,
                            "end": 2271
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 115,
                            "end": 140,
                            "matchedPaperCorpusId": "6025826"
                        },
                        {
                            "start": 140,
                            "end": 164,
                            "matchedPaperCorpusId": "10019526"
                        },
                        {
                            "start": 373,
                            "end": 397,
                            "matchedPaperCorpusId": "6025826"
                        },
                        {
                            "start": 558,
                            "end": 572,
                            "matchedPaperCorpusId": "10019526"
                        },
                        {
                            "start": 896,
                            "end": 918,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1289,
                            "end": 1311,
                            "matchedPaperCorpusId": "16587947"
                        },
                        {
                            "start": 1388,
                            "end": 1409,
                            "matchedPaperCorpusId": "15709889"
                        },
                        {
                            "start": 1466,
                            "end": 1482,
                            "matchedPaperCorpusId": "6788641"
                        },
                        {
                            "start": 1614,
                            "end": 1631,
                            "matchedPaperCorpusId": "5001921"
                        },
                        {
                            "start": 1636,
                            "end": 1654,
                            "matchedPaperCorpusId": "49210924"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9326171875
                },
                {
                    "corpus_id": "235097309",
                    "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                    "text": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.",
                    "score": 0.6942756379677875,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8955078125
                }
            ],
            "relevance_judgement": 0.9326171875,
            "relevance_judgment_input_expanded": "# Title: Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Ramakanth Pasunuru, Mengwen Liu, Mohit Bansal, Sujith Ravi, Markus Dreyer\n## Abstract\nThis paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.\n## Related Work\nResearchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani and Bloedorn, 1997;Radev and McKeown, 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. Mani and Bloedorn (1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and McKeown (1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, Erkan and Radev (2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). Dasgupta et al. (2013) incorporated dependency graph features into their sentence relation graphs. Baralis et al. (2013) built graphs over sets of terms, rather than sentences. Li et al. (2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. Liu et al. (2015) and Liao et al. (2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com",
            "reference_string": "[235097309 | Pasunuru et al. | 2021 | Citations: 73]"
        },
        {
            "title": "Discourse-Aware Neural Extractive Text Summarization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 53,
            "citation_count": 282,
            "influential_citation_count": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.451.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.14142, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34837371",
                    "name": "Jiacheng Xu"
                },
                {
                    "authorId": "144702900",
                    "name": "Zhe Gan"
                },
                {
                    "authorId": "145215470",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "46700348",
                    "name": "Jingjing Liu"
                }
            ],
            "abstract": "Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.",
            "corpus_id": 219036690,
            "sentences": [
                {
                    "corpus_id": "219036690",
                    "title": "Discourse-Aware Neural Extractive Text Summarization",
                    "text": "The use of discourse theory for text summarization has been explored before. Louis et al. (2010) examined the benefit of graph structure provided by discourse relations for text summarization. Hirao et al. (2013); Yoshida et al. (2014) formulated the summarization problem as the trimming of the document discourse tree. Durrett et al. (2016) presented a system of sentence extraction and compression with ILP methods using discourse structure. Li et al. (2016) demonstrated that using EDUs as units of content selection leads to stronger summarization performance. Compared with them, our proposed method is the first neural end-to-end summarization model using EDUs as the selection basis. \n\nGraph-based Summarization Graph approach has been explored in text summarization over decades. LexRank introduced a stochastic graphbased method for computing relative importance of textual units (Erkan and Radev, 2004). Yasunaga et al. (2017) employed a GCN on the relation graphs with sentence embeddings obtained from RNN. Tan et al. (2017) also proposed graphbased attention in abstractive summarization model. Fernandes et al. (2018) developed a framework to reason long-distance relationships for text summarization.",
                    "score": 0.7037248146557771,
                    "section_title": "Discourse & Summarization",
                    "char_start_offset": 23346,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 76
                        },
                        {
                            "start": 77,
                            "end": 192
                        },
                        {
                            "start": 193,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 691
                        },
                        {
                            "start": 694,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 1108
                        },
                        {
                            "start": 1109,
                            "end": 1216
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 77,
                            "end": 96,
                            "matchedPaperCorpusId": "2810111"
                        },
                        {
                            "start": 193,
                            "end": 212,
                            "matchedPaperCorpusId": "18505561"
                        },
                        {
                            "start": 214,
                            "end": 235,
                            "matchedPaperCorpusId": "6181569"
                        },
                        {
                            "start": 321,
                            "end": 342,
                            "matchedPaperCorpusId": "5125975"
                        },
                        {
                            "start": 445,
                            "end": 461,
                            "matchedPaperCorpusId": "3937849"
                        },
                        {
                            "start": 890,
                            "end": 913,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 915,
                            "end": 937,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 1020,
                            "end": 1037,
                            "matchedPaperCorpusId": "26698484"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: Discourse-Aware Neural Extractive Text Summarization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu\n## Abstract\nRecently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.\n## Discourse & Summarization\nThe use of discourse theory for text summarization has been explored before. Louis et al. (2010) examined the benefit of graph structure provided by discourse relations for text summarization. Hirao et al. (2013); Yoshida et al. (2014) formulated the summarization problem as the trimming of the document discourse tree. Durrett et al. (2016) presented a system of sentence extraction and compression with ILP methods using discourse structure. Li et al. (2016) demonstrated that using EDUs as units of content selection leads to stronger summarization performance. Compared with them, our proposed method is the first neural end-to-end summarization model using EDUs as the selection basis. \n\nGraph-based Summarization Graph approach has been explored in text summarization over decades. LexRank introduced a stochastic graphbased method for computing relative importance of textual units (Erkan and Radev, 2004). Yasunaga et al. (2017) employed a GCN on the relation graphs with sentence embeddings obtained from RNN. Tan et al. (2017) also proposed graphbased attention in abstractive summarization model. Fernandes et al. (2018) developed a framework to reason long-distance relationships for text summarization.",
            "reference_string": "[219036690 | Xu et al. | 2019 | Citations: 282]"
        },
        {
            "title": "Fast Document Summarization using Locality Sensitive Hashing and Memory Access Efficient Node Ranking",
            "venue": "",
            "year": 2016,
            "reference_count": 23,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ijece.iaescore.com/index.php/IJECE/article/download/447/322",
                "status": "HYBRID",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.11591/IJECE.V6I3.PP945-954?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.11591/IJECE.V6I3.PP945-954, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2620240",
                    "name": "Ercan Canhasi"
                }
            ],
            "abstract": "Text modeling and sentence selection are the fundamental steps of a typical extractive document summarization algorithm.\u00a0\u00a0 The common text modeling method connects a pair of sentences based on their similarities.\u00a0\u00a0 Even thought it can effectively represent the sentence similarity graph of given document(s) its big drawback is a large time complexity of $O(n^2)$, where n represents the number of sentences.\u00a0\u00a0 The quadratic time complexity makes it impractical for large documents.\u00a0\u00a0 In this paper we propose the fast approximation algorithms for the text modeling and the sentence selection.\u00a0\u00a0 Our text modeling algorithm reduces the time complexity to near-linear time by rapidly finding the most similar sentences to form the sentences similarity graph.\u00a0\u00a0 In doing so we utilized Locality-Sensitive Hashing, a fast algorithm for the approximate nearest neighbor search.\u00a0\u00a0 For the sentence selection step we propose a simple memory-access-efficient node ranking method based on the idea of scanning sequentially only the neighborhood arrays.\u00a0\u00a0\u00a0 Experimentally, we show that sacrificing a rather small percentage of recall and precision in the quality of the produced summary can reduce the quadratic to sub-linear time complexity.\u00a0\u00a0 We see the big potential of proposed method in text summarization for mobile devices and big text data summarization for internet of things on cloud.\u00a0\u00a0 In our experiments, beside evaluating the presented method on the standard general and query multi-document summarization tasks, we also tested it on few alternative summarization tasks including general and query, timeline, and comparative summarization.",
            "corpus_id": 63821954,
            "sentences": [
                {
                    "corpus_id": "63821954",
                    "title": "Fast Document Summarization using Locality Sensitive Hashing and Memory Access Efficient Node Ranking",
                    "text": "Comparative multi document summarization (CDS) is first proposed in [10] to summarize differences between comparable document groups. [10] presents a sentence selection strategy modeled by means of conditional entropy, which precisely discriminates the documents in different groups. \n\nGraph-based methods like TextRank [12] and PageRank [13] model a document or a set of documents as a text similarity graph, constructed by taking sentences as vertices and the similarity between sentences as edge weights. They take into account the global information and recursively calculate the sentence significance from the entire text graph rather than simply relying on unconnected individual sentences. From an NLP perspective, extractive summarization embodies two criteria: sentence relevance and sentence redundancy. Graph-based sentence ranking algorithms successfully merge both of these criteria into a single framework, by utilizing the so-called graph-based lexical centrality principle. Graph-based ranking algorithms were also used in query-focused summarization when it became a popular research topic. For instance, a topic-sensitive version of LexRank is proposed by [14]. It integrates the relevance of a sentence to the query into LexRank to get a biased PageRank ranking. \n\nSimilar work to ours [21] presents a new principled and versatile summarization framework MDS using the submodal function. This framework can deal with different summarization tasks, including generic, query-focused, updated, comparative summarization. The empirical results show that this framework outperforms the other rivals in the generic summarization and is competitive in other summarization tasks. In [22] authors have investigated the use of maximum entropy, naive-Bayes, support vector machine models and a hybrid machine model for multi-document automatic text summarization.",
                    "score": 0.6487639875301161,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 6811,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 283
                        },
                        {
                            "start": 286,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 813
                        },
                        {
                            "start": 814,
                            "end": 989
                        },
                        {
                            "start": 990,
                            "end": 1107
                        },
                        {
                            "start": 1108,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1281
                        },
                        {
                            "start": 1284,
                            "end": 1406
                        },
                        {
                            "start": 1407,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1690
                        },
                        {
                            "start": 1691,
                            "end": 1871
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 68,
                            "end": 72,
                            "matchedPaperCorpusId": "8834940"
                        },
                        {
                            "start": 134,
                            "end": 138,
                            "matchedPaperCorpusId": "8834940"
                        },
                        {
                            "start": 338,
                            "end": 342,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1174,
                            "end": 1178,
                            "matchedPaperCorpusId": "17636912"
                        },
                        {
                            "start": 1305,
                            "end": 1309,
                            "matchedPaperCorpusId": "15990379"
                        },
                        {
                            "start": 1694,
                            "end": 1698,
                            "matchedPaperCorpusId": "7992772"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92529296875
                }
            ],
            "relevance_judgement": 0.92529296875,
            "relevance_judgment_input_expanded": "# Title: Fast Document Summarization using Locality Sensitive Hashing and Memory Access Efficient Node Ranking\n# Venue: \n# Authors: Ercan Canhasi\n## Abstract\nText modeling and sentence selection are the fundamental steps of a typical extractive document summarization algorithm.\u00a0\u00a0 The common text modeling method connects a pair of sentences based on their similarities.\u00a0\u00a0 Even thought it can effectively represent the sentence similarity graph of given document(s) its big drawback is a large time complexity of $O(n^2)$, where n represents the number of sentences.\u00a0\u00a0 The quadratic time complexity makes it impractical for large documents.\u00a0\u00a0 In this paper we propose the fast approximation algorithms for the text modeling and the sentence selection.\u00a0\u00a0 Our text modeling algorithm reduces the time complexity to near-linear time by rapidly finding the most similar sentences to form the sentences similarity graph.\u00a0\u00a0 In doing so we utilized Locality-Sensitive Hashing, a fast algorithm for the approximate nearest neighbor search.\u00a0\u00a0 For the sentence selection step we propose a simple memory-access-efficient node ranking method based on the idea of scanning sequentially only the neighborhood arrays.\u00a0\u00a0\u00a0 Experimentally, we show that sacrificing a rather small percentage of recall and precision in the quality of the produced summary can reduce the quadratic to sub-linear time complexity.\u00a0\u00a0 We see the big potential of proposed method in text summarization for mobile devices and big text data summarization for internet of things on cloud.\u00a0\u00a0 In our experiments, beside evaluating the presented method on the standard general and query multi-document summarization tasks, we also tested it on few alternative summarization tasks including general and query, timeline, and comparative summarization.\n## RELATED WORK\nComparative multi document summarization (CDS) is first proposed in [10] to summarize differences between comparable document groups. [10] presents a sentence selection strategy modeled by means of conditional entropy, which precisely discriminates the documents in different groups. \n\nGraph-based methods like TextRank [12] and PageRank [13] model a document or a set of documents as a text similarity graph, constructed by taking sentences as vertices and the similarity between sentences as edge weights. They take into account the global information and recursively calculate the sentence significance from the entire text graph rather than simply relying on unconnected individual sentences. From an NLP perspective, extractive summarization embodies two criteria: sentence relevance and sentence redundancy. Graph-based sentence ranking algorithms successfully merge both of these criteria into a single framework, by utilizing the so-called graph-based lexical centrality principle. Graph-based ranking algorithms were also used in query-focused summarization when it became a popular research topic. For instance, a topic-sensitive version of LexRank is proposed by [14]. It integrates the relevance of a sentence to the query into LexRank to get a biased PageRank ranking. \n\nSimilar work to ours [21] presents a new principled and versatile summarization framework MDS using the submodal function. This framework can deal with different summarization tasks, including generic, query-focused, updated, comparative summarization. The empirical results show that this framework outperforms the other rivals in the generic summarization and is competitive in other summarization tasks. In [22] authors have investigated the use of maximum entropy, naive-Bayes, support vector machine models and a hybrid machine model for multi-document automatic text summarization.",
            "reference_string": "[63821954 | Canhasi | 2016 | Citations: 4]"
        },
        {
            "title": "Graph-based Neural Multi-Document Summarization",
            "venue": "Conference on Computational Natural Language Learning",
            "year": 2017,
            "reference_count": 48,
            "citation_count": 258,
            "influential_citation_count": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/K17-1045.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.06681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "19168196",
                    "name": "Michihiro Yasunaga"
                },
                {
                    "authorId": "144142360",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "19266011",
                    "name": "Kshitijh Meelu"
                },
                {
                    "authorId": "18081101",
                    "name": "Ayush Pareek"
                },
                {
                    "authorId": "2093939303",
                    "name": "K. Srinivasan"
                },
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                }
            ],
            "abstract": "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.",
            "corpus_id": 6532096,
            "sentences": [
                {
                    "corpus_id": "6532096",
                    "title": "Graph-based Neural Multi-Document Summarization",
                    "text": "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.",
                    "score": 0.6708483787317311,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.92041015625,
            "relevance_judgment_input_expanded": "# Title: Graph-based Neural Multi-Document Summarization\n# Venue: Conference on Computational Natural Language Learning\n# Authors: Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, K. Srinivasan, Dragomir R. Radev\n## Abstract\nWe propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.\n",
            "reference_string": "[6532096 | Yasunaga et al. | 2017 | Citations: 258]"
        },
        {
            "title": "Movie Review Summarization Using Supervised Learning and Graph-Based Ranking Algorithm",
            "venue": "Computational Intelligence and Neuroscience",
            "year": 2020,
            "reference_count": 68,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://downloads.hindawi.com/journals/cin/2020/7526580.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7288188, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1561421122",
                    "name": "Atif Khan"
                },
                {
                    "authorId": "1750930091",
                    "name": "Muhammad Adnan Gul"
                },
                {
                    "authorId": "3060705",
                    "name": "M. Zareei"
                },
                {
                    "authorId": "9820762",
                    "name": "R. Rajesh"
                },
                {
                    "authorId": "1411333558",
                    "name": "Asim Zeb"
                },
                {
                    "authorId": "2057127260",
                    "name": "Muhammad Naeem"
                },
                {
                    "authorId": "1411821449",
                    "name": "Yousaf Saeed"
                },
                {
                    "authorId": "1680372",
                    "name": "N. Salim"
                }
            ],
            "abstract": "With the growing information on web, online movie review is becoming a significant information resource for Internet users. However, online users post thousands of movie reviews on daily basis and it is hard for them to manually summarize the reviews. Movie review mining and summarization is one of the challenging tasks in natural language processing. Therefore, an automatic approach is desirable to summarize the lengthy movie reviews, and it will allow users to quickly recognize the positive and negative aspects of a movie. This study employs a feature extraction technique called bag of words (BoW) to extract features from movie reviews and represent the reviews as a vector space model or feature vector. The next phase uses Na\u00efve Bayes machine learning algorithm to classify the movie reviews (represented as feature vector) into positive and negative. Next, an undirected weighted graph is constructed from the pairwise semantic similarities between classified review sentences in such a way that the graph nodes represent review sentences, while the edges of graph indicate semantic similarity weight. The weighted graph-based ranking algorithm (WGRA) is applied to compute the rank score for each review sentence in the graph. Finally, the top ranked sentences (graph nodes) are chosen based on highest rank scores to produce the extractive summary. Experimental results reveal that the proposed approach is superior to other state-of-the-art approaches.",
            "corpus_id": 219690773,
            "sentences": [
                {
                    "corpus_id": "219690773",
                    "title": "Movie Review Summarization Using Supervised Learning and Graph-Based Ranking Algorithm",
                    "text": "e final summary is produced based on sentences containing the relevant keywords. A statistical method was proposed in [37] for news article summarization. e sentences in the news documents are scored based on different features such length of sentence, first sentence of news article, title of news article, proper nouns, and term frequency. \n\nComputational Intelligence and Neuroscience e top scored sentences are selected to produce a summary. e authors in [38] presented a pattern-based method for news article summarization. \n\nIn recent years, various graph-based methods have attracted more attention and effectively attempted for text summarization. ese methods utilize PageRank algorithm [39] and its variants to give rank/score to graph nodes, which represent sentences or passages. e authors in [40] proposed a connectivity graph, which assumes that nodes only carry significant information if they are connected to many other nodes. e authors in [41] introduced a Lex-PageRank approach that is based on eigenvector centrality, which constructs a sentence connectivity matrix and utilizes similar algorithm like PageRank to find the significant sentences for summary. A similar algorithm to PageRank was also proposed in [42] which finds salient sentences for summary generation. \n\ne authors in [24] presented a graph-based approach, which integrates surface features with text content and investigates subtopic features in multiple documents to incorporate them into the graph-based ranking algorithm. A multidocument summarization approach based on affinity graph [43] exploits similar algorithm to PageRank and computes sentence scores in the affinity graph based on information richness. e authors in [44] demonstrated a document-sensitive graph model for multidocument generic summarization and highlighted the impact of global document set information at sentence level. A weighted graph model for generic multidocument summarization introduced in [45] combines sentence ranking and sentence clustering methods. e authors in [46] presented a graphbased method for multidocument summarization of Vietnamese documents and employed traditional PageRank algorithm to rank the important sentences. e authors in [47] demonstrated an event graph-based approach for multidocument extractive summarization. However, the approach requires the construction of hand crafted rules for argument extraction, which is a time consuming process and may limit its application to a specific domain.",
                    "score": 0.5864611784724577,
                    "section_title": "Related Work",
                    "char_start_offset": 14204,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 80
                        },
                        {
                            "start": 81,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 341
                        },
                        {
                            "start": 344,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 528
                        },
                        {
                            "start": 531,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1511
                        },
                        {
                            "start": 1512,
                            "end": 1700
                        },
                        {
                            "start": 1701,
                            "end": 1885
                        },
                        {
                            "start": 1886,
                            "end": 2026
                        },
                        {
                            "start": 2027,
                            "end": 2207
                        },
                        {
                            "start": 2208,
                            "end": 2312
                        },
                        {
                            "start": 2313,
                            "end": 2493
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 118,
                            "end": 122,
                            "matchedPaperCorpusId": "14486205"
                        },
                        {
                            "start": 459,
                            "end": 463,
                            "matchedPaperCorpusId": "3780782"
                        },
                        {
                            "start": 804,
                            "end": 808,
                            "matchedPaperCorpusId": "9177142"
                        },
                        {
                            "start": 1230,
                            "end": 1234,
                            "matchedPaperCorpusId": "8878897"
                        },
                        {
                            "start": 1304,
                            "end": 1308,
                            "matchedPaperCorpusId": "22342754"
                        },
                        {
                            "start": 1575,
                            "end": 1579,
                            "matchedPaperCorpusId": "5457260"
                        },
                        {
                            "start": 1714,
                            "end": 1718,
                            "matchedPaperCorpusId": "35133176"
                        },
                        {
                            "start": 1963,
                            "end": 1967,
                            "matchedPaperCorpusId": "39384736"
                        },
                        {
                            "start": 2040,
                            "end": 2044,
                            "matchedPaperCorpusId": "26333220"
                        },
                        {
                            "start": 2221,
                            "end": 2225,
                            "matchedPaperCorpusId": "45592507"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91845703125
                }
            ],
            "relevance_judgement": 0.91845703125,
            "relevance_judgment_input_expanded": "# Title: Movie Review Summarization Using Supervised Learning and Graph-Based Ranking Algorithm\n# Venue: Computational Intelligence and Neuroscience\n# Authors: Atif Khan, Muhammad Adnan Gul, M. Zareei, R. Rajesh, Asim Zeb, Muhammad Naeem, Yousaf Saeed, N. Salim\n## Abstract\nWith the growing information on web, online movie review is becoming a significant information resource for Internet users. However, online users post thousands of movie reviews on daily basis and it is hard for them to manually summarize the reviews. Movie review mining and summarization is one of the challenging tasks in natural language processing. Therefore, an automatic approach is desirable to summarize the lengthy movie reviews, and it will allow users to quickly recognize the positive and negative aspects of a movie. This study employs a feature extraction technique called bag of words (BoW) to extract features from movie reviews and represent the reviews as a vector space model or feature vector. The next phase uses Na\u00efve Bayes machine learning algorithm to classify the movie reviews (represented as feature vector) into positive and negative. Next, an undirected weighted graph is constructed from the pairwise semantic similarities between classified review sentences in such a way that the graph nodes represent review sentences, while the edges of graph indicate semantic similarity weight. The weighted graph-based ranking algorithm (WGRA) is applied to compute the rank score for each review sentence in the graph. Finally, the top ranked sentences (graph nodes) are chosen based on highest rank scores to produce the extractive summary. Experimental results reveal that the proposed approach is superior to other state-of-the-art approaches.\n## Related Work\ne final summary is produced based on sentences containing the relevant keywords. A statistical method was proposed in [37] for news article summarization. e sentences in the news documents are scored based on different features such length of sentence, first sentence of news article, title of news article, proper nouns, and term frequency. \n\nComputational Intelligence and Neuroscience e top scored sentences are selected to produce a summary. e authors in [38] presented a pattern-based method for news article summarization. \n\nIn recent years, various graph-based methods have attracted more attention and effectively attempted for text summarization. ese methods utilize PageRank algorithm [39] and its variants to give rank/score to graph nodes, which represent sentences or passages. e authors in [40] proposed a connectivity graph, which assumes that nodes only carry significant information if they are connected to many other nodes. e authors in [41] introduced a Lex-PageRank approach that is based on eigenvector centrality, which constructs a sentence connectivity matrix and utilizes similar algorithm like PageRank to find the significant sentences for summary. A similar algorithm to PageRank was also proposed in [42] which finds salient sentences for summary generation. \n\ne authors in [24] presented a graph-based approach, which integrates surface features with text content and investigates subtopic features in multiple documents to incorporate them into the graph-based ranking algorithm. A multidocument summarization approach based on affinity graph [43] exploits similar algorithm to PageRank and computes sentence scores in the affinity graph based on information richness. e authors in [44] demonstrated a document-sensitive graph model for multidocument generic summarization and highlighted the impact of global document set information at sentence level. A weighted graph model for generic multidocument summarization introduced in [45] combines sentence ranking and sentence clustering methods. e authors in [46] presented a graphbased method for multidocument summarization of Vietnamese documents and employed traditional PageRank algorithm to rank the important sentences. e authors in [47] demonstrated an event graph-based approach for multidocument extractive summarization. However, the approach requires the construction of hand crafted rules for argument extraction, which is a time consuming process and may limit its application to a specific domain.",
            "reference_string": "[219690773 | Khan et al. | 2020 | Citations: 16]"
        },
        {
            "title": "Large-Margin Learning of Submodular Summarization Methods",
            "venue": "arXiv.org",
            "year": 2011,
            "reference_count": 28,
            "citation_count": 6,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1110.2162, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2195688",
                    "name": "Ruben Sipos"
                },
                {
                    "authorId": "2248631",
                    "name": "Pannagadatta K. Shivaswamy"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ],
            "abstract": "In this paper, we present a supervised learning approach to training submodular scoring functions for extractive multi-document summarization. By taking a structured predicition approach, we provide a large-margin method that directly optimizes a convex relaxation of the desired performance measure. The learning method applies to all submodular summarization methods, and we demonstrate its effectiveness for both pairwise as well as coverage-based scoring functions on multiple datasets. Compared to state-of-the-art functions that were tuned manually, our method significantly improves performance and enables high-fidelity models with numbers of parameters well beyond what could reasonbly be tuned by hand.",
            "corpus_id": 16689087,
            "sentences": [
                {
                    "corpus_id": "16689087",
                    "title": "Large-Margin Learning of Submodular Summarization Methods",
                    "text": "Work on extractive summarization spans a large range of approaches. Starting with unsupervised methods, one of the widely known approaches is MMR [12]. It uses a greedy approach for selection and considers the trade-off between relevance and redundancy. Later it was extended [13] to support multi-document settings by incorporating additional information available in this case. Good results can be achieved by reformulating this as a knapsack packing problem and solving it using dynamic programing [14]. \n\nA popular stohastic graph-based summarization method is LexRank [15]. It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank [16] is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank [17]. Sentence extraction can also be implemented using other graph based scoring approaches [18] such as HITS [19] and positional power functions. \n\nGraph based methods can also be paired with clustering such as in CollabSum [20]. This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities. Another clustering based algorithm [21] is diversity based extension of MMR that finds diversity by clustering and then proceeds to reduce redundancy by selecting a representative for each cluster. \n\nThe manually tuned sentence pairwise model [22,23] we took inspiration from is based on budgeted submodular optimization. A summary is produced by maximizing an objective function that includes coverage and redundancy terms. Coverage is defined as the sum of sentence similarities between the selected summary and the rest of the sentences, while redundancy is the sum of pairwise intra-summary sentence similarities. Another approach based on submodularity [24] is relying on extracting important keyphrases from citation sentences for a given paper and using them to build the summary. \n\nIn the supervised setting, a lot of early methods [25] made independent binary decisions whether to include a particular sentence in the summary or not. This ignores dependencies between sentences and can result in high redundancy.",
                    "score": 0.614037234100515,
                    "section_title": "Related work",
                    "char_start_offset": 3465,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 68,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 253
                        },
                        {
                            "start": 254,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 506
                        },
                        {
                            "start": 509,
                            "end": 578
                        },
                        {
                            "start": 579,
                            "end": 694
                        },
                        {
                            "start": 695,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 1000
                        },
                        {
                            "start": 1003,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1270
                        },
                        {
                            "start": 1271,
                            "end": 1468
                        },
                        {
                            "start": 1471,
                            "end": 1592
                        },
                        {
                            "start": 1593,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1888
                        },
                        {
                            "start": 1889,
                            "end": 2058
                        },
                        {
                            "start": 2061,
                            "end": 2213
                        },
                        {
                            "start": 2214,
                            "end": 2292
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 146,
                            "end": 150,
                            "matchedPaperCorpusId": "4508623"
                        },
                        {
                            "start": 276,
                            "end": 280,
                            "matchedPaperCorpusId": "8294822"
                        },
                        {
                            "start": 501,
                            "end": 505,
                            "matchedPaperCorpusId": "2798452"
                        },
                        {
                            "start": 573,
                            "end": 577,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 946,
                            "end": 950,
                            "matchedPaperCorpusId": "2100529"
                        },
                        {
                            "start": 964,
                            "end": 968,
                            "matchedPaperCorpusId": "216000619"
                        },
                        {
                            "start": 1079,
                            "end": 1083,
                            "matchedPaperCorpusId": "10228847"
                        },
                        {
                            "start": 1306,
                            "end": 1310,
                            "matchedPaperCorpusId": "6460414"
                        },
                        {
                            "start": 1514,
                            "end": 1518,
                            "matchedPaperCorpusId": "1803710"
                        },
                        {
                            "start": 1929,
                            "end": 1933,
                            "matchedPaperCorpusId": "6494467"
                        },
                        {
                            "start": 2111,
                            "end": 2115,
                            "matchedPaperCorpusId": "5775833"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91796875
                }
            ],
            "relevance_judgement": 0.91796875,
            "relevance_judgment_input_expanded": "# Title: Large-Margin Learning of Submodular Summarization Methods\n# Venue: arXiv.org\n# Authors: Ruben Sipos, Pannagadatta K. Shivaswamy, T. Joachims\n## Abstract\nIn this paper, we present a supervised learning approach to training submodular scoring functions for extractive multi-document summarization. By taking a structured predicition approach, we provide a large-margin method that directly optimizes a convex relaxation of the desired performance measure. The learning method applies to all submodular summarization methods, and we demonstrate its effectiveness for both pairwise as well as coverage-based scoring functions on multiple datasets. Compared to state-of-the-art functions that were tuned manually, our method significantly improves performance and enables high-fidelity models with numbers of parameters well beyond what could reasonbly be tuned by hand.\n## Related work\nWork on extractive summarization spans a large range of approaches. Starting with unsupervised methods, one of the widely known approaches is MMR [12]. It uses a greedy approach for selection and considers the trade-off between relevance and redundancy. Later it was extended [13] to support multi-document settings by incorporating additional information available in this case. Good results can be achieved by reformulating this as a knapsack packing problem and solving it using dynamic programing [14]. \n\nA popular stohastic graph-based summarization method is LexRank [15]. It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank [16] is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank [17]. Sentence extraction can also be implemented using other graph based scoring approaches [18] such as HITS [19] and positional power functions. \n\nGraph based methods can also be paired with clustering such as in CollabSum [20]. This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities. Another clustering based algorithm [21] is diversity based extension of MMR that finds diversity by clustering and then proceeds to reduce redundancy by selecting a representative for each cluster. \n\nThe manually tuned sentence pairwise model [22,23] we took inspiration from is based on budgeted submodular optimization. A summary is produced by maximizing an objective function that includes coverage and redundancy terms. Coverage is defined as the sum of sentence similarities between the selected summary and the rest of the sentences, while redundancy is the sum of pairwise intra-summary sentence similarities. Another approach based on submodularity [24] is relying on extracting important keyphrases from citation sentences for a given paper and using them to build the summary. \n\nIn the supervised setting, a lot of early methods [25] made independent binary decisions whether to include a particular sentence in the summary or not. This ignores dependencies between sentences and can result in high redundancy.",
            "reference_string": "[16689087 | Sipos et al. | 2011 | Citations: 6]"
        },
        {
            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.03978",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.03978, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064325789",
                    "name": "Aiswarya Sankar"
                },
                {
                    "authorId": "145934595",
                    "name": "Ankit R. Chadha"
                }
            ],
            "abstract": "Abstractive multi document summarization has evolved as a task through the basic sequence to sequence approaches to transformer and graph based techniques. Each of these approaches has primarily focused on the issues of multi document information synthe-sis and attention based approaches to extract salient information. A challenge that arises with multi document summarization which is not prevalent in single document summarization is the need to effectively summarize multiple documents that might have con\ufb02icting polarity, sentiment or subjective information about a given topic. In this paper we propose ACM, attribute conditioned multi document summarization,a model that incorporates attribute conditioning modules in order to decouple con\ufb02icting information by conditioning for a certain attribute in the output summary. This approach shows strong gains in ROUGE score over baseline multi document summarization approaches and shows gains in \ufb02uency, informativeness and reduction in repetitiveness as shown through a human annotation analysis study.",
            "corpus_id": 248571519,
            "sentences": [
                {
                    "corpus_id": "248571519",
                    "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                    "text": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013). These papers primarily aim to address de-duplicating information and learning relationships between the different topics shared across documents however none of these architectures are built to deal with conflicting information.",
                    "score": 0.7812433893715174,
                    "section_title": "Multi Document Summarization",
                    "char_start_offset": 8653,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1615
                        },
                        {
                            "start": 1616,
                            "end": 1844
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 205,
                            "end": 231,
                            "matchedPaperCorpusId": "577937"
                        },
                        {
                            "start": 241,
                            "end": 264,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 491,
                            "end": 508,
                            "matchedPaperCorpusId": "10112929"
                        },
                        {
                            "start": 1345,
                            "end": 1362,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1448,
                            "end": 1465,
                            "matchedPaperCorpusId": "218718706"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9169921875
                }
            ],
            "relevance_judgement": 0.9169921875,
            "relevance_judgment_input_expanded": "# Title: ACM - Attribute Conditioning for Abstractive Multi Document Summarization\n# Venue: arXiv.org\n# Authors: Aiswarya Sankar, Ankit R. Chadha\n## Abstract\nAbstractive multi document summarization has evolved as a task through the basic sequence to sequence approaches to transformer and graph based techniques. Each of these approaches has primarily focused on the issues of multi document information synthe-sis and attention based approaches to extract salient information. A challenge that arises with multi document summarization which is not prevalent in single document summarization is the need to effectively summarize multiple documents that might have con\ufb02icting polarity, sentiment or subjective information about a given topic. In this paper we propose ACM, attribute conditioned multi document summarization,a model that incorporates attribute conditioning modules in order to decouple con\ufb02icting information by conditioning for a certain attribute in the output summary. This approach shows strong gains in ROUGE score over baseline multi document summarization approaches and shows gains in \ufb02uency, informativeness and reduction in repetitiveness as shown through a human annotation analysis study.\n## Multi Document Summarization\nMulti document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013). These papers primarily aim to address de-duplicating information and learning relationships between the different topics shared across documents however none of these architectures are built to deal with conflicting information.",
            "reference_string": "[248571519 | Sankar et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Hierarchical Transformers for Multi-Document Summarization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 46,
            "citation_count": 298,
            "influential_citation_count": 41,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1905.13164",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.13164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "39798499",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1747893",
                    "name": "Mirella Lapata"
                }
            ],
            "abstract": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.",
            "corpus_id": 170079112,
            "sentences": [
                {
                    "corpus_id": "170079112",
                    "title": "Hierarchical Transformers for Multi-Document Summarization",
                    "text": "Most previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. \n\nAbstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005;Filippova and Strube, 2008;Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018;Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives (Ma et al., 2016;Chu and Liu, 2018).",
                    "score": 0.7671665279759963,
                    "section_title": "Related Work",
                    "char_start_offset": 5183,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 428
                        },
                        {
                            "start": 429,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 915
                        },
                        {
                            "start": 918,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1520
                        },
                        {
                            "start": 1521,
                            "end": 1662
                        },
                        {
                            "start": 1663,
                            "end": 1983
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 257,
                            "end": 280,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 508,
                            "end": 531,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 646,
                            "end": 668,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 1150,
                            "end": 1178,
                            "matchedPaperCorpusId": "16188305"
                        },
                        {
                            "start": 1178,
                            "end": 1205,
                            "matchedPaperCorpusId": "14909308"
                        },
                        {
                            "start": 1205,
                            "end": 1223,
                            "matchedPaperCorpusId": "8377315"
                        },
                        {
                            "start": 1325,
                            "end": 1343,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 1343,
                            "end": 1363,
                            "matchedPaperCorpusId": "21850704"
                        },
                        {
                            "start": 1363,
                            "end": 1385,
                            "matchedPaperCorpusId": "52144157"
                        },
                        {
                            "start": 1385,
                            "end": 1410,
                            "matchedPaperCorpusId": "4406182"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9169921875
                },
                {
                    "corpus_id": "170079112",
                    "title": "Hierarchical Transformers for Multi-Document Summarization",
                    "text": "Aside from the difficulties in obtaining training data, a major obstacle to the application of end-to-end models to multi-document summarization is the sheer size and number of source documents which can be very large. As a result, it is practically infeasible (given memory limitations of current hardware) to train a model which encodes all of them into vectors and subsequently generates a summary from them. Liu et al. (2018) propose a two-stage architecture, where an extractive model first selects a subset of salient passages, and subsequently an abstractive model generates the summary while conditioning on the extracted subset. The selected passages are concatenated into a flat sequence and the Transformer (Vaswani et al., 2017), an architecture well-suited to language modeling over long sequences, is used to arXiv:1905.13164v1 [cs.CL] 30 May 2019 decode the summary. \n\nAlthough the model of Liu et al. (2018) takes an important first step towards abstractive multidocument summarization, it still considers the multiple input documents as a concatenated flat sequence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light (Radev, 2000). The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004;Christensen et al., 2013;Wan, 2008;Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. \n\nIn this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner.",
                    "score": 0.6125524835617523,
                    "section_title": "Introduction",
                    "char_start_offset": 1905,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 218
                        },
                        {
                            "start": 219,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 637
                        },
                        {
                            "start": 638,
                            "end": 881
                        },
                        {
                            "start": 884,
                            "end": 1181
                        },
                        {
                            "start": 1182,
                            "end": 1369
                        },
                        {
                            "start": 1370,
                            "end": 1699
                        },
                        {
                            "start": 1700,
                            "end": 1928
                        },
                        {
                            "start": 1931,
                            "end": 2075
                        },
                        {
                            "start": 2076,
                            "end": 2215
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 412,
                            "end": 429,
                            "matchedPaperCorpusId": "39871772"
                        },
                        {
                            "start": 718,
                            "end": 740,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 906,
                            "end": 923,
                            "matchedPaperCorpusId": "39871772"
                        },
                        {
                            "start": 1355,
                            "end": 1368,
                            "matchedPaperCorpusId": "10103200"
                        },
                        {
                            "start": 1615,
                            "end": 1638,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1663,
                            "end": 1673,
                            "matchedPaperCorpusId": "17446655"
                        },
                        {
                            "start": 1673,
                            "end": 1698,
                            "matchedPaperCorpusId": "8437725"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88720703125
                }
            ],
            "relevance_judgement": 0.9169921875,
            "relevance_judgment_input_expanded": "# Title: Hierarchical Transformers for Multi-Document Summarization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yang Liu, Mirella Lapata\n## Abstract\nIn this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.\n## Introduction\nAside from the difficulties in obtaining training data, a major obstacle to the application of end-to-end models to multi-document summarization is the sheer size and number of source documents which can be very large. As a result, it is practically infeasible (given memory limitations of current hardware) to train a model which encodes all of them into vectors and subsequently generates a summary from them. Liu et al. (2018) propose a two-stage architecture, where an extractive model first selects a subset of salient passages, and subsequently an abstractive model generates the summary while conditioning on the extracted subset. The selected passages are concatenated into a flat sequence and the Transformer (Vaswani et al., 2017), an architecture well-suited to language modeling over long sequences, is used to arXiv:1905.13164v1 [cs.CL] 30 May 2019 decode the summary. \n\nAlthough the model of Liu et al. (2018) takes an important first step towards abstractive multidocument summarization, it still considers the multiple input documents as a concatenated flat sequence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light (Radev, 2000). The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004;Christensen et al., 2013;Wan, 2008;Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. \n\nIn this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner.\n\n## Related Work\nMost previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. \n\nAbstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005;Filippova and Strube, 2008;Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018;Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives (Ma et al., 2016;Chu and Liu, 2018).",
            "reference_string": "[170079112 | Liu et al. | 2019 | Citations: 298]"
        },
        {
            "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 37,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18992, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "67081502",
                    "name": "Qianren Mao"
                },
                {
                    "authorId": "2263933839",
                    "name": "Shaobo Zhao"
                },
                {
                    "authorId": "2264274905",
                    "name": "Jiarui Li"
                },
                {
                    "authorId": "2263823705",
                    "name": "Xiaolei Gu"
                },
                {
                    "authorId": "2263839211",
                    "name": "Shizhu He"
                },
                {
                    "authorId": "2264733691",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2263794896",
                    "name": "Jianxin Li"
                }
            ],
            "abstract": "Pre-trained sentence representations are crucial for identifying significant sentences in unsupervised document extractive summarization. However, the traditional two-step paradigm of pre-training and sentence-ranking, creates a gap due to differing optimization objectives. To address this issue, we argue that utilizing pre-trained embeddings derived from a process specifically designed to optimize cohensive and distinctive sentence representations helps rank significant sentences. To do so, we propose a novel graph pre-training auto-encoder to obtain sentence embeddings by explicitly modelling intra-sentential distinctive features and inter-sentential cohesive features through sentence-word bipartite graphs. These pre-trained sentence representations are then utilized in a graph-based ranking algorithm for unsupervised summarization. Our method produces predominant performance for unsupervised summarization frameworks by providing summary-worthy sentence representations. It surpasses heavy BERT- or RoBERTa-based sentence representations in downstream tasks.",
            "corpus_id": 264817506,
            "sentences": [
                {
                    "corpus_id": "264817506",
                    "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders",
                    "text": "There are several simple unsupervised summarization extraction frameworks, including TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), as well as more robust graph-based ranking methods such as Pac-Sum (Zheng and Lapata, 2019), FAR (Liang et al., 2021), DASG (Liu et al., 2021). Graph-based ranking methods take sentence representations as input, using the algorithm of graph-based sentence centrality ranking for sentence selection. We now introduce extractive summarization backbones. \n\n\u2022 TextRank and LexRank utilize PageRank to calculate node centrality based on a Markov chain model recursively. \n\n\u2022 PacSum (Zheng and Lapata, 2019) constructs graphs with directed edges. The rationale behind this approach is that the centrality of two nodes is influenced by their relative position in the document, as illustrated by Equation 15. \n\n\u2022 DASG (Liu et al., 2021) selects sentences for summarization based on the similarities and relative distances among neighbouring sentences. It incorporates a graph edge weighting scheme to Equation 15, using a coefficient that maps a pair of sentence indices to a value calculated by their relative distance. \n\n\u2022 FAR (Liang et al., 2021) modifies Equation 15by applying a facet-aware centrality-based ranking model to filter out insignificant sentences. FAR also incorporates a similarity constraint between candidate summary representation and document representation to ensure the selected sentences are semantically related to the entire text, thereby facilitating summarization. \n\nThe main distinction among the extractive frameworks mentioned above lies in their centrality algorithms. A comprehensive comparison of these algorithms can be found in Appendix 8.",
                    "score": 0.6333964434166657,
                    "section_title": "Backbones of Summarization Approaches",
                    "char_start_offset": 14687,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 509
                        },
                        {
                            "start": 512,
                            "end": 623
                        },
                        {
                            "start": 626,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 858
                        },
                        {
                            "start": 861,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1170
                        },
                        {
                            "start": 1173,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1544
                        },
                        {
                            "start": 1547,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1727
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 94,
                            "end": 120,
                            "matchedPaperCorpusId": "577937"
                        },
                        {
                            "start": 133,
                            "end": 156,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 225,
                            "end": 249,
                            "matchedPaperCorpusId": "182952892"
                        },
                        {
                            "start": 255,
                            "end": 275,
                            "matchedPaperCorpusId": "236477774"
                        },
                        {
                            "start": 282,
                            "end": 300,
                            "matchedPaperCorpusId": "235792514"
                        },
                        {
                            "start": 868,
                            "end": 886,
                            "matchedPaperCorpusId": "235792514"
                        },
                        {
                            "start": 1179,
                            "end": 1199,
                            "matchedPaperCorpusId": "236477774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.916015625
                }
            ],
            "relevance_judgement": 0.916015625,
            "relevance_judgment_input_expanded": "# Title: Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Qianren Mao, Shaobo Zhao, Jiarui Li, Xiaolei Gu, Shizhu He, Bo Li, Jianxin Li\n## Abstract\nPre-trained sentence representations are crucial for identifying significant sentences in unsupervised document extractive summarization. However, the traditional two-step paradigm of pre-training and sentence-ranking, creates a gap due to differing optimization objectives. To address this issue, we argue that utilizing pre-trained embeddings derived from a process specifically designed to optimize cohensive and distinctive sentence representations helps rank significant sentences. To do so, we propose a novel graph pre-training auto-encoder to obtain sentence embeddings by explicitly modelling intra-sentential distinctive features and inter-sentential cohesive features through sentence-word bipartite graphs. These pre-trained sentence representations are then utilized in a graph-based ranking algorithm for unsupervised summarization. Our method produces predominant performance for unsupervised summarization frameworks by providing summary-worthy sentence representations. It surpasses heavy BERT- or RoBERTa-based sentence representations in downstream tasks.\n## Backbones of Summarization Approaches\nThere are several simple unsupervised summarization extraction frameworks, including TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), as well as more robust graph-based ranking methods such as Pac-Sum (Zheng and Lapata, 2019), FAR (Liang et al., 2021), DASG (Liu et al., 2021). Graph-based ranking methods take sentence representations as input, using the algorithm of graph-based sentence centrality ranking for sentence selection. We now introduce extractive summarization backbones. \n\n\u2022 TextRank and LexRank utilize PageRank to calculate node centrality based on a Markov chain model recursively. \n\n\u2022 PacSum (Zheng and Lapata, 2019) constructs graphs with directed edges. The rationale behind this approach is that the centrality of two nodes is influenced by their relative position in the document, as illustrated by Equation 15. \n\n\u2022 DASG (Liu et al., 2021) selects sentences for summarization based on the similarities and relative distances among neighbouring sentences. It incorporates a graph edge weighting scheme to Equation 15, using a coefficient that maps a pair of sentence indices to a value calculated by their relative distance. \n\n\u2022 FAR (Liang et al., 2021) modifies Equation 15by applying a facet-aware centrality-based ranking model to filter out insignificant sentences. FAR also incorporates a similarity constraint between candidate summary representation and document representation to ensure the selected sentences are semantically related to the entire text, thereby facilitating summarization. \n\nThe main distinction among the extractive frameworks mentioned above lies in their centrality algorithms. A comprehensive comparison of these algorithms can be found in Appendix 8.",
            "reference_string": "[264817506 | Mao et al. | 2023 | Citations: 2]"
        },
        {
            "title": "A Survey of Automatic Text Summarization: Progress, Process and Challenges",
            "venue": "IEEE Access",
            "year": 2021,
            "reference_count": 362,
            "citation_count": 88,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09623462.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2021.3129786?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2021.3129786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348260813",
                    "name": "M. F. Mridha"
                },
                {
                    "authorId": "2149534558",
                    "name": "Aklima Akter Lima"
                },
                {
                    "authorId": "2086569",
                    "name": "Prof. Dr. Kamruddin Nur"
                },
                {
                    "authorId": "2152905283",
                    "name": "S. Das"
                },
                {
                    "authorId": "2663201",
                    "name": "M. Hasan"
                },
                {
                    "authorId": "2055992775",
                    "name": "Muhammad Mohsin Kabir"
                }
            ],
            "abstract": "With the evolution of the Internet and multimedia technology, the amount of text data has increased exponentially. This text volume is a precious source of information and knowledge that needs to be efficiently summarized. Text summarization is the method to reduce the source text into a compact variant, preserving its knowledge and the actual meaning. Here we thoroughly investigate the automatic text summarization (ATS) and summarize the widely recognized ATS architectures. This paper outlines extractive and abstractive text summarization technologies and provides a deep taxonomy of the ATS domain. The taxonomy presents the classical ATS algorithms to modern deep learning ATS architectures. Every modern text summarization approach\u2019s workflow and significance are reviewed with the limitations with potential recovery methods, including the feature extraction approaches, datasets, performance measurement techniques, and challenges of the ATS domain, etc. In addition, this paper concisely presents the past, present, and future research directions in the ATS domain.",
            "corpus_id": 244510269,
            "sentences": [
                {
                    "corpus_id": "244510269",
                    "title": "A Survey of Automatic Text Summarization: Progress, Process and Challenges",
                    "text": "The graph-based method can be applied to both extractive and abstractive text summarization. This approach is an unsupervised learning method that rates the required sentences or terms using a graph. The purpose of the graphical process is to extract the most relevant sentences from a single text [152]. \n\nGraph-based ranking algorithms determine the relevance of a vertex in a graph based on global information iteratively extracted from the entire chart. When it comes to text summarization, specific graph-based techniques are applied. 1) LexRank: LexRank is a probabilistic graph-based technique for calculating sentence significance based on the notion of eigenvector centrality in a graph representation of phrases for natural language processing. It is a connectivity matrix based on intra-sentence cosine similarity that is utilized as the adjacency matrix in a sentence graph representation [65]. [257], [258], [259], [260], [261] proposed the LexRank algorithm for graph-based text summarization task. 2) Hyperlink-Induced Topic Search (HITS): Hyperlinkinduced topic search is a link analysis algorithm that determines the authority and hub values. The results for the search query are retrieved, and then the computation is performed only on this set of results. A hub value is the total of the scaled authority values of the pages it points to, and an authority value is the sum of the scaled authority values of the pages it points to [262]. Some articles focused on the HITS ranking algorithm for graphbased text summarization tasks, as can be seen in [263], [264], [265], and [266]. 3) PageRank: The PageRank algorithm utilizes the inbound links of specified pages to measure their significance or quality to rank the search results. PageRank links more weight based on the importance of the page from which it originates [267]. Some articles proposed the PageRank algorithm for summarization [268], [269], [270], [271], [272]. 4) TextRank: TextRank is an unsupervised method for automatic text summarization to extract the most important keywords from a document.",
                    "score": 0.6545187748871646,
                    "section_title": "E. EXTRACTIVE + ABSTRACTIVE 1) Graph Based Method",
                    "char_start_offset": 48908,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 92
                        },
                        {
                            "start": 93,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 304
                        },
                        {
                            "start": 307,
                            "end": 457
                        },
                        {
                            "start": 458,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 754
                        },
                        {
                            "start": 755,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1012
                        },
                        {
                            "start": 1013,
                            "end": 1159
                        },
                        {
                            "start": 1160,
                            "end": 1274
                        },
                        {
                            "start": 1275,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1598
                        },
                        {
                            "start": 1599,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 1943
                        },
                        {
                            "start": 1944,
                            "end": 2080
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 298,
                            "end": 303,
                            "matchedPaperCorpusId": "29980788"
                        },
                        {
                            "start": 901,
                            "end": 905,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 907,
                            "end": 912,
                            "matchedPaperCorpusId": "17636912"
                        },
                        {
                            "start": 914,
                            "end": 919,
                            "matchedPaperCorpusId": "40873863"
                        },
                        {
                            "start": 921,
                            "end": 926,
                            "matchedPaperCorpusId": "63875451"
                        },
                        {
                            "start": 928,
                            "end": 933,
                            "matchedPaperCorpusId": "17951450"
                        },
                        {
                            "start": 935,
                            "end": 940,
                            "matchedPaperCorpusId": "38751984"
                        },
                        {
                            "start": 1449,
                            "end": 1454,
                            "matchedPaperCorpusId": "12222596"
                        },
                        {
                            "start": 1567,
                            "end": 1572,
                            "matchedPaperCorpusId": "21346422"
                        },
                        {
                            "start": 1574,
                            "end": 1579,
                            "matchedPaperCorpusId": "2100529"
                        },
                        {
                            "start": 1581,
                            "end": 1586,
                            "matchedPaperCorpusId": "5540065"
                        },
                        {
                            "start": 1592,
                            "end": 1597,
                            "matchedPaperCorpusId": "18376399"
                        },
                        {
                            "start": 1909,
                            "end": 1914,
                            "matchedPaperCorpusId": "213789058"
                        },
                        {
                            "start": 1916,
                            "end": 1921,
                            "matchedPaperCorpusId": "69927855"
                        },
                        {
                            "start": 1923,
                            "end": 1928,
                            "matchedPaperCorpusId": "33925572"
                        },
                        {
                            "start": 1930,
                            "end": 1935,
                            "matchedPaperCorpusId": "14944858"
                        },
                        {
                            "start": 1937,
                            "end": 1942,
                            "matchedPaperCorpusId": "15513453"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: A Survey of Automatic Text Summarization: Progress, Process and Challenges\n# Venue: IEEE Access\n# Authors: M. F. Mridha, Aklima Akter Lima, Prof. Dr. Kamruddin Nur, S. Das, M. Hasan, Muhammad Mohsin Kabir\n## Abstract\nWith the evolution of the Internet and multimedia technology, the amount of text data has increased exponentially. This text volume is a precious source of information and knowledge that needs to be efficiently summarized. Text summarization is the method to reduce the source text into a compact variant, preserving its knowledge and the actual meaning. Here we thoroughly investigate the automatic text summarization (ATS) and summarize the widely recognized ATS architectures. This paper outlines extractive and abstractive text summarization technologies and provides a deep taxonomy of the ATS domain. The taxonomy presents the classical ATS algorithms to modern deep learning ATS architectures. Every modern text summarization approach\u2019s workflow and significance are reviewed with the limitations with potential recovery methods, including the feature extraction approaches, datasets, performance measurement techniques, and challenges of the ATS domain, etc. In addition, this paper concisely presents the past, present, and future research directions in the ATS domain.\n## E. EXTRACTIVE + ABSTRACTIVE 1) Graph Based Method\nThe graph-based method can be applied to both extractive and abstractive text summarization. This approach is an unsupervised learning method that rates the required sentences or terms using a graph. The purpose of the graphical process is to extract the most relevant sentences from a single text [152]. \n\nGraph-based ranking algorithms determine the relevance of a vertex in a graph based on global information iteratively extracted from the entire chart. When it comes to text summarization, specific graph-based techniques are applied. 1) LexRank: LexRank is a probabilistic graph-based technique for calculating sentence significance based on the notion of eigenvector centrality in a graph representation of phrases for natural language processing. It is a connectivity matrix based on intra-sentence cosine similarity that is utilized as the adjacency matrix in a sentence graph representation [65]. [257], [258], [259], [260], [261] proposed the LexRank algorithm for graph-based text summarization task. 2) Hyperlink-Induced Topic Search (HITS): Hyperlinkinduced topic search is a link analysis algorithm that determines the authority and hub values. The results for the search query are retrieved, and then the computation is performed only on this set of results. A hub value is the total of the scaled authority values of the pages it points to, and an authority value is the sum of the scaled authority values of the pages it points to [262]. Some articles focused on the HITS ranking algorithm for graphbased text summarization tasks, as can be seen in [263], [264], [265], and [266]. 3) PageRank: The PageRank algorithm utilizes the inbound links of specified pages to measure their significance or quality to rank the search results. PageRank links more weight based on the importance of the page from which it originates [267]. Some articles proposed the PageRank algorithm for summarization [268], [269], [270], [271], [272]. 4) TextRank: TextRank is an unsupervised method for automatic text summarization to extract the most important keywords from a document.",
            "reference_string": "[244510269 | Mridha et al. | 2021 | Citations: 88]"
        },
        {
            "title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 0,
            "citation_count": 60,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.06508",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.06508, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "10430740",
                    "name": "Leonardo F. R. Ribeiro"
                }
            ],
            "abstract": "Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.",
            "corpus_id": 264614849,
            "sentences": [
                {
                    "corpus_id": "264614849",
                    "title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations",
                    "text": "-based Representations for Summarization. A growing body of work focuses on using graph-based representations for improving summarization. Whereas different approaches encode graphs into neural models for multi-document summarization (Fan et al., 2019;Li et al., 2020;Pasunuru et al., 2021;Chen et al., 2021), AMR structures have been shown to benefit both document representation and summary generation (Liu et al., 2015;Liao et al., 2018;Hardy and Vlachos, 2018;Dohare et al., 2018) and have the potential of improving controllability in summarization . The above works are related to FACTGRAPH as they use semantic graphs for content representation, but also different because they utilize graphs for the downstream summarization task, whereas FACTGRAPH employ them for factuality evaluation.\n\nSemantic Representations for Factuality Evaluation. More closely to our work, Goodrich et al. (2019) extract tuples from the document and summary and measure the factual consistency by overlapping metrics. Recently, dependency arc entailment (DAE, Goyal and Durrett, 2020) is used to measure subsentence-level factuality by classifying pairs of words defined by dependency arcs which often describe semantic relations. However, FACTGRAPH is considerably different from those approaches, since it explicitly encodes the entire graph semantic structure into the model. Moreover, while DAE considers semantic edge relations of the summary only, FACTGRAPH encodes the semantic structures of both the input document and summary leading to better factuality performance at both sentence and subsentence levels.",
                    "score": 0.6498881218233836,
                    "section_title": "Related Work",
                    "char_start_offset": 7114,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 234,
                            "end": 252,
                            "matchedPaperCorpusId": "202785778"
                        },
                        {
                            "start": 252,
                            "end": 268,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 268,
                            "end": 290,
                            "matchedPaperCorpusId": "235097309"
                        },
                        {
                            "start": 290,
                            "end": 308,
                            "matchedPaperCorpusId": "239768911"
                        },
                        {
                            "start": 404,
                            "end": 422,
                            "matchedPaperCorpusId": "5001921"
                        },
                        {
                            "start": 422,
                            "end": 440,
                            "matchedPaperCorpusId": "49210924"
                        },
                        {
                            "start": 440,
                            "end": 464,
                            "matchedPaperCorpusId": "52111191"
                        },
                        {
                            "start": 464,
                            "end": 484,
                            "matchedPaperCorpusId": "51867906"
                        },
                        {
                            "start": 875,
                            "end": 897,
                            "matchedPaperCorpusId": "173188138"
                        },
                        {
                            "start": 1045,
                            "end": 1068,
                            "matchedPaperCorpusId": "222291532"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91015625
                }
            ],
            "relevance_judgement": 0.91015625,
            "relevance_judgment_input_expanded": "# Title: FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Leonardo F. R. Ribeiro\n## Abstract\nDespite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.\n## Related Work\n-based Representations for Summarization. A growing body of work focuses on using graph-based representations for improving summarization. Whereas different approaches encode graphs into neural models for multi-document summarization (Fan et al., 2019;Li et al., 2020;Pasunuru et al., 2021;Chen et al., 2021), AMR structures have been shown to benefit both document representation and summary generation (Liu et al., 2015;Liao et al., 2018;Hardy and Vlachos, 2018;Dohare et al., 2018) and have the potential of improving controllability in summarization . The above works are related to FACTGRAPH as they use semantic graphs for content representation, but also different because they utilize graphs for the downstream summarization task, whereas FACTGRAPH employ them for factuality evaluation.\n\nSemantic Representations for Factuality Evaluation. More closely to our work, Goodrich et al. (2019) extract tuples from the document and summary and measure the factual consistency by overlapping metrics. Recently, dependency arc entailment (DAE, Goyal and Durrett, 2020) is used to measure subsentence-level factuality by classifying pairs of words defined by dependency arcs which often describe semantic relations. However, FACTGRAPH is considerably different from those approaches, since it explicitly encodes the entire graph semantic structure into the model. Moreover, while DAE considers semantic edge relations of the summary only, FACTGRAPH encodes the semantic structures of both the input document and summary leading to better factuality performance at both sentence and subsentence levels.",
            "reference_string": "[264614849 | Ribeiro | 2022 | Citations: 60]"
        },
        {
            "title": "Text Summarization Using FrameNet-Based Semantic Graph Model",
            "venue": "Scientific Programming",
            "year": 2016,
            "reference_count": 37,
            "citation_count": 25,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1155/2016/5130603",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1155/2016/5130603?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1155/2016/5130603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2110981656",
                    "name": "Xu Han"
                },
                {
                    "authorId": "143750477",
                    "name": "Tao Lv"
                },
                {
                    "authorId": "48430746",
                    "name": "Zhirui Hu"
                },
                {
                    "authorId": "2115554077",
                    "name": "Xinyan Wang"
                },
                {
                    "authorId": "50097174",
                    "name": "Cong Wang"
                }
            ],
            "abstract": "Text summarization is to generate a condensed version of the original document. The major issues for text summarization are eliminating redundant information, identifying important difference among documents, and recovering the informative content. This paper proposes a Semantic Graph Model which exploits the semantic information of sentence using FSGM. FSGM treats sentences as vertexes while the semantic relationship as the edges. It uses FrameNet and word embedding to calculate the similarity of sentences. This method assigns weight to both sentence nodes and edges. After all, it proposes an improved method to rank these sentences, considering both internal and external information. The experimental results show that the applicability of the model to summarize text is feasible and effective.",
            "corpus_id": 13237013,
            "sentences": [
                {
                    "corpus_id": "13237013",
                    "title": "Text Summarization Using FrameNet-Based Semantic Graph Model",
                    "text": "Another approach of unsupervised extract-based summarization [11,16] uses graph-based model. TextRank [17] and LexRank [18] are first two graph-based models applied in text summarization, which use the PageRank-like algorithms to mark sentences. Then, other researchers have integrated the statistical and linguistic features to drive the sentence selection process, for example, the sentence position [19], term frequency [20], topic signature [21], lexical chains [22], and syntactic patterns [7,23]. Ko and Seo [24] composed two sentences nearby into a bigram. Those bigrams were supposed to be context information. First, they extracted the bigrams by using the sentence extraction model. Then they used another extraction module to extract sentences from them. The ClusterCMRW and ClusterHITS models calculated the sentences scores by considering the clusterlevel information in the graph-based ranking algorithm. Canhasi and Kononenko [25] improve matrix decomposition by employing the archetypal analysis for generic multidocument summarization. While coming to the document set, there must be more consideration about the document-level influence. But it did not consider the relationship between words and sentences. The DsR model [26] achieved it by using document-sensitive graph-based ranking model. But this method did not get a satisfied result. Yin et al. improved the summarization quality by adding extra information which came from the query-extraction scenario. Goyal et al. [27] take Bernoulli model of randomness to index weights of sentences taking the context into consideration. The method proposed in [28] decomposed sentences by semantic role analysis, but while building the model, it did not use graphbased algorithms. \n\nHowever, most of these graph-based methods only consider the relation of keyword cooccurrence, without considering the sentence-level dependency syntax. Those papers which use semantic information do not utilize the semantic information in the sentence-level. Thus, how to take advantage of the relationship between sentences needs further research. In this paper, it proposes sentence-level Semantic Graph Model. FSGM can build the relationships between sentences in a better way. Several experiments show ideal results in our model.",
                    "score": 0.6233365306618417,
                    "section_title": "Related Work",
                    "char_start_offset": 5921,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 92
                        },
                        {
                            "start": 93,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 502
                        },
                        {
                            "start": 503,
                            "end": 563
                        },
                        {
                            "start": 564,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1155
                        },
                        {
                            "start": 1156,
                            "end": 1225
                        },
                        {
                            "start": 1226,
                            "end": 1311
                        },
                        {
                            "start": 1312,
                            "end": 1359
                        },
                        {
                            "start": 1360,
                            "end": 1480
                        },
                        {
                            "start": 1481,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1746
                        },
                        {
                            "start": 1749,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 2008
                        },
                        {
                            "start": 2009,
                            "end": 2098
                        },
                        {
                            "start": 2099,
                            "end": 2162
                        },
                        {
                            "start": 2163,
                            "end": 2230
                        },
                        {
                            "start": 2231,
                            "end": 2283
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 61,
                            "end": 65,
                            "matchedPaperCorpusId": "2642018"
                        },
                        {
                            "start": 65,
                            "end": 68,
                            "matchedPaperCorpusId": "1750917"
                        },
                        {
                            "start": 102,
                            "end": 106,
                            "matchedPaperCorpusId": "6524990"
                        },
                        {
                            "start": 119,
                            "end": 123,
                            "matchedPaperCorpusId": "11847258"
                        },
                        {
                            "start": 402,
                            "end": 406,
                            "matchedPaperCorpusId": "11989149"
                        },
                        {
                            "start": 423,
                            "end": 427,
                            "matchedPaperCorpusId": "5519987"
                        },
                        {
                            "start": 445,
                            "end": 449,
                            "matchedPaperCorpusId": "8598694"
                        },
                        {
                            "start": 466,
                            "end": 470,
                            "matchedPaperCorpusId": "1584325"
                        },
                        {
                            "start": 495,
                            "end": 498,
                            "matchedPaperCorpusId": "15709889"
                        },
                        {
                            "start": 498,
                            "end": 501,
                            "matchedPaperCorpusId": "503868"
                        },
                        {
                            "start": 514,
                            "end": 518,
                            "matchedPaperCorpusId": "16153195"
                        },
                        {
                            "start": 941,
                            "end": 945,
                            "matchedPaperCorpusId": "207057042"
                        },
                        {
                            "start": 1240,
                            "end": 1244,
                            "matchedPaperCorpusId": "35133176"
                        },
                        {
                            "start": 1494,
                            "end": 1498,
                            "matchedPaperCorpusId": "17333959"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9091796875
                }
            ],
            "relevance_judgement": 0.9091796875,
            "relevance_judgment_input_expanded": "# Title: Text Summarization Using FrameNet-Based Semantic Graph Model\n# Venue: Scientific Programming\n# Authors: Xu Han, Tao Lv, Zhirui Hu, Xinyan Wang, Cong Wang\n## Abstract\nText summarization is to generate a condensed version of the original document. The major issues for text summarization are eliminating redundant information, identifying important difference among documents, and recovering the informative content. This paper proposes a Semantic Graph Model which exploits the semantic information of sentence using FSGM. FSGM treats sentences as vertexes while the semantic relationship as the edges. It uses FrameNet and word embedding to calculate the similarity of sentences. This method assigns weight to both sentence nodes and edges. After all, it proposes an improved method to rank these sentences, considering both internal and external information. The experimental results show that the applicability of the model to summarize text is feasible and effective.\n## Related Work\nAnother approach of unsupervised extract-based summarization [11,16] uses graph-based model. TextRank [17] and LexRank [18] are first two graph-based models applied in text summarization, which use the PageRank-like algorithms to mark sentences. Then, other researchers have integrated the statistical and linguistic features to drive the sentence selection process, for example, the sentence position [19], term frequency [20], topic signature [21], lexical chains [22], and syntactic patterns [7,23]. Ko and Seo [24] composed two sentences nearby into a bigram. Those bigrams were supposed to be context information. First, they extracted the bigrams by using the sentence extraction model. Then they used another extraction module to extract sentences from them. The ClusterCMRW and ClusterHITS models calculated the sentences scores by considering the clusterlevel information in the graph-based ranking algorithm. Canhasi and Kononenko [25] improve matrix decomposition by employing the archetypal analysis for generic multidocument summarization. While coming to the document set, there must be more consideration about the document-level influence. But it did not consider the relationship between words and sentences. The DsR model [26] achieved it by using document-sensitive graph-based ranking model. But this method did not get a satisfied result. Yin et al. improved the summarization quality by adding extra information which came from the query-extraction scenario. Goyal et al. [27] take Bernoulli model of randomness to index weights of sentences taking the context into consideration. The method proposed in [28] decomposed sentences by semantic role analysis, but while building the model, it did not use graphbased algorithms. \n\nHowever, most of these graph-based methods only consider the relation of keyword cooccurrence, without considering the sentence-level dependency syntax. Those papers which use semantic information do not utilize the semantic information in the sentence-level. Thus, how to take advantage of the relationship between sentences needs further research. In this paper, it proposes sentence-level Semantic Graph Model. FSGM can build the relationships between sentences in a better way. Several experiments show ideal results in our model.",
            "reference_string": "[13237013 | Han et al. | 2016 | Citations: 25]"
        },
        {
            "title": "A Review on Automatic Text Summarization Approaches",
            "venue": "Journal of Computer Science",
            "year": 2016,
            "reference_count": 87,
            "citation_count": 72,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://thescipub.com/pdf/10.3844/jcssp.2016.178.190",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3844/jcssp.2016.178.190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3844/jcssp.2016.178.190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1734844",
                    "name": "Y. J. Kumar"
                },
                {
                    "authorId": "2445567",
                    "name": "O. Goh"
                },
                {
                    "authorId": "2136088",
                    "name": "H. Basiron"
                },
                {
                    "authorId": "3388053",
                    "name": "Ngo Hea Choon"
                },
                {
                    "authorId": "21154901",
                    "name": "Puspalata C. Suppiah"
                }
            ],
            "abstract": "It has been more than 50 years since the initial investigation on automatic text summarization was started. Various techniques have been successfully used to extract the important contents from text document to represent document summary. In this study, we review some of the studies that have been conducted in this still-developing research area. It covers the basics of text summarization, the types of summarization, the methods that have been used and some areas in which text summarization has been applied. Furthermore, this paper also reviews the significant efforts which have been put in studies concerning sentence extraction, domain specific summarization and multi document summarization and provides the theoretical explanation and the fundamental concepts related to it. In addition, the advantages and limitations concerning the approaches commonly used for text summarization are also highlighted in this study.",
            "corpus_id": 33696521,
            "sentences": [
                {
                    "corpus_id": "33696521",
                    "title": "A Review on Automatic Text Summarization Approaches",
                    "text": "Graph theory is simply used to model the connections or links that exist between objects. Generally, a graph can be denoted in the form of G = (V, E), where V represents the graph's vertex or node and E is the edge between each vertex. In the context of text documents, vertex represents sentences and an edge is the weight between two sentences. Using this approach, documents can therefore be represented as a graph where each sentence becomes the vertex and the weight between each vertex corresponds to the similarity between the two sentences. \n\nAs in most literature concerning graph based approach, the most widely used similarity measure is the cosine similarity measure (Erkan and Radev, 2004). An edge then exists if the similarity weight is above some predefined threshold. Figure 5 shows an example graph for multi document. Once the graph is constructed for a set of documents, important sentences will then be identified; it follows the idea that a sentence is considered important if it is strongly connected to many other sentences. \n\nThis approach differs from the cluster based approach where sentences are ranked based on its closeness to cluster centroid. Two well-known graph based ranking algorithms is the HITS algorithm (Kleinberg, 1999) and the Google's PageRank (Brin and Page, 2012). Both methods have been traditionally used in Web-link analysis and social networks. Lexrank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are two successful graph-based ranking systems that implement these algorithms. \n\nFurther studies have been carried to make improvement through modification in the ranking algorithm. Wan and Yang (2006) assigned different weights to intra-document links and inter-document links. \n\nThey gave more priority to sentence with high interdocument links. In later work by Hariharan and Srinivasan (2009), they approached the graph based method differently i.e., by discounting the already selected sentence by removing it from further consideration when they rank the remaining sentences in the document. \n\nApart from sentence level information, Wan (2008) and Wei et al. (2010) devised a document-sensitive graph model to explore document impact on the graphbased summarization, by incorporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process.",
                    "score": 0.5912989591546349,
                    "section_title": "Graph Based Method",
                    "char_start_offset": 26533,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 89
                        },
                        {
                            "start": 90,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 548
                        },
                        {
                            "start": 551,
                            "end": 703
                        },
                        {
                            "start": 704,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 1048
                        },
                        {
                            "start": 1051,
                            "end": 1175
                        },
                        {
                            "start": 1176,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1546
                        },
                        {
                            "start": 1549,
                            "end": 1649
                        },
                        {
                            "start": 1650,
                            "end": 1746
                        },
                        {
                            "start": 1749,
                            "end": 1815
                        },
                        {
                            "start": 1816,
                            "end": 2065
                        },
                        {
                            "start": 2068,
                            "end": 2371
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 679,
                            "end": 701,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1244,
                            "end": 1261,
                            "matchedPaperCorpusId": "216000619"
                        },
                        {
                            "start": 1288,
                            "end": 1309,
                            "matchedPaperCorpusId": "911040"
                        },
                        {
                            "start": 1403,
                            "end": 1426,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1650,
                            "end": 1669,
                            "matchedPaperCorpusId": "5457260"
                        },
                        {
                            "start": 1833,
                            "end": 1864,
                            "matchedPaperCorpusId": "62241768"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90771484375
                }
            ],
            "relevance_judgement": 0.90771484375,
            "relevance_judgment_input_expanded": "# Title: A Review on Automatic Text Summarization Approaches\n# Venue: Journal of Computer Science\n# Authors: Y. J. Kumar, O. Goh, H. Basiron, Ngo Hea Choon, Puspalata C. Suppiah\n## Abstract\nIt has been more than 50 years since the initial investigation on automatic text summarization was started. Various techniques have been successfully used to extract the important contents from text document to represent document summary. In this study, we review some of the studies that have been conducted in this still-developing research area. It covers the basics of text summarization, the types of summarization, the methods that have been used and some areas in which text summarization has been applied. Furthermore, this paper also reviews the significant efforts which have been put in studies concerning sentence extraction, domain specific summarization and multi document summarization and provides the theoretical explanation and the fundamental concepts related to it. In addition, the advantages and limitations concerning the approaches commonly used for text summarization are also highlighted in this study.\n## Graph Based Method\nGraph theory is simply used to model the connections or links that exist between objects. Generally, a graph can be denoted in the form of G = (V, E), where V represents the graph's vertex or node and E is the edge between each vertex. In the context of text documents, vertex represents sentences and an edge is the weight between two sentences. Using this approach, documents can therefore be represented as a graph where each sentence becomes the vertex and the weight between each vertex corresponds to the similarity between the two sentences. \n\nAs in most literature concerning graph based approach, the most widely used similarity measure is the cosine similarity measure (Erkan and Radev, 2004). An edge then exists if the similarity weight is above some predefined threshold. Figure 5 shows an example graph for multi document. Once the graph is constructed for a set of documents, important sentences will then be identified; it follows the idea that a sentence is considered important if it is strongly connected to many other sentences. \n\nThis approach differs from the cluster based approach where sentences are ranked based on its closeness to cluster centroid. Two well-known graph based ranking algorithms is the HITS algorithm (Kleinberg, 1999) and the Google's PageRank (Brin and Page, 2012). Both methods have been traditionally used in Web-link analysis and social networks. Lexrank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are two successful graph-based ranking systems that implement these algorithms. \n\nFurther studies have been carried to make improvement through modification in the ranking algorithm. Wan and Yang (2006) assigned different weights to intra-document links and inter-document links. \n\nThey gave more priority to sentence with high interdocument links. In later work by Hariharan and Srinivasan (2009), they approached the graph based method differently i.e., by discounting the already selected sentence by removing it from further consideration when they rank the remaining sentences in the document. \n\nApart from sentence level information, Wan (2008) and Wei et al. (2010) devised a document-sensitive graph model to explore document impact on the graphbased summarization, by incorporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process.",
            "reference_string": "[33696521 | Kumar et al. | 2016 | Citations: 72]"
        },
        {
            "title": "Learning with fuzzy hypergraphs: A topical approach to query-oriented text summarization",
            "venue": "Information Sciences",
            "year": 2019,
            "reference_count": 53,
            "citation_count": 26,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1906.09445",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.09445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "29364954",
                    "name": "H. V. Lierde"
                },
                {
                    "authorId": "144134805",
                    "name": "T. Chow"
                }
            ],
            "abstract": null,
            "corpus_id": 182712867,
            "sentences": [
                {
                    "corpus_id": "182712867",
                    "title": "Learning with fuzzy hypergraphs: A topical approach to query-oriented text summarization",
                    "text": "Extractive summarizers aggregate important sentences in a corpus while abstractive summarizers generate new summaries after identifying important information [26]. As abstractive summarization requires extensive Natural Language Processing, most summarizers to date are based on extractive approaches. \n\nMethods of extractive summarization generally fall into two categories, namely featurebased and graph-based approaches. Feature-based methods train a model to predict the score of each sentence based on feature representations of sentences (term frequency, sentence position [26], etc.). Graph-based methods define graphs in which nodes are sentences and edges represent similarities between sentences. Sentence scores are then given by node centrality measures on the graph [11,27]. The advantages of graph-based summarization over feature-based summarization are that it does not require labelled corpora, and it is based on the global structure of links between sentences of the corpus rather than local features. \n\nThe earliest graph-based summarizer, called LexRank [11], defines edges as term co-occurrence relationships between sentences. Then, PageRank algorithm is applied to compute relevance scores of sentences. Adapting this idea for the task of query-focused summarization, topic sensitive LexRank [27] introduces a query bias in probabilities of transition, which results in higher scores for sentences that are similar to the query. Sim-ilarly, [36] proposes a manifold ranking algorithm in which scores are popagated accross a graph including both sentences and the query as vertices. To remove redundancies in summaries, [23] proposes a new node ranking algorithm called DivRank, which tends to select dissimilar sentences. While early graph-based algorithms only involved sentences, a bipartite graph model is proposed in [37], involving both sentences and terms as vertices and it applies HITS algorithm to score sentences. [40] combines this idea with a PageRank-like method to score sentences, terms and documents simultaneously. \n\nWhile early methods build sentence graphs based on co-occurrence of terms in sentences only, later approaches infer higher level relationships. These methods include sentence clusters in the graph construction, namely groups of similar sentences.",
                    "score": 0.6551739141305327,
                    "section_title": "Related work",
                    "char_start_offset": 10671,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 301
                        },
                        {
                            "start": 304,
                            "end": 423
                        },
                        {
                            "start": 424,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 1020
                        },
                        {
                            "start": 1023,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1452
                        },
                        {
                            "start": 1453,
                            "end": 1605
                        },
                        {
                            "start": 1606,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1947
                        },
                        {
                            "start": 1948,
                            "end": 2055
                        },
                        {
                            "start": 2058,
                            "end": 2201
                        },
                        {
                            "start": 2202,
                            "end": 2304
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 779,
                            "end": 783,
                            "matchedPaperCorpusId": "2962090"
                        },
                        {
                            "start": 783,
                            "end": 786,
                            "matchedPaperCorpusId": "10274362"
                        },
                        {
                            "start": 1075,
                            "end": 1079,
                            "matchedPaperCorpusId": "2962090"
                        },
                        {
                            "start": 1316,
                            "end": 1320,
                            "matchedPaperCorpusId": "10274362"
                        },
                        {
                            "start": 1465,
                            "end": 1469,
                            "matchedPaperCorpusId": "11840355"
                        },
                        {
                            "start": 1643,
                            "end": 1647,
                            "matchedPaperCorpusId": "11977708"
                        },
                        {
                            "start": 1845,
                            "end": 1849,
                            "matchedPaperCorpusId": "9849366"
                        },
                        {
                            "start": 1948,
                            "end": 1952,
                            "matchedPaperCorpusId": "35133176"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90625
                }
            ],
            "relevance_judgement": 0.90625,
            "relevance_judgment_input_expanded": "# Title: Learning with fuzzy hypergraphs: A topical approach to query-oriented text summarization\n# Venue: Information Sciences\n# Authors: H. V. Lierde, T. Chow\n## Abstract\nNone\n## Related work\nExtractive summarizers aggregate important sentences in a corpus while abstractive summarizers generate new summaries after identifying important information [26]. As abstractive summarization requires extensive Natural Language Processing, most summarizers to date are based on extractive approaches. \n\nMethods of extractive summarization generally fall into two categories, namely featurebased and graph-based approaches. Feature-based methods train a model to predict the score of each sentence based on feature representations of sentences (term frequency, sentence position [26], etc.). Graph-based methods define graphs in which nodes are sentences and edges represent similarities between sentences. Sentence scores are then given by node centrality measures on the graph [11,27]. The advantages of graph-based summarization over feature-based summarization are that it does not require labelled corpora, and it is based on the global structure of links between sentences of the corpus rather than local features. \n\nThe earliest graph-based summarizer, called LexRank [11], defines edges as term co-occurrence relationships between sentences. Then, PageRank algorithm is applied to compute relevance scores of sentences. Adapting this idea for the task of query-focused summarization, topic sensitive LexRank [27] introduces a query bias in probabilities of transition, which results in higher scores for sentences that are similar to the query. Sim-ilarly, [36] proposes a manifold ranking algorithm in which scores are popagated accross a graph including both sentences and the query as vertices. To remove redundancies in summaries, [23] proposes a new node ranking algorithm called DivRank, which tends to select dissimilar sentences. While early graph-based algorithms only involved sentences, a bipartite graph model is proposed in [37], involving both sentences and terms as vertices and it applies HITS algorithm to score sentences. [40] combines this idea with a PageRank-like method to score sentences, terms and documents simultaneously. \n\nWhile early methods build sentence graphs based on co-occurrence of terms in sentences only, later approaches infer higher level relationships. These methods include sentence clusters in the graph construction, namely groups of similar sentences.",
            "reference_string": "[182712867 | Lierde et al. | 2019 | Citations: 26]"
        },
        {
            "title": "Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.15327, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135688409",
                    "name": "Haopeng Zhang"
                },
                {
                    "authorId": "1718428",
                    "name": "Jiawei Zhang"
                }
            ],
            "abstract": "Unsupervised document summarization has reacquired lots of attention in recent years thanks to its simplicity and data independence. In this paper, we propose a graph-based unsupervised approach for extractive document summarization. Instead of ranking sentences by salience and extracting sentences one by one, our approach works at a summary-level by utilizing graph centrality and centroid. We first extract summary candidates as subgraphs based on centrality from the sentence graph and then select from the summary candidates by matching to the centroid. We perform extensive experiments on two bench-marked summarization datasets, and the results demonstrate the effectiveness of our model compared to state-of-theart baselines.",
            "corpus_id": 232404911,
            "sentences": [
                {
                    "corpus_id": "232404911",
                    "title": "Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization",
                    "text": "The core of extractive summarization is to model the relation between sentences in a document, which falls in graph models' strength. In general, graph-based summarization models represent a document (or a cluster of documents) as a graph, with nodes representing sentences (or discourse) and edge weights representing sentence similarities. \n\nUnsupervised graph summarization methods rely on graph connectivity (centrality) or node representations (centroid) (Radev et al., 2004;Rossiello et al., 2017) to score and rank sentences. Popular centrality-based methods include TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and PACSUM (Zheng and Lapata, 2019). Details of centrality-based and centroid-based summarization will be discussed in Section 3. \n\nResearchers also explore supervised graph neural network (GNN) on the task (Yasunaga et al., 2017;Xu et al., 2019;Wang et al., 2020). Our paper follows this line of works on developing novel unsupervised graph models for single document summarization.",
                    "score": 0.7746069485875133,
                    "section_title": "Graph-based Summarization",
                    "char_start_offset": 4919,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 341
                        },
                        {
                            "start": 344,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 772
                        },
                        {
                            "start": 775,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1026
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 480,
                            "end": 503,
                            "matchedPaperCorpusId": "2346086"
                        },
                        {
                            "start": 583,
                            "end": 609,
                            "matchedPaperCorpusId": "577937"
                        },
                        {
                            "start": 619,
                            "end": 642,
                            "matchedPaperCorpusId": "506350"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9052734375
                }
            ],
            "relevance_judgement": 0.9052734375,
            "relevance_judgment_input_expanded": "# Title: Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization\n# Venue: arXiv.org\n# Authors: Haopeng Zhang, Jiawei Zhang\n## Abstract\nUnsupervised document summarization has reacquired lots of attention in recent years thanks to its simplicity and data independence. In this paper, we propose a graph-based unsupervised approach for extractive document summarization. Instead of ranking sentences by salience and extracting sentences one by one, our approach works at a summary-level by utilizing graph centrality and centroid. We first extract summary candidates as subgraphs based on centrality from the sentence graph and then select from the summary candidates by matching to the centroid. We perform extensive experiments on two bench-marked summarization datasets, and the results demonstrate the effectiveness of our model compared to state-of-theart baselines.\n## Graph-based Summarization\nThe core of extractive summarization is to model the relation between sentences in a document, which falls in graph models' strength. In general, graph-based summarization models represent a document (or a cluster of documents) as a graph, with nodes representing sentences (or discourse) and edge weights representing sentence similarities. \n\nUnsupervised graph summarization methods rely on graph connectivity (centrality) or node representations (centroid) (Radev et al., 2004;Rossiello et al., 2017) to score and rank sentences. Popular centrality-based methods include TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and PACSUM (Zheng and Lapata, 2019). Details of centrality-based and centroid-based summarization will be discussed in Section 3. \n\nResearchers also explore supervised graph neural network (GNN) on the task (Yasunaga et al., 2017;Xu et al., 2019;Wang et al., 2020). Our paper follows this line of works on developing novel unsupervised graph models for single document summarization.",
            "reference_string": "[232404911 | Zhang et al. | 2021 | Citations: 0]"
        },
        {
            "title": "Vietnamese multi-document summary using subgraph selection approach - VLSP 2022 AbMuSu Shared Task",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 12,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.14827",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.14827, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2220690681",
                    "name": "Huu-Thin Nguyen"
                },
                {
                    "authorId": "2101316144",
                    "name": "Tam Doan Thanh"
                },
                {
                    "authorId": "2220630783",
                    "name": "Cam-Van Thi Nguyen"
                }
            ],
            "abstract": "Document summarization is a task to generate afluent, condensed summary for a document, andkeep important information. A cluster of documents serves as the input for multi-document summarizing (MDS), while the cluster summary serves as the output. In this paper, we focus on transforming the extractive MDS problem into subgraph selection. Approaching the problem in the form of graphs helps to capture simultaneously the relationship between sentences in the same document and between sentences in the same cluster based on exploiting the overall graph structure and selected subgraphs. Experiments have been implemented on the Vietnamese dataset published in VLSP Evaluation Campaign 2022. This model currently results in the top 10 participating teams reported on the ROUGH-2 $F\\_1$ measure on the public test set.",
            "corpus_id": 259262067,
            "sentences": [
                {
                    "corpus_id": "259262067",
                    "title": "Vietnamese multi-document summary using subgraph selection approach - VLSP 2022 AbMuSu Shared Task",
                    "text": "In multi-document summarization (MDS), the input is a set of documents, and the output is a summary that describes important information in a coherent and non-redundant manner. It is a complex problem that has gained attention from the research community. In recent times, there have been significant improvements in MDS due to the availability of MDS datasets and advances in pretraining approaches. Extractive and abstractive summarizing are two well-known methods for multi-document summarization. Abstractive summarizing methods try to succinctly summarize the substance of the texts, whereas extractive summarization systems aim to extract prominent snippets, phrases, or sections from documents. \n\nGraphs capturing relationships between textual units are of great benefit to MDS, which can help create more concise, informative, and coherent summaries from multiple sources documents. Furthermore, graphs can be easily constructed by representing sentences or paragraph as graph nodes and edges. LexRank (Erkan and Radev, 2004) computes sentence importance based on a lexical similarity graph of sentences. Graph representations of documents such as discourse graph based on discourse relations (Christensen et al., 2013). \n\nIn the VLSP 2022 Evaluation Campaign, Vietnamese Abstractive multi-document summarization (AbMuSu) Shared Task (Tran et al., 2022) is proposed to promote the development of research on abstractive multi-document summarization for Vietnamese text. The Vietnamese abstractive multidocument summarization task's goal is to develop summarizing systems that could automatically generate abstractive summaries for a collection of documents on a given topic. The model produces an abstractive summary that is connected to the input, which is a collection of news items on the same topic. In this paper, we approach by switching back to the graph-based extractive problem, specifically, extracting summaries for multi-document by selecting sub-graphs based on the constructed graph. Inspired by study (Chen et al., 2021), we exploit more edge weights representing the relationship of sentences in the same document and train and improve the model for Vietnamese dataset.",
                    "score": 0.6986807413560044,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 400
                        },
                        {
                            "start": 401,
                            "end": 500
                        },
                        {
                            "start": 501,
                            "end": 701
                        },
                        {
                            "start": 704,
                            "end": 890
                        },
                        {
                            "start": 891,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1228
                        },
                        {
                            "start": 1231,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1682
                        },
                        {
                            "start": 1683,
                            "end": 1811
                        },
                        {
                            "start": 1812,
                            "end": 2005
                        },
                        {
                            "start": 2006,
                            "end": 2193
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1010,
                            "end": 1032,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1201,
                            "end": 1227,
                            "matchedPaperCorpusId": "337730"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90478515625
                }
            ],
            "relevance_judgement": 0.90478515625,
            "relevance_judgment_input_expanded": "# Title: Vietnamese multi-document summary using subgraph selection approach - VLSP 2022 AbMuSu Shared Task\n# Venue: arXiv.org\n# Authors: Huu-Thin Nguyen, Tam Doan Thanh, Cam-Van Thi Nguyen\n## Abstract\nDocument summarization is a task to generate afluent, condensed summary for a document, andkeep important information. A cluster of documents serves as the input for multi-document summarizing (MDS), while the cluster summary serves as the output. In this paper, we focus on transforming the extractive MDS problem into subgraph selection. Approaching the problem in the form of graphs helps to capture simultaneously the relationship between sentences in the same document and between sentences in the same cluster based on exploiting the overall graph structure and selected subgraphs. Experiments have been implemented on the Vietnamese dataset published in VLSP Evaluation Campaign 2022. This model currently results in the top 10 participating teams reported on the ROUGH-2 $F\\_1$ measure on the public test set.\n## Introduction\nIn multi-document summarization (MDS), the input is a set of documents, and the output is a summary that describes important information in a coherent and non-redundant manner. It is a complex problem that has gained attention from the research community. In recent times, there have been significant improvements in MDS due to the availability of MDS datasets and advances in pretraining approaches. Extractive and abstractive summarizing are two well-known methods for multi-document summarization. Abstractive summarizing methods try to succinctly summarize the substance of the texts, whereas extractive summarization systems aim to extract prominent snippets, phrases, or sections from documents. \n\nGraphs capturing relationships between textual units are of great benefit to MDS, which can help create more concise, informative, and coherent summaries from multiple sources documents. Furthermore, graphs can be easily constructed by representing sentences or paragraph as graph nodes and edges. LexRank (Erkan and Radev, 2004) computes sentence importance based on a lexical similarity graph of sentences. Graph representations of documents such as discourse graph based on discourse relations (Christensen et al., 2013). \n\nIn the VLSP 2022 Evaluation Campaign, Vietnamese Abstractive multi-document summarization (AbMuSu) Shared Task (Tran et al., 2022) is proposed to promote the development of research on abstractive multi-document summarization for Vietnamese text. The Vietnamese abstractive multidocument summarization task's goal is to develop summarizing systems that could automatically generate abstractive summaries for a collection of documents on a given topic. The model produces an abstractive summary that is connected to the input, which is a collection of news items on the same topic. In this paper, we approach by switching back to the graph-based extractive problem, specifically, extracting summaries for multi-document by selecting sub-graphs based on the constructed graph. Inspired by study (Chen et al., 2021), we exploit more edge weights representing the relationship of sentences in the same document and train and improve the model for Vietnamese dataset.",
            "reference_string": "[259262067 | Nguyen et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Multi-document Summarization Using Bipartite Graphs",
            "venue": "TextGraphs@EMNLP",
            "year": 2014,
            "reference_count": 39,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3115/v1/w14-3703",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/W14-3703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2567321",
                    "name": "Daraksha Parveen"
                },
                {
                    "authorId": "31380436",
                    "name": "M. Strube"
                }
            ],
            "abstract": "In this paper, we introduce a novel graph based technique for topic based multidocument summarization. We transform documents into a bipartite graph where one set of nodes represents entities and the other set of nodes represents sentences. To obtain the summary we apply a ranking technique to the bipartite graph which is followed by an optimization step. We test the performance of our method on several DUC datasets and compare it to the stateof-the-art.",
            "corpus_id": 8437725,
            "sentences": [
                {
                    "corpus_id": "8437725",
                    "title": "Multi-document Summarization Using Bipartite Graphs",
                    "text": "A graph-based representation of documents for summarization is adopted by various approaches. For instance, TextRank by Mihalcea and Tarau (2004) applies the PageRank algorithm (Brin and Page, 1998) to extract important sentences for single document summarization. This ranking algorithm proclaims the importance of a sentence by considering the global information which is computed recursively from the entire graph. Later, the graph is converted into a weighted graph in which the weights are calculated by measuring the similarity of sentences (Mihalcea, 2004). Similarly, in the LexRank approach (Erkan and Radev, 2004), documents are represented as a similarity graph in which the sentences are nodes and these sentences are then ranked according to centrality measures. The three centrality measures used are degree, LexRank with threshold and continuous LexRank. LexRank is a measure to calculate ranks using the similarity graph of sentences. It is also known as lexical PageRank. The summarization approach developed by Gong and Liu (2001) is also based on ranking sentences where important sentences are selected using a relevance measure and latent semantic analysis. \n\nLater, for better performance, sentences are classified according to their existence in their final summary in binary format i.e. 1 (belongs to summary) and 0 (doesn't belong to summary) (Shen et al., 2007;Gong and Liu, 2001). Here, the sentences are projected as feature vectors and conditional random fields are used to classify them. During document processing, most informative sentences are selected by the summarizer (Shen et al., 2007). Fattah and Ren (2009) also consid-ers summarization as two class classification problem. They use a genetic algorithm and mathematical regression to select appropriate weights for the features and used different classification technique for e.g. feed forward neural network, probablistic neural network and Gaussian mixture models. \n\nIn the summarization task, optimization of the three properties discussed in Section 1, relevance, non-redundancy and readability, is required. This is a global inference problem, which can be solved by two approaches. Firstly, relevance and redundancy can be optimized simultaneously.",
                    "score": 0.7289567635751519,
                    "section_title": "Related work",
                    "char_start_offset": 3693,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 94,
                            "end": 264
                        },
                        {
                            "start": 265,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 564
                        },
                        {
                            "start": 565,
                            "end": 775
                        },
                        {
                            "start": 776,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 950
                        },
                        {
                            "start": 951,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1178
                        },
                        {
                            "start": 1181,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1624
                        },
                        {
                            "start": 1625,
                            "end": 1713
                        },
                        {
                            "start": 1714,
                            "end": 1870
                        },
                        {
                            "start": 1871,
                            "end": 1956
                        },
                        {
                            "start": 1959,
                            "end": 2102
                        },
                        {
                            "start": 2103,
                            "end": 2177
                        },
                        {
                            "start": 2178,
                            "end": 2244
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 547,
                            "end": 563,
                            "matchedPaperCorpusId": "2100529"
                        },
                        {
                            "start": 600,
                            "end": 623,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1029,
                            "end": 1048,
                            "matchedPaperCorpusId": "7003631"
                        },
                        {
                            "start": 1368,
                            "end": 1387,
                            "matchedPaperCorpusId": "126818"
                        },
                        {
                            "start": 1387,
                            "end": 1406,
                            "matchedPaperCorpusId": "7003631"
                        },
                        {
                            "start": 1604,
                            "end": 1623,
                            "matchedPaperCorpusId": "126818"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90283203125
                }
            ],
            "relevance_judgement": 0.90283203125,
            "relevance_judgment_input_expanded": "# Title: Multi-document Summarization Using Bipartite Graphs\n# Venue: TextGraphs@EMNLP\n# Authors: Daraksha Parveen, M. Strube\n## Abstract\nIn this paper, we introduce a novel graph based technique for topic based multidocument summarization. We transform documents into a bipartite graph where one set of nodes represents entities and the other set of nodes represents sentences. To obtain the summary we apply a ranking technique to the bipartite graph which is followed by an optimization step. We test the performance of our method on several DUC datasets and compare it to the stateof-the-art.\n## Related work\nA graph-based representation of documents for summarization is adopted by various approaches. For instance, TextRank by Mihalcea and Tarau (2004) applies the PageRank algorithm (Brin and Page, 1998) to extract important sentences for single document summarization. This ranking algorithm proclaims the importance of a sentence by considering the global information which is computed recursively from the entire graph. Later, the graph is converted into a weighted graph in which the weights are calculated by measuring the similarity of sentences (Mihalcea, 2004). Similarly, in the LexRank approach (Erkan and Radev, 2004), documents are represented as a similarity graph in which the sentences are nodes and these sentences are then ranked according to centrality measures. The three centrality measures used are degree, LexRank with threshold and continuous LexRank. LexRank is a measure to calculate ranks using the similarity graph of sentences. It is also known as lexical PageRank. The summarization approach developed by Gong and Liu (2001) is also based on ranking sentences where important sentences are selected using a relevance measure and latent semantic analysis. \n\nLater, for better performance, sentences are classified according to their existence in their final summary in binary format i.e. 1 (belongs to summary) and 0 (doesn't belong to summary) (Shen et al., 2007;Gong and Liu, 2001). Here, the sentences are projected as feature vectors and conditional random fields are used to classify them. During document processing, most informative sentences are selected by the summarizer (Shen et al., 2007). Fattah and Ren (2009) also consid-ers summarization as two class classification problem. They use a genetic algorithm and mathematical regression to select appropriate weights for the features and used different classification technique for e.g. feed forward neural network, probablistic neural network and Gaussian mixture models. \n\nIn the summarization task, optimization of the three properties discussed in Section 1, relevance, non-redundancy and readability, is required. This is a global inference problem, which can be solved by two approaches. Firstly, relevance and redundancy can be optimized simultaneously.",
            "reference_string": "[8437725 | Parveen et al. | 2014 | Citations: 16]"
        },
        {
            "title": "Self-Supervised and Controlled Multi-Document Opinion Summarization",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 86,
            "citation_count": 48,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.eacl-main.141.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.14754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218938",
                    "name": "Hady ElSahar"
                },
                {
                    "authorId": "3443469",
                    "name": "Maximin Coavoux"
                },
                {
                    "authorId": "2907260",
                    "name": "Matthias Gall\u00e9"
                },
                {
                    "authorId": "120419790",
                    "name": "Jos Rozen"
                }
            ],
            "abstract": "We address the problem of unsupervised abstractive summarization of collections of user generated reviews through self-supervision and control. We propose a self-supervised setup that considers an individual document as a target summary for a set of similar documents. This setting makes training simpler than previous approaches by relying only on standard log-likelihood loss and mainstream models. We address the problem of hallucinations through the use of control codes, to steer the generation towards more coherent and relevant summaries.",
            "corpus_id": 216867290,
            "sentences": [
                {
                    "corpus_id": "216867290",
                    "title": "Self-Supervised and Controlled Multi-Document Opinion Summarization",
                    "text": "Unsupervised Opinion Summarization Unsupervised Multi-Document summarization methods encompass both extractive and abstractive approaches. Extractive summarization consists in selecting a few sentences from the input documents to form the output summary. Radev et al. (2004) proposed to rank sentences according to their relevance to the whole input, representing sentences as tfidf bags of words and the input as the centroid vector of its sentences. Recent refinements of this approach include using distributed word representations (Rossiello et al., 2017) or ranking whole summaries instead of individual sentences (Gholipour Ghalandari, 2017). Graph-based methods, such as LexRank (Erkan & Radev, 2004) or TextRank (Mihalcea & Tarau, 2004;Zheng & Lapata, 2019), work by constructing a graph whose nodes are the sentences from the input documents and whose edges indicate a high word overlap between two sentences. Then, they use the PageRank algorithm to extract the sentences with the highest centrality. In contrast to these methods, we focus on abstractive summarization methods.\n\nAbstractive methods for summarization are in principle able to generate new words and sentences that do not occur in the input documents and therefore produce more fluent text. Non-neural abstractive methods (Ganesan et al., 2010;Nayeem et al., 2018) are also graph-based, but construct graphs whose nodes are word types and edges indicate the immediate precedence relationship between two instantiations of the word type in a sentence. The summary is extracted by finding salient paths in the graph.\n\nRecently, a few approaches for neural unsupervised abstractive summarization have been proposed. Chu & Liu (2019, MeanSum) introduced a summarization system based on a review autoencoder. At inference time, MeanSum encodes every review for a product to a vector, computes the centroid of reviews' vectors and uses this centroid to seed the decoder and generate a summary. However, averaging representations of statements that are sometimes contradictory tends to confuse the decoder, and to lead it to rely on only language modeling for generating the output summary, thus ignoring the input signal. To deal with this limitation, Coavoux et al. (2019",
                    "score": 0.8421632895129887,
                    "section_title": "Related Work",
                    "char_start_offset": 3723,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 535,
                            "end": 559,
                            "matchedPaperCorpusId": "2346086"
                        },
                        {
                            "start": 720,
                            "end": 744,
                            "matchedPaperCorpusId": "15641339"
                        },
                        {
                            "start": 744,
                            "end": 765,
                            "matchedPaperCorpusId": "182952892"
                        },
                        {
                            "start": 1319,
                            "end": 1339,
                            "matchedPaperCorpusId": "52011473"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90185546875
                }
            ],
            "relevance_judgement": 0.90185546875,
            "relevance_judgment_input_expanded": "# Title: Self-Supervised and Controlled Multi-Document Opinion Summarization\n# Venue: Conference of the European Chapter of the Association for Computational Linguistics\n# Authors: Hady ElSahar, Maximin Coavoux, Matthias Gall\u00e9, Jos Rozen\n## Abstract\nWe address the problem of unsupervised abstractive summarization of collections of user generated reviews through self-supervision and control. We propose a self-supervised setup that considers an individual document as a target summary for a set of similar documents. This setting makes training simpler than previous approaches by relying only on standard log-likelihood loss and mainstream models. We address the problem of hallucinations through the use of control codes, to steer the generation towards more coherent and relevant summaries.\n## Related Work\nUnsupervised Opinion Summarization Unsupervised Multi-Document summarization methods encompass both extractive and abstractive approaches. Extractive summarization consists in selecting a few sentences from the input documents to form the output summary. Radev et al. (2004) proposed to rank sentences according to their relevance to the whole input, representing sentences as tfidf bags of words and the input as the centroid vector of its sentences. Recent refinements of this approach include using distributed word representations (Rossiello et al., 2017) or ranking whole summaries instead of individual sentences (Gholipour Ghalandari, 2017). Graph-based methods, such as LexRank (Erkan & Radev, 2004) or TextRank (Mihalcea & Tarau, 2004;Zheng & Lapata, 2019), work by constructing a graph whose nodes are the sentences from the input documents and whose edges indicate a high word overlap between two sentences. Then, they use the PageRank algorithm to extract the sentences with the highest centrality. In contrast to these methods, we focus on abstractive summarization methods.\n\nAbstractive methods for summarization are in principle able to generate new words and sentences that do not occur in the input documents and therefore produce more fluent text. Non-neural abstractive methods (Ganesan et al., 2010;Nayeem et al., 2018) are also graph-based, but construct graphs whose nodes are word types and edges indicate the immediate precedence relationship between two instantiations of the word type in a sentence. The summary is extracted by finding salient paths in the graph.\n\nRecently, a few approaches for neural unsupervised abstractive summarization have been proposed. Chu & Liu (2019, MeanSum) introduced a summarization system based on a review autoencoder. At inference time, MeanSum encodes every review for a product to a vector, computes the centroid of reviews' vectors and uses this centroid to seed the decoder and generate a summary. However, averaging representations of statements that are sometimes contradictory tends to confuse the decoder, and to lead it to rely on only language modeling for generating the output summary, thus ignoring the input signal. To deal with this limitation, Coavoux et al. (2019",
            "reference_string": "[216867290 | ElSahar et al. | 2020 | Citations: 48]"
        },
        {
            "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
            "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
            "year": 2025,
            "reference_count": 89,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/taslp.2024.3490375",
                "status": "HYBRID",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASLP.2024.3490375?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASLP.2024.3490375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "134327204",
                    "name": "Luca Ragazzi"
                },
                {
                    "authorId": "2237646991",
                    "name": "Gianluca Moro"
                },
                {
                    "authorId": "2132084411",
                    "name": "Lorenzo Valgimigli"
                },
                {
                    "authorId": "2328891652",
                    "name": "Riccardo Fiorani"
                }
            ],
            "abstract": "Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-Seek-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-Seek-2 across seven datasets spanning various domains\u2014including news articles, lawsuits, government reports, and scientific texts\u2014under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics.",
            "corpus_id": 273784877,
            "sentences": [
                {
                    "corpus_id": "273784877",
                    "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
                    "text": "Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-Seek-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-Seek-2 across seven datasets spanning various domains\u2014including news articles, lawsuits, government reports, and scientific texts\u2014under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics.",
                    "score": 0.6394827631866076,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.900390625
                },
                {
                    "corpus_id": "273784877",
                    "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
                    "text": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge Luca Ragazzi , Gianluca Moro , Lorenzo Valgimigli, and Riccardo Fiorani Abstract-Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-SEEK-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-SEEK-2 across seven datasets spanning various domains-including news articles, lawsuits, government reports, and scientific texts-under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics. \n\nIndex Terms-Multi-document summarization, graphenhanced transformer, few-shot learning.",
                    "score": 0.6801524546308297,
                    "section_title": "body",
                    "char_start_offset": 1,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 517
                        },
                        {
                            "start": 518,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 899
                        },
                        {
                            "start": 900,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1208
                        },
                        {
                            "start": 1209,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1573
                        },
                        {
                            "start": 1574,
                            "end": 1755
                        },
                        {
                            "start": 1758,
                            "end": 1845
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8759765625
                }
            ],
            "relevance_judgement": 0.900390625,
            "relevance_judgment_input_expanded": "# Title: Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge\n# Venue: IEEE Transactions on Audio, Speech, and Language Processing\n# Authors: Luca Ragazzi, Gianluca Moro, Lorenzo Valgimigli, Riccardo Fiorani\n## Abstract\nAbstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-Seek-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-Seek-2 across seven datasets spanning various domains\u2014including news articles, lawsuits, government reports, and scientific texts\u2014under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics.\n## body\nCross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge Luca Ragazzi , Gianluca Moro , Lorenzo Valgimigli, and Riccardo Fiorani Abstract-Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-SEEK-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-SEEK-2 across seven datasets spanning various domains-including news articles, lawsuits, government reports, and scientific texts-under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics. \n\nIndex Terms-Multi-document summarization, graphenhanced transformer, few-shot learning.",
            "reference_string": "[273784877 | Ragazzi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1609/aaai.v39i24.34796",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i24.34796?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i24.34796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116765029",
                    "name": "Zhi Zhang"
                },
                {
                    "authorId": "2290114013",
                    "name": "Yan Liu"
                },
                {
                    "authorId": "2240541436",
                    "name": "Sheng-hua Zhong"
                },
                {
                    "authorId": "2265358574",
                    "name": "Gong Chen"
                },
                {
                    "authorId": "2283200843",
                    "name": "Yu Yang"
                },
                {
                    "authorId": "2312268442",
                    "name": "Jiannong Cao"
                }
            ],
            "abstract": "Literature reviews play a crucial role in scientific research for understanding the current state of research, identifying gaps, and guiding future studies on specific topics. However, the process of conducting a comprehensive literature review is yet time-consuming. This paper proposes a novel framework, collaborative knowledge minigraph agents (CKMAs), to automate scholarly literature reviews. A novel prompt-based algorithm, the knowledge minigraph construction agent (KMCA), is designed to identify relations between concepts from academic literature and automatically constructs knowledge minigraphs. By leveraging the capabilities of large language models on constructed knowledge minigraphs, the multiple path summarization agent (MPSA) efficiently organizes concepts and relations from different viewpoints to generate literature review paragraphs. We evaluate CKMAs on three benchmark datasets. Experimental results show the effectiveness of the proposed method, further revealing promising applications of LLMs in scientific research.",
            "corpus_id": 273963834,
            "sentences": [
                {
                    "corpus_id": "273963834",
                    "title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation",
                    "text": "To generate a summary that is representative of the overall content, graph-based methods construct external graphs to assist document representation and cross-document relation modeling, achieving promising progress. In this regard, LexRank (Erkan and Radev 2004) and TextRank (Mihalcea and Tarau 2004) first introduced graphs to extractive text summarization in 2004. They compute sentence importance using a graph representation of sentences to extract salient textual units from documents as summarization. In 2020, Wang et al. (Wang et al. 2020) propose to extract salient textual units from documents as summarization using a heterogeneous graph consisting of semantic nodes at several granularity levels of documents. In 2022, Wang et al. (Wang et al. 2022) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template to achieve state-of-the-art performance. \n\nHowever, to the best of our knowledge, no existing work integrates LLMs into graph-based methods to leverage their natural language understanding capabilities for improved graph construction and summary generation.",
                    "score": 0.7962415516539114,
                    "section_title": "Graphs in MSDS Tasks",
                    "char_start_offset": 4625,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 723
                        },
                        {
                            "start": 724,
                            "end": 925
                        },
                        {
                            "start": 928,
                            "end": 1142
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 241,
                            "end": 263,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 510,
                            "end": 517,
                            "matchedPaperCorpusId": "216342059"
                        },
                        {
                            "start": 745,
                            "end": 762,
                            "matchedPaperCorpusId": "252185277"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89453125
                }
            ],
            "relevance_judgement": 0.89453125,
            "relevance_judgment_input_expanded": "# Title: Mixture of Knowledge Minigraph Agents for Literature Review Generation\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Zhi Zhang, Yan Liu, Sheng-hua Zhong, Gong Chen, Yu Yang, Jiannong Cao\n## Abstract\nLiterature reviews play a crucial role in scientific research for understanding the current state of research, identifying gaps, and guiding future studies on specific topics. However, the process of conducting a comprehensive literature review is yet time-consuming. This paper proposes a novel framework, collaborative knowledge minigraph agents (CKMAs), to automate scholarly literature reviews. A novel prompt-based algorithm, the knowledge minigraph construction agent (KMCA), is designed to identify relations between concepts from academic literature and automatically constructs knowledge minigraphs. By leveraging the capabilities of large language models on constructed knowledge minigraphs, the multiple path summarization agent (MPSA) efficiently organizes concepts and relations from different viewpoints to generate literature review paragraphs. We evaluate CKMAs on three benchmark datasets. Experimental results show the effectiveness of the proposed method, further revealing promising applications of LLMs in scientific research.\n## Graphs in MSDS Tasks\nTo generate a summary that is representative of the overall content, graph-based methods construct external graphs to assist document representation and cross-document relation modeling, achieving promising progress. In this regard, LexRank (Erkan and Radev 2004) and TextRank (Mihalcea and Tarau 2004) first introduced graphs to extractive text summarization in 2004. They compute sentence importance using a graph representation of sentences to extract salient textual units from documents as summarization. In 2020, Wang et al. (Wang et al. 2020) propose to extract salient textual units from documents as summarization using a heterogeneous graph consisting of semantic nodes at several granularity levels of documents. In 2022, Wang et al. (Wang et al. 2022) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template to achieve state-of-the-art performance. \n\nHowever, to the best of our knowledge, no existing work integrates LLMs into graph-based methods to leverage their natural language understanding capabilities for improved graph construction and summary generation.",
            "reference_string": "[273963834 | Zhang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Topic-Selective Graph Network for Topic-Focused Summarization",
            "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.13106",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.13106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1764406",
                    "name": "Shi Zesheng"
                },
                {
                    "authorId": "2110348767",
                    "name": "Yucheng Zhou"
                }
            ],
            "abstract": "Due to the success of the pre-trained language model (PLM), existing PLM-based summarization models show their powerful generative capability. However, these models are trained on general-purpose summarization datasets, leading to generated summaries failing to satisfy the needs of different readers. To generate summaries with topics, many efforts have been made on topic-focused summarization. However, these works generate a summary only guided by a prompt comprising topic words. Despite their success, these methods still ignore the disturbance of sentences with non-relevant topics and only conduct cross-interaction between tokens by attention module. To address this issue, we propose a topic-arc recognition objective and topic-selective graph network. First, the topic-arc recognition objective is used to model training, which endows the capability to discriminate topics for the model. Moreover, the topic-selective graph network can conduct topic-guided cross-interaction on sentences based on the results of topic-arc recognition. In the experiments, we conduct extensive evaluations on NEWTS and COVIDET datasets. Results show that our methods achieve state-of-the-art performance.",
            "corpus_id": 257219819,
            "sentences": [
                {
                    "corpus_id": "257219819",
                    "title": "Topic-Selective Graph Network for Topic-Focused Summarization",
                    "text": "Graph neural network [20] has been valued in the field of deep learning for its excellent processing ability on unstructured data and node-centric information aggregation mode. With the advance of graph neural networks, there are many graph networks with special structures, e.g., GCN [21], GAT [27], HAN [30] and r-GCN [21]. Moreover, GNN is often used for downstream tasks such as text classification, information extraction, and text generation. In text summarization, Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled.",
                    "score": 0.642988666757816,
                    "section_title": "Graph Neural Network",
                    "char_start_offset": 7591,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 325
                        },
                        {
                            "start": 326,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 880
                        },
                        {
                            "start": 881,
                            "end": 1043
                        },
                        {
                            "start": 1044,
                            "end": 1276
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 21,
                            "end": 25,
                            "matchedPaperCorpusId": "4675456"
                        },
                        {
                            "start": 285,
                            "end": 289,
                            "matchedPaperCorpusId": "5458500"
                        },
                        {
                            "start": 295,
                            "end": 299,
                            "matchedPaperCorpusId": "3292002"
                        },
                        {
                            "start": 305,
                            "end": 309,
                            "matchedPaperCorpusId": "6857205"
                        },
                        {
                            "start": 320,
                            "end": 324,
                            "matchedPaperCorpusId": "5458500"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89453125
                }
            ],
            "relevance_judgement": 0.89453125,
            "relevance_judgment_input_expanded": "# Title: Topic-Selective Graph Network for Topic-Focused Summarization\n# Venue: Pacific-Asia Conference on Knowledge Discovery and Data Mining\n# Authors: Shi Zesheng, Yucheng Zhou\n## Abstract\nDue to the success of the pre-trained language model (PLM), existing PLM-based summarization models show their powerful generative capability. However, these models are trained on general-purpose summarization datasets, leading to generated summaries failing to satisfy the needs of different readers. To generate summaries with topics, many efforts have been made on topic-focused summarization. However, these works generate a summary only guided by a prompt comprising topic words. Despite their success, these methods still ignore the disturbance of sentences with non-relevant topics and only conduct cross-interaction between tokens by attention module. To address this issue, we propose a topic-arc recognition objective and topic-selective graph network. First, the topic-arc recognition objective is used to model training, which endows the capability to discriminate topics for the model. Moreover, the topic-selective graph network can conduct topic-guided cross-interaction on sentences based on the results of topic-arc recognition. In the experiments, we conduct extensive evaluations on NEWTS and COVIDET datasets. Results show that our methods achieve state-of-the-art performance.\n## Graph Neural Network\nGraph neural network [20] has been valued in the field of deep learning for its excellent processing ability on unstructured data and node-centric information aggregation mode. With the advance of graph neural networks, there are many graph networks with special structures, e.g., GCN [21], GAT [27], HAN [30] and r-GCN [21]. Moreover, GNN is often used for downstream tasks such as text classification, information extraction, and text generation. In text summarization, Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled.",
            "reference_string": "[257219819 | Zesheng et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.10416, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2073437",
                    "name": "Pancheng Wang"
                },
                {
                    "authorId": "2145340498",
                    "name": "Shasha Li"
                },
                {
                    "authorId": "2214849368",
                    "name": "Dong Li"
                },
                {
                    "authorId": "2186557380",
                    "name": "Kehan Long"
                },
                {
                    "authorId": "1762106",
                    "name": "Jintao Tang"
                },
                {
                    "authorId": "2257363292",
                    "name": "Ting Wang"
                }
            ],
            "abstract": "Automatically condensing multiple topic-related scientific papers into a succinct and concise summary is referred to as Multi-Document Scientific Summarization (MDSS). Currently, while commonly used abstractive MDSS methods can generate flexible and coherent summaries, the difficulty in handling global information and the lack of guidance during decoding still make it challenging to generate better summaries. To alleviate these two shortcomings, this paper introduces summary candidates into MDSS, utilizing the global information of the document set and additional guidance from the summary candidates to guide the decoding process. Our insights are twofold: Firstly, summary candidates can provide instructive information from both positive and negative perspectives, and secondly, selecting higher-quality candidates from multiple options contributes to producing better summaries. Drawing on the insights, we propose a summary candidates fusion framework - Disentangling Instructive information from Ranked candidates (DIR) for MDSS. Specifically, DIR first uses a specialized pairwise comparison method towards multiple candidates to pick out those of higher quality. Then DIR disentangles the instructive information of summary candidates into positive and negative latent variables with Conditional Variational Autoencoder. These variables are further incorporated into the decoder to guide generation. We evaluate our approach with three different types of Transformer-based models and three different types of candidates, and consistently observe noticeable performance improvements according to automatic and human evaluation. More analyses further demonstrate the effectiveness of our model in handling global information and enhancing decoding controllability.",
            "corpus_id": 269157041,
            "sentences": [
                {
                    "corpus_id": "269157041",
                    "title": "Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization",
                    "text": "Multi-Document Scientific Summarization (MDSS) involves consolidating scattered information from multiple papers.According to the paradigm adopted, previous studies can be categorized into graph-based [5,36], flat-based [23,31], and hierarchical-based methods [6,30].Concretely, graph-based methods construct external graphs (e.g., entity-relation graph, AMR, TF-IDF) to assist document representation and cross-document relation modeling.In this regard, Chen et al. [5] leverage graphs to encode source articles, constructing keyphrase graphs and using a multi-level contrastive learning strategy to improve the accuracy of generated summaries.\n\nWang et al. [36] incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template.Flat-based methods simply concatenate multiple documents, thus they can leverage state-of-the-art pre-trained summarization models.Moro et al. [23] select top  documents with a dense retriever and use a BART model [13] to produce the summary.Shi et al. [31] leverage Large Language Model (LLM) to expand the query for reference retrieval and adopt an instruction-tuning method to guide LLM to generate summary.Compared with flatbased methods, hierarchical-based methods take the hierarchical structure of document clusters into account, thus preserving crossdocument relations and obtaining semantic-rich representations.Chen et al. [6] apply word representations and cross-document relationships to model document-level representations, and design a hierarchical decoding strategy.Shen et al. [30] modify the BART model by incorporating document-level representations in the encoder and decoder to facilitate multi-document interactions.However, we argue that all the above works fail to explore external guidance information for better global semantic control and summary decoding.",
                    "score": 0.6370807583492244,
                    "section_title": "RELATED WORK 2.1 Multi-Document Scientific Summarization",
                    "char_start_offset": 6345,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 113,
                            "end": 267
                        },
                        {
                            "start": 267,
                            "end": 439
                        },
                        {
                            "start": 439,
                            "end": 645
                        },
                        {
                            "start": 647,
                            "end": 785
                        },
                        {
                            "start": 785,
                            "end": 916
                        },
                        {
                            "start": 916,
                            "end": 1027
                        },
                        {
                            "start": 1027,
                            "end": 1195
                        },
                        {
                            "start": 1195,
                            "end": 1406
                        },
                        {
                            "start": 1406,
                            "end": 1567
                        },
                        {
                            "start": 1567,
                            "end": 1723
                        },
                        {
                            "start": 1723,
                            "end": 1868
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 201,
                            "end": 204,
                            "matchedPaperCorpusId": "249097545"
                        },
                        {
                            "start": 204,
                            "end": 207,
                            "matchedPaperCorpusId": "252185277"
                        },
                        {
                            "start": 220,
                            "end": 224,
                            "matchedPaperCorpusId": "248780330"
                        },
                        {
                            "start": 224,
                            "end": 227,
                            "matchedPaperCorpusId": "266176619"
                        },
                        {
                            "start": 260,
                            "end": 263,
                            "matchedPaperCorpusId": "236460044"
                        },
                        {
                            "start": 467,
                            "end": 470,
                            "matchedPaperCorpusId": "249097545"
                        },
                        {
                            "start": 659,
                            "end": 663,
                            "matchedPaperCorpusId": "252185277"
                        },
                        {
                            "start": 928,
                            "end": 932,
                            "matchedPaperCorpusId": "248780330"
                        },
                        {
                            "start": 999,
                            "end": 1003,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 1038,
                            "end": 1042,
                            "matchedPaperCorpusId": "266176619"
                        },
                        {
                            "start": 1418,
                            "end": 1421,
                            "matchedPaperCorpusId": "236460044"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8935546875
                }
            ],
            "relevance_judgement": 0.8935546875,
            "relevance_judgment_input_expanded": "# Title: Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Pancheng Wang, Shasha Li, Dong Li, Kehan Long, Jintao Tang, Ting Wang\n## Abstract\nAutomatically condensing multiple topic-related scientific papers into a succinct and concise summary is referred to as Multi-Document Scientific Summarization (MDSS). Currently, while commonly used abstractive MDSS methods can generate flexible and coherent summaries, the difficulty in handling global information and the lack of guidance during decoding still make it challenging to generate better summaries. To alleviate these two shortcomings, this paper introduces summary candidates into MDSS, utilizing the global information of the document set and additional guidance from the summary candidates to guide the decoding process. Our insights are twofold: Firstly, summary candidates can provide instructive information from both positive and negative perspectives, and secondly, selecting higher-quality candidates from multiple options contributes to producing better summaries. Drawing on the insights, we propose a summary candidates fusion framework - Disentangling Instructive information from Ranked candidates (DIR) for MDSS. Specifically, DIR first uses a specialized pairwise comparison method towards multiple candidates to pick out those of higher quality. Then DIR disentangles the instructive information of summary candidates into positive and negative latent variables with Conditional Variational Autoencoder. These variables are further incorporated into the decoder to guide generation. We evaluate our approach with three different types of Transformer-based models and three different types of candidates, and consistently observe noticeable performance improvements according to automatic and human evaluation. More analyses further demonstrate the effectiveness of our model in handling global information and enhancing decoding controllability.\n## RELATED WORK 2.1 Multi-Document Scientific Summarization\nMulti-Document Scientific Summarization (MDSS) involves consolidating scattered information from multiple papers.According to the paradigm adopted, previous studies can be categorized into graph-based [5,36], flat-based [23,31], and hierarchical-based methods [6,30].Concretely, graph-based methods construct external graphs (e.g., entity-relation graph, AMR, TF-IDF) to assist document representation and cross-document relation modeling.In this regard, Chen et al. [5] leverage graphs to encode source articles, constructing keyphrase graphs and using a multi-level contrastive learning strategy to improve the accuracy of generated summaries.\n\nWang et al. [36] incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template.Flat-based methods simply concatenate multiple documents, thus they can leverage state-of-the-art pre-trained summarization models.Moro et al. [23] select top  documents with a dense retriever and use a BART model [13] to produce the summary.Shi et al. [31] leverage Large Language Model (LLM) to expand the query for reference retrieval and adopt an instruction-tuning method to guide LLM to generate summary.Compared with flatbased methods, hierarchical-based methods take the hierarchical structure of document clusters into account, thus preserving crossdocument relations and obtaining semantic-rich representations.Chen et al. [6] apply word representations and cross-document relationships to model document-level representations, and design a hierarchical decoding strategy.Shen et al. [30] modify the BART model by incorporating document-level representations in the encoder and decoder to facilitate multi-document interactions.However, we argue that all the above works fail to explore external guidance information for better global semantic control and summary decoding.",
            "reference_string": "[269157041 | Wang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Extract with Order for Coherent Multi-Document Summarization",
            "venue": "TextGraphs@ACL",
            "year": 2017,
            "reference_count": 23,
            "citation_count": 39,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/W17-2407.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.06542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1807355",
                    "name": "Mir Tafseer Nayeem"
                },
                {
                    "authorId": "1735475",
                    "name": "Yllias Chali"
                }
            ],
            "abstract": "In this work, we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases. Furthermore, we propose a model to tackle summary coherence for increasing readability. We conduct experiments on the Document Understanding Conference (DUC) 2004 datasets using ROUGE toolkit. Our experiments demonstrate that the methods bring significant improvements over the state of the art methods in terms of informativity and coherence.",
            "corpus_id": 3230186,
            "sentences": [
                {
                    "corpus_id": "3230186",
                    "title": "Extract with Order for Coherent Multi-Document Summarization",
                    "text": "During a decade, several extractive approaches have been developed for automatic summary generation that implement a number of machine learning, graph-based and optimization techniques. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods of computing sentence importance for text summarization. The RegSum system (Hong and Nenkova, 2014) employs a supervised model for predicting word importance. Treating multidocument summarization as a submodular maximization problem has proven successful by (Lin and Bilmes, 2011). Unfortunately, none of the above systems care about the coherence of the final extracted summary. \n\nIn very recent works using neural network, (Cheng and Lapata, 2016) proposed an attentional encoder-decoder and (Nallapati et al., 2017) used a simple recurrent network based sequence classifier to solve the problem of extractive summarization. However, they are limited to single document settings, where sentences are implicitly ordered according to the sentence position. (Parveen and Strube, 2015;Parveen et al., 2015) arXiv:1706.06542v1 [cs.CL] 12 Jun 2017 proposed graph-based techniques to tackle coherence, which is also limited to single document summarization. Moreover, a recent work (Wang et al., 2016) actually proposed a multi-document summarization system that combines both coherence and informativeness but this system is limited to syntactic linkages between entities. \n\nIn this paper, we implement a rank based sentence selection using continuous vector representations along with key-phrases. We also model the coherence using semantic relations between entities and sentences to increase the readability.",
                    "score": 0.7478110419431592,
                    "section_title": "Related Work",
                    "char_start_offset": 1654,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 563
                        },
                        {
                            "start": 564,
                            "end": 661
                        },
                        {
                            "start": 664,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1038
                        },
                        {
                            "start": 1039,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1450
                        },
                        {
                            "start": 1453,
                            "end": 1576
                        },
                        {
                            "start": 1577,
                            "end": 1689
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 194,
                            "end": 217,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 540,
                            "end": 562,
                            "matchedPaperCorpusId": "320371"
                        },
                        {
                            "start": 707,
                            "end": 731,
                            "matchedPaperCorpusId": "1499080"
                        },
                        {
                            "start": 776,
                            "end": 800,
                            "matchedPaperCorpusId": "6405271"
                        },
                        {
                            "start": 1259,
                            "end": 1278,
                            "matchedPaperCorpusId": "2867700"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88916015625
                }
            ],
            "relevance_judgement": 0.88916015625,
            "relevance_judgment_input_expanded": "# Title: Extract with Order for Coherent Multi-Document Summarization\n# Venue: TextGraphs@ACL\n# Authors: Mir Tafseer Nayeem, Yllias Chali\n## Abstract\nIn this work, we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases. Furthermore, we propose a model to tackle summary coherence for increasing readability. We conduct experiments on the Document Understanding Conference (DUC) 2004 datasets using ROUGE toolkit. Our experiments demonstrate that the methods bring significant improvements over the state of the art methods in terms of informativity and coherence.\n## Related Work\nDuring a decade, several extractive approaches have been developed for automatic summary generation that implement a number of machine learning, graph-based and optimization techniques. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods of computing sentence importance for text summarization. The RegSum system (Hong and Nenkova, 2014) employs a supervised model for predicting word importance. Treating multidocument summarization as a submodular maximization problem has proven successful by (Lin and Bilmes, 2011). Unfortunately, none of the above systems care about the coherence of the final extracted summary. \n\nIn very recent works using neural network, (Cheng and Lapata, 2016) proposed an attentional encoder-decoder and (Nallapati et al., 2017) used a simple recurrent network based sequence classifier to solve the problem of extractive summarization. However, they are limited to single document settings, where sentences are implicitly ordered according to the sentence position. (Parveen and Strube, 2015;Parveen et al., 2015) arXiv:1706.06542v1 [cs.CL] 12 Jun 2017 proposed graph-based techniques to tackle coherence, which is also limited to single document summarization. Moreover, a recent work (Wang et al., 2016) actually proposed a multi-document summarization system that combines both coherence and informativeness but this system is limited to syntactic linkages between entities. \n\nIn this paper, we implement a rank based sentence selection using continuous vector representations along with key-phrases. We also model the coherence using semantic relations between entities and sentences to increase the readability.",
            "reference_string": "[3230186 | Nayeem et al. | 2017 | Citations: 39]"
        },
        {
            "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022,
            "reference_count": 43,
            "citation_count": 29,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3477495.3532065",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.13339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46772896",
                    "name": "Xiuying Chen"
                },
                {
                    "authorId": "1753416672",
                    "name": "Hind Alamro"
                },
                {
                    "authorId": "9255404",
                    "name": "Li Mingzhe"
                },
                {
                    "authorId": "2112311595",
                    "name": "Shen Gao"
                },
                {
                    "authorId": "2055864368",
                    "name": "Rui Yan"
                },
                {
                    "authorId": "2118502950",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2928371",
                    "name": "Xiangliang Zhang"
                }
            ],
            "abstract": "The related work section is an important component of a scientific paper, which highlights the contribution of the target paper in the context of the reference papers. Authors can save their time and effort by using the automatically generated related work section as a draft to complete the final related work. Most of the existing related work section generation methods rely on extracting off-the-shelf sentences to make a comparative discussion about the target work and the reference papers. However, such sentences need to be written in advance and are hard to obtain in practice. Hence, in this paper, we propose an abstractive target-aware related work generator (TAG), which can generate related work sections consisting of new sentences. Concretely, we first propose a target-aware graph encoder, which models the relationships between reference papers and the target paper with target-centered attention mechanisms. In the decoding process, we propose a hierarchical decoder that attends to the nodes of different levels in the graph with keyphrases as semantic indicators. Finally, to generate a more informative related work, we propose multi-level contrastive optimization objectives, which aim to maximize the mutual information between the generated related work with the references and minimize that with non-references. Extensive experiments on two public scholar datasets show that the proposed model brings substantial improvements over several strong baselines in terms of automatic and tailored human evaluations.",
            "corpus_id": 249097545,
            "sentences": [
                {
                    "corpus_id": "249097545",
                    "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning",
                    "text": "We evaluate our models against popular and state-of-the-art multidocument summarization baselines and related work generation techniques. We adopt the LEAD baseline which selects the first sentence of each reference as the summary as a baseline, and ORACLE as an upper bound of extractive summarization systems. \n\nOur extractive baselines include: LexRank [11]: a graph-based extractive model that computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. \n\nNES [37]: a neural data-driven summarizer that extracts sentences from reference papers based on target paper. A joint contextdriven attention mechanism is proposed to measure the contextual relevance within full texts. \n\nBertSumEXT [25]: an extractive summarization model with BERT encoder, which is able to express the semantics of a document and obtain representations for its sentences. \n\nAbstractive baseline models include: BertSumABS [25]: an abstractive summarization system built on BERT with a new fine-tuning schedule that adopts different optimizers for the encoder and the decoder. \n\nMGSum [20]: a multi-granularity interaction network for abstractive multi-document summarization. The multi granularities include document-level, sentence-level, and word-level. \n\nEMS [42]: an entity-aware model for abstractive multi-document summarization with BERT encoder. It augments the classical Transformer based encoder-decoder framework with a graph consisting of paragraph nodes and entity nodes. EMS uses the same keyphrase extraction tool as ours. \n\nRRG [5]: an abstractive related work generator based on reference papers. It augments the RNN encoder with a relation-aware graph between multiple references. The relation graph and the document representation are refined iteratively. \n\nThe essential difference between our model and baselines is that we explicitly model the relationship between the target work and the references. To make a fair comparison, for baselines that have relatively good performance, we give an additional with.Target (w.T ) version which takes references and target paper as input. Thus, the baseline models treat the references and the target paper as multiple inputs without distinguishing them.",
                    "score": 0.6943763815392784,
                    "section_title": "Baselines",
                    "char_start_offset": 23597,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 311
                        },
                        {
                            "start": 314,
                            "end": 513
                        },
                        {
                            "start": 516,
                            "end": 626
                        },
                        {
                            "start": 627,
                            "end": 735
                        },
                        {
                            "start": 738,
                            "end": 906
                        },
                        {
                            "start": 909,
                            "end": 1110
                        },
                        {
                            "start": 1113,
                            "end": 1210
                        },
                        {
                            "start": 1211,
                            "end": 1290
                        },
                        {
                            "start": 1293,
                            "end": 1388
                        },
                        {
                            "start": 1389,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1572
                        },
                        {
                            "start": 1575,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1733
                        },
                        {
                            "start": 1734,
                            "end": 1809
                        },
                        {
                            "start": 1812,
                            "end": 1957
                        },
                        {
                            "start": 1958,
                            "end": 2136
                        },
                        {
                            "start": 2137,
                            "end": 2252
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 356,
                            "end": 360,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 520,
                            "end": 524,
                            "matchedPaperCorpusId": "53083244"
                        },
                        {
                            "start": 749,
                            "end": 753,
                            "matchedPaperCorpusId": "201304248"
                        },
                        {
                            "start": 957,
                            "end": 961,
                            "matchedPaperCorpusId": "201304248"
                        },
                        {
                            "start": 1119,
                            "end": 1123,
                            "matchedPaperCorpusId": "220045815"
                        },
                        {
                            "start": 1297,
                            "end": 1301,
                            "matchedPaperCorpusId": "236478143"
                        },
                        {
                            "start": 1579,
                            "end": 1582,
                            "matchedPaperCorpusId": "236460044"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88720703125
                }
            ],
            "relevance_judgement": 0.88720703125,
            "relevance_judgment_input_expanded": "# Title: Target-aware Abstractive Related Work Generation with Contrastive Learning\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Xiuying Chen, Hind Alamro, Li Mingzhe, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang\n## Abstract\nThe related work section is an important component of a scientific paper, which highlights the contribution of the target paper in the context of the reference papers. Authors can save their time and effort by using the automatically generated related work section as a draft to complete the final related work. Most of the existing related work section generation methods rely on extracting off-the-shelf sentences to make a comparative discussion about the target work and the reference papers. However, such sentences need to be written in advance and are hard to obtain in practice. Hence, in this paper, we propose an abstractive target-aware related work generator (TAG), which can generate related work sections consisting of new sentences. Concretely, we first propose a target-aware graph encoder, which models the relationships between reference papers and the target paper with target-centered attention mechanisms. In the decoding process, we propose a hierarchical decoder that attends to the nodes of different levels in the graph with keyphrases as semantic indicators. Finally, to generate a more informative related work, we propose multi-level contrastive optimization objectives, which aim to maximize the mutual information between the generated related work with the references and minimize that with non-references. Extensive experiments on two public scholar datasets show that the proposed model brings substantial improvements over several strong baselines in terms of automatic and tailored human evaluations.\n## Baselines\nWe evaluate our models against popular and state-of-the-art multidocument summarization baselines and related work generation techniques. We adopt the LEAD baseline which selects the first sentence of each reference as the summary as a baseline, and ORACLE as an upper bound of extractive summarization systems. \n\nOur extractive baselines include: LexRank [11]: a graph-based extractive model that computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. \n\nNES [37]: a neural data-driven summarizer that extracts sentences from reference papers based on target paper. A joint contextdriven attention mechanism is proposed to measure the contextual relevance within full texts. \n\nBertSumEXT [25]: an extractive summarization model with BERT encoder, which is able to express the semantics of a document and obtain representations for its sentences. \n\nAbstractive baseline models include: BertSumABS [25]: an abstractive summarization system built on BERT with a new fine-tuning schedule that adopts different optimizers for the encoder and the decoder. \n\nMGSum [20]: a multi-granularity interaction network for abstractive multi-document summarization. The multi granularities include document-level, sentence-level, and word-level. \n\nEMS [42]: an entity-aware model for abstractive multi-document summarization with BERT encoder. It augments the classical Transformer based encoder-decoder framework with a graph consisting of paragraph nodes and entity nodes. EMS uses the same keyphrase extraction tool as ours. \n\nRRG [5]: an abstractive related work generator based on reference papers. It augments the RNN encoder with a relation-aware graph between multiple references. The relation graph and the document representation are refined iteratively. \n\nThe essential difference between our model and baselines is that we explicitly model the relationship between the target work and the references. To make a fair comparison, for baselines that have relatively good performance, we give an additional with.Target (w.T ) version which takes references and target paper as input. Thus, the baseline models treat the references and the target paper as multiple inputs without distinguishing them.",
            "reference_string": "[249097545 | Chen et al. | 2022 | Citations: 29]"
        },
        {
            "title": "Automatic Text Summarization by Providing Coverage, Non-Redundancy, and Novelty Using Sentence Graph",
            "venue": "Journal of Information Technology Research",
            "year": 2022,
            "reference_count": 48,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.igi-global.com/ViewTitle.aspx?TitleId=282713&isxn=9781683180340",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4018/jitr.2022010108?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4018/jitr.2022010108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2038115655",
                    "name": "P. Krishnaveni"
                },
                {
                    "authorId": "2286571202",
                    "name": "S. R. Balasundaram"
                }
            ],
            "abstract": "The day-to-day growth of online information necessitates intensive research in automatic text summarization (ATS). The ATS software produces summary text by extracting important information from the original text. With the help of summaries, users can easily read and understand the documents of interest. Most of the approaches for ATS used only local properties of text. Moreover, the numerous properties make the sentence selection difficult and complicated. So this article uses a graph based summarization to utilize structural and global properties of text. It introduces maximal clique based sentence selection (MCBSS) algorithm to select important and non-redundant sentences that cover all concepts of the input text for summary. The MCBSS algorithm finds novel information using maximal cliques (MCs). The experimental results of recall oriented understudy for gisting evaluation (ROUGE) on Timeline dataset show that the proposed work outperforms the existing graph algorithms Bushy Path (BP), Aggregate Similarity (AS), and TextRank (TR).",
            "corpus_id": 240268841,
            "sentences": [
                {
                    "corpus_id": "240268841",
                    "title": "Automatic Text Summarization by Providing Coverage, Non-Redundancy, and Novelty Using Sentence Graph",
                    "text": "In a graph based summarization model, a node can be scored using information from the global graph. First, Mani and Bloedorn (1997) proposed graph representation of text. They described a new mechanism for summarizing the similarities and differences between a pair of related documents. Another work used the knowledge of text structure for producing summaries by automatic passage extraction (Salton, Singhal, Mitra, & Buckley, 1997). The earlier iterative graph algorithms are TextRank (Mihalcea & Tarau, 2004) and LexRank (Erkan & Radev, 2004). They can be applied to the summarization of a single or multiple documents in any language (Mihalcea & Tarau, 2005). Even though they are the best graph ranking algorithms, they have high time complexity. Some recent graph based ranking research works are Calvo et al. (2018), Feiyue andXinchen (2018), andTixier et al. (2017). \n\nThe work of Sornil and Gree-ut (2006) constructs an undirected document graph from cosine similarity using the Hopfield Network algorithm for text segment ranking. One research work produces automatic summaries using graph algorithms and the shortest path algorithm and compares them (Khushboo, Dharaskar, & Chandak, 2010). It concludes that the shortest path algorithm is the best one, because it generates a smooth summary. The work of Chen and Zhao (2014) constructs a two layer, phrase-sentence graph. It utilizes sentence relevance and phrase relevance information to produce a summary. The work of Zheng and Bai (2014) considers text summarization as a problem of finding the key paths composed of essential information and merges the common paths to remove redundancy. Han et al. (2016) propose the FrameNet-based semantic graph model. It uses FrameNet to calculate sentence similarity and assigns weights to both sentences and edges. After ranking, it selects summary sentences. Oliveira et al. (2016) present a comparative analysis of eighteen sentence scoring features. They compute the importance of a sentence in extractive single and multi-document summarizations.",
                    "score": 0.6551313737257639,
                    "section_title": "BACKGRoUNd",
                    "char_start_offset": 3190,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 99
                        },
                        {
                            "start": 100,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 548
                        },
                        {
                            "start": 549,
                            "end": 665
                        },
                        {
                            "start": 666,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 876
                        },
                        {
                            "start": 879,
                            "end": 1042
                        },
                        {
                            "start": 1043,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1304
                        },
                        {
                            "start": 1305,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1470
                        },
                        {
                            "start": 1471,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1721
                        },
                        {
                            "start": 1722,
                            "end": 1820
                        },
                        {
                            "start": 1821,
                            "end": 1865
                        },
                        {
                            "start": 1866,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2056
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 394,
                            "end": 435,
                            "matchedPaperCorpusId": "12250431"
                        },
                        {
                            "start": 526,
                            "end": 547,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 640,
                            "end": 664,
                            "matchedPaperCorpusId": "8878897"
                        },
                        {
                            "start": 805,
                            "end": 824,
                            "matchedPaperCorpusId": "44132297"
                        },
                        {
                            "start": 826,
                            "end": 836,
                            "matchedPaperCorpusId": "125164049"
                        },
                        {
                            "start": 836,
                            "end": 855,
                            "matchedPaperCorpusId": "125164049"
                        },
                        {
                            "start": 855,
                            "end": 875,
                            "matchedPaperCorpusId": "42957184"
                        },
                        {
                            "start": 1163,
                            "end": 1201,
                            "matchedPaperCorpusId": "21346422"
                        },
                        {
                            "start": 1483,
                            "end": 1503,
                            "matchedPaperCorpusId": "35197081"
                        },
                        {
                            "start": 1866,
                            "end": 1888,
                            "matchedPaperCorpusId": "36306586"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88720703125
                }
            ],
            "relevance_judgement": 0.88720703125,
            "relevance_judgment_input_expanded": "# Title: Automatic Text Summarization by Providing Coverage, Non-Redundancy, and Novelty Using Sentence Graph\n# Venue: Journal of Information Technology Research\n# Authors: P. Krishnaveni, S. R. Balasundaram\n## Abstract\nThe day-to-day growth of online information necessitates intensive research in automatic text summarization (ATS). The ATS software produces summary text by extracting important information from the original text. With the help of summaries, users can easily read and understand the documents of interest. Most of the approaches for ATS used only local properties of text. Moreover, the numerous properties make the sentence selection difficult and complicated. So this article uses a graph based summarization to utilize structural and global properties of text. It introduces maximal clique based sentence selection (MCBSS) algorithm to select important and non-redundant sentences that cover all concepts of the input text for summary. The MCBSS algorithm finds novel information using maximal cliques (MCs). The experimental results of recall oriented understudy for gisting evaluation (ROUGE) on Timeline dataset show that the proposed work outperforms the existing graph algorithms Bushy Path (BP), Aggregate Similarity (AS), and TextRank (TR).\n## BACKGRoUNd\nIn a graph based summarization model, a node can be scored using information from the global graph. First, Mani and Bloedorn (1997) proposed graph representation of text. They described a new mechanism for summarizing the similarities and differences between a pair of related documents. Another work used the knowledge of text structure for producing summaries by automatic passage extraction (Salton, Singhal, Mitra, & Buckley, 1997). The earlier iterative graph algorithms are TextRank (Mihalcea & Tarau, 2004) and LexRank (Erkan & Radev, 2004). They can be applied to the summarization of a single or multiple documents in any language (Mihalcea & Tarau, 2005). Even though they are the best graph ranking algorithms, they have high time complexity. Some recent graph based ranking research works are Calvo et al. (2018), Feiyue andXinchen (2018), andTixier et al. (2017). \n\nThe work of Sornil and Gree-ut (2006) constructs an undirected document graph from cosine similarity using the Hopfield Network algorithm for text segment ranking. One research work produces automatic summaries using graph algorithms and the shortest path algorithm and compares them (Khushboo, Dharaskar, & Chandak, 2010). It concludes that the shortest path algorithm is the best one, because it generates a smooth summary. The work of Chen and Zhao (2014) constructs a two layer, phrase-sentence graph. It utilizes sentence relevance and phrase relevance information to produce a summary. The work of Zheng and Bai (2014) considers text summarization as a problem of finding the key paths composed of essential information and merges the common paths to remove redundancy. Han et al. (2016) propose the FrameNet-based semantic graph model. It uses FrameNet to calculate sentence similarity and assigns weights to both sentences and edges. After ranking, it selects summary sentences. Oliveira et al. (2016) present a comparative analysis of eighteen sentence scoring features. They compute the importance of a sentence in extractive single and multi-document summarizations.",
            "reference_string": "[240268841 | Krishnaveni et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
            "venue": "International Conference on Computational Linguistics",
            "year": 2022,
            "reference_count": 34,
            "citation_count": 15,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2209.04319",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.04319, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2073437",
                    "name": "Pancheng Wang"
                },
                {
                    "authorId": "2145340498",
                    "name": "Shasha Li"
                },
                {
                    "authorId": "2116489",
                    "name": "Kunyuan Pang"
                },
                {
                    "authorId": "50670961",
                    "name": "Liangliang He"
                },
                {
                    "authorId": "2108821455",
                    "name": "Dong Li"
                },
                {
                    "authorId": "1762106",
                    "name": "Jintao Tang"
                },
                {
                    "authorId": "38972135",
                    "name": "Ting Wang"
                }
            ],
            "abstract": "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.",
            "corpus_id": 252185277,
            "sentences": [
                {
                    "corpus_id": "252185277",
                    "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
                    "text": "In this work, we propose a knowledge graph-centric Transformer-based model for MDSS. Our model is able to incorporate knowledge graph information into the paper encoding process with a graph updater and an entity-sentence updater, and introduce a two-stage decoder including a KGtext generator and a summary generator to guide the summary decoding with knowledge graph information. Experiments show that the proposed model significantly outperforms all strong baselines and achieves the best result on the Multi-Xscience dataset. \n\nIn the future, we will explore other more intuitive and effective methods to incorporate graph information in both the encoding and decoding phase of summary generation. 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles.",
                    "score": 0.6264271071933782,
                    "section_title": "Conclusion",
                    "char_start_offset": 25793,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 529
                        },
                        {
                            "start": 532,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 1115
                        },
                        {
                            "start": 1116,
                            "end": 1229
                        },
                        {
                            "start": 1230,
                            "end": 1352
                        },
                        {
                            "start": 1353,
                            "end": 1493
                        },
                        {
                            "start": 1494,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1691
                        },
                        {
                            "start": 1692,
                            "end": 1874
                        },
                        {
                            "start": 1875,
                            "end": 1997
                        },
                        {
                            "start": 1998,
                            "end": 2103
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 867,
                            "end": 885,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 1122,
                            "end": 1140,
                            "matchedPaperCorpusId": "220045815"
                        },
                        {
                            "start": 1362,
                            "end": 1378,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1542,
                            "end": 1562,
                            "matchedPaperCorpusId": "209405420"
                        },
                        {
                            "start": 1710,
                            "end": 1728,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 1930,
                            "end": 1951,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88427734375
                }
            ],
            "relevance_judgement": 0.88427734375,
            "relevance_judgment_input_expanded": "# Title: Multi-Document Scientific Summarization from a Knowledge Graph-Centric View\n# Venue: International Conference on Computational Linguistics\n# Authors: Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, Ting Wang\n## Abstract\nMulti-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.\n## Conclusion\nIn this work, we propose a knowledge graph-centric Transformer-based model for MDSS. Our model is able to incorporate knowledge graph information into the paper encoding process with a graph updater and an entity-sentence updater, and introduce a two-stage decoder including a KGtext generator and a summary generator to guide the summary decoding with knowledge graph information. Experiments show that the proposed model significantly outperforms all strong baselines and achieves the best result on the Multi-Xscience dataset. \n\nIn the future, we will explore other more intuitive and effective methods to incorporate graph information in both the encoding and decoding phase of summary generation. 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles.",
            "reference_string": "[252185277 | Wang et al. | 2022 | Citations: 15]"
        },
        {
            "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
            "venue": "Discret. Math. Algorithms Appl.",
            "year": 2023,
            "reference_count": 205,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.09411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258949345",
                    "name": "Guanghua Wang"
                },
                {
                    "authorId": "2258731583",
                    "name": "Weili Wu"
                }
            ],
            "abstract": "In recent years, deep learning has revolutionized natural language processing (NLP) by enabling the development of models that can learn complex representations of language data, leading to significant improvements in performance across a wide range of NLP tasks. Deep learning models for NLP typically use large amounts of data to train deep neural networks, allowing them to learn the patterns and relationships in language data. This is in contrast to traditional NLP approaches, which rely on hand-engineered features and rules to perform NLP tasks. The ability of deep neural networks to learn hierarchical representations of language data, handle variable-length input sequences, and perform well on large datasets makes them well-suited for NLP applications. Driven by the exponential growth of textual data and the increasing demand for condensed, coherent, and informative summaries, text summarization has been a critical research area in the field of NLP. Applying deep learning to text summarization refers to the use of deep neural networks to perform text summarization tasks. In this survey, we begin with a review of fashionable text summarization tasks in recent years, including extractive, abstractive, multi-document, and so on. Next, we discuss most deep learning-based models and their experimental results on these tasks. The paper also covers datasets and data representation for summarization tasks. Finally, we delve into the opportunities and challenges associated with summarization tasks and their corresponding methodologies, aiming to inspire future research efforts to advance the field further. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific setting.",
            "corpus_id": 264146402,
            "sentences": [
                {
                    "corpus_id": "264146402",
                    "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
                    "text": "Multi-document Single-document summarization and multi-document summarization are two distinct tasks within text summarization domain based on source document quantity. Single-document summarization focuses on generating a summary from a single input document, while multi-document summarization aims to create a summary by aggregating information from multiple related documents [41,111,149]. Within the single-document system, the objective is to condense the main ideas and essential information contained in that specific document. On the other hand, multi-document summarization tasks require identifying and combining the most relevant and nonredundant information from a set of documents, often covering the same topic or event. This means multi-document summarization has additional challenges, such as maintaining cross-document coherence, efficiently handling larger volumes of information, and processing redundancy across documents. These complexities make multi-document summarization generally more difficult than single-document summarization. \n\nTraditional approaches usually employ extractive techniques both in the context of single-document and multi-document summarization. Graph-based methods can be applied to both single-document and multi-document summarization by representing the relationships between sentences in one document or several documents as a graph, with sentences as nodes and the edges as the similarity between the sentences. The systems [46,96,141,171] then use algorithms like PageRank, HITS, or LexRank to identify the most important sentences in the graph, which are then extracted and combined to form the summary. Fig. 2. The weighted cosine similarity graph [46] was generated for the cluster, based on the subset of d1003t from DUC 2004, which is a dataset for multi-document summarization tasks. The notation used in the figure is as follows: 'd' represents document and 's' represents sentence. For instance, d2s3 denotes the third sentence of document 2. \n\nFor single-document summarization, position-based methods [44,82,95] exploit the position of sentences within the document to identify important content. Additionally, the TF-IDF approach [60,115,156] weighs the importance of words in main Surveying Text Summarization with Deep Learning 7 the document based on their frequency and rarity.",
                    "score": 0.655251715896275,
                    "section_title": "Source Document Quantity: Single-document vs",
                    "char_start_offset": 10281,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 393
                        },
                        {
                            "start": 394,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 944
                        },
                        {
                            "start": 945,
                            "end": 1058
                        },
                        {
                            "start": 1061,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1659
                        },
                        {
                            "start": 1660,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 1944
                        },
                        {
                            "start": 1945,
                            "end": 2005
                        },
                        {
                            "start": 2008,
                            "end": 2161
                        },
                        {
                            "start": 2162,
                            "end": 2347
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 380,
                            "end": 384,
                            "matchedPaperCorpusId": "243638031"
                        },
                        {
                            "start": 1478,
                            "end": 1482,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1482,
                            "end": 1485,
                            "matchedPaperCorpusId": "8014853"
                        },
                        {
                            "start": 1485,
                            "end": 1489,
                            "matchedPaperCorpusId": "269225"
                        },
                        {
                            "start": 1489,
                            "end": 1493,
                            "matchedPaperCorpusId": "21346422"
                        },
                        {
                            "start": 1705,
                            "end": 1709,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 2066,
                            "end": 2070,
                            "matchedPaperCorpusId": "1177942"
                        },
                        {
                            "start": 2070,
                            "end": 2073,
                            "matchedPaperCorpusId": "5775833"
                        },
                        {
                            "start": 2073,
                            "end": 2076,
                            "matchedPaperCorpusId": "40166767"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88330078125
                }
            ],
            "relevance_judgement": 0.88330078125,
            "relevance_judgment_input_expanded": "# Title: Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review\n# Venue: Discret. Math. Algorithms Appl.\n# Authors: Guanghua Wang, Weili Wu\n## Abstract\nIn recent years, deep learning has revolutionized natural language processing (NLP) by enabling the development of models that can learn complex representations of language data, leading to significant improvements in performance across a wide range of NLP tasks. Deep learning models for NLP typically use large amounts of data to train deep neural networks, allowing them to learn the patterns and relationships in language data. This is in contrast to traditional NLP approaches, which rely on hand-engineered features and rules to perform NLP tasks. The ability of deep neural networks to learn hierarchical representations of language data, handle variable-length input sequences, and perform well on large datasets makes them well-suited for NLP applications. Driven by the exponential growth of textual data and the increasing demand for condensed, coherent, and informative summaries, text summarization has been a critical research area in the field of NLP. Applying deep learning to text summarization refers to the use of deep neural networks to perform text summarization tasks. In this survey, we begin with a review of fashionable text summarization tasks in recent years, including extractive, abstractive, multi-document, and so on. Next, we discuss most deep learning-based models and their experimental results on these tasks. The paper also covers datasets and data representation for summarization tasks. Finally, we delve into the opportunities and challenges associated with summarization tasks and their corresponding methodologies, aiming to inspire future research efforts to advance the field further. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific setting.\n## Source Document Quantity: Single-document vs\nMulti-document Single-document summarization and multi-document summarization are two distinct tasks within text summarization domain based on source document quantity. Single-document summarization focuses on generating a summary from a single input document, while multi-document summarization aims to create a summary by aggregating information from multiple related documents [41,111,149]. Within the single-document system, the objective is to condense the main ideas and essential information contained in that specific document. On the other hand, multi-document summarization tasks require identifying and combining the most relevant and nonredundant information from a set of documents, often covering the same topic or event. This means multi-document summarization has additional challenges, such as maintaining cross-document coherence, efficiently handling larger volumes of information, and processing redundancy across documents. These complexities make multi-document summarization generally more difficult than single-document summarization. \n\nTraditional approaches usually employ extractive techniques both in the context of single-document and multi-document summarization. Graph-based methods can be applied to both single-document and multi-document summarization by representing the relationships between sentences in one document or several documents as a graph, with sentences as nodes and the edges as the similarity between the sentences. The systems [46,96,141,171] then use algorithms like PageRank, HITS, or LexRank to identify the most important sentences in the graph, which are then extracted and combined to form the summary. Fig. 2. The weighted cosine similarity graph [46] was generated for the cluster, based on the subset of d1003t from DUC 2004, which is a dataset for multi-document summarization tasks. The notation used in the figure is as follows: 'd' represents document and 's' represents sentence. For instance, d2s3 denotes the third sentence of document 2. \n\nFor single-document summarization, position-based methods [44,82,95] exploit the position of sentences within the document to identify important content. Additionally, the TF-IDF approach [60,115,156] weighs the importance of words in main Surveying Text Summarization with Deep Learning 7 the document based on their frequency and rarity.",
            "reference_string": "[264146402 | Wang et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
            "venue": "The Web Conference",
            "year": 2024,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651262",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3589335.3651262?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3589335.3651262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2163451228",
                    "name": "Yutong Qu"
                }
            ],
            "abstract": "With the development of information technology, a large amount of information and corpora has been incrementally sparked from the Web, stimulating an increasingly high demand for summarizing. Document Summarization is one of Natural Language Processing tasks, which aims to generate abridged versions of a given single or multiple documents as concise and coherent as possible while preserving salient information from the source texts. Recent research in the area has started to use knowledge graphs as they can capture more factual and applicable information from more facets along with source information, benefiting fact consistency and informativeness of generated summaries, rather than just from a linguistic perspective. However, there is no explicit investigation of the effects of different kinds of knowledge graphs on document summarization. The proposed method is to use structured informative and knowledgeable auxiliary information, especially knowledge graphs, into pre-trained summarization models, advancing summary qualities. Expected outcomes are exploring knowledge and knowledge graph incorporation for multi-document summarization, and achieving more informative, coherent, and factually consistent summaries.",
            "corpus_id": 269762702,
            "sentences": [
                {
                    "corpus_id": "269762702",
                    "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
                    "text": "Along with the prosperity of knowledge-aware research in the natural language processing field, more and more document summarization models attempted to incorporate knowledge graphs to enhance the quality of generated summaries.SDS with KG Gunel et al. [6]  MDS with KG Zhou et al. [26] presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. [18] presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News [5] and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model [13] with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. [18] utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
                    "score": 0.5761256505528667,
                    "section_title": "STATE OF THE ART",
                    "char_start_offset": 6022,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 228,
                            "end": 547
                        },
                        {
                            "start": 547,
                            "end": 740
                        },
                        {
                            "start": 740,
                            "end": 879
                        },
                        {
                            "start": 879,
                            "end": 1129
                        },
                        {
                            "start": 1129,
                            "end": 1335
                        },
                        {
                            "start": 1335,
                            "end": 1466
                        },
                        {
                            "start": 1466,
                            "end": 1729
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 253,
                            "end": 256,
                            "matchedPaperCorpusId": "204735695"
                        },
                        {
                            "start": 282,
                            "end": 286,
                            "matchedPaperCorpusId": "236478143"
                        },
                        {
                            "start": 895,
                            "end": 899,
                            "matchedPaperCorpusId": "235097309"
                        },
                        {
                            "start": 1112,
                            "end": 1115,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 1210,
                            "end": 1214,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 1525,
                            "end": 1529,
                            "matchedPaperCorpusId": "235097309"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88330078125
                }
            ],
            "relevance_judgement": 0.88330078125,
            "relevance_judgment_input_expanded": "# Title: Leveraging Knowledge-aware Methodologies for Multi-document Summarization\n# Venue: The Web Conference\n# Authors: Yutong Qu\n## Abstract\nWith the development of information technology, a large amount of information and corpora has been incrementally sparked from the Web, stimulating an increasingly high demand for summarizing. Document Summarization is one of Natural Language Processing tasks, which aims to generate abridged versions of a given single or multiple documents as concise and coherent as possible while preserving salient information from the source texts. Recent research in the area has started to use knowledge graphs as they can capture more factual and applicable information from more facets along with source information, benefiting fact consistency and informativeness of generated summaries, rather than just from a linguistic perspective. However, there is no explicit investigation of the effects of different kinds of knowledge graphs on document summarization. The proposed method is to use structured informative and knowledgeable auxiliary information, especially knowledge graphs, into pre-trained summarization models, advancing summary qualities. Expected outcomes are exploring knowledge and knowledge graph incorporation for multi-document summarization, and achieving more informative, coherent, and factually consistent summaries.\n## STATE OF THE ART\nAlong with the prosperity of knowledge-aware research in the natural language processing field, more and more document summarization models attempted to incorporate knowledge graphs to enhance the quality of generated summaries.SDS with KG Gunel et al. [6]  MDS with KG Zhou et al. [26] presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. [18] presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News [5] and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model [13] with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. [18] utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
            "reference_string": "[269762702 | Qu | 2024 | Citations: 0]"
        },
        {
            "title": "Query-oriented text summarization based on hypergraph transversals",
            "venue": "Information Processing & Management",
            "year": 2019,
            "reference_count": 73,
            "citation_count": 55,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1902.00672",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.00672, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "29364954",
                    "name": "H. V. Lierde"
                },
                {
                    "authorId": "144134805",
                    "name": "T. Chow"
                }
            ],
            "abstract": null,
            "corpus_id": 59599960,
            "sentences": [
                {
                    "corpus_id": "59599960",
                    "title": "Query-oriented text summarization based on hypergraph transversals",
                    "text": "While early models focused on the task of single document summarization, recent systems generally produce summaries of corpora of documents [11]. Similarly, the focus has shifted from generic summarization to the more realistic task of query-oriented summarization, in which a summary is produced with the essential information contained in a corpus that is also relevant to a user-defined query [12]. \n\nSummarization systems are further divided into two classes, namely abstractive and extractive models. Extractive summarizers identify relevant sentences in the original corpus and produce summaries by aggregating these sentences [11]. In contrast, an abstractive summarizer identifies conceptual information in the corpus and reformulates a summary from scratch [12]. Since abstractive approaches require advanced natural language processing, the majority of existing summarization systems consist of extractive models. \n\nExtractive summarizers differ in the method used to identify relevant sentences, which leads to a classification of models as either feature-based or graph-based approaches. Feature-based methods represent the sentences with a set of predefined features such as the sentence position, the sentence length or the presence of cue phrases [13]. Then, they train a model to compute relevance scores for the sentences based on their features. Since feature-based approaches generally require datasets with labelled sentences which are hard to produce [12], unsupervised graph-based methods have attracted growing interest in recent years. \n\nGraph-based summarizers represent the sentences of a corpus as the nodes of a graph with the edges modelling relationships of similarity between the sentences [1]. Then, graph-based algorithms are applied to identify relevant sentences. The models generally differ in the type of relationship captured by the graph or in the sentence selection approach. Most graph-based models define the edges connecting sentences based on the co-occurrence of terms in pairs of sentences [1,3,4]. Then, important sentences are identified either based on node ranking algorithms, or using a global optimization approach. Methods based on node ranking compute individual relevance scores for the sentences and build summaries with highly scored sentences. The earliest such summarizer, LexRank [1], applies the PageRank algorithm to compute sentence scores.",
                    "score": 0.6988661088861764,
                    "section_title": "Background and related work",
                    "char_start_offset": 7277,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 401
                        },
                        {
                            "start": 404,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 923
                        },
                        {
                            "start": 926,
                            "end": 1099
                        },
                        {
                            "start": 1100,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1559
                        },
                        {
                            "start": 1562,
                            "end": 1725
                        },
                        {
                            "start": 1726,
                            "end": 1798
                        },
                        {
                            "start": 1799,
                            "end": 1915
                        },
                        {
                            "start": 1916,
                            "end": 2044
                        },
                        {
                            "start": 2045,
                            "end": 2167
                        },
                        {
                            "start": 2168,
                            "end": 2301
                        },
                        {
                            "start": 2302,
                            "end": 2403
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 140,
                            "end": 144,
                            "matchedPaperCorpusId": "47179013"
                        },
                        {
                            "start": 396,
                            "end": 400,
                            "matchedPaperCorpusId": "556431"
                        },
                        {
                            "start": 633,
                            "end": 637,
                            "matchedPaperCorpusId": "47179013"
                        },
                        {
                            "start": 766,
                            "end": 770,
                            "matchedPaperCorpusId": "556431"
                        },
                        {
                            "start": 1262,
                            "end": 1266,
                            "matchedPaperCorpusId": "7992772"
                        },
                        {
                            "start": 1472,
                            "end": 1476,
                            "matchedPaperCorpusId": "556431"
                        },
                        {
                            "start": 1721,
                            "end": 1724,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 2036,
                            "end": 2039,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 2039,
                            "end": 2041,
                            "matchedPaperCorpusId": "10274362"
                        },
                        {
                            "start": 2041,
                            "end": 2043,
                            "matchedPaperCorpusId": "27105029"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87890625
                }
            ],
            "relevance_judgement": 0.87890625,
            "relevance_judgment_input_expanded": "# Title: Query-oriented text summarization based on hypergraph transversals\n# Venue: Information Processing & Management\n# Authors: H. V. Lierde, T. Chow\n## Abstract\nNone\n## Background and related work\nWhile early models focused on the task of single document summarization, recent systems generally produce summaries of corpora of documents [11]. Similarly, the focus has shifted from generic summarization to the more realistic task of query-oriented summarization, in which a summary is produced with the essential information contained in a corpus that is also relevant to a user-defined query [12]. \n\nSummarization systems are further divided into two classes, namely abstractive and extractive models. Extractive summarizers identify relevant sentences in the original corpus and produce summaries by aggregating these sentences [11]. In contrast, an abstractive summarizer identifies conceptual information in the corpus and reformulates a summary from scratch [12]. Since abstractive approaches require advanced natural language processing, the majority of existing summarization systems consist of extractive models. \n\nExtractive summarizers differ in the method used to identify relevant sentences, which leads to a classification of models as either feature-based or graph-based approaches. Feature-based methods represent the sentences with a set of predefined features such as the sentence position, the sentence length or the presence of cue phrases [13]. Then, they train a model to compute relevance scores for the sentences based on their features. Since feature-based approaches generally require datasets with labelled sentences which are hard to produce [12], unsupervised graph-based methods have attracted growing interest in recent years. \n\nGraph-based summarizers represent the sentences of a corpus as the nodes of a graph with the edges modelling relationships of similarity between the sentences [1]. Then, graph-based algorithms are applied to identify relevant sentences. The models generally differ in the type of relationship captured by the graph or in the sentence selection approach. Most graph-based models define the edges connecting sentences based on the co-occurrence of terms in pairs of sentences [1,3,4]. Then, important sentences are identified either based on node ranking algorithms, or using a global optimization approach. Methods based on node ranking compute individual relevance scores for the sentences and build summaries with highly scored sentences. The earliest such summarizer, LexRank [1], applies the PageRank algorithm to compute sentence scores.",
            "reference_string": "[59599960 | Lierde et al. | 2019 | Citations: 55]"
        },
        {
            "title": "Automated text summarisation and evidence-based medicine: A survey of two domains",
            "venue": "arXiv.org",
            "year": 2017,
            "reference_count": 171,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.08162, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48673524",
                    "name": "A. Sarker"
                },
                {
                    "authorId": "1702289",
                    "name": "Diego Moll\u00e1 Aliod"
                },
                {
                    "authorId": "145212976",
                    "name": "C\u00e9cile Paris"
                }
            ],
            "abstract": "The practice of evidence-based medicine (EBM) urges medical practitioners to utilise the latest research evidence when making clinical decisions. Because of the massive and growing volume of published research on various medical topics, practitioners often find themselves overloaded with information. As such, natural language processing research has recently commenced exploring techniques for performing medical domain-specific automated text summarisation (ATS) techniques-- targeted towards the task of condensing large medical texts. However, the development of effective summarisation techniques for this task requires cross-domain knowledge. We present a survey of EBM, the domain-specific needs for EBM, automated summarisation techniques, and how they have been applied hitherto. We envision that this survey will serve as a first resource for the development of future operational text summarisation techniques for EBM.",
            "corpus_id": 38851848,
            "sentences": [
                {
                    "corpus_id": "38851848",
                    "title": "Automated text summarisation and evidence-based medicine: A survey of two domains",
                    "text": "MMR was initially utilised for document retrieval and is given by the following formula: \n\nwhere, as explained by Carbonell and Goldstein [1998]: \"Q is a query; R = IR(C, Q, \u03b8), i.e., the ranked list of documents retrieved by an IR system given a document collection C and a relevance threshold \u03b8; S is the subset of documents in R already selected; R\\S is the set difference, i.e., the set of unselected documents in R; Sim 1 is the similarity metric used in document retrieval and relevance ranking between documents and a query; and Sim 2 can be the same or a different metric.\" \n\nGraph-based approaches have also been applied to text summarisation [Mani and Bloedorn 1997;Mani and Maybury 1999;Erkan and Radev 2004;Leskovec et al. 2005], with Mani and Bloedorn [1997] being the pioneers in this area. In their approach, the authors use nodes to represent words and edges between nodes represent relationships. The summaries generated can be topic driven, and there is no text in the summaries. Instead, the summary content is represented via as nodes and edges that represent contents and relations between them. When summarising a pair of documents, common nodes represent same words or synonyms, while difference nodes are those that are not common. Sentence selection from the graph is computed from the average activated weights of the covered words: for a sentence s, its score in terms of coverage of common nodes is given by the following formula: \n\nwhere c(s) = {w|w \u2208 Common s}. The score for differences is calculated similarly. The sentences with higher common and difference scores are selected for the final summary. Erkan and Radev [2004] presented the LexRank system for multi-document summarisation, which is a graph-based system using a connected, undirected graph to represent documents. A similar method, suitable for single-document summarisation only, was proposed by Mihalcea and Tarau [2004]. Other graph based approaches have been proposed, both in the medical domain [Reeve et al. 2007;Fiszman et al. 2004] and outside it [Litvak and Last 2008].",
                    "score": 0.5584698788811178,
                    "section_title": "3.3.3",
                    "char_start_offset": 41068,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 91,
                            "end": 581
                        },
                        {
                            "start": 584,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1116
                        },
                        {
                            "start": 1117,
                            "end": 1255
                        },
                        {
                            "start": 1256,
                            "end": 1458
                        },
                        {
                            "start": 1461,
                            "end": 1491
                        },
                        {
                            "start": 1492,
                            "end": 1542
                        },
                        {
                            "start": 1543,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1809
                        },
                        {
                            "start": 1810,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 2074
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 114,
                            "end": 144,
                            "matchedPaperCorpusId": "4508623"
                        },
                        {
                            "start": 698,
                            "end": 719,
                            "matchedPaperCorpusId": "2962090"
                        },
                        {
                            "start": 719,
                            "end": 740,
                            "matchedPaperCorpusId": "16136677"
                        },
                        {
                            "start": 1634,
                            "end": 1656,
                            "matchedPaperCorpusId": "2962090"
                        },
                        {
                            "start": 1893,
                            "end": 1918,
                            "matchedPaperCorpusId": "14130036"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87890625
                }
            ],
            "relevance_judgement": 0.87890625,
            "relevance_judgment_input_expanded": "# Title: Automated text summarisation and evidence-based medicine: A survey of two domains\n# Venue: arXiv.org\n# Authors: A. Sarker, Diego Moll\u00e1 Aliod, C\u00e9cile Paris\n## Abstract\nThe practice of evidence-based medicine (EBM) urges medical practitioners to utilise the latest research evidence when making clinical decisions. Because of the massive and growing volume of published research on various medical topics, practitioners often find themselves overloaded with information. As such, natural language processing research has recently commenced exploring techniques for performing medical domain-specific automated text summarisation (ATS) techniques-- targeted towards the task of condensing large medical texts. However, the development of effective summarisation techniques for this task requires cross-domain knowledge. We present a survey of EBM, the domain-specific needs for EBM, automated summarisation techniques, and how they have been applied hitherto. We envision that this survey will serve as a first resource for the development of future operational text summarisation techniques for EBM.\n## 3.3.3\nMMR was initially utilised for document retrieval and is given by the following formula: \n\nwhere, as explained by Carbonell and Goldstein [1998]: \"Q is a query; R = IR(C, Q, \u03b8), i.e., the ranked list of documents retrieved by an IR system given a document collection C and a relevance threshold \u03b8; S is the subset of documents in R already selected; R\\S is the set difference, i.e., the set of unselected documents in R; Sim 1 is the similarity metric used in document retrieval and relevance ranking between documents and a query; and Sim 2 can be the same or a different metric.\" \n\nGraph-based approaches have also been applied to text summarisation [Mani and Bloedorn 1997;Mani and Maybury 1999;Erkan and Radev 2004;Leskovec et al. 2005], with Mani and Bloedorn [1997] being the pioneers in this area. In their approach, the authors use nodes to represent words and edges between nodes represent relationships. The summaries generated can be topic driven, and there is no text in the summaries. Instead, the summary content is represented via as nodes and edges that represent contents and relations between them. When summarising a pair of documents, common nodes represent same words or synonyms, while difference nodes are those that are not common. Sentence selection from the graph is computed from the average activated weights of the covered words: for a sentence s, its score in terms of coverage of common nodes is given by the following formula: \n\nwhere c(s) = {w|w \u2208 Common s}. The score for differences is calculated similarly. The sentences with higher common and difference scores are selected for the final summary. Erkan and Radev [2004] presented the LexRank system for multi-document summarisation, which is a graph-based system using a connected, undirected graph to represent documents. A similar method, suitable for single-document summarisation only, was proposed by Mihalcea and Tarau [2004]. Other graph based approaches have been proposed, both in the medical domain [Reeve et al. 2007;Fiszman et al. 2004] and outside it [Litvak and Last 2008].",
            "reference_string": "[38851848 | Sarker et al. | 2017 | Citations: 5]"
        },
        {
            "title": "Grapharizer: A Graph-Based Technique for Extractive Multi-Document Summarization",
            "venue": "Electronics",
            "year": 2023,
            "reference_count": 48,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/12/8/1895/pdf?version=1681740423",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics12081895?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics12081895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "70642020",
                    "name": "Zakia Jalil"
                },
                {
                    "authorId": "2128893061",
                    "name": "Muhammad Nasir"
                },
                {
                    "authorId": "2268000",
                    "name": "M. Alazab"
                },
                {
                    "authorId": "2152458353",
                    "name": "Jamal Nasir"
                },
                {
                    "authorId": "1742471",
                    "name": "Tehmina Amjad"
                },
                {
                    "authorId": "2078200186",
                    "name": "Abdullah Alqammaz"
                }
            ],
            "abstract": "In the age of big data, there is increasing growth of data on the Internet. It becomes frustrating for users to locate the desired data. Therefore, text summarization emerges as a solution to this problem. It summarizes and presents the users with the gist of the provided documents. However, summarizer systems face challenges, such as poor grammaticality, missing important information, and redundancy, particularly in multi-document summarization. This study involves the development of a graph-based extractive generic MDS technique, named Grapharizer (GRAPH-based summARIZER), focusing on resolving these challenges. Grapharizer addresses the grammaticality problems of the summary using lemmatization during pre-processing. Furthermore, synonym mapping, multi-word expression mapping, and anaphora and cataphora resolution, contribute positively to improving the grammaticality of the generated summary. Challenges, such as redundancy and proper coverage of all topics, are dealt with to achieve informativity and representativeness. Grapharizer is a novel approach which can also be used in combination with different machine learning models. The system was tested on DUC 2004 and Recent News Article datasets against various state-of-the-art techniques. Use of Grapharizer with machine learning increased accuracy by up to 23.05% compared with different baseline techniques on ROUGE scores. Expert evaluation of the proposed system indicated the accuracy to be more than 55%.",
            "corpus_id": 258231820,
            "sentences": [],
            "relevance_judgement": 0.8779296875,
            "relevance_judgment_input_expanded": "# Title: Grapharizer: A Graph-Based Technique for Extractive Multi-Document Summarization\n# Venue: Electronics\n# Authors: Zakia Jalil, Muhammad Nasir, M. Alazab, Jamal Nasir, Tehmina Amjad, Abdullah Alqammaz\n## Abstract\nIn the age of big data, there is increasing growth of data on the Internet. It becomes frustrating for users to locate the desired data. Therefore, text summarization emerges as a solution to this problem. It summarizes and presents the users with the gist of the provided documents. However, summarizer systems face challenges, such as poor grammaticality, missing important information, and redundancy, particularly in multi-document summarization. This study involves the development of a graph-based extractive generic MDS technique, named Grapharizer (GRAPH-based summARIZER), focusing on resolving these challenges. Grapharizer addresses the grammaticality problems of the summary using lemmatization during pre-processing. Furthermore, synonym mapping, multi-word expression mapping, and anaphora and cataphora resolution, contribute positively to improving the grammaticality of the generated summary. Challenges, such as redundancy and proper coverage of all topics, are dealt with to achieve informativity and representativeness. Grapharizer is a novel approach which can also be used in combination with different machine learning models. The system was tested on DUC 2004 and Recent News Article datasets against various state-of-the-art techniques. Use of Grapharizer with machine learning increased accuracy by up to 23.05% compared with different baseline techniques on ROUGE scores. Expert evaluation of the proposed system indicated the accuracy to be more than 55%.\n",
            "reference_string": "[258231820 | Jalil et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Clustering cliques for graph-based summarization of the biomedical research literature",
            "venue": "BMC Bioinformatics",
            "year": 2013,
            "reference_count": 57,
            "citation_count": 28,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/1471-2105-14-182",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC3682874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48213346",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "1719666",
                    "name": "M. Fiszman"
                },
                {
                    "authorId": "50877008",
                    "name": "Dongwook Shin"
                },
                {
                    "authorId": "2982296",
                    "name": "Bartlomiej Wilkowski"
                },
                {
                    "authorId": "1693891",
                    "name": "Thomas C. Rindflesch"
                }
            ],
            "abstract": "Graph-based notions are increasingly used in biomedical data mining and knowledge discovery tasks. In this paper, we present a clique-clustering method to automatically summarize graphs of semantic predications produced from PubMed citations (titles and abstracts). SemRep is used to extract semantic predications from the citations returned by a PubMed search. Cliques were identified from frequently occurring predications with highly connected arguments filtered by degree centrality. Themes contained in the summary were identified with a hierarchical clustering algorithm based on common arguments shared among cliques. The validity of the clusters in the summaries produced was compared to the Silhouette-generated baseline for cohesion, separation and overall validity. The theme labels were also compared to a reference standard produced with major MeSH headings. For 11 topics in the testing data set, the overall validity of clusters from the system summary was 10% better than the baseline (43% versus 33%). While compared to the reference standard from MeSH headings, the results for recall, precision and F-score were 0.64, 0.65, and 0.65 respectively.",
            "corpus_id": 10274390,
            "sentences": [
                {
                    "corpus_id": "10274390",
                    "title": "Clustering cliques for graph-based summarization of the biomedical research literature",
                    "text": "Most frequency-based methods provide extractive summaries composed of source sentences containing frequently occurring content units. Nenkova and Vanderwende [15] assessed the contribution of frequency of occurrence to summarization, which is considerable. Reeve et al. [16,17] further exploit domain ontologies to identify salient information.\n\nRecently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. [18][19][20]) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank [18] and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary.\n\nIn addition to text, biomedical data can also be represented as a graph, with nodes representing biological entities (e.g. genes or proteins) and edges associations between them. For example, protein-protein interactions can be successfully modeled by a graph. Based on the recognition of cohesive subgroups (such as cliques), gene or protein complexes can be extracted to help predict protein interactions or find gene-disease relations [22,23].",
                    "score": 0.5645308172251217,
                    "section_title": "Identifying salient source content",
                    "char_start_offset": 5658,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 274,
                            "end": 277,
                            "matchedPaperCorpusId": "14968839"
                        },
                        {
                            "start": 1044,
                            "end": 1048,
                            "matchedPaperCorpusId": "2962090"
                        },
                        {
                            "start": 1048,
                            "end": 1052,
                            "matchedPaperCorpusId": "268069298"
                        },
                        {
                            "start": 1052,
                            "end": 1056,
                            "matchedPaperCorpusId": "13664986"
                        },
                        {
                            "start": 1177,
                            "end": 1181,
                            "matchedPaperCorpusId": "2962090"
                        },
                        {
                            "start": 1869,
                            "end": 1873,
                            "matchedPaperCorpusId": "7883745"
                        },
                        {
                            "start": 1873,
                            "end": 1876,
                            "matchedPaperCorpusId": "303600"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.876953125
                }
            ],
            "relevance_judgement": 0.876953125,
            "relevance_judgment_input_expanded": "# Title: Clustering cliques for graph-based summarization of the biomedical research literature\n# Venue: BMC Bioinformatics\n# Authors: Han Zhang, M. Fiszman, Dongwook Shin, Bartlomiej Wilkowski, Thomas C. Rindflesch\n## Abstract\nGraph-based notions are increasingly used in biomedical data mining and knowledge discovery tasks. In this paper, we present a clique-clustering method to automatically summarize graphs of semantic predications produced from PubMed citations (titles and abstracts). SemRep is used to extract semantic predications from the citations returned by a PubMed search. Cliques were identified from frequently occurring predications with highly connected arguments filtered by degree centrality. Themes contained in the summary were identified with a hierarchical clustering algorithm based on common arguments shared among cliques. The validity of the clusters in the summaries produced was compared to the Silhouette-generated baseline for cohesion, separation and overall validity. The theme labels were also compared to a reference standard produced with major MeSH headings. For 11 topics in the testing data set, the overall validity of clusters from the system summary was 10% better than the baseline (43% versus 33%). While compared to the reference standard from MeSH headings, the results for recall, precision and F-score were 0.64, 0.65, and 0.65 respectively.\n## Identifying salient source content\nMost frequency-based methods provide extractive summaries composed of source sentences containing frequently occurring content units. Nenkova and Vanderwende [15] assessed the contribution of frequency of occurrence to summarization, which is considerable. Reeve et al. [16,17] further exploit domain ontologies to identify salient information.\n\nRecently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. [18][19][20]) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank [18] and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary.\n\nIn addition to text, biomedical data can also be represented as a graph, with nodes representing biological entities (e.g. genes or proteins) and edges associations between them. For example, protein-protein interactions can be successfully modeled by a graph. Based on the recognition of cohesive subgroups (such as cliques), gene or protein complexes can be extracted to help predict protein interactions or find gene-disease relations [22,23].",
            "reference_string": "[10274390 | Zhang et al. | 2013 | Citations: 28]"
        },
        {
            "title": "Better Highlighting: Creating Sub-Sentence Summary Highlights",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2020,
            "reference_count": 56,
            "citation_count": 12,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2010.10566",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.10566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2173531",
                    "name": "Sangwoo Cho"
                },
                {
                    "authorId": "50982080",
                    "name": "Kaiqiang Song"
                },
                {
                    "authorId": "40475596",
                    "name": "Chen Li"
                },
                {
                    "authorId": "2111505433",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "1691260",
                    "name": "H. Foroosh"
                },
                {
                    "authorId": "144544919",
                    "name": "Fei Liu"
                }
            ],
            "abstract": "Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short. In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion. Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights. To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets. Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.",
            "corpus_id": 224818792,
            "sentences": [
                {
                    "corpus_id": "224818792",
                    "title": "Better Highlighting: Creating Sub-Sentence Summary Highlights",
                    "text": "We compare our method with strong extractive and abstractive summarization systems for multidocument summarization, results are shown in Tables 3 and 5. DPP (Kulesza and Taskar, 2012) and variant DPP-BERT (Cho et al., 2019b) use determinantal point processes to extract whole sentences from a set of documents. SumBasic (Vanderwende et al., 2007) is an extractive approach leveraging the  fact that frequently occurring words are more likely to be included in the summary. KL-Sum (Haghighi and Vanderwende, 2009) is a greedy approach that iteratively adds sentences to the summary to minimize KL divergence. LexRank (Erkan and Radev, 2004) is a graph-based approach estimating sentence importance based on eigenvector centrality. All of these methods extract whole sentences rather than segments from a set of documents. We further consider abstractive summarization methods. Opinosis (Ganesan et al., 2010) creates a word co-occurrence graph and searches for a graph path to generate an abstract. PointerGen (See et al., 2017) learns to reuse source words or predict new words. The documents are concatenated to serve as input. CopyTrans (Gehrmann et al., 2018) uses a 4layer Transformer for the encoder and decoder. Hi-MAP (Fabbri et al., 2019) introduces an end-to-end hierarchical attention model to generate abstracts from multi-document inputs. \n\nWe explore two variants of our proposed method, called HL-XLNetSegs and HL-TreeSegs, focusing on highlighting summary segments. The former utilizes XLNet to extract a set of partially-overlapping segments from a sentence; the latter decomposes a sentence constituent parse tree into subtrees and collect text segments governed by the subtrees. An illustration is shown in Figure 3. Constituent parse trees are obtained using the Stanford parser (Manning et al., 2014). In both cases, the segments are passed to DPP, which identifies a set of important and non-redundant segments as highlights.",
                    "score": 0.5920870718797637,
                    "section_title": "Summarization Results",
                    "char_start_offset": 23800,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 310
                        },
                        {
                            "start": 311,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 729
                        },
                        {
                            "start": 730,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1350
                        },
                        {
                            "start": 1353,
                            "end": 1480
                        },
                        {
                            "start": 1481,
                            "end": 1696
                        },
                        {
                            "start": 1697,
                            "end": 1821
                        },
                        {
                            "start": 1822,
                            "end": 1946
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 205,
                            "end": 224,
                            "matchedPaperCorpusId": "204901346"
                        },
                        {
                            "start": 320,
                            "end": 346,
                            "matchedPaperCorpusId": "10496513"
                        },
                        {
                            "start": 480,
                            "end": 511,
                            "matchedPaperCorpusId": "678258"
                        },
                        {
                            "start": 616,
                            "end": 638,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 885,
                            "end": 907,
                            "matchedPaperCorpusId": "988010"
                        },
                        {
                            "start": 1009,
                            "end": 1027,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 1139,
                            "end": 1162,
                            "matchedPaperCorpusId": "52144157"
                        },
                        {
                            "start": 1225,
                            "end": 1246,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 1798,
                            "end": 1820,
                            "matchedPaperCorpusId": "14068874"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.873046875
                }
            ],
            "relevance_judgement": 0.873046875,
            "relevance_judgment_input_expanded": "# Title: Better Highlighting: Creating Sub-Sentence Summary Highlights\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Sangwoo Cho, Kaiqiang Song, Chen Li, Dong Yu, H. Foroosh, Fei Liu\n## Abstract\nAmongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short. In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion. Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights. To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets. Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.\n## Summarization Results\nWe compare our method with strong extractive and abstractive summarization systems for multidocument summarization, results are shown in Tables 3 and 5. DPP (Kulesza and Taskar, 2012) and variant DPP-BERT (Cho et al., 2019b) use determinantal point processes to extract whole sentences from a set of documents. SumBasic (Vanderwende et al., 2007) is an extractive approach leveraging the  fact that frequently occurring words are more likely to be included in the summary. KL-Sum (Haghighi and Vanderwende, 2009) is a greedy approach that iteratively adds sentences to the summary to minimize KL divergence. LexRank (Erkan and Radev, 2004) is a graph-based approach estimating sentence importance based on eigenvector centrality. All of these methods extract whole sentences rather than segments from a set of documents. We further consider abstractive summarization methods. Opinosis (Ganesan et al., 2010) creates a word co-occurrence graph and searches for a graph path to generate an abstract. PointerGen (See et al., 2017) learns to reuse source words or predict new words. The documents are concatenated to serve as input. CopyTrans (Gehrmann et al., 2018) uses a 4layer Transformer for the encoder and decoder. Hi-MAP (Fabbri et al., 2019) introduces an end-to-end hierarchical attention model to generate abstracts from multi-document inputs. \n\nWe explore two variants of our proposed method, called HL-XLNetSegs and HL-TreeSegs, focusing on highlighting summary segments. The former utilizes XLNet to extract a set of partially-overlapping segments from a sentence; the latter decomposes a sentence constituent parse tree into subtrees and collect text segments governed by the subtrees. An illustration is shown in Figure 3. Constituent parse trees are obtained using the Stanford parser (Manning et al., 2014). In both cases, the segments are passed to DPP, which identifies a set of important and non-redundant segments as highlights.",
            "reference_string": "[224818792 | Cho et al. | 2020 | Citations: 12]"
        },
        {
            "title": "AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.05537",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.05537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152584752",
                    "name": "Guan-Hua Wang"
                },
                {
                    "authorId": "2187784939",
                    "name": "Weihua Li"
                },
                {
                    "authorId": "2075378060",
                    "name": "E. Lai"
                },
                {
                    "authorId": "71203570",
                    "name": "Quan-wei Bai"
                }
            ],
            "abstract": "The rapid growth of information on the Internet has led to an overwhelming amount of opinions and comments on various activities, products, and services. This makes it difficult and time-consuming for users to process all the available information when making decisions. Text summarization, a Natural Language Processing (NLP) task, has been widely explored to help users quickly retrieve relevant information by generating short and salient content from long or multiple documents. Recent advances in pre-trained language models, such as ChatGPT, have demonstrated the potential of Large Language Models (LLMs) in text generation. However, LLMs require massive amounts of data and resources and are challenging to implement as offline applications. Furthermore, existing text summarization approaches often lack the ``adaptive\"nature required to capture diverse aspects in opinion summarization, which is particularly detrimental to users with specific requirements or preferences. In this paper, we propose an Aspect-adaptive Knowledge-based Opinion Summarization model for product reviews, which effectively captures the adaptive nature required for opinion summarization. The model generates aspect-oriented summaries given a set of reviews for a particular product, efficiently providing users with useful information on specific aspects they are interested in, ensuring the generated summaries are more personalized and informative. Extensive experiments have been conducted using real-world datasets to evaluate the proposed model. The results demonstrate that our model outperforms state-of-the-art approaches and is adaptive and efficient in generating summaries that focus on particular aspects, enabling users to make well-informed decisions and catering to their diverse interests and preferences.",
            "corpus_id": 259129368,
            "sentences": [
                {
                    "corpus_id": "259129368",
                    "title": "AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization",
                    "text": "\u2022 LexRank [24] is an unsupervised graph-based summarization method. It employs a ranking algorithm to determine node centrality. In LexRank, sentences are treated as nodes to form a graph with weighted edges calculated using tf-idf. In our work, following the settings from [31], we also use BERT [30] and SentiNeuron [34] vectors to calculate the adjacency matrices. \u2022 Opinosis [27] is a graph-based summarization framework that generates concise abstractive summaries of highly redundant opinions. It assumes no domain knowledge and leverages mostly the word order in the existing text. \u2022 Meansum [22] is an unsupervised neural model for multi-document summarization. \n\nIt proposes an end-to-end architecture with an auto-encoder, where the mean of input review representations decodes into a reasonable summary review without relying on any review-specific features. \u2022 Copycat [13] is a summarization model based on the pointer-generator mechanism [5]. It follows the intuition of controlling the \"amount of novelty\" during summary generation. With this intuition, they define a hierarchical variational autoencoder model to produce summaries that reflect common opinions. \u2022 QT [31] enhances the ability to control the summarization process by leveraging the properties of quantized space to generate aspect-specific summaries.",
                    "score": 0.7609829837369937,
                    "section_title": "Baselines",
                    "char_start_offset": 24227,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 68,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 367
                        },
                        {
                            "start": 368,
                            "end": 499
                        },
                        {
                            "start": 500,
                            "end": 588
                        },
                        {
                            "start": 589,
                            "end": 669
                        },
                        {
                            "start": 672,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1175
                        },
                        {
                            "start": 1176,
                            "end": 1330
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 10,
                            "end": 14,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 274,
                            "end": 278,
                            "matchedPaperCorpusId": "227745131"
                        },
                        {
                            "start": 297,
                            "end": 301,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 379,
                            "end": 383,
                            "matchedPaperCorpusId": "988010"
                        },
                        {
                            "start": 599,
                            "end": 603,
                            "matchedPaperCorpusId": "59413781"
                        },
                        {
                            "start": 880,
                            "end": 884,
                            "matchedPaperCorpusId": "218883429"
                        },
                        {
                            "start": 951,
                            "end": 954,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 1181,
                            "end": 1185,
                            "matchedPaperCorpusId": "227745131"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87255859375
                }
            ],
            "relevance_judgement": 0.87255859375,
            "relevance_judgment_input_expanded": "# Title: AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization\n# Venue: arXiv.org\n# Authors: Guan-Hua Wang, Weihua Li, E. Lai, Quan-wei Bai\n## Abstract\nThe rapid growth of information on the Internet has led to an overwhelming amount of opinions and comments on various activities, products, and services. This makes it difficult and time-consuming for users to process all the available information when making decisions. Text summarization, a Natural Language Processing (NLP) task, has been widely explored to help users quickly retrieve relevant information by generating short and salient content from long or multiple documents. Recent advances in pre-trained language models, such as ChatGPT, have demonstrated the potential of Large Language Models (LLMs) in text generation. However, LLMs require massive amounts of data and resources and are challenging to implement as offline applications. Furthermore, existing text summarization approaches often lack the ``adaptive\"nature required to capture diverse aspects in opinion summarization, which is particularly detrimental to users with specific requirements or preferences. In this paper, we propose an Aspect-adaptive Knowledge-based Opinion Summarization model for product reviews, which effectively captures the adaptive nature required for opinion summarization. The model generates aspect-oriented summaries given a set of reviews for a particular product, efficiently providing users with useful information on specific aspects they are interested in, ensuring the generated summaries are more personalized and informative. Extensive experiments have been conducted using real-world datasets to evaluate the proposed model. The results demonstrate that our model outperforms state-of-the-art approaches and is adaptive and efficient in generating summaries that focus on particular aspects, enabling users to make well-informed decisions and catering to their diverse interests and preferences.\n## Baselines\n\u2022 LexRank [24] is an unsupervised graph-based summarization method. It employs a ranking algorithm to determine node centrality. In LexRank, sentences are treated as nodes to form a graph with weighted edges calculated using tf-idf. In our work, following the settings from [31], we also use BERT [30] and SentiNeuron [34] vectors to calculate the adjacency matrices. \u2022 Opinosis [27] is a graph-based summarization framework that generates concise abstractive summaries of highly redundant opinions. It assumes no domain knowledge and leverages mostly the word order in the existing text. \u2022 Meansum [22] is an unsupervised neural model for multi-document summarization. \n\nIt proposes an end-to-end architecture with an auto-encoder, where the mean of input review representations decodes into a reasonable summary review without relying on any review-specific features. \u2022 Copycat [13] is a summarization model based on the pointer-generator mechanism [5]. It follows the intuition of controlling the \"amount of novelty\" during summary generation. With this intuition, they define a hierarchical variational autoencoder model to produce summaries that reflect common opinions. \u2022 QT [31] enhances the ability to control the summarization process by leveraging the properties of quantized space to generate aspect-specific summaries.",
            "reference_string": "[259129368 | Wang et al. | 2023 | Citations: 0]"
        },
        {
            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
            "venue": "Journal of Artificial Intelligence Research",
            "year": 2004,
            "reference_count": 41,
            "citation_count": 3097,
            "influential_citation_count": 368,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/10396/24901",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1109.2128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2158159",
                    "name": "G\u00fcnes Erkan"
                },
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                }
            ],
            "abstract": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
            "corpus_id": 506350,
            "sentences": [
                {
                    "corpus_id": "506350",
                    "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                    "text": "Researchers have also tried to integrate machine learning into summarization as more features have been proposed and more training data have become available (Kupiec, Pedersen, & Chen, 1995;Lin, 1999;Osborne, 2002;Daum\u00e9 III & Marcu, 2004). \n\nOur summarization approach in this paper is to assess the centrality of each sentence in a cluster and extract the most important ones to include in the summary. We investigate different ways of defining the lexical centrality principle in multi-document summarization, which measures centrality in terms of lexical properties of the sentences. \n\nIn Section 2, we present centroid-based summarization, a well-known method for judging sentence centrality. Then we introduce three new measures for centrality, Degree, LexRank with threshold, and continuous LexRank, inspired from the \"prestige\" concept in social networks. We propose a graph representation of a document cluster, where vertices represent the sentences and edges are defined in terms of the similarity relation between pairs of sentences. This representation enables us to make use of several centrality heuristics defined on graphs. We compare our new methods with centroid-based summarization using a featurebased generic summarization toolkit, MEAD, and show that our new features outperform Centroid in most of the cases. Test data for our experiments are taken from 2003 and 2004 summarization evaluations of Document Understanding Conferences (DUC) to compare our system with other state-of-the-art summarization systems and human performance as well.",
                    "score": 0.585025165242841,
                    "section_title": "Introduction",
                    "char_start_offset": 3867,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 242,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 586
                        },
                        {
                            "start": 589,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1331
                        },
                        {
                            "start": 1332,
                            "end": 1563
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 158,
                            "end": 190,
                            "matchedPaperCorpusId": "5775833"
                        },
                        {
                            "start": 190,
                            "end": 200,
                            "matchedPaperCorpusId": "2010014"
                        },
                        {
                            "start": 200,
                            "end": 214,
                            "matchedPaperCorpusId": "7353825"
                        },
                        {
                            "start": 214,
                            "end": 238,
                            "matchedPaperCorpusId": "6773914"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87255859375
                }
            ],
            "relevance_judgement": 0.87255859375,
            "relevance_judgment_input_expanded": "# Title: LexRank: Graph-based Lexical Centrality as Salience in Text Summarization\n# Venue: Journal of Artificial Intelligence Research\n# Authors: G\u00fcnes Erkan, Dragomir R. Radev\n## Abstract\nWe introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.\n## Introduction\nResearchers have also tried to integrate machine learning into summarization as more features have been proposed and more training data have become available (Kupiec, Pedersen, & Chen, 1995;Lin, 1999;Osborne, 2002;Daum\u00e9 III & Marcu, 2004). \n\nOur summarization approach in this paper is to assess the centrality of each sentence in a cluster and extract the most important ones to include in the summary. We investigate different ways of defining the lexical centrality principle in multi-document summarization, which measures centrality in terms of lexical properties of the sentences. \n\nIn Section 2, we present centroid-based summarization, a well-known method for judging sentence centrality. Then we introduce three new measures for centrality, Degree, LexRank with threshold, and continuous LexRank, inspired from the \"prestige\" concept in social networks. We propose a graph representation of a document cluster, where vertices represent the sentences and edges are defined in terms of the similarity relation between pairs of sentences. This representation enables us to make use of several centrality heuristics defined on graphs. We compare our new methods with centroid-based summarization using a featurebased generic summarization toolkit, MEAD, and show that our new features outperform Centroid in most of the cases. Test data for our experiments are taken from 2003 and 2004 summarization evaluations of Document Understanding Conferences (DUC) to compare our system with other state-of-the-art summarization systems and human performance as well.",
            "reference_string": "[506350 | Erkan et al. | 2004 | Citations: 3097]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "272831675",
            "title": "GLARE: Guided LexRank for Advanced Retrieval in Legal Analysis",
            "text": "LexRank (Erkan and Radev, 2004), an unsupervised graph-based method, is initially applied to the document to reduce it to a length corresponding to 35% of the original size. The second step uses the PEGASUS neural language model (Zhang et al., 2020) for text summarization. \n\nPEGASUS is based on the transformer (Vaswani et al., 2023) architecture. It follows the encoder-decoder pattern, where the encoder processes the original text to extract contextual representations while the decoder generates the summary based on these representations. The encoder comprises several layers of encoding, while the decoder also has several layers to generate a token-by-token summary. A linear layer is applied to map the decoder outputs to the vocabulary, allowing the prediction of the following words in the summary. In the method proposed by Dalal et al. (2023), the extractive summary generated by the LexRank model is divided into sentences and fed to the PEGASUS model. The model then derives paraphrases for each sentence, making it simpler to understand. The sentences are then merged to provide a final abstract summary. \n\nOur work is related to Dalal et al. (2023), who used the graph-based LexRank algorithm to create the legal document summary. However, as a distinguishing feature of our method, we modified the original LexRank algorithm, seeking a way to guide it in the summarization process and thus improve the generated summary. \n\nSatwick Gupta et al. (2022) compare summary techniques and apply them to legal documents. The work method covers the stages of Dataset Acquisition, Data Cleaning, Text summarization, Summary evaluation, and Comparative analysis. In the extractive summarization stage, the following techniques are compared: LexRank, TextRank, Reduction, Luhn, Edmundson, Latent Semantic Analysis, and SumBasic. The results show that graph-based approaches such as LexRank and TextRank perform better than approaches based on frequency. Similarly to Satwick Gupta et al. (2022), we attest to the good performance of the graph-based approach but make a comparison with methods that implement topic modeling.",
            "score": 0.9058380642312172,
            "section_title": "Related work",
            "char_start_offset": 17348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 31,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8134765625
        },
        {
            "corpus_id": "216867290",
            "title": "Self-Supervised and Controlled Multi-Document Opinion Summarization",
            "text": "Unsupervised Opinion Summarization Unsupervised Multi-Document summarization methods encompass both extractive and abstractive approaches. Extractive summarization consists in selecting a few sentences from the input documents to form the output summary. Radev et al. (2004) proposed to rank sentences according to their relevance to the whole input, representing sentences as tfidf bags of words and the input as the centroid vector of its sentences. Recent refinements of this approach include using distributed word representations (Rossiello et al., 2017) or ranking whole summaries instead of individual sentences (Gholipour Ghalandari, 2017). Graph-based methods, such as LexRank (Erkan & Radev, 2004) or TextRank (Mihalcea & Tarau, 2004;Zheng & Lapata, 2019), work by constructing a graph whose nodes are the sentences from the input documents and whose edges indicate a high word overlap between two sentences. Then, they use the PageRank algorithm to extract the sentences with the highest centrality. In contrast to these methods, we focus on abstractive summarization methods.\n\nAbstractive methods for summarization are in principle able to generate new words and sentences that do not occur in the input documents and therefore produce more fluent text. Non-neural abstractive methods (Ganesan et al., 2010;Nayeem et al., 2018) are also graph-based, but construct graphs whose nodes are word types and edges indicate the immediate precedence relationship between two instantiations of the word type in a sentence. The summary is extracted by finding salient paths in the graph.\n\nRecently, a few approaches for neural unsupervised abstractive summarization have been proposed. Chu & Liu (2019, MeanSum) introduced a summarization system based on a review autoencoder. At inference time, MeanSum encodes every review for a product to a vector, computes the centroid of reviews' vectors and uses this centroid to seed the decoder and generate a summary. However, averaging representations of statements that are sometimes contradictory tends to confuse the decoder, and to lead it to rely on only language modeling for generating the output summary, thus ignoring the input signal. To deal with this limitation, Coavoux et al. (2019",
            "score": 0.8421632895129887,
            "section_title": "Related Work",
            "char_start_offset": 3723,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 559,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 720,
                    "end": 744,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 744,
                    "end": 765,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 1319,
                    "end": 1339,
                    "matchedPaperCorpusId": "52011473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "254408844",
            "title": "Scientific Paper Extractive Summarization Enhanced by Citation Graphs",
            "text": "Extractive summarization aims to generate a summary by integrating the salient sentences in the document. Traditional extractive summarization methods are mostly unsupervised, which directly select sentences based on explicit features or graphbased methods. Unsupervised methods can save a lot of manual work and expenses since it does not demand labeled data. More recently, benefiting from the success of neural sequence models and large-scale datasets, neural network-based supervised summarization models have been proposed (Gao et al., 2019(Gao et al., , 2020)). More recetly, Liu and Lapata (2019) and Xu et al. (2020) showcased how pre-trained models can be applied in extractive text summarization tasks. These models often achieve better performance since labeled data is provided. \n\nResearch on summarizing scientific articles has been studied for decades (Nenkova and McKeown, 2011). In the unsupervised domain, Cohan and Goharian (2015) proposed to extract citationcontexts from citing articles, which does not help draft an abstract when the paper has not been cited yet. Dong et al. (2021b) came up with an unsupervised graph-based ranking model for extractive summarization. As for supervised methods, Subramanian et al. (2019) performed an extractive step before generating a summary, which is then used as guidance for abstractive summarization. Xiao and Carenini (2019) incorporated both the global context and local context to summarize papers. \n\nEarly approaches for extractive summarization, such as LexRank and TextRank (Mihalcea and Tarau, 2004), have taken advantage of graph structures with inter-sentence cosine similarity. As for the neural-based approaches, Koncel-Kedziorski et al. (2019) designed a graph attention-based transformer encoder to generate a summary with the help of knowledge graphs extracted from scientific texts. Graph modeling is also explored in abstractive summarization. For example, An et al. (2021) proposed a citation graph-based summarization model which incorporates inter-document information of the source paper and its references.",
            "score": 0.8095898493316377,
            "section_title": "Related Work",
            "char_start_offset": 4850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 528,
                    "end": 545,
                    "matchedPaperCorpusId": "202676734"
                },
                {
                    "start": 545,
                    "end": 566,
                    "matchedPaperCorpusId": "218581875"
                },
                {
                    "start": 582,
                    "end": 603,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 608,
                    "end": 624,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 923,
                    "end": 948,
                    "matchedPaperCorpusId": "5523604"
                },
                {
                    "start": 1085,
                    "end": 1104,
                    "matchedPaperCorpusId": "231592914"
                },
                {
                    "start": 1363,
                    "end": 1387,
                    "matchedPaperCorpusId": "201697450"
                },
                {
                    "start": 1542,
                    "end": 1568,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1686,
                    "end": 1717,
                    "matchedPaperCorpusId": "102354588"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75830078125
        },
        {
            "corpus_id": "273963834",
            "title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation",
            "text": "To generate a summary that is representative of the overall content, graph-based methods construct external graphs to assist document representation and cross-document relation modeling, achieving promising progress. In this regard, LexRank (Erkan and Radev 2004) and TextRank (Mihalcea and Tarau 2004) first introduced graphs to extractive text summarization in 2004. They compute sentence importance using a graph representation of sentences to extract salient textual units from documents as summarization. In 2020, Wang et al. (Wang et al. 2020) propose to extract salient textual units from documents as summarization using a heterogeneous graph consisting of semantic nodes at several granularity levels of documents. In 2022, Wang et al. (Wang et al. 2022) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template to achieve state-of-the-art performance. \n\nHowever, to the best of our knowledge, no existing work integrates LLMs into graph-based methods to leverage their natural language understanding capabilities for improved graph construction and summary generation.",
            "score": 0.7962415516539114,
            "section_title": "Graphs in MSDS Tasks",
            "char_start_offset": 4625,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1142
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 263,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 510,
                    "end": 517,
                    "matchedPaperCorpusId": "216342059"
                },
                {
                    "start": 745,
                    "end": 762,
                    "matchedPaperCorpusId": "252185277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "236478143",
            "title": "Entity-Aware Abstractive Multi-Document Summarization",
            "text": "Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan and Radev, 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Wan (2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. Fan et al. (2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. Huang et al. (2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. Li et al. (2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, Wang et al. (2020a) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization. Our work is partly similar to theirs, but we construct heterogeneous graphs composed of text unit nodes and entity nodes for abstractive multi-document summarization.",
            "score": 0.7897238584291848,
            "section_title": "Graph-based Document Summarization",
            "char_start_offset": 5732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1791
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 206,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 333,
                    "end": 343,
                    "matchedPaperCorpusId": "17446655"
                },
                {
                    "start": 723,
                    "end": 740,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 814,
                    "end": 836,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 988,
                    "end": 1005,
                    "matchedPaperCorpusId": "202785778"
                },
                {
                    "start": 1166,
                    "end": 1185,
                    "matchedPaperCorpusId": "218487279"
                },
                {
                    "start": 1340,
                    "end": 1356,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1471,
                    "end": 1490,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "248571519",
            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
            "text": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013). These papers primarily aim to address de-duplicating information and learning relationships between the different topics shared across documents however none of these architectures are built to deal with conflicting information.",
            "score": 0.7812433893715174,
            "section_title": "Multi Document Summarization",
            "char_start_offset": 8653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 231,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 241,
                    "end": 264,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 491,
                    "end": 508,
                    "matchedPaperCorpusId": "10112929"
                },
                {
                    "start": 1345,
                    "end": 1362,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1448,
                    "end": 1465,
                    "matchedPaperCorpusId": "218718706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9169921875
        },
        {
            "corpus_id": "250374734",
            "title": "Two-phase Multi-document Event Summarization on Core Event Graphs",
            "text": "Existing multi-document summarization methods mainly focus on sentence-level summarization and can be categorized into extractive and abstractive methods. Most extractive methods are operated over graph-based representations of sentences or passages with edge weights computed by tf-idf (Erkan et al., 2004), discourse relations (Christensen et al., 2013) or sentence embeddings (Yasunaga et al., 2017b) and then a specific algorithm is further adopted for ranking text units for inclusion in the final summary. More recently, some extractive summarization works also utilize graph convolutional networks for salient sentences estimation (Kipf et al., 2016) and sentence ordering (Yin et al., 2019). Abstractive models, especially neural abstractive ones, have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Lewis et al., 2019). However, the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward due to the lack of sufficient training data and the computational challenge of processing multiple documents. Intuitively, graph-based extractive methods are suitable to identify relationship of different sentences or documents and extract salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting. Hence, abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020). Our model, which also combines a graph-based extractive module and neural abstractive sequence-to-sequence architecture, is a higher level summarization and focuses on the core event summarization from multiple documents, which is more challenging.",
            "score": 0.7811344608117196,
            "section_title": "Multi-document Summarization",
            "char_start_offset": 5936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1710
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 307,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 821,
                    "end": 839,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 839,
                    "end": 859,
                    "matchedPaperCorpusId": "21850704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "232404911",
            "title": "Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization",
            "text": "The core of extractive summarization is to model the relation between sentences in a document, which falls in graph models' strength. In general, graph-based summarization models represent a document (or a cluster of documents) as a graph, with nodes representing sentences (or discourse) and edge weights representing sentence similarities. \n\nUnsupervised graph summarization methods rely on graph connectivity (centrality) or node representations (centroid) (Radev et al., 2004;Rossiello et al., 2017) to score and rank sentences. Popular centrality-based methods include TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and PACSUM (Zheng and Lapata, 2019). Details of centrality-based and centroid-based summarization will be discussed in Section 3. \n\nResearchers also explore supervised graph neural network (GNN) on the task (Yasunaga et al., 2017;Xu et al., 2019;Wang et al., 2020). Our paper follows this line of works on developing novel unsupervised graph models for single document summarization.",
            "score": 0.7746069485875133,
            "section_title": "Graph-based Summarization",
            "char_start_offset": 4919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1026
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 503,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 583,
                    "end": 609,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 619,
                    "end": 642,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9052734375
        },
        {
            "corpus_id": "218718706",
            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
            "text": "Multi-document summarization (MDS) brings great challenges to the widely used sequence-tosequence (Seq2Seq) neural architecture as it requires effective representation of multiple input documents and content organization of long summaries. For MDS, different documents may contain the same content, include additional information, and present complementary or contradictory information (Radev, 2000). So different from single document summarization (SDS), cross-document links are very important in extracting salient information, detecting redundancy and generating overall coherent summaries for MDS. Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models. However, they are not well studied by most abstractive approaches, especially the end-to-end neural approaches. Few work has studied the effectiveness of explicit graph representations on neural abstractive MDS. \n\nIn this paper, we develop a neural abstractive MDS model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries. Our model augments the end-toend neural architecture with the ability to incorporate well-established graphs into both the document representation and summary generation processes. Specifically, a graph-informed attention mechanism is developed to incorporate graphs into the document encoding process, which enables our model to capture richer cross-document relations. Furthermore, graphs are utilized to guide the summary generation process via a hierarchical graph attention mechanism, which takes advantage of the explicit graph structure to help organize the summary content. Benefiting from the graph modeling, our model can extract salient information from long documents and generate coherent summaries more effectively. We experiment with three types of graph representations, including similarity graph, topic graph and discourse graph, which all significantly improve the MDS performance.",
            "score": 0.7743836116186534,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1420
                },
                {
                    "start": 1423,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2360
                },
                {
                    "start": 2361,
                    "end": 2531
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 399,
                    "matchedPaperCorpusId": "10103200"
                },
                {
                    "start": 1036,
                    "end": 1059,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1109,
                    "end": 1135,
                    "matchedPaperCorpusId": "337730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "235669622",
            "title": "TWAG: A Topic-Guided Wikipedia Abstract Generator",
            "text": "Multi-document summarization is a classic and challenging problem in natural language processing, which aims to distill an informative and coherent summary from a set of input documents. Compared with single-document summarization, the input documents may contain redundant or even contradictory information (Radev, 2000).\n\nEarly high-quality multi-document summarization datasets are annotated by humans, e.g., datasets for Document Understanding Conference (DUC) and Text Analysis Conference (TAC). These datasets are too small to build neural models, and most of the early works take an extractive method, attempting to build graphs with interparagraph relations and choose the most salient textual units. The graph could be built with various information, e.g., TF-IDF similarity (Erkan and Radev, 2004), discourse relation (Mihalcea and Tarau, 2004), document-sentence two-layer relations (Wan, 2008), multi-modal (Wan and Xiao, 2009) and query information (Cai and Li, 2012). Recently, there emerge attempts to incorporate neural models, e.g., Yasunaga et al. (2017) builds a discourse graph and represents textual units upon the graph convolutional network (GCN) (Kipf and Welling, 2017), and Yin et al. (2019) adopts the entity linking technique to capture global dependencies between sentences and ranks the sentences with a neural graph-based model.\n\nIn contrast, early abstractive models using sentence-fusion and paraphrasing (Filippova and Strube, 2008;Banerjee et al., 2015;Bing et al., 2015) achieve less success. Inspired by the recent success of single-document abstractive models (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;Huang et al., 2020), some works (Liu et al., 2018;Zhang et al., 2018) try to transfer single-document models to multi-document settings to alleviate the limitations of small-scale datasets. Specifically, Liu et al. (2018) defines Wikipedia generation problem and contributes the large-scale WikiSum dataset. Fabbri et al. (",
            "score": 0.7716922301758129,
            "section_title": "Multi-document Summarization",
            "char_start_offset": 4947,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 308,
                    "end": 321,
                    "matchedPaperCorpusId": "10103200"
                },
                {
                    "start": 784,
                    "end": 807,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 828,
                    "end": 854,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 894,
                    "end": 905,
                    "matchedPaperCorpusId": "17446655"
                },
                {
                    "start": 919,
                    "end": 938,
                    "matchedPaperCorpusId": "58509157"
                },
                {
                    "start": 962,
                    "end": 980,
                    "matchedPaperCorpusId": "14383376"
                },
                {
                    "start": 1050,
                    "end": 1072,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1200,
                    "end": 1217,
                    "matchedPaperCorpusId": "199466313"
                },
                {
                    "start": 1438,
                    "end": 1466,
                    "matchedPaperCorpusId": "14909308"
                },
                {
                    "start": 1466,
                    "end": 1488,
                    "matchedPaperCorpusId": "15795297"
                },
                {
                    "start": 1488,
                    "end": 1506,
                    "matchedPaperCorpusId": "8377315"
                },
                {
                    "start": 1598,
                    "end": 1616,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1616,
                    "end": 1636,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 1636,
                    "end": 1658,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1658,
                    "end": 1677,
                    "matchedPaperCorpusId": "218487279"
                },
                {
                    "start": 1690,
                    "end": 1708,
                    "matchedPaperCorpusId": "3608234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "254246336",
            "title": "NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization",
            "text": "We investigate the performance of several baselines and state-of-the-art neural summarization models on NARRASUM. We include both extractive and abstractive models. For extractive models, we use the following methods: RANDOM selects n sentences from the document randomly. LEAD selects the top-n sentences from the document to compose the summary. This is a strong baseline for news summarization. TEXTRANK (Mihalcea and Tarau, 2004) is a graphbased extractive summarization model based on PageRank (Brin and Page, 1998) in a graph representation of sentences. LEXRANK (Erkan and Radev, 2004) is another graph-based extractive summarization model based on eigenvector centrality . HSG (Wang et al., 2020) is a heterogeneous graphbased neural extractive summarization model that uses word co-occurrence to enhance sentence contextual representation. PRESUMM (Liu and Lapata, 2019) relies on a pretrained language model to enhance the sentence representation during text encoding and extractive summarization. We choose BERT (Devlin et al., 2019), ROBERTA (Liu et al., 2019), and LONG-FORMER (Beltagy et al., 2020) as the pre-trained models. BERT and RoBERTa limit the input length to be shorter than 512 tokens, while Longformer can accept up to 4,096 tokens. \n\nFor abstractive models, we use the following pre-trained sequence-to-sequence models: BART (Lewis et al., 2020), T5 (Raffel et al., 2020), PEGASUS (Zhang et al., 2020), and LED (Beltagy et al., 2020). The input length of the first three models is limited to 512 (base version) or 1,024 (large version). LED uses Longformer as the encoder and therefore can accept up to 4,096 tokens as input.",
            "score": 0.7694018948883484,
            "section_title": "Baseline Models",
            "char_start_offset": 22609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1652
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 432,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 569,
                    "end": 591,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 685,
                    "end": 703,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 857,
                    "end": 879,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1023,
                    "end": 1044,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1352,
                    "end": 1372,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1377,
                    "end": 1398,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1408,
                    "end": 1428,
                    "matchedPaperCorpusId": "209405420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "266244733",
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "text": "Yet another set of studies, such as those employing ExpandRank, utilize information from adjacent documents to enhance the sentence graph of a target document (Yang et al., 2018).The current approach maximizes the utility of both local document information and external data for evaluating the significance of sentences (Wan & Xiao, 2010). The method also incorporates topic information to bolster summarization efficacy. A distinguishing feature of this methodology is its incorporation of adaptive depth and breadth, facilitating the handling of long-distance dependencies across multiple layers. Additionally, the model adjusts the level of connectivity for each graph neural network (GNN) layer according to the local neighborhood size of individual nodes. Moreover, topic probability distribution values are employed as edge features between topic nodes and sentence nodes, thereby enabling the latter to aggregate more pertinent topics. Separate research has introduced innovative graph models for text summarization (Ferreira et al., 2013), constructing graphs based on a four-dimensional framework: similarity, semantic similarity, co-citations, and textual information. With regard to modifications in prior values, diverse strategies exist for assessing the preliminary importance of nodes within the graph. For example, a biased TextRank algorithm has been proposed to extract focused content by modifying the random restart probability based on node relevance to the targeted task (Kazemi, P\u00e9rez-Rosas & Mihalcea, 2020). Such a modification skews the algorithm toward selecting nodes of higher relevance. This particular methodology begins by scoring sentences using a pretrained model, employing these scores to establish the preliminary importance of the sentences within the graph. \n\nIn recent years, the advent of distributed representations in natural language processing (Mikolov et al., 2013;Devlin et al., 2018) has led to a proliferation of deep learning-based techniques for automatic summarization (An et al., 2021;Liu, 2019). These methods have made strides in enhancing the efficacy of automatic document summarization, yet the task of managing long-distance dependencies continues to pose a significant challenge, and the training procedures are often resource-intensive. Notwithstanding, textual semantic information can be efficiently mapped as nodes within a graph-based framework. This configuration augments the model's information flow and elevates the coherence of the generated summaries.",
            "score": 0.7688439433050402,
            "section_title": "Heterogeneous graph for NLP",
            "char_start_offset": 7442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2297
                },
                {
                    "start": 2298,
                    "end": 2410
                },
                {
                    "start": 2411,
                    "end": 2522
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 178,
                    "matchedPaperCorpusId": "53088171"
                },
                {
                    "start": 320,
                    "end": 338,
                    "matchedPaperCorpusId": "207178257"
                },
                {
                    "start": 1023,
                    "end": 1046,
                    "matchedPaperCorpusId": "11394330"
                },
                {
                    "start": 2021,
                    "end": 2038,
                    "matchedPaperCorpusId": "233169121"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80322265625
        },
        {
            "corpus_id": "170079112",
            "title": "Hierarchical Transformers for Multi-Document Summarization",
            "text": "Most previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. \n\nAbstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005;Filippova and Strube, 2008;Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018;Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives (Ma et al., 2016;Chu and Liu, 2018).",
            "score": 0.7671665279759963,
            "section_title": "Related Work",
            "char_start_offset": 5183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 280,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 508,
                    "end": 531,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 646,
                    "end": 668,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1150,
                    "end": 1178,
                    "matchedPaperCorpusId": "16188305"
                },
                {
                    "start": 1178,
                    "end": 1205,
                    "matchedPaperCorpusId": "14909308"
                },
                {
                    "start": 1205,
                    "end": 1223,
                    "matchedPaperCorpusId": "8377315"
                },
                {
                    "start": 1325,
                    "end": 1343,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1343,
                    "end": 1363,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 1363,
                    "end": 1385,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1385,
                    "end": 1410,
                    "matchedPaperCorpusId": "4406182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9169921875
        },
        {
            "corpus_id": "202889056",
            "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
            "text": "The idea of using multiple embeddings has been employed at the word level. Kiela et al. (2018) use an attention mechanism to combine the embeddings for each word for the task of natural language inference. Xu et al. (2018); Bollegala et al. (2015) concatenate the embeddings of each word into a vector before feeding a neural network for the tasks of aspect extraction and sentiment analysis. To our knowledge, we are the first to combine multiple types of sentence embeddings. \n\nExtractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988). \n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges.",
            "score": 0.7665279306179295,
            "section_title": "Related Work",
            "char_start_offset": 23272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1957
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 94,
                    "matchedPaperCorpusId": "52166626"
                },
                {
                    "start": 206,
                    "end": 222,
                    "matchedPaperCorpusId": "44009215"
                },
                {
                    "start": 224,
                    "end": 247,
                    "matchedPaperCorpusId": "14116842"
                },
                {
                    "start": 615,
                    "end": 627,
                    "matchedPaperCorpusId": "10103200"
                },
                {
                    "start": 719,
                    "end": 741,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 945,
                    "end": 965,
                    "matchedPaperCorpusId": "5457260"
                },
                {
                    "start": 965,
                    "end": 989,
                    "matchedPaperCorpusId": "17064982"
                },
                {
                    "start": 991,
                    "end": 1010,
                    "matchedPaperCorpusId": "9849366"
                },
                {
                    "start": 1073,
                    "end": 1090,
                    "matchedPaperCorpusId": "11977708"
                },
                {
                    "start": 1209,
                    "end": 1234,
                    "matchedPaperCorpusId": "337730"
                },
                {
                    "start": 1378,
                    "end": 1403,
                    "matchedPaperCorpusId": "60514661"
                },
                {
                    "start": 1495,
                    "end": 1526,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 1665,
                    "end": 1682,
                    "matchedPaperCorpusId": "1207010"
                },
                {
                    "start": 1710,
                    "end": 1724,
                    "matchedPaperCorpusId": "5879376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9345703125
        },
        {
            "corpus_id": "259129368",
            "title": "AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization",
            "text": "\u2022 LexRank [24] is an unsupervised graph-based summarization method. It employs a ranking algorithm to determine node centrality. In LexRank, sentences are treated as nodes to form a graph with weighted edges calculated using tf-idf. In our work, following the settings from [31], we also use BERT [30] and SentiNeuron [34] vectors to calculate the adjacency matrices. \u2022 Opinosis [27] is a graph-based summarization framework that generates concise abstractive summaries of highly redundant opinions. It assumes no domain knowledge and leverages mostly the word order in the existing text. \u2022 Meansum [22] is an unsupervised neural model for multi-document summarization. \n\nIt proposes an end-to-end architecture with an auto-encoder, where the mean of input review representations decodes into a reasonable summary review without relying on any review-specific features. \u2022 Copycat [13] is a summarization model based on the pointer-generator mechanism [5]. It follows the intuition of controlling the \"amount of novelty\" during summary generation. With this intuition, they define a hierarchical variational autoencoder model to produce summaries that reflect common opinions. \u2022 QT [31] enhances the ability to control the summarization process by leveraging the properties of quantized space to generate aspect-specific summaries.",
            "score": 0.7609829837369937,
            "section_title": "Baselines",
            "char_start_offset": 24227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1330
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "227745131"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 599,
                    "end": 603,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 880,
                    "end": 884,
                    "matchedPaperCorpusId": "218883429"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1181,
                    "end": 1185,
                    "matchedPaperCorpusId": "227745131"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87255859375
        },
        {
            "corpus_id": "49210924",
            "title": "Abstract Meaning Representation for Multi-Document Summarization",
            "text": "We perform experiments on standard multi-document summarization datasets2 , prepared by the NIST researchers for DUC/TAC competitions and later exploited by various summarization studies (Nenkova and McKeown, 2011;Hong et al., 2014;Yogatama et al., 2015). A summarization instance includes generating a text summary containing 100 words or less from a cluster of 10 source documents discussing a single topic. 4 human reference summaries are provided for each cluster of documents; they are created by NIST assessors. We use the datasets from DUC-03, DUC-04, TAC-09, TAC-10, and TAC-11 in this study, containing 30/50/44/46/44 clusters of documents respectively. \n\nWe compare our AMR summarization framework with a number of extractive (ext- * ) and abstractive (abs- * ) summarization systems, including the most recent neural encoder-decoder architecture (See et al., 2017). They are described as follows. \n\n\u2022 ext-LexRank (Erkan and Radev, 2004) is a graph-based approach that computes sentence importance based on the concept of eigenvector centrality in a graph representation of source sentences; \n\n\u2022 ext-SumBasic (Vanderwende et al., 2007) is an extractive approach that assumes words occurring frequently in a document cluster have a higher chance of being included in the summary; \n\n\u2022 ext-KL-Sum (Haghighi and Vanderwende, 2009) describes a method that greedily adds sentences to the summary so long as it decreases the KL divergence; \n\n\u2022 abs-Opinosis (Ganesan et al., 2010) 1: Summary graph prediction results on the DUC-04 dataset. The scores measure how well the predicted summary graphs match reference summary graphs on nodes and edges. Reference summary graphs are created by parsing reference summary sentences using the CAMR parser. \"Oracle\" results are obtained by performing only cost-based decoding. They an upper bound for the respective approaches. \n\n\u2022 abs-Pointer-Generator (See et al., 2017) describes a neural encoder-decoder architecture.",
            "score": 0.7564904813164149,
            "section_title": "Datasets and Baselines",
            "char_start_offset": 18843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1442
                },
                {
                    "start": 1445,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 232,
                    "matchedPaperCorpusId": "16482037"
                },
                {
                    "start": 232,
                    "end": 254,
                    "matchedPaperCorpusId": "12194143"
                },
                {
                    "start": 857,
                    "end": 875,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 924,
                    "end": 946,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1119,
                    "end": 1145,
                    "matchedPaperCorpusId": "10496513"
                },
                {
                    "start": 1304,
                    "end": 1336,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 1460,
                    "end": 1482,
                    "matchedPaperCorpusId": "988010"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "145993667",
            "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis",
            "text": "In a more feature-based fashion, Zhou et al. [13] applied ESA to query-focussed text summarisation and integrated an ESA-based technique and traditional sentence features to score document sentences using machine learning algorithms. \n\nIn addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization [14,15,18,19,20,21,22,23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev [14] proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization [18,19,20]. In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. [19] and Wan [20] considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts [24,25,28,22]. Of this, Azadani et al. [22] and Plaza et al. [24] have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28].",
            "score": 0.7540794343974762,
            "section_title": "Related Works",
            "char_start_offset": 7625,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 49,
                    "matchedPaperCorpusId": "45403764"
                },
                {
                    "start": 388,
                    "end": 392,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 395,
                    "end": 398,
                    "matchedPaperCorpusId": "16493063"
                },
                {
                    "start": 398,
                    "end": 401,
                    "matchedPaperCorpusId": "35133176"
                },
                {
                    "start": 401,
                    "end": 404,
                    "matchedPaperCorpusId": "17224077"
                },
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "49217427"
                },
                {
                    "start": 559,
                    "end": 563,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1072,
                    "end": 1076,
                    "matchedPaperCorpusId": "16493063"
                },
                {
                    "start": 1076,
                    "end": 1079,
                    "matchedPaperCorpusId": "35133176"
                },
                {
                    "start": 1079,
                    "end": 1082,
                    "matchedPaperCorpusId": "17224077"
                },
                {
                    "start": 1371,
                    "end": 1375,
                    "matchedPaperCorpusId": "35133176"
                },
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "17224077"
                },
                {
                    "start": 1655,
                    "end": 1659,
                    "matchedPaperCorpusId": "5674504"
                },
                {
                    "start": 1662,
                    "end": 1665,
                    "matchedPaperCorpusId": "16459894"
                },
                {
                    "start": 1665,
                    "end": 1668,
                    "matchedPaperCorpusId": "49217427"
                },
                {
                    "start": 1694,
                    "end": 1698,
                    "matchedPaperCorpusId": "49217427"
                },
                {
                    "start": 1716,
                    "end": 1720,
                    "matchedPaperCorpusId": "5674504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "3230186",
            "title": "Extract with Order for Coherent Multi-Document Summarization",
            "text": "During a decade, several extractive approaches have been developed for automatic summary generation that implement a number of machine learning, graph-based and optimization techniques. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods of computing sentence importance for text summarization. The RegSum system (Hong and Nenkova, 2014) employs a supervised model for predicting word importance. Treating multidocument summarization as a submodular maximization problem has proven successful by (Lin and Bilmes, 2011). Unfortunately, none of the above systems care about the coherence of the final extracted summary. \n\nIn very recent works using neural network, (Cheng and Lapata, 2016) proposed an attentional encoder-decoder and (Nallapati et al., 2017) used a simple recurrent network based sequence classifier to solve the problem of extractive summarization. However, they are limited to single document settings, where sentences are implicitly ordered according to the sentence position. (Parveen and Strube, 2015;Parveen et al., 2015) arXiv:1706.06542v1 [cs.CL] 12 Jun 2017 proposed graph-based techniques to tackle coherence, which is also limited to single document summarization. Moreover, a recent work (Wang et al., 2016) actually proposed a multi-document summarization system that combines both coherence and informativeness but this system is limited to syntactic linkages between entities. \n\nIn this paper, we implement a rank based sentence selection using continuous vector representations along with key-phrases. We also model the coherence using semantic relations between entities and sentences to increase the readability.",
            "score": 0.7478110419431592,
            "section_title": "Related Work",
            "char_start_offset": 1654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1689
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 217,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 540,
                    "end": 562,
                    "matchedPaperCorpusId": "320371"
                },
                {
                    "start": 707,
                    "end": 731,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 776,
                    "end": 800,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1259,
                    "end": 1278,
                    "matchedPaperCorpusId": "2867700"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "207848011",
            "title": "Transforming Wikipedia Into Augmented Data for Query-Focused Summarization",
            "text": "A wide range of unsupervised approaches has been proposed for extractive summarization. Surface features, such as n-gram overlapping, term frequency, document frequency, sentence positions [12], sentence length [11], and TF-IDF cosine similarity [4]. Maximum Marginal Relevance (MMR) [1] greedily selects sentences and considered the trade-off between saliency and redundancy. McDonald [3] treat sentence selection as an optimization problem and solve it using Integer Linear Programming (ILP). Lin and Bilmes [35] propose using submodular functions to maximize an objective function that considers the trade-off between coverage and redundancy terms. \n\nGraph-based models make use of various inter-sentence and query-sentence relationships are also widely applied in the extractive summarization area. LexRank [2] scores sentences in a graph of sentence similarities. Wan and Xiao [4] apply manifold ranking to make use of the sentence-to-sentence and sentence-to-document relationships and the sentence-toquery relationships. We also model the above mentioned relationships, except for the cross-document relationships, like a graph at token level, which are aggregated into distributed representations of sentences. \n\nSupervised methods with machine learning techniques [8]- [10] are also used to better estimate sentence importance. In recent years, few deep neural networks based approaches have been used for extractive document summarization. Cao et al. [11] propose an attention-base model that jointly handles sentence salience ranking and query relevance ranking. It automatically generates distributed representations for sentences as well as the document. To leverage contextual relations for sentence modeling, Ren et al. [12] propose CRSum that learns sentence representations and context representations jointly with a two-level attention mechanism. Xu and Lapata [14] propose a coarse-to-fine framework which progressively estimates sentence relevance. Kulkarni et al. [36] propose SIBERT that extends HIBERT [37] to query-focused multi-document summarization by introducing a cross-document infusion layer and incorporating queries as additional contexts. The small data size is the main obstacle to develop neural models for query-focused summarization. \n\nWikipedia Wikipedia provides rich resources for exploring various natural language processing tasks.",
            "score": 0.7471777973384639,
            "section_title": "VII. RELATED WORK",
            "char_start_offset": 29955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2271
                },
                {
                    "start": 2274,
                    "end": 2374
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 193,
                    "matchedPaperCorpusId": "4500716"
                },
                {
                    "start": 211,
                    "end": 215,
                    "matchedPaperCorpusId": "8244856"
                },
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "58509157"
                },
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "2798452"
                },
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "1803710"
                },
                {
                    "start": 811,
                    "end": 814,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 882,
                    "end": 885,
                    "matchedPaperCorpusId": "58509157"
                },
                {
                    "start": 1278,
                    "end": 1282,
                    "matchedPaperCorpusId": "12090675"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "matchedPaperCorpusId": "8244856"
                },
                {
                    "start": 1735,
                    "end": 1739,
                    "matchedPaperCorpusId": "4500716"
                },
                {
                    "start": 1879,
                    "end": 1883,
                    "matchedPaperCorpusId": "226262229"
                },
                {
                    "start": 1985,
                    "end": 1989,
                    "matchedPaperCorpusId": "237458528"
                },
                {
                    "start": 2025,
                    "end": 2029,
                    "matchedPaperCorpusId": "155100086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.833984375
        },
        {
            "corpus_id": "235187219",
            "title": "Analysis of GraphSum\u2019s Attention Weights to Improve the Explainability of Multi-Document Summarization",
            "text": "Multi-document Summarization (MDS) refers to the task of providing a concise representation of multiple documents with overlapping textual content [14]. Besides transformer-based models for summarization [16,17], approaches on graph neural networks and specifically using knowledge graphs have gained popularity [8,12,20,21] while recent models combine graph neural networks with a transformer architecture [8,12].\n\nGenerally, we distinguish extractive and abstractive MDS. In extractive MDS, a model is trained to select the most relevant input sentences or paragraphs in order to create an optimal summary with regard to salience and coherence [21]. In comparison, abstractive MDS is able to generate new texts which are not present in the input documents, which improve the summary quality [8]. For extractive MDS, a graph-based representation can help to detect salient vertices, so that they are included in the generated summary to cover all important information. In abstractive MDS, salient vertices can guide the summarization process via attention mechanisms of the transformer architecture. To this end, the text from multiple documents is represented as independent, so-called textual units. The graph's vertices represent the textual units such as paragraphs and sentences, while the edges model a semantic relationship between the units like a text-based cosine similarity. Different textual units have been used for graph-based text summarization in the past, as discourse relations [8,20], sentences [3,21], or paragraphs [12].\n\nRecently, Li et al. [12] proposed GraphSum, a state of the art graph-based MDS, which utilizes a graph structure based on paragraphs as textual units to improve the transformer architecture and guide the summary generation process [12]. Paragraphs are assumed to divide text into contiguous topics. Leveraging interparagraph relations can provide the model additional information for detecting contextual relations between topics. The GraphSum model showed substantial improvements compared to strong MDS baselines such as LEAD-3 and LexRank [5]. The authors of Graph-Sum [12] considered paragraph-level textual units as vertices for their graph structure. Also Huang et al. [8] used paragraph-level representations for the graph-based MDS task",
            "score": 0.7342863415464693,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "8448956"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 646,
                    "end": 650,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1516,
                    "end": 1519,
                    "matchedPaperCorpusId": "337730"
                },
                {
                    "start": 1519,
                    "end": 1522,
                    "matchedPaperCorpusId": "6532096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "270808749",
            "title": "Abstractive Summarization Model for Summarizing Scientific Article",
            "text": "To evaluate the performance of the proposed model GBAS, experiments were conducted in a baseline approach for both abstractive and extractive methods.These methods are summarized as follows:\n\n\u2022 TextRank [40] is a graph-based method applied to extractive methods.\n\n\u2022 LexRank [41] is a graph-based method that calculates based on cosine similarity.Based on the eigenvector centrality, the graph's representation of the sentences is shown.\n\n\u2022 LSA [42] is an algebraic method for extractive methods.This model is performed in three stages: input matrix 91258 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.creation, singular value decomposition (SVD), and sentence selection.\n\n\u2022 A fine-tuned Text-to-Text Transfer Transformer (T5) model is used for text summarization trained with 4515 English news articles. 1\n\n\u2022 Billsum [45] is a T5-based fine-tuned model on the Billsum dataset which consists of US Congressional bills.\n\n\u2022 BART [43], which uses the standard sequence-tosequence Transformer architecture, is effective when the model is fine-tuned for the abstractive summarization task.The BART fine-tuned model is trained to summarize scientific articles. 2\n\n\u2022 GraphWriter [8] is a graph-based tool that generates a summary from the title for abstractive methods.",
            "score": 0.7324751041754359,
            "section_title": "B. BASELINE METHODS",
            "char_start_offset": 21987,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 190
                },
                {
                    "start": 192,
                    "end": 262
                },
                {
                    "start": 264,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 436
                },
                {
                    "start": 438,
                    "end": 495
                },
                {
                    "start": 495,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 750
                },
                {
                    "start": 752,
                    "end": 885
                },
                {
                    "start": 887,
                    "end": 997
                },
                {
                    "start": 999,
                    "end": 1163
                },
                {
                    "start": 1163,
                    "end": 1235
                },
                {
                    "start": 1237,
                    "end": 1341
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 207,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 444,
                    "end": 448,
                    "matchedPaperCorpusId": "7003631"
                },
                {
                    "start": 897,
                    "end": 901,
                    "matchedPaperCorpusId": "257687578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "8437725",
            "title": "Multi-document Summarization Using Bipartite Graphs",
            "text": "A graph-based representation of documents for summarization is adopted by various approaches. For instance, TextRank by Mihalcea and Tarau (2004) applies the PageRank algorithm (Brin and Page, 1998) to extract important sentences for single document summarization. This ranking algorithm proclaims the importance of a sentence by considering the global information which is computed recursively from the entire graph. Later, the graph is converted into a weighted graph in which the weights are calculated by measuring the similarity of sentences (Mihalcea, 2004). Similarly, in the LexRank approach (Erkan and Radev, 2004), documents are represented as a similarity graph in which the sentences are nodes and these sentences are then ranked according to centrality measures. The three centrality measures used are degree, LexRank with threshold and continuous LexRank. LexRank is a measure to calculate ranks using the similarity graph of sentences. It is also known as lexical PageRank. The summarization approach developed by Gong and Liu (2001) is also based on ranking sentences where important sentences are selected using a relevance measure and latent semantic analysis. \n\nLater, for better performance, sentences are classified according to their existence in their final summary in binary format i.e. 1 (belongs to summary) and 0 (doesn't belong to summary) (Shen et al., 2007;Gong and Liu, 2001). Here, the sentences are projected as feature vectors and conditional random fields are used to classify them. During document processing, most informative sentences are selected by the summarizer (Shen et al., 2007). Fattah and Ren (2009) also consid-ers summarization as two class classification problem. They use a genetic algorithm and mathematical regression to select appropriate weights for the features and used different classification technique for e.g. feed forward neural network, probablistic neural network and Gaussian mixture models. \n\nIn the summarization task, optimization of the three properties discussed in Section 1, relevance, non-redundancy and readability, is required. This is a global inference problem, which can be solved by two approaches. Firstly, relevance and redundancy can be optimized simultaneously.",
            "score": 0.7289567635751519,
            "section_title": "Related work",
            "char_start_offset": 3693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 563,
                    "matchedPaperCorpusId": "2100529"
                },
                {
                    "start": 600,
                    "end": 623,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1029,
                    "end": 1048,
                    "matchedPaperCorpusId": "7003631"
                },
                {
                    "start": 1368,
                    "end": 1387,
                    "matchedPaperCorpusId": "126818"
                },
                {
                    "start": 1387,
                    "end": 1406,
                    "matchedPaperCorpusId": "7003631"
                },
                {
                    "start": 1604,
                    "end": 1623,
                    "matchedPaperCorpusId": "126818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "255125107",
            "title": "GAE-ISUMM: Unsupervised Graph-based Summarization for Indian Languages",
            "text": "Document summarization aims to minimize the content in a text document and preserve the salient information. There are usually two categories of summarization techniques: Extractive [1] and Abstractive [2]. Extractive summarization extracts salient text from the document, whereas Abstractive summarization concisely paraphrases the information contained in the document. Understanding text's contextual and semantic representation is one of the main challenges for effective summaries. Traditional methods extract summaries based on hand-crafted features, including term Frequency [3], Sentence Position, and Length [4], and largely depend on the availability of natural language processing (NLP) tools.\n\nSignificant progress has been made in single document extractive summarization by using recent popular deep learning models such as recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [5], attention-based models [6]. In particular, most of these models follow encoderdecoder-based approaches to generate summaries in a supervised or unsupervised setting [7]. However, these models lack in capturing the global context and the long-distance 1 https://github.com/scsmuhio/Summarization sentence relationships in a document. Modeling long-range inter-sentence relationships with transformer-based models is still challenging [8] and requires massive computation and memory. Graph-based approaches assist in overcoming these limitations by capturing the long-term dependencies and treating the document as a graph [9].\n\nIn NLP, the recently proposed graph neural networks (GNNs) have been used to capture the cross-sentence relationships for summarization [10]. Another work proposed a Graph-Clustering framework to summarize financial text documents [11]. In particular, graph convolutional networks (GCNs) have proven their dominance in several NLP tasks such as text classification [12], and semantic role labelling [13]. GCNs are capable of drawing syntactic information, exploiting long-range multi-word relations, and have been deployed on document-word relationships [14]. More recently, Kipf et al. [15] built a graph autoencoder (GA",
            "score": 0.7276719666608085,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 185,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 582,
                    "end": 585,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 871,
                    "end": 874,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "4500716"
                },
                {
                    "start": 1083,
                    "end": 1086,
                    "matchedPaperCorpusId": "223953446"
                },
                {
                    "start": 1681,
                    "end": 1685,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 1776,
                    "end": 1780,
                    "matchedPaperCorpusId": "246081124"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7060546875
        },
        {
            "corpus_id": "227905355",
            "title": "Identifying Implicit Quotes for Unsupervised Extractive Summarization of Conversations",
            "text": "As baseline models, we employ TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), KLSum (Haghighi and Vanderwende, 2009), PacSum (Zheng and Lapata, 2019), Lead, and Random. \n\nTextRank and LexRank are graph-centrality based methods that have long been considered as strong methods for unsupervised summarization. \n\nPacSum is an improved model of TextRank, which harnesses the position of sentences as a feature. KLSum employs the Kullbuck-Leibler divergence to constrain extracted sentences and the source text to have the similar word distribution. Lead is a simple method that extracts the first few sentences from the source text but is considered as a strong baseline for the summarization of news articles. PacSum and LexRank leverage idf. We compute idf using the validation data. \n\nAs another baseline, we employ IQETextRank; the TextRank model that leverages cosine similarities of sentence vectors of IQE's Encoder as similarities between sentences. This is added to verify that the success of our model is not only because our model uses neural networks.",
            "score": 0.7229413840867054,
            "section_title": "Baseline",
            "char_start_offset": 15186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1081
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 98,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 147,
                    "end": 171,
                    "matchedPaperCorpusId": "182952892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54541015625
        },
        {
            "corpus_id": "245827982",
            "title": "An Unsupervised Masking Objective for Abstractive Multi-Document News Summarization",
            "text": "We compare to multiple popular unsupervised extractive approaches2 like Fabbri et al. ( 2019) along with a recent neural unsupervised approach. \n\nLede-k We concatenate the first k sentences from each article cluster together to form the summary, which has proven to be a straightforward, but effective baseline for summarization systems (See et al., 2017). \n\nLexRank Erkan and Radev (2004)'s LexRank computes sentence importance via eigenvector centrality in a graph representation of the sentences in each multi-document cluster. TextRank TextRank (Mihalcea and Tarau, 2004) is another graph-based method that ranks sentences for extraction, originally proposed for singledocument summarization. MMR Maximal Marginal Relevance (Carbonell and Goldstein, 1998) ranks candidate sentences based on a balance between relevance and redundancy. Top-ranked sentences are appended to form an extractive summary. MeanSum MeanSum3 is a completely unsupervised method introduced by Chu and Liu (2019) for summarizing Yelp and Amazon reviews. It is trained to simultaneously auto-encode reviews and produce a summary that is semantically similar to the input reviews using their mean-pooled representation. We remove their review rating classifier and adapt it to the Multi-News dataset. \n\nSource Doc with Best Overlap We simply return the source document with the highest bigram overlap (i.e., the document that would be masked out and predicted). This tests how effective our model is at fusing the information between source documents as opposed to just generating a new, separate document on the same topic.",
            "score": 0.7215216831601864,
            "section_title": "Unsupervised Baselines",
            "char_start_offset": 8242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 146,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1599
                }
            ],
            "ref_mentions": [
                {
                    "start": 337,
                    "end": 355,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 367,
                    "end": 389,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 549,
                    "end": 574,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 971,
                    "end": 989,
                    "matchedPaperCorpusId": "59413781"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "254952632",
            "title": "Feature Based Automatic Text Summarization Methods: A Comprehensive State-of-the-Art Survey",
            "text": "In these methods, the document is represented as a graph of sentences. The sentences represent the nodes. The edges represent the similarity between the nodes. The similarity between words can be represented using some similarity measures like cosine similarity ( [6], [19], [20]). Graph-based techniques are prevalent for extractive summarizers. Popular summarizers such as TextRank [21], LexRank [19] and [22] use a graph-based approach. The sentences are then scored based on the properties of the graph. The summary of such methods is shown in TABLE 7.",
            "score": 0.7177020926565464,
            "section_title": "4) GRAPH-BASED TECHNIQUES",
            "char_start_offset": 18988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 556
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "71594"
                },
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 275,
                    "end": 279,
                    "matchedPaperCorpusId": "192571560"
                },
                {
                    "start": 384,
                    "end": 388,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "213789058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "506350",
            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
            "text": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
            "score": 0.7135865969141049,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77880859375
        },
        {
            "corpus_id": "174799390",
            "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
            "text": "First We concatenate the first sentence of each article in a document cluster as the system summary. \n\nFor our dataset, First-k means the first k sentences from each source article will be concatenated as the summary. Due to the difference in gold summary length, we only use First-1 for DUC, as others would exceed the average summary length. \n\nLexRank Initially proposed by (Erkan and Radev, 2004), LexRank is a graph-based method for computing relative importance in extractive summarization. \n\nTextRank Introduced by (Mihalcea and Tarau, 2004), TextRank is a graph-based ranking model. Sentence importance scores are computed based on eigenvector centrality within a global graph from the corpus. MMR In addition to incorporating MMR in our pointer generator network, we use this original method as an extractive summarization baseline. When testing on DUC data, we set these extractive methods to give an output of 100 tokens and 300 tokens for Multi-News data.",
            "score": 0.7125985034386204,
            "section_title": "Baseline and Extractive Methods",
            "char_start_offset": 18653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 103,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 966
                }
            ],
            "ref_mentions": [
                {
                    "start": 376,
                    "end": 399,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 521,
                    "end": 547,
                    "matchedPaperCorpusId": "577937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7138671875
        },
        {
            "corpus_id": "214442368",
            "title": "Aspect-Aware Multimodal Summarization for Chinese E-Commerce Products",
            "text": "We compare several text-based extractive and abstractive summarization methods. \n\nLead extracts the first 80 characters (according to the length of the ground-truth summary) as the summary. LexRank (Erkan and Radev 2011) is an unsupervised graph-based ranking algorithm for extractive summarization. Seq2seq is a standard text-based seq2seq model with an attention mechanism. Pointer-Generator (PG) is a text-based seq2seq network model with an attention and copying mechanism. MASS (Song et al. 2019) is the-state-of-the-art method for text summarization based on masked language modeling.",
            "score": 0.7124328093617603,
            "section_title": "Experiment Comparative Methods",
            "char_start_offset": 18800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 82,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 590
                }
            ],
            "ref_mentions": [
                {
                    "start": 483,
                    "end": 500,
                    "matchedPaperCorpusId": "146808476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59716796875
        },
        {
            "corpus_id": "222133287",
            "title": "Corpora Evaluation and System Bias detection in Multi Document Summarization",
            "text": "To identify bias in system-generated summaries, we study a few non-neural extractive and neural abstractive summarization systems, which are extensively used for multi-document summarization. \n\n\u2022 LexRank (Erkan and Radev, 2004) is a graph based algorithm that computes the importance of a sentence using the concept of eigen vector centrality in a graphical representation of text. \n\n\u2022 TextRank (Mihalcea and Tarau, 2004) runs a modified version of PageRank (Brin and Page, 1998) on a weighted graph, consisting of nodes as sentences and edges as similarities between sentences. \n\n\u2022 Maximal Marginal Relevance (MMR) (Carbinell and Goldstein, 2017) is an extractive summarization system that ranks sentences based on higher relevance while considering the novelty of the sentence to reduce redundancy. \n\n\u2022 ICSISumm (Gillick et al., 2008) optimizes the summary coverage by adopting a linear optimization framework. It finds a globally optimal summary using the most important concepts covered in the document. \n\n\u2022 Pointer Generator (PG) network (See et al., 2017) is a sequence-to-sequence summarization model which allows both copying words from the source by pointing or generating words from a fixed vocabulary. \n\n\u2022 Pointer Generator-MMR: PG-MMR (Lebanoff et al., 2018) uses MMR along with PG for better coverage and redundancy mitigation. \n\n\u2022 Hi-Map: Hierarchical MMR-Attention PG model (Fabbri et al., 2019) extends the work of PG and MMR. MMR scores are calculated at word level and incorporated in the attention weights for a better summary generation. \n\n\u2022 Bottom-up Abstractive Summarization (CopyTransformer) (Gehrmann et al., 2018) uses transformer parameters proposed by (Vaswani et al., 2017); but one of the attention heads chosen randomly acts as a copy distribution. 4 Inferences from Corpus Metrics \n\n\u2022 News derived corpora show a strong layout bias where significant reference information is contained in the introductory sentences of the candidate documents (Fig. 2).",
            "score": 0.7110127950040511,
            "section_title": "MDS Systems",
            "char_start_offset": 11664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1812
                },
                {
                    "start": 1815,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 226,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 395,
                    "end": 420,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 458,
                    "end": 479,
                    "matchedPaperCorpusId": "7587743"
                },
                {
                    "start": 1043,
                    "end": 1061,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1247,
                    "end": 1270,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 1616,
                    "end": 1639,
                    "matchedPaperCorpusId": "52144157"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "Table 3 and Table 4 demonstrate the ROUGE F-scores for our proposed model and cutting-edge summarization systems on the arXiv and PubMed datasets respectively. The results are categorized into six distinct segments for clarity. Traditional extractive summarization models, including SumBasic, LexRank, LSA, and Oracle, form the first group. As expected, Oracle ranks the highest across all metrics due to its optimized approach of selecting sentences that have the maximum overlap with reference summaries. Neural abstractive summarization models form the second group, excelling at rephrasing content but often struggling with long documents due to difficulties in capturing document-wide semantics. Neural extractive summarization models, the third group, demonstrate improvements over traditional approaches but lack mechanisms to incorporate global contextual information effectively. The fourth segment consists of BERT-based extractive summarization models, including BERTSUMEXT as well as MatchSum models. However, these models have issues of fixed input length and quadratic computational complexity of BERT's self-attention mechanism, which are unsuitable for large scientific documents. The fifth segment includes neural graph-based models that comprise Topic-GraphSum and HeterGraphLongSum. The core idea behind this research is the use of heterogeneous graphs to capture the inter-sentence relations and inter-document semantics in order to achieve remarkable performance. \n\nLastly, the proposed Global and Local Context-aware Heterogeneous Graph Neural Network yields promising results on both the arXiv and PubMed datasets for the task of summarizing lengthy scientific documents. It efficiently fuses the global and local semantic knowledge within a graph-based structure augmented with word, sentence, and topic nodes. \n\nAdditionally, it utilizes relatively fewer resources while prioritizing scalability. \n\nOur model consistently outperforms traditional extractive models and achieves higher ROUGE scores compared to both neural abstractive and extractive models. This success is largely due to its potential to handle lengthy scientific documents beyond the input length restrictions. By leveraging GloVe embeddings, CNNs, and BiLSTMs, the model effectively captures the local dependencies as well as the overall context.",
            "score": 0.7089048828309055,
            "section_title": "1) PERFORMANCE INSIGHT ON ARXIV AND PUBMED",
            "char_start_offset": 43198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53857421875
        },
        {
            "corpus_id": "219036690",
            "title": "Discourse-Aware Neural Extractive Text Summarization",
            "text": "The use of discourse theory for text summarization has been explored before. Louis et al. (2010) examined the benefit of graph structure provided by discourse relations for text summarization. Hirao et al. (2013); Yoshida et al. (2014) formulated the summarization problem as the trimming of the document discourse tree. Durrett et al. (2016) presented a system of sentence extraction and compression with ILP methods using discourse structure. Li et al. (2016) demonstrated that using EDUs as units of content selection leads to stronger summarization performance. Compared with them, our proposed method is the first neural end-to-end summarization model using EDUs as the selection basis. \n\nGraph-based Summarization Graph approach has been explored in text summarization over decades. LexRank introduced a stochastic graphbased method for computing relative importance of textual units (Erkan and Radev, 2004). Yasunaga et al. (2017) employed a GCN on the relation graphs with sentence embeddings obtained from RNN. Tan et al. (2017) also proposed graphbased attention in abstractive summarization model. Fernandes et al. (2018) developed a framework to reason long-distance relationships for text summarization.",
            "score": 0.7037248146557771,
            "section_title": "Discourse & Summarization",
            "char_start_offset": 23346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1216
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 96,
                    "matchedPaperCorpusId": "2810111"
                },
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "18505561"
                },
                {
                    "start": 214,
                    "end": 235,
                    "matchedPaperCorpusId": "6181569"
                },
                {
                    "start": 321,
                    "end": 342,
                    "matchedPaperCorpusId": "5125975"
                },
                {
                    "start": 445,
                    "end": 461,
                    "matchedPaperCorpusId": "3937849"
                },
                {
                    "start": 890,
                    "end": 913,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 915,
                    "end": 937,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1020,
                    "end": 1037,
                    "matchedPaperCorpusId": "26698484"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "This category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization. \n\nTable 2 in this study shows the Rouge F-scores for various models. The first segment of Table 2 demonstrates results for traditional summarization models and Oracle. The second segment presents the results of neural abstractive summarization models. The third segment reveal the results of Match-Sum, a sophisticated BERT-based summarizing model. The fourth segment illustrates the results achieved by recent advanced graph-based models for abstractive summarization. The last segment of Table 2 presents the results of our proposed model. \n\nOur model significantly outperformed conventional extractive summarization models on all ROUGE metrics, as shown in Table 2. This achievement extends to state-of-the-art neural abstractive and extractive models, where our approach also achieved higher ROUGE scores. This improvement highlights the benefit of integrating global semantic information with a dedicated graph layer. This layer facilitates the model's ability to generate summaries by considering the broader context within the document. \n\nAdditionally, traditional sequence-to-sequence (seq2seq) models with attention and pointer networks often struggle with lengthy scientific documents. This limitation arises from the challenges encoders face in capturing long-range dependencies within long texts [57]. Our model overcomes this limitation. Furthermore, our model surpasses the advanced BERT-based Match-Sum model, which experiences performance drops on the PubMed scientific dataset. Match-Sum's difficulty lies in grasping semantic and global information, hindering its ability to interpret the meaning of sentences and summaries. In contrast, our model can learn semantic information and leverage latent topics to focus on salient in long documents. \n\nOur model's performance is compared with highly advanced neural graph-based models for both extractive and abstractive summarization, which utilize rich semantic information. Our model achieved near-identical results in R-1, comparable results in R-2, and surpassed Topic-GraphSum in R-L.",
            "score": 0.7035774339401363,
            "section_title": "5) GRAPH BASED MODELS",
            "char_start_offset": 32157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2007
                },
                {
                    "start": 2010,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "235097475"
                },
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "250012667"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "235097475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77001953125
        },
        {
            "corpus_id": "59599960",
            "title": "Query-oriented text summarization based on hypergraph transversals",
            "text": "While early models focused on the task of single document summarization, recent systems generally produce summaries of corpora of documents [11]. Similarly, the focus has shifted from generic summarization to the more realistic task of query-oriented summarization, in which a summary is produced with the essential information contained in a corpus that is also relevant to a user-defined query [12]. \n\nSummarization systems are further divided into two classes, namely abstractive and extractive models. Extractive summarizers identify relevant sentences in the original corpus and produce summaries by aggregating these sentences [11]. In contrast, an abstractive summarizer identifies conceptual information in the corpus and reformulates a summary from scratch [12]. Since abstractive approaches require advanced natural language processing, the majority of existing summarization systems consist of extractive models. \n\nExtractive summarizers differ in the method used to identify relevant sentences, which leads to a classification of models as either feature-based or graph-based approaches. Feature-based methods represent the sentences with a set of predefined features such as the sentence position, the sentence length or the presence of cue phrases [13]. Then, they train a model to compute relevance scores for the sentences based on their features. Since feature-based approaches generally require datasets with labelled sentences which are hard to produce [12], unsupervised graph-based methods have attracted growing interest in recent years. \n\nGraph-based summarizers represent the sentences of a corpus as the nodes of a graph with the edges modelling relationships of similarity between the sentences [1]. Then, graph-based algorithms are applied to identify relevant sentences. The models generally differ in the type of relationship captured by the graph or in the sentence selection approach. Most graph-based models define the edges connecting sentences based on the co-occurrence of terms in pairs of sentences [1,3,4]. Then, important sentences are identified either based on node ranking algorithms, or using a global optimization approach. Methods based on node ranking compute individual relevance scores for the sentences and build summaries with highly scored sentences. The earliest such summarizer, LexRank [1], applies the PageRank algorithm to compute sentence scores.",
            "score": 0.6988661088861764,
            "section_title": "Background and related work",
            "char_start_offset": 7277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2301
                },
                {
                    "start": 2302,
                    "end": 2403
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "47179013"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "556431"
                },
                {
                    "start": 633,
                    "end": 637,
                    "matchedPaperCorpusId": "47179013"
                },
                {
                    "start": 766,
                    "end": 770,
                    "matchedPaperCorpusId": "556431"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "7992772"
                },
                {
                    "start": 1472,
                    "end": 1476,
                    "matchedPaperCorpusId": "556431"
                },
                {
                    "start": 1721,
                    "end": 1724,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 2036,
                    "end": 2039,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 2039,
                    "end": 2041,
                    "matchedPaperCorpusId": "10274362"
                },
                {
                    "start": 2041,
                    "end": 2043,
                    "matchedPaperCorpusId": "27105029"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "259262067",
            "title": "Vietnamese multi-document summary using subgraph selection approach - VLSP 2022 AbMuSu Shared Task",
            "text": "In multi-document summarization (MDS), the input is a set of documents, and the output is a summary that describes important information in a coherent and non-redundant manner. It is a complex problem that has gained attention from the research community. In recent times, there have been significant improvements in MDS due to the availability of MDS datasets and advances in pretraining approaches. Extractive and abstractive summarizing are two well-known methods for multi-document summarization. Abstractive summarizing methods try to succinctly summarize the substance of the texts, whereas extractive summarization systems aim to extract prominent snippets, phrases, or sections from documents. \n\nGraphs capturing relationships between textual units are of great benefit to MDS, which can help create more concise, informative, and coherent summaries from multiple sources documents. Furthermore, graphs can be easily constructed by representing sentences or paragraph as graph nodes and edges. LexRank (Erkan and Radev, 2004) computes sentence importance based on a lexical similarity graph of sentences. Graph representations of documents such as discourse graph based on discourse relations (Christensen et al., 2013). \n\nIn the VLSP 2022 Evaluation Campaign, Vietnamese Abstractive multi-document summarization (AbMuSu) Shared Task (Tran et al., 2022) is proposed to promote the development of research on abstractive multi-document summarization for Vietnamese text. The Vietnamese abstractive multidocument summarization task's goal is to develop summarizing systems that could automatically generate abstractive summaries for a collection of documents on a given topic. The model produces an abstractive summary that is connected to the input, which is a collection of news items on the same topic. In this paper, we approach by switching back to the graph-based extractive problem, specifically, extracting summaries for multi-document by selecting sub-graphs based on the constructed graph. Inspired by study (Chen et al., 2021), we exploit more edge weights representing the relationship of sentences in the same document and train and improve the model for Vietnamese dataset.",
            "score": 0.6986807413560044,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 1010,
                    "end": 1032,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1201,
                    "end": 1227,
                    "matchedPaperCorpusId": "337730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "259292631",
            "title": "Cogito Ergo Summ: Abstractive Summarization of Biomedical Papers via Semantic Parsing Graphs and Consistency Rewards",
            "text": "Abstractive Summarization Sequence-to-sequence architectures founded on self-supervised pre-trained transformers have been responsible for a keen impetus in implicitly learning abstractive summarization procedures (Zhang et al. 2020a), comprising multi-document (Moro et al. 2022) and low-resource settings (Moro and Ragazzi 2022;Moro et al. 2023). Nevertheless, modern solutions are highly prone to hallucinating content (Cao et al. 2018;Maynez et al. 2020) or falling back on extraction (See, Liu, and Manning 2017). Gaining an understanding of semantics and context is becoming a prerogative, but a model trained purely on form cannot learn meaning (Bender and Koller 2020)-even with more data and huge architectural dimensions. \n\nGraph-enhanced Summarization Graph structures have long been studied for implementing summarization subtasks (i.e., information extraction, content selection, sur-1 Inspired by the first principle of Ren\u00e9 Descartes's philosophy, we coin the name Cogito Ergo Summ to emphasize the researched neural network capacity of \"thinking\" about the inner semantics of the text-via joint text-graph reasoning-before summarizing. face realization), registering distinct benefits depending on their composition. Early techniques for extractive summarization build intra- (Mihalcea and Tarau 2004) and interdocument (Wan 2008) cosine similarity connectivity networks to identify salient sentences. Late hybrid neural systems mainly stand on the shoulders of run-of-the-mill GNNs (Wu et al. 2021a), exploiting graph-based attention (Tan, Wan, and Xiao 2017) and heterogeneous word-/sentencelevel nodes (Wang et al. 2020). As for abstractive summarization, the community has attempted a medley of graphical document representations, from dependency (Wu et al. 2021b), sentiment (Moro et al. 2018), and coreference links (Balachandran et al. 2021) to latent co-occurrences (Frisoni and Moro 2020), discourse relations (Li et al. 2020; Chen and Yang 2021) and citation networks (An et al. 2021).",
            "score": 0.6968953175772189,
            "section_title": "Related Work",
            "char_start_offset": 5255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 234,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 422,
                    "end": 439,
                    "matchedPaperCorpusId": "19198109"
                },
                {
                    "start": 489,
                    "end": 517,
                    "matchedPaperCorpusId": "202541447"
                },
                {
                    "start": 652,
                    "end": 676,
                    "matchedPaperCorpusId": "211029226"
                },
                {
                    "start": 1292,
                    "end": 1316,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1499,
                    "end": 1515,
                    "matchedPaperCorpusId": "235187330"
                },
                {
                    "start": 1551,
                    "end": 1576,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 1621,
                    "end": 1639,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.591796875
        },
        {
            "corpus_id": "235097309",
            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
            "text": "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani and Bloedorn, 1997;Radev and McKeown, 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. Mani and Bloedorn (1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and McKeown (1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, Erkan and Radev (2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). Dasgupta et al. (2013) incorporated dependency graph features into their sentence relation graphs. Baralis et al. (2013) built graphs over sets of terms, rather than sentences. Li et al. (2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. Liu et al. (2015) and Liao et al. (2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com",
            "score": 0.6949899016033594,
            "section_title": "Related Work",
            "char_start_offset": 4341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 140,
                    "matchedPaperCorpusId": "6025826"
                },
                {
                    "start": 140,
                    "end": 164,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 373,
                    "end": 397,
                    "matchedPaperCorpusId": "6025826"
                },
                {
                    "start": 558,
                    "end": 572,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 896,
                    "end": 918,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1289,
                    "end": 1311,
                    "matchedPaperCorpusId": "16587947"
                },
                {
                    "start": 1388,
                    "end": 1409,
                    "matchedPaperCorpusId": "15709889"
                },
                {
                    "start": 1466,
                    "end": 1482,
                    "matchedPaperCorpusId": "6788641"
                },
                {
                    "start": 1614,
                    "end": 1631,
                    "matchedPaperCorpusId": "5001921"
                },
                {
                    "start": 1636,
                    "end": 1654,
                    "matchedPaperCorpusId": "49210924"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "249097545",
            "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning",
            "text": "We evaluate our models against popular and state-of-the-art multidocument summarization baselines and related work generation techniques. We adopt the LEAD baseline which selects the first sentence of each reference as the summary as a baseline, and ORACLE as an upper bound of extractive summarization systems. \n\nOur extractive baselines include: LexRank [11]: a graph-based extractive model that computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. \n\nNES [37]: a neural data-driven summarizer that extracts sentences from reference papers based on target paper. A joint contextdriven attention mechanism is proposed to measure the contextual relevance within full texts. \n\nBertSumEXT [25]: an extractive summarization model with BERT encoder, which is able to express the semantics of a document and obtain representations for its sentences. \n\nAbstractive baseline models include: BertSumABS [25]: an abstractive summarization system built on BERT with a new fine-tuning schedule that adopts different optimizers for the encoder and the decoder. \n\nMGSum [20]: a multi-granularity interaction network for abstractive multi-document summarization. The multi granularities include document-level, sentence-level, and word-level. \n\nEMS [42]: an entity-aware model for abstractive multi-document summarization with BERT encoder. It augments the classical Transformer based encoder-decoder framework with a graph consisting of paragraph nodes and entity nodes. EMS uses the same keyphrase extraction tool as ours. \n\nRRG [5]: an abstractive related work generator based on reference papers. It augments the RNN encoder with a relation-aware graph between multiple references. The relation graph and the document representation are refined iteratively. \n\nThe essential difference between our model and baselines is that we explicitly model the relationship between the target work and the references. To make a fair comparison, for baselines that have relatively good performance, we give an additional with.Target (w.T ) version which takes references and target paper as input. Thus, the baseline models treat the references and the target paper as multiple inputs without distinguishing them.",
            "score": 0.6943763815392784,
            "section_title": "Baselines",
            "char_start_offset": 23597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2252
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "53083244"
                },
                {
                    "start": 749,
                    "end": 753,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 957,
                    "end": 961,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1119,
                    "end": 1123,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 1579,
                    "end": 1582,
                    "matchedPaperCorpusId": "236460044"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "235097309",
            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
            "text": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.",
            "score": 0.6942756379677875,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "Conventional truncation models frequently lose crucial meaning, whereas hierarchical and distributed representations fail to effectively capture both global and local contextual dependencies. Graph-based models that incorporate heterogeneous structures aim to improve interconnectivity, but they still face challenges in effectively modeling fine-grained semantic relationships. \n\nDomain-specific approaches like SciBERTSUM and citation graph-based methods often fail to generalize effectively across diverse document types. Additionally, cluster-based and pre-training methods, despite their potential, involve substantial computational overhead, making them impractical for real-time summarization. These challenges emphasize the need for a more efficient and scalable solution that combines semantic-rich representations with a graph-based framework to advance extractive summarization for long scientific documents. \n\nThis study aims to fill this gap by proposing a computationally efficient Heterogeneous Graph Neural Network (HGNN) for extractive summarization of scientific texts. The framework combines GloVe embeddings, CNNs, and BiLSTMs for local and long-range dependencies, enhanced LDA for global context, and a Graph Attention Network (GAT) to refine node representations in a heterogeneous graph. This is the first work to integrate LDA, CNN, and BiLSTM encoders in a GAT-based model for scientific text summarization.",
            "score": 0.694085715818653,
            "section_title": "C. LONG DOCUMENT EXTRACTIVE SUMMARIZATION",
            "char_start_offset": 18898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1433
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67333984375
        },
        {
            "corpus_id": "216552978",
            "title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization",
            "text": "Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016;Narayan et al., 2018;Arumae and Liu, 2018;Zhong et al., 2019a;Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. \n\nTo effectively extract the summary-worthy sentences from a document, a core step is to model the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016;Nallapati et al., 2017;Zhou et al., 2018). However, RNNs-based models are usually hard to capture sentence-level long-distance dependency, especially in the case of the long document or multidocuments. One more intuitive way is to model the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts have been made in various ways. Early traditional work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Recently, some works account for discourse inter-sentential relationships when building summarization graphs, such as the Approximate Discourse Graph (ADG) with sentence personalization features (Yasunaga et al., 2017) and Rhetorical Structure Theory (RST) graph (Xu et al., 2019). However, they usually rely on external tools and need to take account of the error propagation problem. A more straightforward way is to create a sentence-level fully-connected graph. To some extent, the Transformer encoder (Vaswani et al., 2017) used in recent work (Zhong et al., 2019a;Liu and Lapata, 2019b) can be classified into this type, which learns the pairwise interaction between sentences. Despite their success, how to construct an effective graph structure for summarization remains an open question.",
            "score": 0.6874870675631514,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 249,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 249,
                    "end": 270,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 270,
                    "end": 291,
                    "matchedPaperCorpusId": "44064978"
                },
                {
                    "start": 291,
                    "end": 311,
                    "matchedPaperCorpusId": "195833318"
                },
                {
                    "start": 311,
                    "end": 333,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 699,
                    "end": 723,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 723,
                    "end": 746,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 746,
                    "end": 764,
                    "matchedPaperCorpusId": "49656757"
                },
                {
                    "start": 1256,
                    "end": 1279,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1293,
                    "end": 1319,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1827,
                    "end": 1849,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1870,
                    "end": 1891,
                    "matchedPaperCorpusId": "195833318"
                },
                {
                    "start": 1891,
                    "end": 1913,
                    "matchedPaperCorpusId": "201304248"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78564453125
        },
        {
            "corpus_id": "269791107",
            "title": "Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level",
            "text": "Graph neural networks have been widely used for extractive summarization due to their flexibility and scalability.Dong et al. (2020) proposed an unsupervised graph-based model that combines both sentence similarity and hierarchical discourse structure to rank sentences.Cui et al. (2020) injected latent topic information into graph neural networks to further improve performance.Wang et al. (2020) constructed a word-document heterogeneous graph using word nodes as intermediate to connect sentences.Zhang et al. (2022) proposed a hypergraph transformer to model long-distance dependency while emphasizing the importance of high-order inter-sentence relations in extraction summarization.Our paper follows this line of work, but the main difference is that our approach combines both intra-sentence relations and high-order crosssentence relations and efficiently leverages the hierarchical discourse structure of scientific documents to learn sentence representations that incorporate both local and global information.",
            "score": 0.686354921189123,
            "section_title": "Graph based Summarization",
            "char_start_offset": 7172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 380
                },
                {
                    "start": 380,
                    "end": 501
                },
                {
                    "start": 501,
                    "end": 689
                },
                {
                    "start": 689,
                    "end": 1021
                }
            ],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 398,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "26698484",
            "title": "Abstractive Document Summarization with a Graph-Based Attentional Neural Model",
            "text": "In this paper we tackle the challenging task of abstractive document summarization, which is still less investigated to date. We study the difficulty of the abstractive document summarization task, and address the need of finding salient content from the original document, which is overlooked by previous studies. We propose a novel graph-based attention mechanism in a hierarchical encoderdecoder framework, and propose a hierarchical beam search algorithm to generate multi-sentence summary. Extensive experiments verify the effectiveness of the proposed method. Experimental results on two large-scale datasets demonstrate our method achieves state-of-the-art abstractive document summarization performance. It is also able to achieve competitive results with state-of-the-art neural extractive summarization models. \n\nThere is lots of future work we can do. An appealing direction is to investigate the neural abstractive method on the multi-document summarization task, which is more challenging and lacks training data. Further endeavor may be needed.",
            "score": 0.6856581686881887,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 30320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1058
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73291015625
        },
        {
            "corpus_id": "275522673",
            "title": "Advanced Text Summarization Model Incorporating NLP Techniques and Feature-Based Scoring",
            "text": "Concise summaries, with sentence scoring being the prevalent method in current practices. These scoring methods fall into three categories: word scoring, sentence scoring, and graph scoring [22]. Word scoring evaluates sentences based on the occurrence frequency of significant words like proper nouns, places, and objects, assigning higher scores to sentences containing these terms [23], [24], [25], [26]. Sentence scoring considers the formatting attributes of words (e.g., bold, italic, underline) [27] and prioritizes sentences beginning with specific phrases like ''Briefly,'' ''Finally,'' or ''As a result,'' deeming them as significant [24]. \n\nFurthermore, the alignment of sentences with the text title boosts their importance in the summary [28], with longer sentences often receiving higher scores [27], [29]. Sentence positioning and the inclusion of numerical data also influence their scoring [24], [26], [30]. \n\nExtractive summarization approaches have been developed to aid individuals with reading challenges, focusing on design and evaluation [31]. Graph-based representations, such as TextRank and LexRank, leverage text intersections for summarization inspired by the PageRank algorithm to identify key sentences through mutual information between term and sentence sets [32], [33]. \n\nThe multi-layered document, sentence, and word representations [34], as well as graph-based document structuring to uncover textual relationships and evaluate against humangenerated summaries, have been explored [35]. Graph-based methods providing semantic coherence by mapping terms and semantic links have been employed, where graph diameters indicate the strength of connections within the text [36], [37]. \n\nIn the biomedical domain, summaries have been generated using concepts and relationships from the Unified Medical Language System, applying the PageRank algorithm to the resultant graph [38]. Enhanced random walk models have also been proposed for summarization. \n\nDespite the focus on extractive summarization, abstractive approaches using RNNs and other methods have been investigated, expanding the scope of text summarization research [39], [40], [41].",
            "score": 0.6840316500907206,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 7815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 649
                },
                {
                    "start": 652,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1302
                },
                {
                    "start": 1305,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 1979
                },
                {
                    "start": 1982,
                    "end": 2173
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 194,
                    "matchedPaperCorpusId": "258476139"
                },
                {
                    "start": 384,
                    "end": 388,
                    "matchedPaperCorpusId": "229465153"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "245200135"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "49217427"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "238807770"
                },
                {
                    "start": 502,
                    "end": 506,
                    "matchedPaperCorpusId": "229944054"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "245200135"
                },
                {
                    "start": 751,
                    "end": 755,
                    "matchedPaperCorpusId": "222317303"
                },
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "229944054"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "209536754"
                },
                {
                    "start": 907,
                    "end": 911,
                    "matchedPaperCorpusId": "245200135"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "238807770"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "261472680"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "213613078"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "253380308"
                },
                {
                    "start": 1517,
                    "end": 1521,
                    "matchedPaperCorpusId": "250572619"
                },
                {
                    "start": 1703,
                    "end": 1707,
                    "matchedPaperCorpusId": "247958026"
                },
                {
                    "start": 1709,
                    "end": 1713,
                    "matchedPaperCorpusId": "233187213"
                },
                {
                    "start": 1903,
                    "end": 1907,
                    "matchedPaperCorpusId": "221340611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "251081212",
            "title": "Attention based Recurrent Neural Network for Nepali Text Summarization",
            "text": "This approach is for generating a query-based summary. A centroid-based text summarization technique works for single as well as multi-document summarization (Radev et al., 2004). A popular extractive text summarization technique called LexRank is an unsupervised approach to text summarization based on graph-based centrality scoring of sentences (Erkan & Radev, 2004). \n\nBesides extractive text summarization, there is increasing attention from researchers on abstractive text summarization in the recent past (Yeasmin et al., 2017), especially after the success of deep neural networks in various applications such as computer vision and image processing (Mishra & Shahi, 2021). Initially, the text summarization task was investigated to generate news headline generation, abstract of research papers, and so on (Edmundson, 1969;DeJong, 1977;Carbonell & Goldstein, 1998). A text summarization method inspired by statistical machine translation was proposed by Banko et al. (2000) using a news corpus of headline-article pairs. They generated the headlines for the news article even shorter than one sentence using statistical term selection and term ordering jointly which their model learns directly from the training corpus. Similarly, a neural network-based approach with a larger dataset of headline-article pairs was implemented by Rush et al., 2015). They achieve state-of-the-art performance on both DUC-2004 and Gigaword datasets which have two sentence level summaries. A further performance boost was reported in Nallapati et al. ( 2016) using a sequence to a sequence-based attentional encoder-decoder neural network. This work was based on an attentional recurrent neural network implemented for a machine translation task (Bahdanau et al., 2014). Furthermore, an LSTM network with attention was implemented for news headline generation (Lopyrev, 2015) A few works on text summarization using the generative adversarial network are also reported in the literature. \n\nFor instance, a text summarization using GAN is proposed by Lin (2004). In this work, they have designed and trained both a generator and discriminator in an endto-end fashion.",
            "score": 0.6823201493582692,
            "section_title": "RELATED WORK",
            "char_start_offset": 7000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1979
                },
                {
                    "start": 1982,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 178,
                    "matchedPaperCorpusId": "10151424"
                },
                {
                    "start": 348,
                    "end": 369,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 512,
                    "end": 534,
                    "matchedPaperCorpusId": "212449070"
                },
                {
                    "start": 658,
                    "end": 680,
                    "matchedPaperCorpusId": "238219952"
                },
                {
                    "start": 815,
                    "end": 832,
                    "matchedPaperCorpusId": "1177942"
                },
                {
                    "start": 845,
                    "end": 873,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 963,
                    "end": 982,
                    "matchedPaperCorpusId": "9952653"
                },
                {
                    "start": 1340,
                    "end": 1358,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 2042,
                    "end": 2052,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61181640625
        },
        {
            "corpus_id": "259138479",
            "title": "Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks",
            "text": "Inspired by human reading habits, Jeon and Strube (2020b) investigate a model to measure a document's coherence by incrementally interpreting sentences. Our work is in line with the above approaches to learning a coherence model based on neural networks. The main difference is that the above neural models focus on extracting features within a single document, whereas our graph-based approach aims to study the effectiveness of correlations between documents. Specifically, rooting on the linguistic definition of text coherence, we model the correlation from the perspective of structural similarities between documents. \n\nGNN-based Methods for NLP. Graph neural networks are a family of neural networks that operate naturally on graphs. Many NLP problems can be expressed with a graph structure, so there is a surge of interest in applying GNNs for NLP tasks. Marcheggiani and Titov (2017) present a Syntactic GCN to learn latent feature representations of words in a sentence over dependency trees for Semantic Role Labeling. Yasunaga et al. (2017) propose a GCN-based multi-document summarization system that exploits the sentence relation information encoded in graph representations of document \n\n+'()'($' ,-./0* +%1,-./023-'4%'($5'* \"#$%&'()6*%1,-./02,-./0 7892'($#:'-;-':5$)5#( \n\nFigure 2: Overview of the proposed approach. Our method identifies a document's graph structure, converts the graph into a subgraph set, constructs a corpus-level graph based on the shared subgraphs between structurally similar documents, and finally encodes those connections with a GCN. For simplicity, we only show three documents and five subgraphs and limit the number of sentences in a document. s u , d i , and g j denote the u-th sentence in a document, the i-th document in the training corpus, and the j-th defined subgraph, respectively. \n\nclusters.",
            "score": 0.6814217354748069,
            "section_title": "Related Work",
            "char_start_offset": 6214,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 57,
                    "matchedPaperCorpusId": "227231546"
                },
                {
                    "start": 1031,
                    "end": 1053,
                    "matchedPaperCorpusId": "6532096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80078125
        },
        {
            "corpus_id": "273784877",
            "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
            "text": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge Luca Ragazzi , Gianluca Moro , Lorenzo Valgimigli, and Riccardo Fiorani Abstract-Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-SEEK-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-SEEK-2 across seven datasets spanning various domains-including news articles, lawsuits, government reports, and scientific texts-under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics. \n\nIndex Terms-Multi-document summarization, graphenhanced transformer, few-shot learning.",
            "score": 0.6801524546308297,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1845
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "239768911",
            "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
            "text": "Most previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Christensen et al. (2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, Yasunaga et al. (2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. Li et al. (2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. Wu et al. (2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries.",
            "score": 0.6791372350565622,
            "section_title": "Graph-based Summarization",
            "char_start_offset": 25715,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 175,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 324,
                    "end": 349,
                    "matchedPaperCorpusId": "337730"
                },
                {
                    "start": 515,
                    "end": 537,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 846,
                    "end": 862,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1079,
                    "end": 1095,
                    "matchedPaperCorpusId": "235187330"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "266244733",
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "text": "The significance and interrelationships among nodes of varying granularities are ascertained through mechanisms of adaptive width and depth. The majority of related studies concentrate on utilizing the encoder-decoder framework, often employing recurrent neural networks for this purpose (Cheng & Lapata, 2016;Nallapati, Zhai & Zhou, 2017;Zhou et al., 2018). Additionally, the adoption of pre-trained language models has gained traction for enabling contextual word representations in summarization tasks (Zhou et al., 2018;Liu & Lapata, 2019b;Zhong et al., 2019b). \n\nGraph structures serve as another intuitive approach for extractive summarization, offering an enhanced capacity to exploit statistical or linguistic correlations between sentences. Initial efforts in this domain were centered on document graphs assembled based on content similarity among sentences, as exemplified by algorithms like LexRank (Erkan & Radev, 2004) and TextRank (Mihalcea & Tarau, 2004). More recent studies have ventured into the dissection of documents into nodes at diversified levels of granularity, employing heterogeneous graph neural networks for this objective.",
            "score": 0.6776616384952274,
            "section_title": "Extractive document summarization",
            "char_start_offset": 4223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1153
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 339,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 911,
                    "end": 932,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 946,
                    "end": 970,
                    "matchedPaperCorpusId": "577937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80908203125
        },
        {
            "corpus_id": "271903777",
            "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization",
            "text": "Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise. Compared with single-document summarization, it's more challenging because an increasing number of input documents will make the source information more redundant and scattered. It also has practical significance, for example, key information of multiple news articles can be generated efficiently. Multi-document summarization approaches can also be applied in other scenarios, such as extracting opinions from social media. \n\nAs the application of MDS becomes more and more widespread, various approaches have been proposed. Non-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary. The main drawback of such approaches is that they retain only a subset of key sentences, which can lead to information loss and may not capture finegrained details. \n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020). Despite their capability in extracting abstract features, neural models are often resource-intensive and require large parallel training datasets. Moreover, recent studies (Pagnoni et al., 2021) have shown that they also suffer from factuality problems. \n\nPre-trained language models use large-scale cor- pora to optimize objective functions, allowing them to acquire more general feature representations. This capability enables them to require only a small amount of data to fine-tune downstream tasks and achieve impressive results.",
            "score": 0.676033339991201,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2089
                },
                {
                    "start": 2092,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 860,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 860,
                    "end": 885,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 885,
                    "end": 908,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 1508,
                    "end": 1529,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1529,
                    "end": 1546,
                    "matchedPaperCorpusId": "222090788"
                },
                {
                    "start": 1546,
                    "end": 1563,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 1778,
                    "end": 1801,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1801,
                    "end": 1818,
                    "matchedPaperCorpusId": "199466313"
                },
                {
                    "start": 1818,
                    "end": 1834,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 2008,
                    "end": 2030,
                    "matchedPaperCorpusId": "233407441"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "252604304",
            "title": "Abstractive Summarization of Broadcast News Stories for Estonian",
            "text": "In broadcast news, the first sentence often gives a general overview of the rest of the story. Hence, an easy way to summarize the story is to simply use the first sentence. Extractive text summarization consists of creating a representation of the input text and scoring the sentences according to a ranking system. High score sentences are extracted, preserving the original order of the text with a determined cut-off length. The extractive approach uses sentences directly from the document, giving higher accuracy and is more straightforward than the abstractive approach. However, the method can lead to redundancy, a lack of cohesion and temporal conflicts in sentences (El-Kassas et al., 2021). \n\nThe field has significantly benefited from the introduction of robust statistical techniques. For example, a stochastic graph-based method for computing the relative importance of textual units for text summarization has been proposed called LexRank (Erkan and Radev, 2011). The technique works by calculating sentence importance from the eigenvector centrality in a sentence graph representation. A connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation. LexRank outperforms other systems and centroid-based methods (Erkan and Radev, 2011).",
            "score": 0.6730482389115173,
            "section_title": "Extractive Baselines.",
            "char_start_offset": 9661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1313
                }
            ],
            "ref_mentions": [
                {
                    "start": 677,
                    "end": 701,
                    "matchedPaperCorpusId": "224955327"
                },
                {
                    "start": 955,
                    "end": 978,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1289,
                    "end": 1312,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "6532096",
            "title": "Graph-based Neural Multi-Document Summarization",
            "text": "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.",
            "score": 0.6708483787317311,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "258654556",
            "title": "A Multi-Granularity Heterogeneous Graph for Extractive Text Summarization",
            "text": "Extractive text summarization selects the most important sentences from a document, preserves their original meaning, and produces an objective and fact-based summary. It is faster and less computationally intensive than abstract summarization techniques. Learning cross-sentence relationships is crucial for extractive text summarization. However, most of the language models currently in use process text data sequentially, which makes it difficult to capture such inter-sentence relations, especially in long documents. This paper proposes an extractive summarization model based on the graph neural network (GNN) to address this problem. The model effectively represents cross-sentence relationships using a graph-structured document representation. In addition to sentence nodes, we introduce two nodes with different granularity in the graph structure, words and topics, which bring different levels of semantic information. The node representations are updated by the graph attention network (GAT). The final summary is obtained using the binary classification of the sentence nodes. Our text summarization method was demonstrated to be highly effective, as supported by the results of our experiments on the CNN/DM and NYT datasets. To be specific, our approach outperformed baseline models of the same type in terms of ROUGE scores on both datasets, indicating the potential of our proposed model for enhancing text summarization tasks.",
            "score": 0.6671340905610615,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80078125
        },
        {
            "corpus_id": "266176619",
            "title": "Towards a Unified Framework for Reference Retrieval and Related Work Generation",
            "text": "We mainly compare our UR 3 WG with three types of baselines: multi-document summarization methods, related work generation methods, and large language models. The multi-document summarization baselines include: LEAD, which concatenates the first sentence of each reference; LexRank (Erkan and Radev, 2004), which extract summarization based on the graph representation of sentences; NES (Wang et al., 2019), a extractive method which measures the relevance via bibliography graph; MGSum (Jin et al., 2020), an abstractive method based on multigranularity interaction network; BertSum (Liu and Lapata, 2019), a summarization system built on a pre-trained BERT (Devlin et al., 2019); EMS (Zhou et al., 2021), which augments the encoder-decoder framework with a heterogeneous graph. \n\nThe related work generation methods include: RRG (Chen et al., 2021) , which enhanced by a iteratively refined relation-aware graph between references; TAG (Chen et al., 2022a), a transformerbased model with a target-aware graph encoder. \n\nThe large language models (LLMs) include flan-t5 (Chung et al., 2022), Llama-7B (Touvron et al., 2023), ChatGLM (Du et al., 2022), Vicuna5 , claude6 , Davinci-text-003, and ChatGPT7 , where the instruction I GEN is taken as input. \n\nTable 5: Results on the TAS2 and TAD dataset. We abbreviate ROUGE as R We underline the best results in each category of baseline methods. w/ T denotes the method which uses both references and abstract as input. All ROUGE scores have a 95% confidence interval as reported by the official R script.",
            "score": 0.6657956077251801,
            "section_title": "A.3 Details for baselines",
            "char_start_offset": 29667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1553
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 305,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 387,
                    "end": 406,
                    "matchedPaperCorpusId": "53083244"
                },
                {
                    "start": 487,
                    "end": 505,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 584,
                    "end": 606,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 659,
                    "end": 680,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 686,
                    "end": 705,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 831,
                    "end": 850,
                    "matchedPaperCorpusId": "236460044"
                },
                {
                    "start": 938,
                    "end": 958,
                    "matchedPaperCorpusId": "249097545"
                },
                {
                    "start": 1134,
                    "end": 1151,
                    "matchedPaperCorpusId": "247519241"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "5674504",
            "title": "Concept-Graph Based Biomedical Automatic Summarization Using Ontologies",
            "text": "Traditionally, automatic summarization methods have been classified in those which generate extracts and those which generate abstracts. Although human summaries are typically abstracts, most of existing systems produce extracts.\n\nExtractive methods build summaries on a superficial analysis of the source. Early summarization systems are based on simple heuristic features, as the position of sentences in the document (Brandow et al., 1995), the frequency of the words they contain (Luhn, 1958;Edmundson, 1969), or the presence of certain cue words or indicative phrases (Edmundson, 1969). Some advanced approaches also employ machine learning techniques to determine the best set of attributes for extraction (Kupiec et al., 1995). Recently, several graph-based methods have been proposed to rank sentences for extraction. LexRank (Erkan and Radev, 2004) is an example of a centroidbased method to multi-document summarization that assess sentence importance based on the concept of eigenvector centrality. It represents the sentences in each document by its tf*idf vectors and computes sentence connectivity using the cosine similarity. Even if results are promising, most of these approaches exhibit important deficiencies which are consequences of not capturing the semantic relations between terms (synonymy, hyperonymy, homonymy, and co-occurs and associatedwith relations).\n\nWe present an extractive method for summarization which attempts to solve this deficiencies. Unlike researches conducted by (Yoo et al., 2007;Erkan and Radev, 2004), which cluster sentences to identify shared topics in multiple documents, in this work we apply clustering to identify groups of concepts closely related. We hypothesize that each cluster represents a theme or topic in the document, and we evaluate three different heuristics to ranking sentences.",
            "score": 0.6656260582433234,
            "section_title": "Previous Work",
            "char_start_offset": 1131,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 420,
                    "end": 442,
                    "matchedPaperCorpusId": "39542260"
                },
                {
                    "start": 484,
                    "end": 496,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 496,
                    "end": 512,
                    "matchedPaperCorpusId": "1177942"
                },
                {
                    "start": 573,
                    "end": 590,
                    "matchedPaperCorpusId": "1177942"
                },
                {
                    "start": 712,
                    "end": 733,
                    "matchedPaperCorpusId": "5775833"
                },
                {
                    "start": 834,
                    "end": 857,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1508,
                    "end": 1526,
                    "matchedPaperCorpusId": "15439904"
                },
                {
                    "start": 1526,
                    "end": 1548,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.826171875
        },
        {
            "corpus_id": "12522650",
            "title": "Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video",
            "text": "Multi-document summarization (MDS) attempts to extract important information for a set of documents related to a topic to generate a short sum-mary. Graph based methods (Mihalcea and Tarau, 2004;Wan and Yang, 2006;Zhang et al., 2016) are commonly used. LexRank (Erkan and Radev, 2011) first builds a graph of the documents, in which each node represents a sentence and the edges represent the relationship between sentences. Then, the importance of each sentence is computed through an iterative random walk.",
            "score": 0.6652928781010318,
            "section_title": "Multi-document Summarization",
            "char_start_offset": 3705,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 195,
                    "end": 214,
                    "matchedPaperCorpusId": "5457260"
                },
                {
                    "start": 214,
                    "end": 233,
                    "matchedPaperCorpusId": "3712220"
                },
                {
                    "start": 261,
                    "end": 284,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "233189611",
            "title": "Extractive Summarization Considering Discourse and Coreference Relations based on Heterogeneous Graph",
            "text": "Automatic summarization aims to condense the information of the input document into a shorter summary. The task has two main paradigms: extractive summarization and abstractive summarization. Generating summary sentences from scratch, abstractive summarizers can generate concise and flexible summaries. However, they also suffer from the problem of not being able to reproduce factual details correctly (See et al., 2017). On the other hand, extractive summarization aims to select salient text spans (mostly sentences) from the input document. Compared to abstractive summarizers, extractive summarizers have the advantage of being efficient and factually reliable. In this paper, we will focus on extractive summarization. \n\nFor extractive summarization, it is crucial to model the relations between text spans throughout the document. Between text spans of different granularity, there exist many different kinds of relations (Figure 1). For example, coreference relations exist between mention phrases of the same entity, and discourse relations exist between Elementary Discourse Units (EDUs) within a document. Due to its complex nature, modeling the various relations among text spans of a document remains an open challenge. \n\nTo capture inter-sentential relations, some recent works utilize recurrent neural networks (RNNs) or Transformer (Vaswani et al., 2017) based encoders on top of the acquired sentence representations (Cheng and Lapata, 2016;Nallapati et al., 2016;Liu and Lapata, 2019). However, empirical observations show that these sentence-level encoders do not bring much performance gain (Liu and Lapata, 2019). Graph structure is an intuitive way to model long-range dependencies among text spans throughout a document. Early works build connectivity graphs based on content similarity between sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004). Some recent works incorporate discourse or coreference relations into the graph structure and utilize graph neural networks (GNNs) to obtain a high-level representation of text spans (Yasunaga et al., 2017;Xu and Durrett, 2019;Xu et al., 2020).",
            "score": 0.6641135060891665,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1349,
                    "end": 1371,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1435,
                    "end": 1459,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 1482,
                    "end": 1503,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1612,
                    "end": 1634,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1829,
                    "end": 1852,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "247348977",
            "title": "Template-Based Headline Generator for Multiple Documents",
            "text": "The Early studies of extract-based summarization were inspired by Pagerank [6]. For example, TextRank [7] and LexRank [8], they computed the salience with a similarity graph of sentences. Li et al. [9] applied the support vector regression model (SVR) for feature selection and weighting to conquer the semantic repeat problem. GreedyKL [10] used Kullback-Lieber (KL) divergence as the criterion for selecting a summary for a given text. New trends of extractive summarizations have been learningbased. G-Flow [11] works on sentence selection and ordering separately. To evaluate the coherence, G-Flow estimates its quality by using an approximate discourse graph (ADG), based on the hand-crafted feature. R2N2 [12] developed a ranking framework for redundant sentences. It transfers input sentences into a binary tree, then processes the binary tree by RNN recursively at each node. Yasunage et al. 1 Our dataset and code are available on: https://github.com/klks0304/mud2h [13] applied a graph convolution network [14] onto their proposed personalized discourse graph, and used GRU to calculate sentence embeddings. The model generates cluster embeddings to fully aggregate features between sentences with a document-level GRU. HSG [15] is a model which constructs a heterogeneous graph containing semantic nodes of different granularity levels apart from sentences. Additionally, HSG can flexibly extend from a single document setting to a multi-document setting. SgSum [16] is a graph based method which works on multi-document summarization by extraction based. SgSum consider the sentences as nodes, then generate a sentence relation graph for input documents. The model outputs a subgraph, then considered it as the result of summarization. ThresSum [17] is a recent publish paper applying powerful encoder to emphasize each sentence. The model utilize supervised variables to select sentences as close to original article meaning as possible, without setting limit to select k sentences.",
            "score": 0.6620205218292555,
            "section_title": "A. EXTRACTIVE SUMMARIZATION",
            "char_start_offset": 3275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 118,
                    "end": 121,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "1207010"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "337730"
                },
                {
                    "start": 711,
                    "end": 715,
                    "matchedPaperCorpusId": "10675728"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "3144218"
                },
                {
                    "start": 1234,
                    "end": 1238,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1473,
                    "end": 1477,
                    "matchedPaperCorpusId": "239768911"
                },
                {
                    "start": 1757,
                    "end": 1761,
                    "matchedPaperCorpusId": "235363524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80517578125
        },
        {
            "corpus_id": "250118028",
            "title": "An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics",
            "text": "The final summary is generated by extracting the top-k sentences ranked by  (  ). Importantly, while there are other classical architectures [38,112], the graph architecture is worth a separate mentioning here due to the fact that (a) it remains as a strong baseline against other advanced architectures, (b) it can effectively incorporate external knowledge as an inductive bias to the calculation of the importance of a sentence and (c) it achieves state-of-the-art result in long document unsupervised extractive summarization setting when integrated with current state-of-the-art pre-trained models [25,69]. Lastly, other than the multi-sentence compression approach [6,54,132] that may be extended to long document summarization tasks, there has been no applicable work on classical graph-based architecture for long document abstractive summarization.",
            "score": 0.6601535413760107,
            "section_title": "Graph Architecture:",
            "char_start_offset": 38367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 857
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "7003631"
                },
                {
                    "start": 145,
                    "end": 149,
                    "matchedPaperCorpusId": "10496513"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "231592914"
                },
                {
                    "start": 607,
                    "end": 610,
                    "matchedPaperCorpusId": "236477774"
                },
                {
                    "start": 671,
                    "end": 674,
                    "matchedPaperCorpusId": "6545121"
                },
                {
                    "start": 674,
                    "end": 677,
                    "matchedPaperCorpusId": "226284006"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "220633461"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "231733241",
            "title": "A Framework for Generating Extractive Summary from Multiple Malayalam Documents",
            "text": "The creation of a single summary from Multiple documents has gained interest since the 1990s, mostly in news articles. Popular Internet news services, for example, Google news or Alta vista news, present clusters of related articles allowing readers to find all stories on a given topic easily. However, these services do not produce summaries. SUM-MONS is the first multi-document summarizing system developed at Columbia University [22]. SUMMONS generates the summaries from multiple documents by merging relevant information about each identified event. Several attempts have been made in developing text summarization systems using different approaches. The common approaches used in literature are the statistical approach, topic-based approach, graph-based approach, and approaches based on machine learning [9]. The statistical-based approach extracts most salient sentences based on the shallow features of text such as the sentence's resemblance to the title, sentence position, presence of numerical data in the sentence, presence of proper nouns (named entities), TF-IDF (Term Frequency Inverse document frequency). Each of the above features assigns some weight to the words. Based on these weights, the scores are assigned to the sentences, and then highly scored sentences are chosen to generate the summary. This technique is language-independent; it can summarize a text in any language. It does not require additional linguistic knowledge. \n\nTopic-based approaches infer topics by observing the distribution of words across documents. LDA is the first topic model that can solve multi-document summarization, and the researchers continuously improve this algorithm. Wu et al. proposed a topic modelingbased approach to extractive automatic summarization [17]. They extracted the candidate sentences related to topic words from a preprocessed novel document. They came up with a critical evaluation function to pick a subset of sentences from the candidate sentences to get a unique summary. This technique requires additional linguistic knowledge. \n\nMany Automatic Text Summarization (ATS) systems apply the graph-based methods for extractive text summarization such as LexRank [23], TextRank [18], and Tex-tRankExt [24]. Graph-based approaches have achieved robust and promising results [25].",
            "score": 0.6598160305061778,
            "section_title": "Related Works",
            "char_start_offset": 5796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2064
                },
                {
                    "start": 2067,
                    "end": 2238
                },
                {
                    "start": 2239,
                    "end": 2310
                }
            ],
            "ref_mentions": [
                {
                    "start": 814,
                    "end": 817,
                    "matchedPaperCorpusId": "24465182"
                },
                {
                    "start": 1771,
                    "end": 1775,
                    "matchedPaperCorpusId": "13752634"
                },
                {
                    "start": 2195,
                    "end": 2199,
                    "matchedPaperCorpusId": "10418456"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "Text summarization is a key area of natural language processing (NLP) that allows users to quickly find relevant information in large datasets [1]. There are two main types: extractive and abstractive summarization [2]. Extractive summarization selects key phrases and sentences from the original text, while abstractive summarization reformulates the content, which may lead to less fluent results. Extractive methods are favored for their factual accuracy and ease of implementation, making them popular among researchers [3], [4]. Moreover, Traditional methods for Extractive Text Summarization (ETS), such as TextRank and LexRank, focus primarily on calculating sentence similarity. The acquisition and modeling of cross-sentence relationships represent a critical phase in the process of generating a summary that encapsulates significantly meaningful sentences from the input content. Over the past few years, neural sequence-to-sequence encoder-decoder frameworks have shown exceptional performance in the domain of extractive summarization. The methodologies introduced by [1] and [2] predominantly utilized the sequence-to-sequence architecture with an encoder-decoder framework. In this setup, each sentence is encoded using various neural components in distinct ways. However, the empirical research showed that when this paradigm was used to define inter-sentence relations, it did not help to improve the summarization performance [3]. \n\nNumerous studies, as indicated by various references, have utilized Recurrent Neural Networks (RNNs) to capture and structure the relationships between sentences. However, recurrent models face troubles in effectively controlling computational factors and capturing long-range connections among words, mainly due to their processing of text in a strictly sequential fashion. \n\nIn recent developments, Graph Neural Networks (GNNs) [4] have drawn significant attention due to their capacity to efficiently model cross-sentence interactions in summarization tasks [3], [5], [6], [7], [8], [9]. It is noted by the researchers of [3] that a crucial step within this framework involves constructing an effective document graph. The study presented by [10] investigates an unsupervised, discourseaware hierarchical and ranking graph-based model, meticulously tailored for comprehensive scientific documents.",
            "score": 0.657944458053206,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 146,
                    "matchedPaperCorpusId": "52221845"
                },
                {
                    "start": 1081,
                    "end": 1084,
                    "matchedPaperCorpusId": "52221845"
                },
                {
                    "start": 2017,
                    "end": 2020,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 2027,
                    "end": 2030,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 2032,
                    "end": 2035,
                    "matchedPaperCorpusId": "252670538"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "226283539",
            "title": "Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm",
            "text": "The graph-based approaches treat text as a network instead of as a simple bag of words and use graph-based ranking methods to generate a summary (Erkan and Radev, 2011;Ouyang et al., 2009;Mihalcea and Tarau, 2004). Optimizationbased methods use techniques such as sparse optimization (Yao et al., 2015;Elhamifar and Vidal, 2013), integer linear programming (ILP) (Qian and Liu, 2013;Berg-Kirkpatrick et al., 2011;Woodsend and Lapata, 2011;Gillick and Favre, 2009) and constraint optimization (Durrett et al., 2016;McDonald, 2007) to reconstruct the summary. \n\nIn this work, we focus on extractive summarization for long documents. Performing automatic text summarization for long documents is especially challenging as obtaining high quality human summaries for long documents is often quite costly and time consuming. Recent works on extractive summarization have been focusing on neural network architectures (Nallapati et al., 2017;Cheng and Lapata, 2016). Although these methods are successful in generating summaries for short documents, they often have difficulties with long input sequences (Shao et al., 2017). \n\nMost recent works have started to investigate neural extractive summarization methods for long documents (Xiao and Carenini, 2019;Wang et al., 2017). However, these methods are supervised and require high quality training data in order to train the neural network models. This creates challenges for domains that do not have massive training datasets. Kedzie et al. (2018) compared recent neural extractive summarization models across different domains including news, personal stories, and medical articles. They found that many sophisticated neural extractive summarizers do not have better performance than those consisting of simpler models, and that word embedding averaging performs equally or better than RNNs or CNNs for sentence embedding. This suggests that a simpler model combined with pre-trained word embeddings show promise for summarizing long documents in domains that have few or no training data.",
            "score": 0.6574233292444533,
            "section_title": "Introduction",
            "char_start_offset": 1752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 188,
                    "matchedPaperCorpusId": "27361296"
                },
                {
                    "start": 188,
                    "end": 213,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 284,
                    "end": 302,
                    "matchedPaperCorpusId": "15593461"
                },
                {
                    "start": 363,
                    "end": 383,
                    "matchedPaperCorpusId": "1846037"
                },
                {
                    "start": 383,
                    "end": 413,
                    "matchedPaperCorpusId": "15467396"
                },
                {
                    "start": 413,
                    "end": 439,
                    "matchedPaperCorpusId": "9945908"
                },
                {
                    "start": 439,
                    "end": 463,
                    "matchedPaperCorpusId": "167874"
                },
                {
                    "start": 514,
                    "end": 529,
                    "matchedPaperCorpusId": "2798452"
                },
                {
                    "start": 911,
                    "end": 935,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1251,
                    "end": 1269,
                    "matchedPaperCorpusId": "21197619"
                },
                {
                    "start": 1473,
                    "end": 1493,
                    "matchedPaperCorpusId": "53083054"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "256174174",
            "title": "Extractive Arabic Text Summarization-Graph-Based Approach",
            "text": "Until convergence, use the graph ranking algorithm to determine a ranking over the graph's nodes. Then, every node is arranged in order of ultimate score. Use the values associated with each vertex to determine ranking and selection. \n\nAs indicated in the third phase, nodes are ranked according to their final scores after specifying the final scores for each node. The best sentences are then chosen to participate in the final summary. Two of the most significant algorithms based on the graph are TexRank [27] and LexRank [28]. We then briefly looked at each of these algorithms. \n\nThe candidate sentences that might be included in the summary are all represented in a graph by the multi-document summarizing system called LexRank. If the similarity between two sentences exceeds a certain level, they are connected in this graph representation. A competitive advantage between two sentences is created if they have specific similarities. This similarity is computed using the function COSINUS. The system then conducts a random walk on the graph after constructing the network to identify the most crucial sentences. \n\nAll graphs that are derived from natural language texts are ranked using the graphbased model TextRank. \n\nTextRank is a single document summarizing system that derives from the Google page ranking [27,29] paradigm. Keywords and sentences are extracted using TextRank. To extract sentences, a completely connected, undirected graph is used. An individual sentence is regarded as a vertex (or node) in a graph. A similarity connection that is calculated as a function of shared concepts is used to draw a line between two texts. Additionally, each edge has a weight that reflects how significant a relationship is. The best sentences are chosen after ranking each sentence according to its score. \n\nGiven a document d, let G = (V, E) be an undirected graph that represents the document d with the set of nodes V and the set of edges E. This is for the text summarizing task. The nodes in this model stand in for the sentences in d. Every edge E ij has a weight W i that denotes how similar the nodes (sentences) V i and V j are to one another. If two sentences satisfy a similarity threshold t and are similar to one another, then they are related. Based on the relationships with other connected nodes, each node in the V graph is also given a salient score.",
            "score": 0.6569639205610626,
            "section_title": "3.",
            "char_start_offset": 18507,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2270
                },
                {
                    "start": 2271,
                    "end": 2381
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1325,
                    "end": 1328,
                    "matchedPaperCorpusId": "7587743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "239768911",
            "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
            "text": "We evaluate our models on both the MultiNews and DUC datasets to validate their effectiveness on dif- ferent types of corpora. The summarization quality is evaluated using ROUGE F1 (Lin, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) between system summaries and gold references as a means of assessing informativeness, and the longest common subsequence (ROUGE-L2) as a means of accessing fluency. Table 1 summarizes the evaluation results on the MultiNews. Several strong extractive and abstractive baselines are evaluated and compared with our models. The first block in the table shows results of extractive methods: LexRank (Erkan and Radev, 2004), MMR (Carbonell and Goldstein, 1998), HeterGraph  and MatchSum (Zhong et al., 2020) which is the previous extractive SOTA model on the MultiNews dataset. The second block shows results of abstractive methods: PG (Lebanoff et al., 2018), Hi-MAP (Fabbri et al., 2019), FT(Flat Transformer) and GraphSum (Li et al., 2020) which is the previous abstractive SOTA model. We report their results following Zhong et al. (2020); ; Li et al. (2020). The last block shows the results of SgSum. Compared with both previous extractive and abstractive SOTA models, SgSum achieves more than 1.1/1.2/0.9 improvements on R-1, R-2 and R-L which demonstrates the effectiveness of our sub-graph selection framework. Furthermore, due to our graph representation and graph-based multi-document encoder, our model has the ability to unify single and multi-document summarization task. In our framework, a single document can also be viewed the same as multidocument input. So our model can be enhanced by feeding extra single-document training data. In the last block, extra means we leverage CNN/DM data 2 -n 2 -m -w 1.2 -c 95 -r 1000 -l 250   (Erkan and Radev, 2004), DPP (Kulesza and Task",
            "score": 0.6568374292320758,
            "section_title": "Evaluation Results",
            "char_start_offset": 17733,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 192,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 648,
                    "end": 671,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 677,
                    "end": 708,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 735,
                    "end": 755,
                    "matchedPaperCorpusId": "215828313"
                },
                {
                    "start": 884,
                    "end": 907,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 916,
                    "end": 937,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 973,
                    "end": 990,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1071,
                    "end": 1090,
                    "matchedPaperCorpusId": "215828313"
                },
                {
                    "start": 1094,
                    "end": 1110,
                    "matchedPaperCorpusId": "218718706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "264817506",
            "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders",
            "text": "Ultimately, the pre-trained sen-tence node representations are utilized for downstream extractive summarization. \n\nOur pre-trained sentence representations obtain superior performance in both single document summarization on the CNN/DailyMail dataset (Hermann et al., 2015) and multiple document summarization on the Multi-News dataset (Sandhaus, 2008) within salient extractive summarization frameworks. i) To our knowledge, we are the first to introduce the bipartite word-sentence graph pretraining method and pioneer bipartite graph pretrained sentence representations in unsupervised extractive summarization. ii) Our pre-trained sentence representation excels in downstream tasks using the same summarization backbones, surpassing heavy BERT-or RoBERTa-based representations and highlighting its superior performance. \n\n2 Background & Related Work",
            "score": 0.6565184598870473,
            "section_title": "\ud835\udc7a \ud835\udc84",
            "char_start_offset": 4414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 115,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 853
                }
            ],
            "ref_mentions": [
                {
                    "start": 251,
                    "end": 273,
                    "matchedPaperCorpusId": "6203757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "247237482",
            "title": "Follow the Timeline! Generating Abstractive and Extractive Timeline Summary in Chronological Order",
            "text": "Despite the focus on abstractive summarization, extractive summarization remains an attractive method. In extractive summarization, Kobayashi et al. [30] propose a summarization method using document-level similarity based on word embeddings. Meanwhile, Filippova et al. [20] use an RNN to delete words in a document for sentence compression. Yan \n\nand Wan [76] propose more meaningful and informative units named frequent deep dependency sub-structure and a topic-sensitive multi-task learning model for multi-doc summarization. Cheng and Lapata [13] propose a general framework for single-document text summarization using a hierarchical article encoder composed with an attentionbased extractor. Following this, Nallapati et al. [47] propose a simple RNN-based sequence classifier that outperforms or matches the state-of-art models at the time. Chen et al. [11] introduce a model which iteratively polishes the document representation on many passes through the document, so as to produce a better summary. In another approach, Narayan et al. [49] use a reinforcement learning method to optimize the ROUGE evaluation metric for text summarization. Ren et al. [52] study the use of sentence relations, e.g., contextual sentence relations, title sentence relations, and query sentence relations, so as to improve the performance of extractive summarization. \n\nManuscript submitted to ACM Recently, pre-trained language models are also applied in summarization for contextual word representations [39,83]. \n\nAnother intuitive structure for extractive summarization is the graph, which can better utilize the statistical or linguistic information between sentences. Early works focus on document graphs constructed with the content similarity among sentences, like LexRank [19] and TextRank [43]. Some recent works aim to incorporate a relational prior into the encoder by graph neural networks (GNNs) [78].",
            "score": 0.655499531919805,
            "section_title": "Extractive Summarization",
            "char_start_offset": 13510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "15448388"
                },
                {
                    "start": 271,
                    "end": 275,
                    "matchedPaperCorpusId": "1992250"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "1786930"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 861,
                    "end": 865,
                    "matchedPaperCorpusId": "52879916"
                },
                {
                    "start": 1047,
                    "end": 1051,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "14001298"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1791,
                    "end": 1795,
                    "matchedPaperCorpusId": "577937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "264146402",
            "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
            "text": "Multi-document Single-document summarization and multi-document summarization are two distinct tasks within text summarization domain based on source document quantity. Single-document summarization focuses on generating a summary from a single input document, while multi-document summarization aims to create a summary by aggregating information from multiple related documents [41,111,149]. Within the single-document system, the objective is to condense the main ideas and essential information contained in that specific document. On the other hand, multi-document summarization tasks require identifying and combining the most relevant and nonredundant information from a set of documents, often covering the same topic or event. This means multi-document summarization has additional challenges, such as maintaining cross-document coherence, efficiently handling larger volumes of information, and processing redundancy across documents. These complexities make multi-document summarization generally more difficult than single-document summarization. \n\nTraditional approaches usually employ extractive techniques both in the context of single-document and multi-document summarization. Graph-based methods can be applied to both single-document and multi-document summarization by representing the relationships between sentences in one document or several documents as a graph, with sentences as nodes and the edges as the similarity between the sentences. The systems [46,96,141,171] then use algorithms like PageRank, HITS, or LexRank to identify the most important sentences in the graph, which are then extracted and combined to form the summary. Fig. 2. The weighted cosine similarity graph [46] was generated for the cluster, based on the subset of d1003t from DUC 2004, which is a dataset for multi-document summarization tasks. The notation used in the figure is as follows: 'd' represents document and 's' represents sentence. For instance, d2s3 denotes the third sentence of document 2. \n\nFor single-document summarization, position-based methods [44,82,95] exploit the position of sentences within the document to identify important content. Additionally, the TF-IDF approach [60,115,156] weighs the importance of words in main Surveying Text Summarization with Deep Learning 7 the document based on their frequency and rarity.",
            "score": 0.655251715896275,
            "section_title": "Source Document Quantity: Single-document vs",
            "char_start_offset": 10281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2005
                },
                {
                    "start": 2008,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2347
                }
            ],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "243638031"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1482,
                    "end": 1485,
                    "matchedPaperCorpusId": "8014853"
                },
                {
                    "start": 1485,
                    "end": 1489,
                    "matchedPaperCorpusId": "269225"
                },
                {
                    "start": 1489,
                    "end": 1493,
                    "matchedPaperCorpusId": "21346422"
                },
                {
                    "start": 1705,
                    "end": 1709,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 2066,
                    "end": 2070,
                    "matchedPaperCorpusId": "1177942"
                },
                {
                    "start": 2070,
                    "end": 2073,
                    "matchedPaperCorpusId": "5775833"
                },
                {
                    "start": 2073,
                    "end": 2076,
                    "matchedPaperCorpusId": "40166767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "182712867",
            "title": "Learning with fuzzy hypergraphs: A topical approach to query-oriented text summarization",
            "text": "Extractive summarizers aggregate important sentences in a corpus while abstractive summarizers generate new summaries after identifying important information [26]. As abstractive summarization requires extensive Natural Language Processing, most summarizers to date are based on extractive approaches. \n\nMethods of extractive summarization generally fall into two categories, namely featurebased and graph-based approaches. Feature-based methods train a model to predict the score of each sentence based on feature representations of sentences (term frequency, sentence position [26], etc.). Graph-based methods define graphs in which nodes are sentences and edges represent similarities between sentences. Sentence scores are then given by node centrality measures on the graph [11,27]. The advantages of graph-based summarization over feature-based summarization are that it does not require labelled corpora, and it is based on the global structure of links between sentences of the corpus rather than local features. \n\nThe earliest graph-based summarizer, called LexRank [11], defines edges as term co-occurrence relationships between sentences. Then, PageRank algorithm is applied to compute relevance scores of sentences. Adapting this idea for the task of query-focused summarization, topic sensitive LexRank [27] introduces a query bias in probabilities of transition, which results in higher scores for sentences that are similar to the query. Sim-ilarly, [36] proposes a manifold ranking algorithm in which scores are popagated accross a graph including both sentences and the query as vertices. To remove redundancies in summaries, [23] proposes a new node ranking algorithm called DivRank, which tends to select dissimilar sentences. While early graph-based algorithms only involved sentences, a bipartite graph model is proposed in [37], involving both sentences and terms as vertices and it applies HITS algorithm to score sentences. [40] combines this idea with a PageRank-like method to score sentences, terms and documents simultaneously. \n\nWhile early methods build sentence graphs based on co-occurrence of terms in sentences only, later approaches infer higher level relationships. These methods include sentence clusters in the graph construction, namely groups of similar sentences.",
            "score": 0.6551739141305327,
            "section_title": "Related work",
            "char_start_offset": 10671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2055
                },
                {
                    "start": 2058,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 779,
                    "end": 783,
                    "matchedPaperCorpusId": "2962090"
                },
                {
                    "start": 783,
                    "end": 786,
                    "matchedPaperCorpusId": "10274362"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "matchedPaperCorpusId": "2962090"
                },
                {
                    "start": 1316,
                    "end": 1320,
                    "matchedPaperCorpusId": "10274362"
                },
                {
                    "start": 1465,
                    "end": 1469,
                    "matchedPaperCorpusId": "11840355"
                },
                {
                    "start": 1643,
                    "end": 1647,
                    "matchedPaperCorpusId": "11977708"
                },
                {
                    "start": 1845,
                    "end": 1849,
                    "matchedPaperCorpusId": "9849366"
                },
                {
                    "start": 1948,
                    "end": 1952,
                    "matchedPaperCorpusId": "35133176"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "240268841",
            "title": "Automatic Text Summarization by Providing Coverage, Non-Redundancy, and Novelty Using Sentence Graph",
            "text": "In a graph based summarization model, a node can be scored using information from the global graph. First, Mani and Bloedorn (1997) proposed graph representation of text. They described a new mechanism for summarizing the similarities and differences between a pair of related documents. Another work used the knowledge of text structure for producing summaries by automatic passage extraction (Salton, Singhal, Mitra, & Buckley, 1997). The earlier iterative graph algorithms are TextRank (Mihalcea & Tarau, 2004) and LexRank (Erkan & Radev, 2004). They can be applied to the summarization of a single or multiple documents in any language (Mihalcea & Tarau, 2005). Even though they are the best graph ranking algorithms, they have high time complexity. Some recent graph based ranking research works are Calvo et al. (2018), Feiyue andXinchen (2018), andTixier et al. (2017). \n\nThe work of Sornil and Gree-ut (2006) constructs an undirected document graph from cosine similarity using the Hopfield Network algorithm for text segment ranking. One research work produces automatic summaries using graph algorithms and the shortest path algorithm and compares them (Khushboo, Dharaskar, & Chandak, 2010). It concludes that the shortest path algorithm is the best one, because it generates a smooth summary. The work of Chen and Zhao (2014) constructs a two layer, phrase-sentence graph. It utilizes sentence relevance and phrase relevance information to produce a summary. The work of Zheng and Bai (2014) considers text summarization as a problem of finding the key paths composed of essential information and merges the common paths to remove redundancy. Han et al. (2016) propose the FrameNet-based semantic graph model. It uses FrameNet to calculate sentence similarity and assigns weights to both sentences and edges. After ranking, it selects summary sentences. Oliveira et al. (2016) present a comparative analysis of eighteen sentence scoring features. They compute the importance of a sentence in extractive single and multi-document summarizations.",
            "score": 0.6551313737257639,
            "section_title": "BACKGRoUNd",
            "char_start_offset": 3190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2056
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 435,
                    "matchedPaperCorpusId": "12250431"
                },
                {
                    "start": 526,
                    "end": 547,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 640,
                    "end": 664,
                    "matchedPaperCorpusId": "8878897"
                },
                {
                    "start": 805,
                    "end": 824,
                    "matchedPaperCorpusId": "44132297"
                },
                {
                    "start": 826,
                    "end": 836,
                    "matchedPaperCorpusId": "125164049"
                },
                {
                    "start": 836,
                    "end": 855,
                    "matchedPaperCorpusId": "125164049"
                },
                {
                    "start": 855,
                    "end": 875,
                    "matchedPaperCorpusId": "42957184"
                },
                {
                    "start": 1163,
                    "end": 1201,
                    "matchedPaperCorpusId": "21346422"
                },
                {
                    "start": 1483,
                    "end": 1503,
                    "matchedPaperCorpusId": "35197081"
                },
                {
                    "start": 1866,
                    "end": 1888,
                    "matchedPaperCorpusId": "36306586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "267061322",
            "title": "Beyond the Frame: Single and mutilple video summarization method with user-defined length",
            "text": "One of the earliest approaches to text summarization is the extraction-based method, which involves selecting the most important sentences or phrases from the original text and combining them to form a summary. In their survey on automatic text summarization, Hovy and Lin [11] provide an overview of various extractionbased methods, including sentence ranking, graph-based methods, and clustering-based methods. These methods typically involve analyzing the text to identify important sentences based on features such as sentence length, word frequency, and semantic similarity. The selected sentences are then combined to create a summary. Some popular extractive summarization methods include: \n\n\u2022 Frequency-based methods, such as TF-IDF and Tex-tRank, that rank sentences based on their frequency or importance in the text. \n\n\u2022 Graph-based methods, such as LexRank and Sum-Basic, which represent the text as a graph and use graph algorithms to identify important sentences. involves generating a summary by paraphrasing the original text using natural language generation techniques. This method is more challenging than extraction-based methods, as it requires the model to understand the meaning of the text and generate new sentences that convey the same information. In their comprehensive study on automatic text summarization, Narayan et al. [12] provide an overview of various abstraction-based methods, including rule-based methods, machine learning-based methods, and deep learning-based methods. These methods typically involve generating summaries using natural language generation techniques, such as language models and neural networks. Some popular abstractive summarization methods include: \n\n\u2022 Sequence-to-sequence models, such as Encoder-Decoder and Transformer, which generate summaries using a neural network that learns to encode the input text into an embedding vector and decode the summary using this embedding vector and a decoder layer.",
            "score": 0.6550232378993699,
            "section_title": "Extractive methods.",
            "char_start_offset": 7445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "148028"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65625
        },
        {
            "corpus_id": "236460044",
            "title": "Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation",
            "text": "To evaluate the performance of our proposed model, we compare it with the following baselines: Extractive Methods: \n\n(1) LEAD: selects the first sentence of each document as the summary as a baseline. ( 2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model. ( 3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT. (4) MGSum-ext (Jin et al., 2020): is a multi-granularity interaction network for extractive multi-document summarization. Abstractive Methods: \n\n(1) PTGen+Cov: combines the sequence-tosequence framework with copy and coverage mechanism in summarization task (See et al., 2017). \n\n(2) TransformerABS: is an abstractive summarization model based on the Transformer (Vaswani et al., 2017). ( 3) BertSumABS (Liu and Lapata, 2019b): is an abstractive summarization network built on BERT. (4) MGSum-abs (Jin et al., 2020): is multi-granularity interaction network for abstractive multi-document summarization. ( 5) GS (Li et al., 2020a): is a neural abstractive multidocument summarization model that leverages well-known graphs to produce abstractive summaries. We use the TF-IDF graph as the input graph.",
            "score": 0.6548906029329232,
            "section_title": "Baselines",
            "char_start_offset": 16914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 117,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1179
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 241,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 306,
                    "end": 329,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 393,
                    "end": 411,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 637,
                    "end": 655,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 742,
                    "end": 764,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 782,
                    "end": 805,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 876,
                    "end": 894,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 991,
                    "end": 1009,
                    "matchedPaperCorpusId": "218718706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "244510269",
            "title": "A Survey of Automatic Text Summarization: Progress, Process and Challenges",
            "text": "The graph-based method can be applied to both extractive and abstractive text summarization. This approach is an unsupervised learning method that rates the required sentences or terms using a graph. The purpose of the graphical process is to extract the most relevant sentences from a single text [152]. \n\nGraph-based ranking algorithms determine the relevance of a vertex in a graph based on global information iteratively extracted from the entire chart. When it comes to text summarization, specific graph-based techniques are applied. 1) LexRank: LexRank is a probabilistic graph-based technique for calculating sentence significance based on the notion of eigenvector centrality in a graph representation of phrases for natural language processing. It is a connectivity matrix based on intra-sentence cosine similarity that is utilized as the adjacency matrix in a sentence graph representation [65]. [257], [258], [259], [260], [261] proposed the LexRank algorithm for graph-based text summarization task. 2) Hyperlink-Induced Topic Search (HITS): Hyperlinkinduced topic search is a link analysis algorithm that determines the authority and hub values. The results for the search query are retrieved, and then the computation is performed only on this set of results. A hub value is the total of the scaled authority values of the pages it points to, and an authority value is the sum of the scaled authority values of the pages it points to [262]. Some articles focused on the HITS ranking algorithm for graphbased text summarization tasks, as can be seen in [263], [264], [265], and [266]. 3) PageRank: The PageRank algorithm utilizes the inbound links of specified pages to measure their significance or quality to rank the search results. PageRank links more weight based on the importance of the page from which it originates [267]. Some articles proposed the PageRank algorithm for summarization [268], [269], [270], [271], [272]. 4) TextRank: TextRank is an unsupervised method for automatic text summarization to extract the most important keywords from a document.",
            "score": 0.6545187748871646,
            "section_title": "E. EXTRACTIVE + ABSTRACTIVE 1) Graph Based Method",
            "char_start_offset": 48908,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2080
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 303,
                    "matchedPaperCorpusId": "29980788"
                },
                {
                    "start": 901,
                    "end": 905,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 907,
                    "end": 912,
                    "matchedPaperCorpusId": "17636912"
                },
                {
                    "start": 914,
                    "end": 919,
                    "matchedPaperCorpusId": "40873863"
                },
                {
                    "start": 921,
                    "end": 926,
                    "matchedPaperCorpusId": "63875451"
                },
                {
                    "start": 928,
                    "end": 933,
                    "matchedPaperCorpusId": "17951450"
                },
                {
                    "start": 935,
                    "end": 940,
                    "matchedPaperCorpusId": "38751984"
                },
                {
                    "start": 1449,
                    "end": 1454,
                    "matchedPaperCorpusId": "12222596"
                },
                {
                    "start": 1567,
                    "end": 1572,
                    "matchedPaperCorpusId": "21346422"
                },
                {
                    "start": 1574,
                    "end": 1579,
                    "matchedPaperCorpusId": "2100529"
                },
                {
                    "start": 1581,
                    "end": 1586,
                    "matchedPaperCorpusId": "5540065"
                },
                {
                    "start": 1592,
                    "end": 1597,
                    "matchedPaperCorpusId": "18376399"
                },
                {
                    "start": 1909,
                    "end": 1914,
                    "matchedPaperCorpusId": "213789058"
                },
                {
                    "start": 1916,
                    "end": 1921,
                    "matchedPaperCorpusId": "69927855"
                },
                {
                    "start": 1923,
                    "end": 1928,
                    "matchedPaperCorpusId": "33925572"
                },
                {
                    "start": 1930,
                    "end": 1935,
                    "matchedPaperCorpusId": "14944858"
                },
                {
                    "start": 1937,
                    "end": 1942,
                    "matchedPaperCorpusId": "15513453"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "256826757",
            "title": "PDSum: Prototype-driven Continuous Summarization of Evolving Multi-document Sets Stream",
            "text": "Single document summarization (SDS) and multi-document summarization (MDS) are the most popular summarization tasks, where the former assumes a single document as input while the latter assumes a set of documents of a certain interest (e.g., topic, query, or theme). Typical SDS and MDS methods can be classified as abstractive or extractive approaches, depending on how the summaries are derived, or as supervised or unsupervised approaches, depending on the use of reference (gold) summaries for model training. \n\nVarious approaches for SDS and MDS have been studied in the literature. Centroid-based methods [13,35,38] are one of the widely used approaches that cluster input documents and pick the most central sentences as a summary. Graph-based methods embed documents in a graph structure [9,32,57,59]. A popular method LexRank [9] constructs a graph by connecting sentences based on their similarities and applies PageRank [34] to extract the most salience sentences. An unsupervised method SummPip [57] with graph clustering and compression techniques shows comparable performances with supervised methods. Recently, deep neural network (DNN)-based methods have been actively proposed [28], where deep reinforcement learning [29], semantic text matching [58], hierarchical transformer [26], or graph neural network [23,45,46] are used, to name a few. While most existing DNNbased methods adopt supervised training with reference summaries, Zhang et al. [55] proposed a self-supervised approach with the Gap Sentence Generation objective. PRIMERA [47] further improves the self-supervision by using the Entity Pyramid for masking sentences and provides state-of-the-art pretrained MDS models. \n\nNevertheless, the existing work for SDS and MDS inherently considers a static and single set of documents for summarization, which fall too short for continuous summarization of streaming documents from evolving sets in EMDS. Moreover, some supervised methods require reference summaries which are not readily available in an online scenario. \n\nAnother related line of work centers on query-focused summarization (QFS) [5,[42][43][44]48].",
            "score": 0.6539634233248726,
            "section_title": "Sigle/multi-document summarization",
            "char_start_offset": 4999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2045
                },
                {
                    "start": 2048,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 611,
                    "end": 615,
                    "matchedPaperCorpusId": "1564849"
                },
                {
                    "start": 615,
                    "end": 618,
                    "matchedPaperCorpusId": "6354619"
                },
                {
                    "start": 618,
                    "end": 621,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 796,
                    "end": 799,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 799,
                    "end": 802,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 802,
                    "end": 805,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 805,
                    "end": 808,
                    "matchedPaperCorpusId": "236512300"
                },
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 1194,
                    "end": 1198,
                    "matchedPaperCorpusId": "226289939"
                },
                {
                    "start": 1234,
                    "end": 1238,
                    "matchedPaperCorpusId": "222090788"
                },
                {
                    "start": 1263,
                    "end": 1267,
                    "matchedPaperCorpusId": "215828313"
                },
                {
                    "start": 1294,
                    "end": 1298,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1324,
                    "end": 1328,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1328,
                    "end": 1331,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1331,
                    "end": 1334,
                    "matchedPaperCorpusId": "235187330"
                },
                {
                    "start": 1462,
                    "end": 1466,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1555,
                    "end": 1559,
                    "matchedPaperCorpusId": "247519084"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "269188314",
            "title": "Prompt-tuning for Clickbait Detection via Text Summarization",
            "text": "Text summarization is a prominent and challenging area in Natural Language Processing (NLP) [50].It involves the task of creating a concise and coherent summary of lengthy text documents, highlighting key points while maintaining the document's overall meaning.This process aims to produce summaries that efficiently capture the primary concepts, enabling a more efficient understanding of extensive textual information.\n\nTwo primary paradigms govern text summarization: extractive and abstractive.In the extractive approach, specific keywords from the input are chosen for output generation, aligning well with this model.Many extractive summarization methods treat the task as a sequence classification problem, utilizing diverse encoders such as recurrent neural networks [51] and pre-trained language models [52].Nallapati et al. introduced SummaRuNNer, utilizing a dual-layer bidirectional GRU-RNN for document extractive summarization [53].They introduced an abstract training method that leverages Rouge score optimization to transform abstract summaries into extractive labels, enabling training exclusively based on human-generated summaries.Xie et al. introduce KeBioSum, utilizing SciBERT to identify PICO elements and incorporating domain knowledge into base PLMs through generative and discriminative adapters [54].The method significantly improves the performance of extractive summarization in the biomedical domain.An alternative view considers extractive summarization as a node classification problem, utilizing graph neural networks to model inter-sentence dependencies [55].Cui et al. present Topic-GraphSum, an extractive text summarization approach that integrates graph neural networks and neural topic models [56].Document encoding utilizes BERT, and the neural topic model (NTM) learns document topics.The graph attention layer captures intersentence relationships, facilitating the efficient extraction of key information and addressing limitations in existing models regarding intersentence relationships and neglect of topic information.\n\nIn the realm of abstractive summarization, significant progress has been achieved through Seq2Seq models [51].These models build a neural network to establish a true relationship between input and output, refining the process with local attention mechanisms and advanced techniques like BERT.",
            "score": 0.6534698948812925,
            "section_title": "Text Summarization",
            "char_start_offset": 18044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 420
                },
                {
                    "start": 422,
                    "end": 498
                },
                {
                    "start": 498,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1151
                },
                {
                    "start": 1151,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1738
                },
                {
                    "start": 1738,
                    "end": 1827
                },
                {
                    "start": 1827,
                    "end": 2065
                },
                {
                    "start": 2067,
                    "end": 2177
                },
                {
                    "start": 2177,
                    "end": 2359
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 96,
                    "matchedPaperCorpusId": "219504970"
                },
                {
                    "start": 941,
                    "end": 945,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "matchedPaperCorpusId": "250713599"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "222310577",
            "title": "Enhancing Extractive Text Summarization with Topic-Aware Graph Neural Networks",
            "text": "Another critical point of summarization is modeling global information, which plays a key role in sentence selection (Xiao and Carenini, 2019). Pre-trained language models can considerably boost the performance of summarization (Liu and Lapata, 2019a;Zhang et al., 2019) since they effectively capture context features. However, they are poor at modeling document-level information, particularly for long documents, because most of them are designed for sentences or a short paragraph (Xu et al., 2019b). \n\nTo tackle the abovementioned weaknesses, this paper proposes a novel graph-based extractive summarization model. First, we encode an entire document with a pre-trained BERT (Devlin et al., 2019) to learn contextual sentence representations, and discover latent topics with a joint neural topic model (NTM;Miao et al., 2017;Srivastava and Sutton, 2017). Second, we build a heterogeneous document graph consisting of sentence and topic nodes, and simultaneously update their representations with a modified graph attention network (GAT; Veli\u010dkovi\u0107 et al., 2017). Third, the representations of sentence nodes are extracted to compute the final labels. Intuitively, our topic-sentence document graph has the following advantages: 1) During the graph propagation, sentence representations can be enriched by topical information, which can be considered as a kind of document-level feature and help our model distil important contents from an entire document. 2) Topic nodes can act as intermediary to bridge longdistance sentences; hence, our model can efficiently capture inter-sentence relationships. We evaluate our model on four standard datasets, including news articles and scientific papers. The experimental results show its effectiveness and superiority. To summarize, our contributions are threefold. \n\n\uf0b7 We conduct a quantitative exploration on the effect of latent topics on document summarization and provide an intuitive understanding of how topical information help summarize documents. \n\n\uf0b7 We propose a novel graph-based neural extractive summarization model, which innovatively incorporates latent topics into graph propagation via a joint neural topic model. To the best of our knowledge, we are the first to propose applying NTM to the extractive text summarization task.",
            "score": 0.6513379479764847,
            "section_title": "Introduction",
            "char_start_offset": 2107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1812
                },
                {
                    "start": 1815,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 142,
                    "matchedPaperCorpusId": "201697450"
                },
                {
                    "start": 228,
                    "end": 251,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 251,
                    "end": 270,
                    "matchedPaperCorpusId": "155100086"
                },
                {
                    "start": 680,
                    "end": 701,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "264614849",
            "title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations",
            "text": "-based Representations for Summarization. A growing body of work focuses on using graph-based representations for improving summarization. Whereas different approaches encode graphs into neural models for multi-document summarization (Fan et al., 2019;Li et al., 2020;Pasunuru et al., 2021;Chen et al., 2021), AMR structures have been shown to benefit both document representation and summary generation (Liu et al., 2015;Liao et al., 2018;Hardy and Vlachos, 2018;Dohare et al., 2018) and have the potential of improving controllability in summarization . The above works are related to FACTGRAPH as they use semantic graphs for content representation, but also different because they utilize graphs for the downstream summarization task, whereas FACTGRAPH employ them for factuality evaluation.\n\nSemantic Representations for Factuality Evaluation. More closely to our work, Goodrich et al. (2019) extract tuples from the document and summary and measure the factual consistency by overlapping metrics. Recently, dependency arc entailment (DAE, Goyal and Durrett, 2020) is used to measure subsentence-level factuality by classifying pairs of words defined by dependency arcs which often describe semantic relations. However, FACTGRAPH is considerably different from those approaches, since it explicitly encodes the entire graph semantic structure into the model. Moreover, while DAE considers semantic edge relations of the summary only, FACTGRAPH encodes the semantic structures of both the input document and summary leading to better factuality performance at both sentence and subsentence levels.",
            "score": 0.6498881218233836,
            "section_title": "Related Work",
            "char_start_offset": 7114,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 252,
                    "matchedPaperCorpusId": "202785778"
                },
                {
                    "start": 252,
                    "end": 268,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 268,
                    "end": 290,
                    "matchedPaperCorpusId": "235097309"
                },
                {
                    "start": 290,
                    "end": 308,
                    "matchedPaperCorpusId": "239768911"
                },
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "5001921"
                },
                {
                    "start": 422,
                    "end": 440,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 440,
                    "end": 464,
                    "matchedPaperCorpusId": "52111191"
                },
                {
                    "start": 464,
                    "end": 484,
                    "matchedPaperCorpusId": "51867906"
                },
                {
                    "start": 875,
                    "end": 897,
                    "matchedPaperCorpusId": "173188138"
                },
                {
                    "start": 1045,
                    "end": 1068,
                    "matchedPaperCorpusId": "222291532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91015625
        },
        {
            "corpus_id": "63821954",
            "title": "Fast Document Summarization using Locality Sensitive Hashing and Memory Access Efficient Node Ranking",
            "text": "Comparative multi document summarization (CDS) is first proposed in [10] to summarize differences between comparable document groups. [10] presents a sentence selection strategy modeled by means of conditional entropy, which precisely discriminates the documents in different groups. \n\nGraph-based methods like TextRank [12] and PageRank [13] model a document or a set of documents as a text similarity graph, constructed by taking sentences as vertices and the similarity between sentences as edge weights. They take into account the global information and recursively calculate the sentence significance from the entire text graph rather than simply relying on unconnected individual sentences. From an NLP perspective, extractive summarization embodies two criteria: sentence relevance and sentence redundancy. Graph-based sentence ranking algorithms successfully merge both of these criteria into a single framework, by utilizing the so-called graph-based lexical centrality principle. Graph-based ranking algorithms were also used in query-focused summarization when it became a popular research topic. For instance, a topic-sensitive version of LexRank is proposed by [14]. It integrates the relevance of a sentence to the query into LexRank to get a biased PageRank ranking. \n\nSimilar work to ours [21] presents a new principled and versatile summarization framework MDS using the submodal function. This framework can deal with different summarization tasks, including generic, query-focused, updated, comparative summarization. The empirical results show that this framework outperforms the other rivals in the generic summarization and is competitive in other summarization tasks. In [22] authors have investigated the use of maximum entropy, naive-Bayes, support vector machine models and a hybrid machine model for multi-document automatic text summarization.",
            "score": 0.6487639875301161,
            "section_title": "RELATED WORK",
            "char_start_offset": 6811,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1281
                },
                {
                    "start": 1284,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "8834940"
                },
                {
                    "start": 134,
                    "end": 138,
                    "matchedPaperCorpusId": "8834940"
                },
                {
                    "start": 338,
                    "end": 342,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1174,
                    "end": 1178,
                    "matchedPaperCorpusId": "17636912"
                },
                {
                    "start": 1305,
                    "end": 1309,
                    "matchedPaperCorpusId": "15990379"
                },
                {
                    "start": 1694,
                    "end": 1698,
                    "matchedPaperCorpusId": "7992772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "258999996",
            "title": "Contrastive Hierarchical Discourse Graph for Scientific Document Summarization",
            "text": "Graph models have been widely applied to extractive summarization due to the capability of modeling cross-sentence relations within a document. The sparsity nature of graph structure also brings scalability and flexibility, making it a good fit for long documents. Graph neural networks' memory costs are generally linear with regard to the input size compared to the quadratic self-attention mechanism. \n\nResearchers have explored supervised graph neural network methods for summarization (Cui and Hu, 2021;Jia et al., 2020;Huang and Kurohashi, 2021;Xie et al., 2022;Phan et al., 2022). Yasunaga et al. (2017) first proposed to use Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) then applied GCN on structural discourse graphs based on RST trees and coreference mentions. Recently, Wang et al. (2020) proposed constructing a word-document heterogeneous graph by using words as the intermediary Figure 1: The overall model architecture of CHANGES. We first construct a hierarchical graph for an input document, and then learn representations with a graph contrastive module and hierarchical graph layers. The concatenation representations of sentence node and its section node will be fused for summary sentence selection. between sentences. Zhang et al. (2022a) proposed to use hypergraph to capture the high-order sentence relations within the document. Our paper follows the series of work but incorporates hierarchical graphs for scientific paper discourse structure modeling and graph contrastive learning for theme-aware sentence representation learning.",
            "score": 0.6460902328466758,
            "section_title": "Graph-based Summarization",
            "char_start_offset": 6565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1600
                }
            ],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 525,
                    "matchedPaperCorpusId": "226262332"
                },
                {
                    "start": 525,
                    "end": 551,
                    "matchedPaperCorpusId": "233189611"
                },
                {
                    "start": 568,
                    "end": 586,
                    "matchedPaperCorpusId": "252818946"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "270076794",
            "title": "The power of graphs in medicine: Introducing BioGraphSum for effective text summarization",
            "text": "State-of-the-art methods techniques were then compared to the performance outcomes obtained using the introduced Bio-GraphSum model.In addition, comparisons were made between summaries generated with an 80 % compression ratio in order to provide a fair comparison with previous similar summarization approaches reported in the literature.\n\nThere are other current and successful studies on biomedical texts in the literature that are graph-based and that provide good results [2,4].However, these studies were excluded from the current study's comparison in order to provide a fair and realistic  [8,25].Therefore, the LexRank algorithm was included in the comparisons as a precursor to graph-based biomedical summarization methods [34].By assigning points to sentences based on the LexRank method, inferential summaries were provided.In the framework of NLP, Erkan et al. suggested a stochastic and graph-based method to determine the relevance of text units.They assessed the significance of sentences in the representative graphs using an eigenvector centrality metric (based on node centrality).The authors showed that LexRank frequently outperformed both degree-based and centroid-based algorithms through their experimental approach.Another competitive and successful method is known as \"Leveraging BERT,\" where BERT and K-Means clustering are used together [35].\n\nAdditionally, the baselines used the unsupervised MultiRank algorithm, which successfully models networks by using word similarity, semantic similarity, and common reference similarity.Using SemRep, MetaMap, and OGER, the MultiGBS algorithm extracts concepts and relationships from UMLS data [8].\n\nThe proposed BioGraphSum model was tested against a corpus created from 450 current scientific research articles published in PubMed.In Table 3 the highest value for each row is presented as bold text.Based on the values for the ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-SU metrics, the BioGraphSum model's results with a compression ratio of 80 % were compared with state-of-the-art models for each performance parameter.",
            "score": 0.6459797693927392,
            "section_title": "Comparison with state-OF-the-art methods",
            "char_start_offset": 34654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 338
                },
                {
                    "start": 340,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 960
                },
                {
                    "start": 960,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1369
                },
                {
                    "start": 1371,
                    "end": 1556
                },
                {
                    "start": 1556,
                    "end": 1667
                },
                {
                    "start": 1669,
                    "end": 1802
                },
                {
                    "start": 1802,
                    "end": 1870
                },
                {
                    "start": 1870,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 479,
                    "matchedPaperCorpusId": "213613078"
                },
                {
                    "start": 479,
                    "end": 481,
                    "matchedPaperCorpusId": "250153151"
                },
                {
                    "start": 597,
                    "end": 600,
                    "matchedPaperCorpusId": "221340611"
                },
                {
                    "start": 600,
                    "end": 603,
                    "matchedPaperCorpusId": "49217427"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1663,
                    "end": 1666,
                    "matchedPaperCorpusId": "221340611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50341796875
        },
        {
            "corpus_id": "253763913",
            "title": "N-GPETS: Neural Attention Graph-Based Pretrained Statistical Model for Extractive Text Summarization",
            "text": "Te authors of [50] developed a hybrid method for producing summaries of long scientifc texts that combined the benefts of both extractive and abstractive designs. Te authors in [51,52] use the deep learning model BERT and RISTECB model to answer important questions related to the COVID-19 research articles. Te authors of [44] demonstrated an excellent tuning-based approach for extractive summarization using the BERT model. Te BERT model was also used by the authors of [7,8,16,36,46] for contextual representation in summarization tasks. Te authors in [53] use the BERT model to automatically generate titles from a huge set of published literature or related work. Additionally, extractive summarization tasks using graph structures have been carried out by exploiting linguistic and statistical information included in sentences [9]. Recent research has combined neural networks with graphs, or (GNNs), and used the encoder-decoder structure for extractive summarization [13,54]. Many researchers nowadays use a heterogeneous graph neural network with multiple updated nodes rather than a homogeneous graph structure with no updated nodes for extractive summarization tasks. Te study [55] proposed a bipartite graph attention network for multihop reading comprehension (RC) across documents that encoded diferent documents and entities together. Te authors in [48] presented an approach that modeled redundancy-aware heterogeneous graphs and refned sentence representation using neural networks for extractive summarization. Te studies [9,56]  4 \n\nComputational Intelligence and Neuroscience between sentences are learned. Te work done by [14] built a sentence-level graph-based model, using BERT for sentence encoding and joint neural network model (NTM) for discovering latent topic information. Te authors in [15] proposed a heterogeneous graph structure for modelling crosssentence relationship between sentences. To represent the relationships between the EDUs, they used three diferent types of nodes, including sentence nodes, EDU nodes, and entity nodes, and RST discourse parsing and leverage external discourse expertise to enhance the model's performance. Te next section goes over the unique model N-GPETS methodology that is proposed in this study in depth.",
            "score": 0.6446619505888246,
            "section_title": "Extractive Text Summarization Approaches and",
            "char_start_offset": 16057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1551
                },
                {
                    "start": 1554,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2276
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "246287960"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "246418639"
                },
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 476,
                    "end": 478,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "247041797"
                },
                {
                    "start": 481,
                    "end": 484,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 484,
                    "end": 487,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 556,
                    "end": 560,
                    "matchedPaperCorpusId": "237588401"
                },
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "49656757"
                },
                {
                    "start": 977,
                    "end": 981,
                    "matchedPaperCorpusId": "222310577"
                },
                {
                    "start": 981,
                    "end": 984,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "158046817"
                },
                {
                    "start": 1366,
                    "end": 1370,
                    "matchedPaperCorpusId": "250012576"
                },
                {
                    "start": 1542,
                    "end": 1545,
                    "matchedPaperCorpusId": "49656757"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "matchedPaperCorpusId": "246588027"
                },
                {
                    "start": 1645,
                    "end": 1649,
                    "matchedPaperCorpusId": "233189611"
                },
                {
                    "start": 1818,
                    "end": 1822,
                    "matchedPaperCorpusId": "220586224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "271791043",
            "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
            "text": "Nonetheless, recurrent models face difficulties in capturing long-range dependencies among sentences and effective handling of computational factors, primarily because they only process input and output sequences in a strictly sequential manner [11]. \n\nIn recent years, there has been a notable achievement in applying Graph Attention Networks (GATs) for the purpose to summarized documents [12]. This success stems from their proficiency in capturing complex inter-sentence relationships within documents [3,[13][14][15][16][17]. To be precise, GNN models demonstrate the capability to effectively model intricate structural data that comprises semantic units (nodes) and the relationships (edges) connecting them. Authors of [18] employed the Rhetorical Structure Theory (RST) graph, which relies on joint extraction and syntactic compression, to model cross-sentence associations in the context of SDS. Reference [19] proposes an unsupervised graph-based model that leverages discourse information for hierarchical ranking, which is specifically tailored for extensive scientific documents, utilizing both intra-connections and inter-connections among sentences. The authors in [20] incorporate three types of nodes, namely Sentence, Entity Division Unit (EDU), and Entity Nodes. They utilize RST discourse parsing to capture the relationships between the EDUs. The researchers of [21] devised a Heterogeneous Transformer based model tailored for extractive summarization of lengthy text, incorporating multi-granularity sparse attention and treating tokens, entities, and sentences as distinct types of nodes in the process. \n\nAnother vital aspect of summarization pertains to modeling global information (topical information), which plays a pivotal role in the selection of sentences [22]. They concurrently trained with latent topics to model global information, employing the Neural Topic Model (NTM) for this purpose. The study conducted by the researchers cited in [23] offers an expanded model that employs NTM for abstractive text summarization. Within the graph structure, topics are extracted using clustering methods like K-Means and Gaussian Mixture Models (GMM).",
            "score": 0.6441904462905309,
            "section_title": "Introduction",
            "char_start_offset": 2340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1628
                },
                {
                    "start": 1631,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 391,
                    "end": 395,
                    "matchedPaperCorpusId": "256921197"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "252670538"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "250301954"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "268699293"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "233189611"
                },
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "269525843"
                },
                {
                    "start": 1789,
                    "end": 1793,
                    "matchedPaperCorpusId": "269700993"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "213817744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.650390625
        },
        {
            "corpus_id": "274896099",
            "title": "Advancements in Natural Language Processing for Automatic Text Summarization",
            "text": "Graph-based methods employ graph theories to construct techniques for summarizing text. The sentences within the documents are depicted as nodes in a directed or an undirected network using typical preprocessing techniques such as stemming and stop word removal. Sentences are connected by edges that are determined by the structure of the sentence [6]. In LexRank and TextRank, this type of representation is commonly employed for the purpose of extractive summarization [7]. \n\nThe construction of a graph is necessary for TextRank, wherein the vertices correspond to the sentences that are to be ranked. The determination of sentence \"similarity\" is based upon the degree of overlap in their respective content. The graph exhibits a high degree of interconnectivity, wherein each edge is imposed a weight that signifies the magnitude of ties between distinct pairs of sentences within the given text. The text is depicted in the form of a weighted graph, and subsequently, a ranking process based on weighted graphs is conducted. After the graph is processed by the ranking algorithm, sentences are organized in a decreasing order according to their score. The sentences with the top scores are selected for insertion in the summary [8]. The LexRank algorithm is likely to provide importance to a sentence that showcases similarity to many other sentences within the given text. The approach entails suggesting a certain sentence by assessing its resemblance to other sentences, leading to an elevated ranking. The methodology employed in this study is founded upon the concept of Eigenvector Centrality and follows an interconnected graph structure. As illustrated in Fig. 2, every sentence is situated at a vertex of the graph, and the weights assigned to the edges are calculated utilizing a cosine similarity metric [9]. \n\nThe LexRank algorithm draws inspiration from the TextRank algorithm and can be considered as an enhanced version of the TextRank algorithm. There exist notable distinctions between the two algorithms, which are subsequently accompanied by shared characteristics. Nevertheless, there are notable distinctions in the ranking of sentences, particularly in the utilization of cosine similarity to assess word overlaps across phrases in LexRank compared to TextRank. Both methods operate on undirected graphs, which is a significant commonality. \n\nThese algorithms continue to be employed and deliberated in scholarly articles, in addition to their application in hybrid methodologies which consists of their own pros and cons.",
            "score": 0.6441222520253986,
            "section_title": "A. Graph Based Methodologies",
            "char_start_offset": 7892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1826
                },
                {
                    "start": 1829,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2369
                },
                {
                    "start": 2372,
                    "end": 2551
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "51792554"
                },
                {
                    "start": 472,
                    "end": 475,
                    "matchedPaperCorpusId": "224955327"
                },
                {
                    "start": 1822,
                    "end": 1825,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77099609375
        },
        {
            "corpus_id": "257219819",
            "title": "Topic-Selective Graph Network for Topic-Focused Summarization",
            "text": "Graph neural network [20] has been valued in the field of deep learning for its excellent processing ability on unstructured data and node-centric information aggregation mode. With the advance of graph neural networks, there are many graph networks with special structures, e.g., GCN [21], GAT [27], HAN [30] and r-GCN [21]. Moreover, GNN is often used for downstream tasks such as text classification, information extraction, and text generation. In text summarization, Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled.",
            "score": 0.642988666757816,
            "section_title": "Graph Neural Network",
            "char_start_offset": 7591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1276
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "4675456"
                },
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "5458500"
                },
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "3292002"
                },
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "5458500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "258999996",
            "title": "Contrastive Hierarchical Discourse Graph for Scientific Document Summarization",
            "text": "Thus, sparse Transformer models like BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) are proposed. \n\nRecently, researchers have also turned to graph neural networks (GNN) as an alternative approach. Graph neural networks have been demonstrated to be effective at tasks with rich relational structure and can preserve global structure information (Yao et al., 2019;Xu et al., 2019;Zhang and Zhang, 2020). By representing a document as a graph, GNNs update and learn sentence representations by message passing, and turn extractive summarization into a node classification problem. Among all attempts, one popular way is to construct crosssentence similarity graphs (Erkan and Radev, 2004;Zheng and Lapata, 2019), which uses sentence representation cosine similarity as edge weights to model cross-sentence semantic relations. Xu et al. (2019) proposed using Rhetorical Structure Theory (RST) trees and coreference mentions to capture cross-sentence discourse relations. Wang et al. (2020) proposed constructing a word-document het-erogeneous graph by using words as the intermediary between sentences. Despite their success, how to construct an effective graph to capture the hierarchical structure for academic papers remains an open question. \n\nTo address the above challenges, we propose CHANGES (Contrastive HierArchical Graph neural network for Extractive Summarization), a hierarchical graph neural network model to fully exploit the section structure of scientific papers. CHANGES first constructs a sentence-section hierarchical graph for a scientific paper, and then learns hierarchical sentence representations by dedicated designed information aggregation with iterative intra-section and inter-section message passing. Inspired by recent advances in contrastive learning (Liu and Liu, 2021;Chen et al., 2020), we also propose a graph contrastive learning module to learn global theme-aware sentence representations and provide fine-grained discriminative information. The local sentence and global section representations are then fused for salient sentence prediction. We validate CHANGES with extensive experiments and analyses on two scientific paper summarization datasets. Experimental results demonstrate the effectiveness of our proposed method. Our main contributions are as follows: \n\n\u2022 We propose a hierarchical graph-based model for long scientific paper extractive summarization.",
            "score": 0.6428065350670433,
            "section_title": "Introduction",
            "char_start_offset": 2113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2283
                },
                {
                    "start": 2284,
                    "end": 2322
                },
                {
                    "start": 2325,
                    "end": 2422
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 66,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 366,
                    "end": 384,
                    "matchedPaperCorpusId": "52284222"
                },
                {
                    "start": 400,
                    "end": 422,
                    "matchedPaperCorpusId": "226262336"
                },
                {
                    "start": 684,
                    "end": 707,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1821,
                    "end": 1839,
                    "matchedPaperCorpusId": "211096730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "277043781",
            "title": "Beyond One-Size-Fits-All Summarization: Customizing Summaries for Diverse Users",
            "text": "Early approaches to automatic text summarization were mostly extractive, where the main idea is dependent on selecting the most important sentences from the source text to create a summary. These methods relied mainly on statistical techniques to identify key information in textual data. \n\nOne of the first methods for summarization was proposed by [6], where the author used word frequency to select important sentences from a document. According to this approach, the importance of a sentence can be measured by counting the frequency of certain content words, or keywords, it contains. The sentences are then given scores according to that frequency and ranked. The highest ranking scores are extracted to generate the summary. [7] built upon this approach by utilizing additional features such as cue words, title and heading words, and sentence location. The method assigned weights to these features to score with linear summation and select sentences to create the summary. \n\nGraph-based Models LexRank [8] and TextRank [9] are foundational approaches that conceptualize documents as graphs and frame extractive summarization as the task of identifying the most central nodes within the graph, drawing inspiration from the PageRank algorithm [10]. \n\nMachine learning techniques have taken the idea of extractive summarization one step further. Machine learning approaches model the summarization as a classification problem. [11] developed a trainable summarizer using a Bayesian classifier to determine the relevance of sentences. In this work, the authors created a naive Bayes classifier which differentiated between sentences to be included in a summary and those not to be included. This classifier uses features of the sentences and is trained on a dataset comprising documents and their respective extractive summaries. Following the popularization of deep learning approaches, neural network models have been applied to extractive summarization tasks. [12] introduced a neural model that employs a hierarchical encoder to capture document structure and a sentence extractor to select important sentences. \n\nRecent advances in transformer-based models have significantly improved extractive summarization. [13] proposed a BERT-based extractive summarization model that achieves state-of-the-art performance by leveraging pre-trained language representations. Their approach demonstrates the effectiveness of fine-tuning pre-trained models for specific summarization tasks.",
            "score": 0.6423391521901588,
            "section_title": "Extractive Summarization",
            "char_start_offset": 3083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2120
                },
                {
                    "start": 2123,
                    "end": 2220
                },
                {
                    "start": 2221,
                    "end": 2373
                },
                {
                    "start": 2374,
                    "end": 2487
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 353,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "1177942"
                },
                {
                    "start": 1011,
                    "end": 1014,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1028,
                    "end": 1031,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1433,
                    "end": 1437,
                    "matchedPaperCorpusId": "5775833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "63238333",
            "title": "Review assessment support in Open Journal System using TextRank",
            "text": "Some of the successful methods of extractive summarization uses supervised approach [3], by learning from a large number of labelled training data to determine how to make a good summary, such as [3,4]. However, the drawback of using such methods is the inability to adapt to a new domains or languages, as it needs an entirely new training data each time. TextRank uses an unsupervised approach where no training data is required to produce a summary, independent of the language used. \n\nIn a graph-based summarization, text is represented as a graph where nodes represent words or sentences, and edges represent the semantic relations between nodes. Graph based ranking model takes into account all the information within a graph, rather than relying only on each local node information [3]. One graph-based algorithm for summarization is LexRank [6], which uses cosine similarity and TF-IDF to calculate the weight of edges, applied to summarize multiple documents. In comparison, TextRank's measurement is based on the number of words two sentences have in common, applied to summarize single documents or to extract important keywords. While graphbased ranking model offers good result, the time it takes to complete the algorithm is proportional to the complexity of the graph, therefore it might not be a very efficient algorithm, timewise [7].",
            "score": 0.6408994915341085,
            "section_title": "Similar methods",
            "char_start_offset": 2225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1351
                }
            ],
            "ref_mentions": [
                {
                    "start": 849,
                    "end": 852,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1347,
                    "end": 1350,
                    "matchedPaperCorpusId": "207721223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "273784877",
            "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
            "text": "Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-Seek-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-Seek-2 across seven datasets spanning various domains\u2014including news articles, lawsuits, government reports, and scientific texts\u2014under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics.",
            "score": 0.6394827631866076,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "53083244",
            "title": "Neural Related Work Summarization with a Joint Context-driven Attention Mechanism",
            "text": "Following are five classical generic solutions, including: 1) Luhn (Luhn, 1958): a heuristic summarization based on word frequency and distribution; 2) MMR (Carbonell and Goldstein, 1998): a diversity-based re-ranking to produce summaries; 3) LexRank (Erkan et al., 2004): a graph-based summary technique inspired by PageRank and HITS; 4) SumBasic (Nenkova and Vanderwende, 2005): a frequency-based summarizer with duplication removal; 5) NltkSum (Acanfora et al., 2014): a natural language tookit (NLTK)-based implementation for summarization. \n\nFor clarity, Luhn, LexRank and SumBasic are analogous to the work of Hu and Wan (2014) which extracts sentences scoring the highest in significance, and they are also contrasted in the latest studies on neural summarizers (Chen et al., 2016;Tan et al., 2017). Meanwhile, MMR often serves as a part/post-processing of existing techniques to avoid the redundancy (Cohan and Goharian, 2017), and we introduce NltkSum to investigate the impact of grammatical/semantic analysis to the automatic related work summarization. Note that former studies specially for this task require extensive human involvements (see Table 1), thus we cannot apply them to such a large dataset of this study.",
            "score": 0.6379145213915254,
            "section_title": "Experimental Setup",
            "char_start_offset": 21832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1230
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 186,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 251,
                    "end": 270,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 447,
                    "end": 470,
                    "matchedPaperCorpusId": "61673212"
                },
                {
                    "start": 616,
                    "end": 633,
                    "matchedPaperCorpusId": "1948673"
                },
                {
                    "start": 788,
                    "end": 805,
                    "matchedPaperCorpusId": "26698484"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "3591956",
            "title": "Live Blog Corpus for Summarization",
            "text": "As benchmark results, we employ methods that have been successfully used for both single and multi-document summarization. Some variants of them have also been applied to update summarization tasks. TF IDF (Luhn, 1958) scores sentences with the TF*IDF of their terms. The best sentences are then greedily extracted. \n\nLexRank (Erkan and Radev, 2004) is a well-known graphbased approach. A similarity graph G(V, E) is constructed where V is the set of sentences and an edge e ij is drawn between sentences v i and v j if and only if the cosine similarity between them is above a given threshold. Sentences are then scored according to their PageRank in G. \n\nLSA (Steinberger and Jezek, 2004) is an approach involving a dimensionality reduction of the term-document matrix via singular value decomposition (SVD).",
            "score": 0.6377034922688626,
            "section_title": "Baselines",
            "char_start_offset": 13655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 810
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 218,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 326,
                    "end": 348,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 661,
                    "end": 689,
                    "matchedPaperCorpusId": "14961089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81396484375
        },
        {
            "corpus_id": "269157041",
            "title": "Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization",
            "text": "Multi-Document Scientific Summarization (MDSS) involves consolidating scattered information from multiple papers.According to the paradigm adopted, previous studies can be categorized into graph-based [5,36], flat-based [23,31], and hierarchical-based methods [6,30].Concretely, graph-based methods construct external graphs (e.g., entity-relation graph, AMR, TF-IDF) to assist document representation and cross-document relation modeling.In this regard, Chen et al. [5] leverage graphs to encode source articles, constructing keyphrase graphs and using a multi-level contrastive learning strategy to improve the accuracy of generated summaries.\n\nWang et al. [36] incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template.Flat-based methods simply concatenate multiple documents, thus they can leverage state-of-the-art pre-trained summarization models.Moro et al. [23] select top  documents with a dense retriever and use a BART model [13] to produce the summary.Shi et al. [31] leverage Large Language Model (LLM) to expand the query for reference retrieval and adopt an instruction-tuning method to guide LLM to generate summary.Compared with flatbased methods, hierarchical-based methods take the hierarchical structure of document clusters into account, thus preserving crossdocument relations and obtaining semantic-rich representations.Chen et al. [6] apply word representations and cross-document relationships to model document-level representations, and design a hierarchical decoding strategy.Shen et al. [30] modify the BART model by incorporating document-level representations in the encoder and decoder to facilitate multi-document interactions.However, we argue that all the above works fail to explore external guidance information for better global semantic control and summary decoding.",
            "score": 0.6370807583492244,
            "section_title": "RELATED WORK 2.1 Multi-Document Scientific Summarization",
            "char_start_offset": 6345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 645
                },
                {
                    "start": 647,
                    "end": 785
                },
                {
                    "start": 785,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1027
                },
                {
                    "start": 1027,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1567
                },
                {
                    "start": 1567,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "249097545"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "252185277"
                },
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "266176619"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "236460044"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "249097545"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "252185277"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "266176619"
                },
                {
                    "start": 1418,
                    "end": 1421,
                    "matchedPaperCorpusId": "236460044"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8935546875
        },
        {
            "corpus_id": "248571519",
            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
            "text": "Abstractive multi document summarization is the task of writing a single summary of the key points and content in multiple related documents. This task has evolved from research in single document abstractive and extractive summarization; however, it faces unique challenges due to input documents having duplicate and conflicting content as well a larger body of text. (Radev et al., 2000). This task has evolved from early approaches using sequence to sequence (Seq2Seq) neural architectures to transformer based architectures with the introduction of large-scale datasets (Liu et al., 2018), (Fabbri et al., 2019). Beyond the introduction of approaches now commonly used for single document abstractive summarization, cross document attention and graphs that capture relations between text in various documents have further improved the state of the art for multi document summarization tasks. (Liu et al., 2018), (Li et al., 2014). These graphs aim to better represent the inter dependencies between articles by representing text spans as nodes in the graph and capturing the relations between these sentences as edge weights. \n\nDespite the advances made with these approaches, a significant challenge remains in multi document summarization with respect to how to deal with contradictory information present in the multiple source documents. It is critical to both learn the relationships between different documents as well as to extract salient information that is consistent with the output viewpoint. This is a situation often faced with summarizing multiple news articles where different viewpoints on an issue can significantly change the semantic structure of the content present in each article making it challenging for the abstractive summarization model to learn the relationships between inconsistent or conflicting information. In this work we define conflicting opinions as a combined measure of the polarity and sentiment of text. By this definition, two pieces of text on the same topic that have a differing sentiment and/ or polarity are determined to have different viewpoints. This definition is used throughout the paper. \n\nThis paper proposes ACM, attribute conditioned multi document summarization, a novel approach to multi document summarization that incorporates an attribute conditioning module with an abstractive multi document summarization model in order to condition for a particular attribute when generating the multi document summary.",
            "score": 0.6368012863720597,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2147
                },
                {
                    "start": 2150,
                    "end": 2474
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 390,
                    "matchedPaperCorpusId": "1320"
                },
                {
                    "start": 917,
                    "end": 934,
                    "matchedPaperCorpusId": "10112929"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79931640625
        },
        {
            "corpus_id": "218470557",
            "title": "HipoRank: Incorporating Hierarchical and Positional Information into Graph-based Unsupervised Long Document Extractive Summarization",
            "text": "Single document summarization aims at shortening a text while preserving the most important ideas of the source document (Nenkova et al., 2011). Current approaches can be either abstractive or extractive. Abstractive models can generate summaries with novel words or phrases that are not present in the source document. In contrast, extractive methods generate summaries by copying text snippets directly from the source document. While abstractive models can be more concise and flexible, extractive models faithfully preserve the original text and are usually more fluent (Kryscinski et al., 2019). \n\nModern supervised neural network-based models have been proposed with extractive strategies weights) in the resulting graph. While LexRank assumes similarity is symmetric and uses undirected edges (Erkan and Radev, 2004), PACSUM assumes earlier sentences in a document are more central and uses directed edges to capture this asymmetry. However, PACSUM faces two challenges which limit its ability to summarize long out-of-domain documents: 1) the lead positional bias is not as prevalent in other domains such as scientific writing; 2) the sentence-level graph representation fails to capture global and topical information found in longer structured documents (Xiao and Carenini, 2019). \n\nDiscourse structure can help address these issues and better identify important sentences in document summarization (Marcu, 1999). More specifically, Lin and Hovy (1997) show that discourse structure leads to positional preferences for important sentences in a document and confirm Baxendale (1958)'s hypothesis that important sentences occur at the start and end of paragraphs. Teufel (1997) re-contextualize these findings for scientific papers that are organized hierarchically, stating that peripheral paragraphs are more likely to contain crucial information in addition to the hypothesis of Baxendale (1958). \n\nBased on these findings in paradigmatic discourse structure of scientific articles, we propose to incorporate discourse structures into graph-based summarization models by augmenting the measure of sentence centrality with: 1) boundary positional information: we propose a boundary positional function on the edge weights in the directed graph, assuming the contribution of one node to another depends on their relative position in the source document.",
            "score": 0.6366775882780074,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 143,
                    "matchedPaperCorpusId": "47179013"
                },
                {
                    "start": 800,
                    "end": 823,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1265,
                    "end": 1290,
                    "matchedPaperCorpusId": "201697450"
                },
                {
                    "start": 1410,
                    "end": 1423,
                    "matchedPaperCorpusId": "14048633"
                },
                {
                    "start": 1444,
                    "end": 1463,
                    "matchedPaperCorpusId": "5519987"
                },
                {
                    "start": 1576,
                    "end": 1592,
                    "matchedPaperCorpusId": "21241987"
                },
                {
                    "start": 1673,
                    "end": 1686,
                    "matchedPaperCorpusId": "11846745"
                },
                {
                    "start": 1891,
                    "end": 1907,
                    "matchedPaperCorpusId": "21241987"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "248496597",
            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
            "text": "Dohare et al. (2018) propose to generate abstractive summaries by first generating AMR graphs (abstract meaning representation) for corresponding input stories, extract summary graphs from the AMR graphs, and lastly create summaries from the summary graphs. \n\nIn the field of unsupervised multi-document summarization, Chu and Liu (2019) propose an end-to-end, unsupervised, abstractive, neural summarization model. Their model consists of two parts: (i) an LSTM-based auto-encoder that learns a representation of each input text and (ii) a summarization module that learns to generate summaries based on the representation encoded by the auto-encoder. The content of all input texts is obtained as the mean of all last hidden and cell states of the LSTM network. This information is then fed into the decoder of the auto-encoder to decode a summary. Lastly, they evaluate average similarity of summary to all input texts by encoding the summary again with the encoder of the auto-encoder and computing the average cosine distance between the hidden states of each encoded input text and the hidden state of the encoded summary. \n\nTheir approach requires no labeled training data to summarize multiple documents. However, it is also based on an assumption that all input texts to be summarized contain similar information. This is often not the case in real world as often times documents to be summarized have different gist. In contrast to neural-based summarization methods, Nayeem et al. (2018) propose an unsupervised multi-document summarization system comprising of word graph-based (Filippova, 2010) sentence fusion and integer linear programming (ILP)-based sentence ranking. They first apply hierarchical agglomerative clustering with complete linkage to all sentences in the documents, where the distance between two sentences are based on their continuous representations. For each cluster of related sentences, they generate abstractive fused sentences by first constructing a word graph and then apply k-shortest path on it. They not only merge similar sentences into a single one but also substitute such lexicals as verbs or nouns with ones having functional similarities, making the resulting sentence more abstractive.",
            "score": 0.6366624532875798,
            "section_title": "Related Work",
            "char_start_offset": 6251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 337,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 1478,
                    "end": 1498,
                    "matchedPaperCorpusId": "52011473"
                },
                {
                    "start": 1590,
                    "end": 1607,
                    "matchedPaperCorpusId": "14750088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "268594273",
            "title": "Unsupervised Extractive Dialogue Summarization in Hyperdimensional Space",
            "text": "Extractive summarization selects n most representative sentences from a document as its summary.For the scope of this paper, we assume n is known.Extractive summarizers are used both as stand-alone systems and, more recently, as part of hierarchical summarization schemes.The integration of extractive summarizers into hybrid summary pipelines demands the summarizers to be both efficient and accurate.Therefore, we explore a wide range of baseline extractive summarizers that specialize in both spectrums.\n\nTextrank [11] is a graph-based summarizer that identifies nodes based on lexical centrality.PacSum [12] also utilizes sentence centrality to identify the most important sentences in a document.Each sentence in the document is embedded with a neural language model and added as a node to a sentence graph.OTExtSum [13] selects a set of summary sentences that minimizes the semantic distance between the candidate summary set and the original document.Following the previous literature, we also include Lead-n [14] as our baseline.Lead-n selects the n-leading sentences as the summary, exploiting the lead bias typically present in documents.We use BERT variants of PacSum and OTExtSum as our baselines for their state-of-the-art performance.",
            "score": 0.636514098827102,
            "section_title": "Extractive summarization",
            "char_start_offset": 3241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 96,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 506
                },
                {
                    "start": 508,
                    "end": 600
                },
                {
                    "start": 600,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 812
                },
                {
                    "start": 812,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1037
                },
                {
                    "start": 1037,
                    "end": 1148
                },
                {
                    "start": 1148,
                    "end": 1248
                }
            ],
            "ref_mentions": [
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "248299685"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "224801419"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72412109375
        },
        {
            "corpus_id": "52272243",
            "title": "Semantic WordRank: Generating Finer Single-Document Summarizations",
            "text": "Unsupervised methods. Relations between words play a central role in unsupervised extractive methods, with the underlying idea that related words \"promote\" each other. In particular, TextRank [14] and LexRank [6] model a document as a sentence graph, and use the PageRank algorithm to rank sentences. TextRank offers robust performance over the DUC-02 dataset. These methods extract sentences based only on sentence scores, without considering topic diversity.\n\nUniformLink [23] builds a sentence graph on a set of similar documents, where a sentence's score is computed based on both with-in document score and cross-document score. URank [22] uses a unified graph-based framework to study both single-document and multi-document summarizations.\n\nE coh [19], as well as T coh [18], use a bipartite graph to represent a document and a different algorithm, Hyperlink-Induced Topic Search (HITS) [11], is used to score sentences. They both treat the summarization problem as an ILP problem, which maximizes the sentence importance, non-redundancy, and coherence at the same time. However, since ILP is an NP-hard problem, obtaining an exact solution to an ILP problem is intractable.\n\nSubmodularity optimization [13] and Latent Semantic Analysis [8] are two other widely used unsupervised techniques for extractive summarizations.\n\nSupervised methods. Traditional supervised machine learning methods often need handcrafted features. These methods include Support Vector Machine and Naive Bayesian Classifier [24]. CP3 [17], which has the same underlying idea as E coh and T coh , mines coherence patterns in a corpus of abstracts, and extracts sentences by solving an ILP problem.\n\nDeep learning methods, able to learn sentence or document representations automatically, have recently been used to score sentences. For example, R2N2 [4] uses a recursive neural network for both word level and sentence level scoring, followed by an ILP optimization strategy for selecting sentences. CNN-W2V [25] is another example, which modifies a convolutional-neural-network (CNN) model of sentence classification [9] to rank sentences",
            "score": 0.6359886643335192,
            "section_title": "Related Work",
            "char_start_offset": 3088,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 209,
                    "end": 212,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "207178257"
                },
                {
                    "start": 640,
                    "end": 644,
                    "matchedPaperCorpusId": "17224077"
                },
                {
                    "start": 754,
                    "end": 758,
                    "matchedPaperCorpusId": "14547015"
                },
                {
                    "start": 777,
                    "end": 781,
                    "matchedPaperCorpusId": "269225"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "216000619"
                },
                {
                    "start": 1210,
                    "end": 1214,
                    "matchedPaperCorpusId": "320371"
                },
                {
                    "start": 1244,
                    "end": 1247,
                    "matchedPaperCorpusId": "7003631"
                },
                {
                    "start": 1506,
                    "end": 1510,
                    "matchedPaperCorpusId": "18517541"
                },
                {
                    "start": 1516,
                    "end": 1520,
                    "matchedPaperCorpusId": "2917544"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "237353454",
            "title": "SummerTime: Text Summarization Toolkit for Non-experts",
            "text": "The following base summarization models are used in SummerTime. They all take a single document and generate a short summary. TextRank (Mihalcea and Tarau, 2004) is a graphbased ranking model that can be used to perform extractive summarization; LexRank (Erkan and Radev, 2004) is also a graphbased extractive summarization model, which is originally developed for multi-document summarization, but can also be applied to a single docu-ment. It uses centrality in a graph representation of sentences to measure their relative importance; BART (Lewis et al., 2020) is an autoencoder model trained with denoising objectives during training. This seq2seq model is constructed with a bidirectional transformer encoder and a left-to-right transformer decoder, which can be fine-tuned to perform abstractive summarization; Pegasus (Zhang et al., 2020a) proposes a new self-supervised pretraining objective for abstractive summarization, by reconstructing the target sentence with the remaining sentences in the document, it also shows strong results in low-resource settings; Longformer (Beltagy et al., 2020) addresses the problem of memory need for self-attention models by using a combination of sliding window attention and global attention to approximate standard self-attention. It is able to support input length of 16K tokens, a large improvement over previous transformer-based models.",
            "score": 0.6356895596667668,
            "section_title": "Single-document Summarization",
            "char_start_offset": 7663,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 160,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 543,
                    "end": 562,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 825,
                    "end": 846,
                    "matchedPaperCorpusId": "209405420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "238582919",
            "title": "Learning to Describe Solutions for Bug Reports Based on Developer Discussions",
            "text": "Supervised Extractive: Using a greedy approach for obtaining noisy extractive summaries (Nallapati, Zhai, and Zhou 2017), we train a supervised extractive summarization model, similar to (Liu and Lapata 2019). LexRank: We use LexRank (Erkan and Radev 2004), an unsupervised graph-based extractive summarization approach. We extract 1 sentence with threshold 0.1. U 1 (Lead 1): This entails simply taking the first sentence of the first utterance, intended to simulate the Lead-1 baseline that is commonly used in summarization. U 1 (Lead 3): This entails simply taking the first 3 sentences of the first utterance, intended to simulate the Lead-3 baseline that is commonly used in summarization. U tg : Since some part of the solution is often mentioned within U tg , we copy this utterance. U tg (Lead 1): Since the length of an utterance is quite different than that of a description (Table 1), we extract only the lead sentence of U tg . U tg (Lead 3): For the reason stated above, we also apply the Lead-3 baseline to this utterance. U tg (Last sentence): Rather than extracting the lead sentence, we try extracting the last sentence of U tg . U tg (Last 3 sentences): Rather than extracting the lead 3 sentences, we try extracting the last 3 sentences of U tg .",
            "score": 0.6354175003094609,
            "section_title": "C.1 Extractive Baselines",
            "char_start_offset": 34897,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 120,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 187,
                    "end": 208,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 234,
                    "end": 256,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5546875
        },
        {
            "corpus_id": "221819240",
            "title": "Extracting Summary Knowledge Graphs from Long Documents",
            "text": "Document summarization models create summaries by identifying the most important sentences from documents (Nallapati, Zhai, and Zhou 2017;Narayan, Cohen, and Lapata 2018) or using a decoder to generate abstractive summaries (Rush, Chopra, and Weston 2015;Celikyilmaz et al. 2018). Although text summarization tasks (Liu and Lapata 2019;Yasunaga et al. 2019) share our objective of distilling crucial information from documents, they mix this objective with the goal of producing fluent natural language text. \n\nWe argue that summarizing entities and relations directly as the first step could decouple the mixed burdens on models and help models to check the factual correctness of a summary. These advantages can benefit other text generation tasks that rely on long document understanding and representation, such as generation grounded on long text. An increasing number of recent works (Wang, Cho, and Lewis 2020;Durmus, He, and Diab 2020;Zhang et al. 2020) have proposed automatically evaluating summarization models by applying information extraction or question answering models to match entities or relations between generated and reference summaries. These newly proposed measures are found to have much higher correlation with human judgements than standard measures. \n\nRecent applications of large pretrained language models such as Ribeiro et al. (2020) and Kale (2020) show the promise of generating fluent and accurate text from knowledge graphs, highlighting the need for identifying correct 1 Papers with Code: paperswithcode.com underlying knowledge representations. In addition, summarized knowledge graphs from multiple documents can be naturally merged by collapsing shared entity nodes to bring even richer information. And such summarized structures can be more easily leveraged to facilitate downstream tasks. \n\nAnother line of work that is closely related to ours is graph-based summarization, which leverages graph structures of documents to facilitate the summary generation (Erkan and Radev 2004;Tan, Wan, and Xiao 2017;Yasunaga et al. 2019;Huang, Wu, and Wang 2020;Xu et al. 2020). These works try to leverage graphs that capture relations between sentences or discourse units. Wang et al. (2020a) incorporate graphs between entities extracted by sentence-level IE systems without considering entity or relation salience.",
            "score": 0.635150650665424,
            "section_title": "Text Summarization",
            "char_start_offset": 4598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1832
                },
                {
                    "start": 1835,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 138,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 138,
                    "end": 170,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 224,
                    "end": 255,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 255,
                    "end": 279,
                    "matchedPaperCorpusId": "4406182"
                },
                {
                    "start": 315,
                    "end": 336,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 917,
                    "end": 943,
                    "matchedPaperCorpusId": "218571335"
                },
                {
                    "start": 943,
                    "end": 961,
                    "matchedPaperCorpusId": "207794190"
                },
                {
                    "start": 2001,
                    "end": 2023,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 2023,
                    "end": 2047,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 2068,
                    "end": 2093,
                    "matchedPaperCorpusId": "218487279"
                },
                {
                    "start": 2093,
                    "end": 2108,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 2206,
                    "end": 2225,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "173990832",
            "title": "Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization",
            "text": "We compare our system with a number of strong summarization baselines (Table 1 and 2). In particular, SumBasic (Vanderwende et al., 2007) is an extractive approach assuming words occurring fre-\n\nOpinosis (Ganesan et al., 2010) 25.15 5.12 8.12 Extract+Rewrite  29.07 6.11 9.20 Pointer-Gen (See et al., 2017) 31.44 6.40 10.20 SumBasic (Vanderwende et al., 2007) 31.58 6.06 10.06 KLSumm (Haghighi et al., 2009) 31.23 7.07 10.56 LexRank (Erkan and Radev, 2004) 33.10 7.50 11.13 DPP (Kulesza and Taskar, 2011  quently in a document cluster are more likely to be included in the summary; KL-Sum (Haghighi and Vanderwende, 2009) is a greedy approach adding a sentence to the summary to minimize KL divergence; and LexRank (Erkan and Radev, 2004) is a graph-based approach computing sentence importance based on eigenvector centrality. We additionally consider abstractive baselines to illustrate how well these systems perform on multi-document summarization: Opinosis (Ganesan et al., 2010) focuses on creating a word cooccurrence graph from the source documents and searching for salient graph paths to create an abstract; Extract+Rewrite  selects sentences using LexRank and condenses each sentence to a title-like summary; Pointer-Gen (See et al., 2017) seeks to generate abstracts by copying words from the source documents and generating novel words not present in the source text.\n\nOur DPP-based framework belongs to a strand of optimization-based methods. In particular, IC-SISumm (Gillick et al., 2009) formulates extractive summarization as integer linear programming; it identifies a globally-optimal set of sentences covering the most important concepts of the source documents; DPP (Kulesza and Taskar, 2011) selects an optimal set of sentences that are representative of the source documents and with maximum diversity, as determined by the determinantal",
            "score": 0.6348663476467242,
            "section_title": "Summarization Results",
            "char_start_offset": 22726,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 226,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 288,
                    "end": 306,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 384,
                    "end": 407,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 433,
                    "end": 456,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 478,
                    "end": 503,
                    "matchedPaperCorpusId": "1793631"
                },
                {
                    "start": 589,
                    "end": 620,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 715,
                    "end": 737,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1232,
                    "end": 1250,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1482,
                    "end": 1504,
                    "matchedPaperCorpusId": "167874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "266164100",
            "title": "Content- and Topology-Aware Representation Learning for Scientific Multi-Literature",
            "text": "Researchers may want to know the whole story of a topic without reading every paper related to it, which falls under the subfield of NLP known as multi-document summarization (MDS). While other multi-document tasks, such as multidocument reading comprehension, which aims to understand the content of multiple documents, are also of interest, our focus in this paper is on MDS to evaluate the learned multi-document rep-resentation. Extractive methods usually produce a summary by selecting the ranked sentences from the given document set (Wan et al., 2015;Mendes et al., 2019;Zhong et al., 2020). For example, Nallapati et al. (2017) treats MDS as a sequence classification problem where each sentence is visited in sequential order and then adopts an RNNbased model to decide whether the sentence should be included in the summarization or not. Liu et al. (2019) conceptualizes MDS to induce a multi-root dependent tree representation of the documents. However, similar sentences may be close in the vector space and share close scores which would cause redundancy problem (Narayan et al., 2018). Recently, graph-based approaches which aim to extract salient textual units from documents based on graph structure representations of sentences are proposed to eliminate this (Chen et al., 2021;Pasunuru et al., 2021). Different from these works, our method leverages document representation with latent semantic information as nodes and their citation relations as edges to construct the multi-document level graph. Then, the message passing mechanism can be resorted to equipping relatedness information for the final representation which can be applied in multi-document tasks.",
            "score": 0.6348469580511242,
            "section_title": "Related Work",
            "char_start_offset": 7118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1680
                }
            ],
            "ref_mentions": [
                {
                    "start": 612,
                    "end": 635,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1276,
                    "end": 1295,
                    "matchedPaperCorpusId": "233169121"
                },
                {
                    "start": 1295,
                    "end": 1317,
                    "matchedPaperCorpusId": "235097309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.814453125
        },
        {
            "corpus_id": "52205000",
            "title": "Learning to Summarize Radiology Findings",
            "text": "For our main experiments, we compare our model against several competitive non-neural and neural systems on the collected dataset. Unless otherwise stated, the baseline models take only the findings section as input.3 S&J-LSA. This is an extractive approach described by Steinberger and Jezek (2004), which applies Latent Semantic Analysis (LSA) to summarization. It first scores \"concept\" clusters by applying singular value decomposition to the termby-sentence co-occurence matrix derived from the passage. Sentences with the top scored concepts are then kept as the summaries. \n\nLexRank. LexRank is another popular extractive model introduced by Erkan and Radev (2004). In LexRank, a passage is represented as a graph of sentences, and a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph. Sentences are scored by the eigenvector centrality in the graph, and the highest scored sentences are kept. \n\nPointer-Generator. We also run the baseline pointer-generator model introduced by See et al. (2017). We find the \"coverage\" mechanism described in the paper did not improve summary quality in our task and therefore did not use it for simplicity. We compare our model with two versions of the pointer-generator model: one with only the findings section as input and another one with the background sections prepended to the findings section as input.",
            "score": 0.6338070072597651,
            "section_title": "Baseline Models",
            "char_start_offset": 14797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1408
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 299,
                    "matchedPaperCorpusId": "14961089"
                },
                {
                    "start": 649,
                    "end": 671,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1041,
                    "end": 1058,
                    "matchedPaperCorpusId": "8314118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65771484375
        },
        {
            "corpus_id": "264817506",
            "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders",
            "text": "There are several simple unsupervised summarization extraction frameworks, including TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), as well as more robust graph-based ranking methods such as Pac-Sum (Zheng and Lapata, 2019), FAR (Liang et al., 2021), DASG (Liu et al., 2021). Graph-based ranking methods take sentence representations as input, using the algorithm of graph-based sentence centrality ranking for sentence selection. We now introduce extractive summarization backbones. \n\n\u2022 TextRank and LexRank utilize PageRank to calculate node centrality based on a Markov chain model recursively. \n\n\u2022 PacSum (Zheng and Lapata, 2019) constructs graphs with directed edges. The rationale behind this approach is that the centrality of two nodes is influenced by their relative position in the document, as illustrated by Equation 15. \n\n\u2022 DASG (Liu et al., 2021) selects sentences for summarization based on the similarities and relative distances among neighbouring sentences. It incorporates a graph edge weighting scheme to Equation 15, using a coefficient that maps a pair of sentence indices to a value calculated by their relative distance. \n\n\u2022 FAR (Liang et al., 2021) modifies Equation 15by applying a facet-aware centrality-based ranking model to filter out insignificant sentences. FAR also incorporates a similarity constraint between candidate summary representation and document representation to ensure the selected sentences are semantically related to the entire text, thereby facilitating summarization. \n\nThe main distinction among the extractive frameworks mentioned above lies in their centrality algorithms. A comprehensive comparison of these algorithms can be found in Appendix 8.",
            "score": 0.6333964434166657,
            "section_title": "Backbones of Summarization Approaches",
            "char_start_offset": 14687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1727
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 120,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 133,
                    "end": 156,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 225,
                    "end": 249,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 255,
                    "end": 275,
                    "matchedPaperCorpusId": "236477774"
                },
                {
                    "start": 282,
                    "end": 300,
                    "matchedPaperCorpusId": "235792514"
                },
                {
                    "start": 868,
                    "end": 886,
                    "matchedPaperCorpusId": "235792514"
                },
                {
                    "start": 1179,
                    "end": 1199,
                    "matchedPaperCorpusId": "236477774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "237353135",
            "title": "Multiplex Graph Neural Network for Extractive Text Summarization",
            "text": "Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate effectiveness of our method.",
            "score": 0.6320595395201758,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65185546875
        },
        {
            "corpus_id": "258041381",
            "title": "Hierarchical Catalogue Generation for Literature Review: A Benchmark",
            "text": "Due to the large input length, we choose an encoderdecoder transformer model that can handle the long text and its backbone model to implement FiD besides various extractive models in our experiments. (I) LexRank (Erkan and Radev, 2004) is an unsupervised extractive summarization approach based on graph-based centrality scoring of sentences. (II) TextRank (Mihalcea and Tarau, 2004) is a graph-based ranking algorithm improved from Google's PageRank (Page et al., 1999) for keyword extraction and document summarization, which uses co-occurrence information (semantics) between words within a document to extract keywords.",
            "score": 0.6318715285714402,
            "section_title": "Baselines",
            "char_start_offset": 19460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 624
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 235,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 358,
                    "end": 383,
                    "matchedPaperCorpusId": "577937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78564453125
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "Modern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents. This approach effectively captured long-range dependencies beyond simple word order. In addition, an attention mechanism is integrated to focus on relevant content for summarization. The authors of [43] employed a network with three distinct node types: sentences, Elementary Discourse Units (EDUs), and entities. They utilized RST discourse parsing to grasp the relationships between EDUs, which provides a deeper comprehension of the document's structure. \n\nThe research in [44] presented a heterogeneous graph network that incorporated information from both words and sentences. The model focused on redundancy dependencies between sentences, and iteratively refined sentence representations through a redundancy-aware graph. This iterative process aimed to enhance the model's ability to capture the essential meaning of each sentence. Recent research examined novel network structures and attention mechanisms for extractive summarization of long documents [45]. One approach utilized a transformer-based architecture within a heterogeneous network and included distinct node types for tokens, entities, and sentences. The study discovered that a multi-granularity sparse attention mechanism assisted in focusing on important relationships between these diverse nodes during the summarization process. \n\nAnother study proposed a novel Multiplex Graph Convolutional Network (M-GCN) architecture [46], which were effective at capturing various kinds of relationships between words and sentences. The model took into account both relationships within sentences (intra-sentential) and relationships between sentences (inter-sentential) to produce better summaries, particularly for long documents. \n\nMoreover, the work of [14] improved the current heterogeneous graph approaches by adding passage nodes alongside word and sentence nodes.",
            "score": 0.6302224516849667,
            "section_title": "C. NEURAL GRAPH-BASED TEXT SUMMARIZATION",
            "char_start_offset": 12480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1875
                },
                {
                    "start": 1878,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2267
                },
                {
                    "start": 2270,
                    "end": 2407
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "212747858"
                },
                {
                    "start": 767,
                    "end": 771,
                    "matchedPaperCorpusId": "233189611"
                },
                {
                    "start": 1045,
                    "end": 1049,
                    "matchedPaperCorpusId": "226262332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76416015625
        },
        {
            "corpus_id": "273025889",
            "title": "FastLexRank: Efficient Lexical Ranking for Structuring Social Media Posts",
            "text": "Models such as PE-GASUS (Zhang et al., 2020) and advanced LLMs like ChatGPT and Llama2 (Touvron et al., 2023) have demonstrated exceptional proficiency in condensing extensive texts into coherent and concise summaries. However, the fixed context windows of transformer-based models constrain their ability to process and distill exceedingly large text corpora. Although innovations like Gemini 1.5 Pro (Team et al., 2023) 2 , with a context window of 1 million tokens, are now operational, the computational load remains substantial for very lengthy documents due to the self-attention mechanism's complexity, denoted by O(n 2 \u2022 d). This complexity underscores the challenges in scaling summarization tasks for extensive texts without incurring significant computational costs, explaining why previous summarization studies mostly focus on single documents/posts/threads rather than the entire relevant corpus. \n\nTherefore, the question becomes, when facing millions of social media posts, how can we quickly identify the most important and representative posts to distill their information? Similar to the idea from Retrieval Augmented Generation (RAG) (Lewis et al., 2020), can we first target the most relevant posts and then ask LLMs to generate a summary based on this content? However, current retrieval models rely heavily on correctly specified queries to perform nearest-neighbor searches and cannot self-rank the posts based on their centrality or representativeness. This is where traditional extractive text summarization methods, like LexRank, can augment text summarization with ordering. \n\nLexRank, introduced by Erkan and Radev (2004), applies the principles of the PageRank algorithm (Brin and Page, 1998) to a graph representation of sentences, calculating the importance of each sentence within the corpus. It uses TF-IDF representations to construct a graph where the nodesrepresent sentences.. As an automatic summarization technique, LexRank has shown a remarkable ability to identify the most salient texts (with high centrality scores) within a set of documents. Despite its limitations in coherence and consistency inherent to extractive approaches, LexRank's ability to pinpoint the most representative text segments is invaluable in data mining and information retrieval.",
            "score": 0.6274168706251195,
            "section_title": "Introduction",
            "char_start_offset": 1805,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 44,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1154,
                    "end": 1174,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1701,
                    "end": 1722,
                    "matchedPaperCorpusId": "7587743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53369140625
        },
        {
            "corpus_id": "1694286",
            "title": "Extracting Core Claims from Scientific Articles",
            "text": "Like keyphrase extraction, automatic document summarization has received a lot of attention over the years. On the one hand, both are similar in the fact that they aim to determine the essence of a text or document. On the other hand, document summarization is more complex because it not only deals with words or phrases but whole sentences and larger bodies of text. There are two main methods of automatic summarization: extractive and abstractive. An extractive summary contains a set of sentences from the document, whereas an abstractive summary can contain material that is not present in the document but is constructed [2,14]. A popular approach to summarize documents nowadays is based on graph representations. TextRank is a well-known graph-based ranking model that is used for both keyword and sentence extraction [10]. The im-portance of words and sentences is based on the relation between them within the constructed graph. In the graph, sentences are represented as vertices. The higher the number of relations of a vertex within the graph, the higher the importance of that vertex. \n\nAnother recent research provides a comparison between an extractive and abstractive approach to document summarization [9]. The extractive approach consists of five steps of which the middle step is topic identification. A summary is generated containing sentences from the document that are considered most relevant. For the abstractive approach, the extractive summary is used as a basis and a word graph is generated that is integrated with that summary to create an abstractive summary. Results show that both approaches perform similar in terms of the information the summaries contain, but that the abstractive summary is more appropriate from a human perspective. Finally, a study performed a quantitative and qualitative assessment of 15 algorithms for sentence scoring [4]. Out of the 15 assessed algorithms, five methods showed the best performance: word frequency, TF-IDF, lexical similarity, sentence length and the TextRank score.",
            "score": 0.6268979408262108,
            "section_title": "Document summarization",
            "char_start_offset": 9565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 628,
                    "end": 631,
                    "matchedPaperCorpusId": "18529031"
                },
                {
                    "start": 631,
                    "end": 634,
                    "matchedPaperCorpusId": "36733653"
                },
                {
                    "start": 1221,
                    "end": 1224,
                    "matchedPaperCorpusId": "14577507"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73876953125
        },
        {
            "corpus_id": "252185277",
            "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
            "text": "In this work, we propose a knowledge graph-centric Transformer-based model for MDSS. Our model is able to incorporate knowledge graph information into the paper encoding process with a graph updater and an entity-sentence updater, and introduce a two-stage decoder including a KGtext generator and a summary generator to guide the summary decoding with knowledge graph information. Experiments show that the proposed model significantly outperforms all strong baselines and achieves the best result on the Multi-Xscience dataset. \n\nIn the future, we will explore other more intuitive and effective methods to incorporate graph information in both the encoding and decoding phase of summary generation. 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles.",
            "score": 0.6264271071933782,
            "section_title": "Conclusion",
            "char_start_offset": 25793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 867,
                    "end": 885,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1122,
                    "end": 1140,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1362,
                    "end": 1378,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1542,
                    "end": 1562,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1710,
                    "end": 1728,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1930,
                    "end": 1951,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "9432914",
            "title": "LexNet: A Graphical Environment for Graph-Based NLP",
            "text": "This interactive presentation describes LexNet, a graphical environment for graph-based NLP developed at the University of Michigan. LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classification). All tools in the collection are based on random walks on lexical graphs, that is graphs where different NLP objects (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment.",
            "score": 0.6262633881883901,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52392578125
        },
        {
            "corpus_id": "225451885",
            "title": "Summarizing Online Movie Reviews: A Machine Learning Approach to Big Data Analytics",
            "text": "e LexRank model is another graph-based ranking algorithm that creates graph representation of sentences and utilizes global information from the whole graph to decide the salience of a vertex (sentence) within a graph. e edge weight is determined from content similarity between sentences. However, our semantic clustering approach utilizes semantic distance between sentences to capture semantically related sentences. \n\nis study employs ROUGE-1 and ROUGE-2 evaluation metrics to compare our proposed semantic graph approach with the state-of-the-art graph-based approaches for summarization, in the context of the generic movie review extractive summarization task. Our proposed approach and other models perform the task of multidocument summarization, since they generate summaries from multiple movie reviews (or documents). \n\ne ROUGE evaluation metric has several variations: ROUGE-N (N \ufffd 1, 2, 3, and 4), ROUGE-S, and ROUGE-L, etc. However, ROUGE-1 and ROUGE-2 are efficiently applied for the multidocument extractive summarization task [60]. ROUGE \u2212 N can be defined [60] as an n-gram recall between a system summary and set of human (reference) summaries and is calculated as follows: \n\nwhere n is the length of the n-gram, gram n , and count match (gram n ) is the maximum number of n-grams that simultaneously occur in a system summary and a set of human summaries. \n\ne precision, recall, and F-measure for the system summary (or the candidate summary) are computed as follows: \n\nRecall \ufffd System Summary\u2229Human Summary Human Summary , (21) Tables 6 and 7 illustrate the comparative evaluation results of the proposed approach and other summarization models based on ROUGE-1 and ROUGE-2 measures, respectively. \n\nese results are achieved on the randomly chosen balanced subset of classified movie reviews as discussed above. For the same subset of movie reviews, we asked 2 Ph.D. students working in area of natural language processing to manually create summaries of 10 sentences. \n\nReferring to the ROUGE-1 results given in Table 6, our proposed clustering technique performs better than other summarization models based on average precision, recall, and F-measure. LexRank produces better summarization results as compared to TextRank.",
            "score": 0.6246799243781721,
            "section_title": "Experimental",
            "char_start_offset": 41059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1990
                },
                {
                    "start": 1993,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 1044,
                    "end": 1048,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62060546875
        },
        {
            "corpus_id": "222310577",
            "title": "Enhancing Extractive Text Summarization with Topic-Aware Graph Neural Networks",
            "text": "In this paper, we systematically explore the effects of latent topics for document summarization, and propose a novel graph-based extractive summarization model, which allows joint learning of latent topics and leverages them to enrich sentence representations via a heterogeneous graph neural network. The experimental results on four well-studied datasets demonstrate that our model not only achieves results on par with state-of-the-art summarization models on news article datasets but also significantly outperforms existing approaches on scientific paper datasets, indicating its strong robustness in various document genres and lengths. Further explorations on incorporating more types of semantic units (e.g. keywords and entities) into document graph for enhancing the performance of summarization will be addressed in our future work.",
            "score": 0.6243318593567986,
            "section_title": "and Future Work",
            "char_start_offset": 24138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 844
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "266244733",
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "text": "Graph neural networks and their associated learning methodologies (Gilmer et al., 2017;Veli\u010dkovi\u0107 et al., 2017) were initially conceived for homogeneous graphs, wherein all nodes share a uniform type. However, real-world applications often involve graphs with diverse types of nodes, or heterogeneous graphs. Recent studies have initiated exploratory efforts to model these complex structures (Shi et al., 2016). For instance, researchers have introduced heterogeneous graph neural networks to encode documents, entities, and candidates simultaneously for tasks like multihop reading comprehension (Tu et al., 2019). Others have focused on semi-supervised short text classification, employing a topic-entity heterogeneous neural graph (Linmei et al., 2019). In the realm of automatic document summarization, various approaches have been proposed, such as designing a heterogeneous graph comprising topic, word, and sentence nodes and employing a Markov chain model for iterative node updates (Wei, 2012). Algorithms like HeteroRank extend existing models (Wang, Chang & Huang, 2019) like TextRank by incorporating additional elements like keywords and sentences. Other models like HDSG Wang et al. (2020) employ words, sentences, and even paragraphs to form graph structures. The HEROS (Jia et al., 2020) model applies graph-based techniques to long-text fields, taking into account input article discourse. These methodologies primarily showcase two categories of variations: those pertaining to graph construction and those modifying the prior values of graph nodes. As for graph construction, the principal alterations relate to vertex granularity and edge weights. Some approaches operate under the assumption that significant sentences are composed of important words, employing a hybrid of graph models and sorting algorithms to rank both words and sentences (Fang et al., 2017). Other methods introduce more complex heterogeneous graphs that include not just words and sentences as nodes, but also integrate topic information. Yet another set of studies, such as those employing ExpandRank, utilize information from adjacent documents to enhance the sentence graph of a target document (Yang et al., 2018).The current approach maximizes the utility of both local document information and external data for evaluating the significance of sentences (Wan & Xiao, 2010).",
            "score": 0.6238945920757137,
            "section_title": "Heterogeneous graph for NLP",
            "char_start_offset": 5408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 87,
                    "matchedPaperCorpusId": "9665943"
                },
                {
                    "start": 393,
                    "end": 411,
                    "matchedPaperCorpusId": "7885409"
                },
                {
                    "start": 735,
                    "end": 756,
                    "matchedPaperCorpusId": "202783478"
                },
                {
                    "start": 992,
                    "end": 1003,
                    "matchedPaperCorpusId": "25794664"
                },
                {
                    "start": 1055,
                    "end": 1082,
                    "matchedPaperCorpusId": "129947442"
                },
                {
                    "start": 1286,
                    "end": 1304,
                    "matchedPaperCorpusId": "226262332"
                },
                {
                    "start": 1865,
                    "end": 1884,
                    "matchedPaperCorpusId": "28442272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "13237013",
            "title": "Text Summarization Using FrameNet-Based Semantic Graph Model",
            "text": "Another approach of unsupervised extract-based summarization [11,16] uses graph-based model. TextRank [17] and LexRank [18] are first two graph-based models applied in text summarization, which use the PageRank-like algorithms to mark sentences. Then, other researchers have integrated the statistical and linguistic features to drive the sentence selection process, for example, the sentence position [19], term frequency [20], topic signature [21], lexical chains [22], and syntactic patterns [7,23]. Ko and Seo [24] composed two sentences nearby into a bigram. Those bigrams were supposed to be context information. First, they extracted the bigrams by using the sentence extraction model. Then they used another extraction module to extract sentences from them. The ClusterCMRW and ClusterHITS models calculated the sentences scores by considering the clusterlevel information in the graph-based ranking algorithm. Canhasi and Kononenko [25] improve matrix decomposition by employing the archetypal analysis for generic multidocument summarization. While coming to the document set, there must be more consideration about the document-level influence. But it did not consider the relationship between words and sentences. The DsR model [26] achieved it by using document-sensitive graph-based ranking model. But this method did not get a satisfied result. Yin et al. improved the summarization quality by adding extra information which came from the query-extraction scenario. Goyal et al. [27] take Bernoulli model of randomness to index weights of sentences taking the context into consideration. The method proposed in [28] decomposed sentences by semantic role analysis, but while building the model, it did not use graphbased algorithms. \n\nHowever, most of these graph-based methods only consider the relation of keyword cooccurrence, without considering the sentence-level dependency syntax. Those papers which use semantic information do not utilize the semantic information in the sentence-level. Thus, how to take advantage of the relationship between sentences needs further research. In this paper, it proposes sentence-level Semantic Graph Model. FSGM can build the relationships between sentences in a better way. Several experiments show ideal results in our model.",
            "score": 0.6233365306618417,
            "section_title": "Related Work",
            "char_start_offset": 5921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 65,
                    "matchedPaperCorpusId": "2642018"
                },
                {
                    "start": 65,
                    "end": 68,
                    "matchedPaperCorpusId": "1750917"
                },
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "6524990"
                },
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "11847258"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "11989149"
                },
                {
                    "start": 423,
                    "end": 427,
                    "matchedPaperCorpusId": "5519987"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "8598694"
                },
                {
                    "start": 466,
                    "end": 470,
                    "matchedPaperCorpusId": "1584325"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "15709889"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "503868"
                },
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "16153195"
                },
                {
                    "start": 941,
                    "end": 945,
                    "matchedPaperCorpusId": "207057042"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "35133176"
                },
                {
                    "start": 1494,
                    "end": 1498,
                    "matchedPaperCorpusId": "17333959"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "253762042",
            "title": "SummVD : An efficient approach for unsupervised topic-based text summarization",
            "text": "Extractive summarization is studied since the late 1950's (Luhn, 1958). Symbolic (Edmundson, 1969) as well as semantic (Barzilay et al., 1999) or statistical (Radev et al., 2000) methods have been successfully used for automatic extractive summarization. Linear integer programming (Gillick and Favre, 2009) and evolutionary algorithms (Bossard and Rodrigues, 2017) have also been adapted to extractive summarization. \n\nTextRank (Mihalcea and Tarau, 2004) is summarization method widely used as a baseline. It is a graph-based method that extracts sentences based on the centrality of their words in a graph representation of the document. \n\nTo the best of our knowledge, (Padmakumar and He, 2021) is one of the most recent unsupervised extractive summarizer. In an empirical study, it outperforms state-of-the-art approaches on different kinds of texts (news, medical, discussions). The model is similar to the query likelihood model described in (Manning et al., 2008) for information retrieval where a language model is used to estimate the probability of a document given a query. Here, the query is replaced by a candidate sentence for extraction in the summary. So, in a greedy process, sentences are added to the output summary according to the language model probability estimation. The language model used in (Padmakumar and He, 2021) is GPT-2. It is fine-tuned on each dataset in order to get the best results. All of their hyper-parameters are tuned on 200 randomly sampled document-summary pairs, in order to optimize the ROUGE F1 measure. It includes the coefficient of relevance and redundancy from their sentence scoring equation and the number of sentences to select for all extractive methods. \n\nSummPip (Zhao et al., 2020) is a graph compression based unsupervised multi-document summarization method . It converts documents into a sentence graph where nodes are the sentences, and edges are constructed based on lexical chains, discourse level markers, exogen semantic information (WordNet), named entity reference and a simple semantic similarity based on word embedding vectors. It allows them to take into account the linguistic and deep neural representation of the documents.",
            "score": 0.6228913391891419,
            "section_title": "Related work 2.1 Extractive summarization",
            "char_start_offset": 2455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1710
                },
                {
                    "start": 1713,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 70,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 81,
                    "end": 98,
                    "matchedPaperCorpusId": "1177942"
                },
                {
                    "start": 119,
                    "end": 142,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 158,
                    "end": 178,
                    "matchedPaperCorpusId": "1320"
                },
                {
                    "start": 282,
                    "end": 307,
                    "matchedPaperCorpusId": "167874"
                },
                {
                    "start": 336,
                    "end": 365,
                    "matchedPaperCorpusId": "3259134"
                },
                {
                    "start": 429,
                    "end": 454,
                    "matchedPaperCorpusId": "262553219"
                },
                {
                    "start": 672,
                    "end": 697,
                    "matchedPaperCorpusId": "231918411"
                },
                {
                    "start": 1318,
                    "end": 1343,
                    "matchedPaperCorpusId": "231918411"
                },
                {
                    "start": 1721,
                    "end": 1739,
                    "matchedPaperCorpusId": "220633461"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "6405271",
            "title": "SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents",
            "text": "Document summarization is an important problem that has many applications in information retrieval and natural language understanding. Summarization techniques are mainly classified into two categories: extractive and abstractive. Extractive methods aim to select salient snippets, sentences or passages from documents, while abstractive summarization techniques aim to concisely paraphrase the information content in the documents. \n\nA vast majority of the literature on document summarization is devoted to extractive summarization. Traditional methods for extractive summarization can be broadly classified into greedy approaches (e.g., (Carbonell and Goldstein 1998)), graph based approaches (e.g., (Radev and Erkan 2004)) and constraint optimization based approaches (e.g., (McDonald 2007)). \n\nRecently, neural network based approaches have become popular for extractive summarization. For example, (Kageback et al. 2014) employed the recursive autoencoder (Socher et al. 2011) to summarize documents, producing best performance on the Opinosis dataset (Ganesan, Zhai, and Han 2010). (Yin and Pei 2015) applied Convolutional Neural Networks (CNN) to project sentences to continuous vector space and then select sentences by minimizing the cost based on their 'prestige' and 'diverseness', on the task of multi-document extractive summarization. Another related work is that of (Cao et al. 2016), who address the problem of query-focused multi-document summarization using CNNs, where they use weighted-sum pooling over sentence * Work was done while the author was an employee at IBM. representations to represent documents. The weights are learned from attention over sentence representations based on the query. \n\nRecently, with the emergence of strong generative neural models for text (Bahdanau, Cho, and Bengio 2014), abstractive techniques are also becoming increasingly popular. For example, (Rush, Chopra, and Weston 2015) proposed an attentional feed-forward network for abstractive summarization of sentences into short headlines.",
            "score": 0.6216046901740201,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1718
                },
                {
                    "start": 1721,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 640,
                    "end": 670,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 703,
                    "end": 725,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 779,
                    "end": 795,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 904,
                    "end": 925,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 962,
                    "end": 982,
                    "matchedPaperCorpusId": "6979578"
                },
                {
                    "start": 1058,
                    "end": 1087,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 1089,
                    "end": 1107,
                    "matchedPaperCorpusId": "17224077"
                },
                {
                    "start": 1382,
                    "end": 1399,
                    "matchedPaperCorpusId": "4508623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "276960874",
            "title": "A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization",
            "text": "The sentence ranking component utilized the LexRank algorithm, which leverages graph theory to rank sentences based on their importance. It was then fed to the text summarization model to generate the final summary. \n\nGraph-based methods utilize graph theories to create text summarization techniques. Following standard preprocessing techniques like stemming and stop word removal, sentences within the documents are depicted as nodes in an undirected graph. Sentences are linked by edges based on sentence structure [9]. Such representation is frequently utilized for extractive summarization in LexRank and TextRank [10]. In the LexRank algorithm, a sentence that is similar to multiple other sentences in the text would probably be considered important. The method recommends a specific sentence based on its similarity to other sentences, resulting in a higher ranking. This method is based on the Eigenvector Centrality. It adheres to a connected graph approach. Each sentence is positioned at a vertex of the graph. The weight on the edges is determined using a cosine similarity metric, as shown in Fig 1 [12]. \n\nIn this study, the PageRank algorithm is imported directly through the NetworkX library, and the necessary modifications were made to develop the LexRank algorithm accordingly. The similarity graph for sentences in LexRank is undirected because cosine similarity, used to measure similarity, is a symmetric relation. This contrasts with the original PageRank approach, which was designed to compute web page prestige. The formula provided below offers a clear overview of the LexRank algorithm that utilizes weighted graphs to rank the sentences. \n\nwhere p(u) is the centrality of node u, adj[u] is the set of nodes that are adjacent to u, N is the total number of nodes in the graph, and d is a \"damping factor\" [13]. \n\nThe author employed count vectorization to create word vectors for this component in the study. Based on the experimentation carried out, it was determined that a damping factor of 0.85 and a similarity threshold for voting of 0.2 should be selected. \n\nThe author identified that the LexRank algorithm is derived from the TextRank algorithm and can be considered an improved version of it. Significant differences exist between the two algorithms, which are then followed by common traits.",
            "score": 0.6215978358866675,
            "section_title": "A. Sentence Ranking Algorithm",
            "char_start_offset": 11627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1667
                },
                {
                    "start": 1670,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2092
                },
                {
                    "start": 2095,
                    "end": 2231
                },
                {
                    "start": 2232,
                    "end": 2331
                }
            ],
            "ref_mentions": [
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "4304278"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "224955327"
                },
                {
                    "start": 1834,
                    "end": 1838,
                    "matchedPaperCorpusId": "7587743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "252211797",
            "title": "Document-aware Positional Encoding and Linguistic-guided Encoding for Abstractive Multi-document Summarization",
            "text": "We compare our proposed method with the following strong baselines: LexRank [24] computes textual unit salience based on the eigenvector centrality algorithm using heuristic features in the similarity graph-based sentence representations. TextRank [25] leverages the graph-based ranking formula, deciding on the importance of a text unit representative within a graph built for information extraction. SummPip [26] constructs sentence graphs by incorporating both linguistic knowledge and deep neural representations. Maximal Marginal Relevance (MMR) [27] combines query relevance and information novelty from source documents, benefiting summarization in reducing redundancy while remaining the most salient information. Bidirectional recurrent neural network (BRNN) superimposes two RNNs of opposing directions on the same output according to RNN states. Transformer [19] follows an encoder-decoder structure based on attention mechanism, which has been extensively utilized in a wide range of natural language processing tasks1 . CopyTransformer restricts abstractive summarizer to copy tokens from source documents. Pointer-Generator (PG) [28] equips with the coverage mechanism between the pointer network and the standard sequence-to-sequence attention model. Hierarchical MMR-Attention Pointer-generator (Hi-MAP) model [16] integrates sentence representatives with hidden-state-based MMR into a standard pointer-generator network, an end-to-end model for abstract summarization. Hierarchical Transformer (HT) [5] captures relationships across multiple paragraphs via the hierarchical Transformer encoders and flat Transformer decoders2 .",
            "score": 0.6206698299071433,
            "section_title": "C. Baselines and Metrics",
            "char_start_offset": 14847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1644
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 551,
                    "end": 555,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 869,
                    "end": 873,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1143,
                    "end": 1147,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1516,
                    "end": 1519,
                    "matchedPaperCorpusId": "170079112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "238583401",
            "title": "Automatic Text Extractive Summarization Based on Graph and Pre-trained Language Model Attention",
            "text": "Graph-based summarization model has been used in several previously published studies. The early works focus on the variant of PageRank [17] algorithm, which assumed that important sentences would connect to other important ones. Textrank [15] and Lexrank [5] adopted such concept, they defined the relation between sentences by tf-idf [9] similarity between sentences and build the text graph, then ranking all the sentences through graph iterating and return the first few sentences as the summaries. \n\nIn current work, the neural network architecture has joined with the graph method thanks to the improvement of sentence representation and graph neural network. [21] applied the GCN model to find the salient sentences. They use three different rules (including sentence similarity) to build the text graph and encode the sentences to a fix-dimensional vector through recurrent network networks (RNN). \n\nHowever, building the sentence relation by rules may be limited to semantic information. The statistic method such as tf-idf or discourse relations [3] just reflect part of the information, but it is insensitive to the word meaning like the case of synonyms or antonyms. We improved this issue by constructing the text graph based on learning. The attention matrix generated by the language model contains much more information, and it is also more flexible in representing the sentence relations in different contexts.",
            "score": 0.61907786288617,
            "section_title": "Related Work",
            "char_start_offset": 3467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1427
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 243,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 1056,
                    "end": 1059,
                    "matchedPaperCorpusId": "337730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86572265625
        },
        {
            "corpus_id": "254408844",
            "title": "Scientific Paper Extractive Summarization Enhanced by Citation Graphs",
            "text": "(1) BertSumEXT (Liu and Lapata, 2019): the extractive summarization model with BERT as the encoder. (2) MGSum-ext (Jin et al., 2020a): the extractive multi-document summarization baseline, which extracts sentences from both the document and the abstracts of the reference papers. We incorporate this baseline to see if sentences from the reference papers are useful. (3) EMSum (Zhou et al., 2021): the abstractive multi-document summarization model, which takes the paper with references as multiple input documents. (4) CGSum (An et al., 2021): the abstractive summarization baseline with citation graph as input, which is also the paper that proposes the SSN dataest. ( 5) HSG (Wang et al., 2020): the heterogeneous graph-based neural network for extractive summarization.",
            "score": 0.617478927685926,
            "section_title": "Supervised summarization baselines:",
            "char_start_offset": 17397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 774
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 133,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 377,
                    "end": 396,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 527,
                    "end": 544,
                    "matchedPaperCorpusId": "233169121"
                },
                {
                    "start": 679,
                    "end": 698,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84326171875
        },
        {
            "corpus_id": "259289652",
            "title": "Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction",
            "text": "Human language is highly ambiguous, with multiple ways to express the same concept unit, where the underlying meaning is oftentimes altered by high-level linguistic constructs. Additionally, a single sentence may incorporate various predicate-argument structures. Despite these observations, current language models only consider the superficial organization of the text document, which is almost irrelevant to identifying its real and deeper semantic content [13]. Climbing towards natural language understanding, an increasing number of researchers argue that a model trained purely on the form will never learn the meaning, lacking signals to learn non-linguistic relations [33]. \n\nTo this end, structured representations allow different quality improvements (e.g., coherence, factuality, low redundancy, long-range dependencies, informativeness, consistency) depending on how they are constructed. In particular, semantic parsing graphs normalize lexical and syntactic variations, providing formal meaning representations capable of decoupling concept units (what to say) from language competencies (how to say it). \n\nGraph structures have long been used for extractive summarization. In this sense, early approaches, such as Tex-tRank [34], propose unsupervised keyword and sentence extraction methods exploiting graph-based ranking algorithms to determine each vertex's importance. Extensions have been devised to incorporate document-level information [35] or introduce graph-based attention into encoderdecoder architectures [36]. As for abstractive summaries, results are mostly built on the cross-cutting success of graph neural networks (GNNs), a famed class of deep learning methods designed to process graph-represented data without imposing linearization or hierarchical constraints. Fernandes et al. [37] combine sequence encoders with GNNs feed with weakly-structured data inferred by the text through off-theshell NLP tools, including named entity recognition and coreference resolution; the final model compares favorably with baselines using only the sequential or graphical structure. Structured summarization also relates to the graph verbalization trend [38][39][40], where inputs may originate from knowledge graphs, information extraction or semantic parsing techniques.",
            "score": 0.61711102492037,
            "section_title": "Graph-Enhanced Summarization",
            "char_start_offset": 7392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "202539891"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "211029226"
                },
                {
                    "start": 1459,
                    "end": 1463,
                    "matchedPaperCorpusId": "17446655"
                },
                {
                    "start": 1533,
                    "end": 1537,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 1815,
                    "end": 1819,
                    "matchedPaperCorpusId": "53216170"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "264817506",
            "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders",
            "text": "Traditional extractive summarization methods are mostly unsupervised (Yin and Pei, 2015;Nallapati et al., 2017;Zheng and Lapata, 2019;Zhong et al., 2019;Mao et al., 2022). Among them, graphbased sentential ranking methods are widely used. Two popular algorithms for single-document summarization are unsupervised LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004), estimating the centrality score of each sentence node among the textual context nodes. \n\nIn contrast to LexRank and TextRank constructing an undirected sentence graph, the model of PacSum (Zheng and Lapata, 2019) builds a directed graph. Its sentence centrality is computed by aggregating its incoming and outgoing edge weights: \n\nwhere hyper-parameters \u03bb 1 , \u03bb 2 are different weights for forwardand backward-looking directed edges and \u03bb 1 + \u03bb 2 = 1. e i, j is the weights of the edges e i, j \u2208 E and is computed using word co-occurrence statistics, such as the similarity score. Building upon the achievements of Pac-Sum (Zheng and Lapata, 2019), recent models such as FAR (Liang et al., 2021) and DASG (Liu et al., 2021) have aimed to improve extractive summarization by integrating centrality algorithms. These models primarily focus on seeking central sentences based on semantic facets (Liang et al., 2021) or sentence positions (Liu et al., 2021).",
            "score": 0.6160891414868769,
            "section_title": "Sentence Ranking Summarization",
            "char_start_offset": 5302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1340
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 88,
                    "matchedPaperCorpusId": "6026194"
                },
                {
                    "start": 88,
                    "end": 111,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 111,
                    "end": 134,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 134,
                    "end": 153,
                    "matchedPaperCorpusId": "195833318"
                },
                {
                    "start": 153,
                    "end": 170,
                    "matchedPaperCorpusId": "249437354"
                },
                {
                    "start": 321,
                    "end": 344,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 358,
                    "end": 384,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 574,
                    "end": 598,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 1009,
                    "end": 1033,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 1061,
                    "end": 1081,
                    "matchedPaperCorpusId": "236477774"
                },
                {
                    "start": 1091,
                    "end": 1109,
                    "matchedPaperCorpusId": "235792514"
                },
                {
                    "start": 1278,
                    "end": 1298,
                    "matchedPaperCorpusId": "236477774"
                },
                {
                    "start": 1321,
                    "end": 1339,
                    "matchedPaperCorpusId": "235792514"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "248218467",
            "title": "Summarization with Graphical Elements",
            "text": "The field of automatic summarization can classically be divided into abstractive (e.g., Gehrmann et al., 2018;Lewis et al., 2020) and extractive summarization (e.g., Narayan et al., 2018b;Ju et al., 2021). In our work the summaries cannot be generated by simply copying from the input document, classifying our task as a form of abstractive summarization. \n\nModeling. Many recent automatic summarization methods rely on some variety of neural sequence to sequence modeling (e.g., Cheng and Lapata, 2016;Vaswani et al., 2017;Lewis et al., 2020;Xue et al., 2021). Summaries produced by recent approaches such as BART (Lewis et al., 2020) and T5 (Xue et al., 2021) are of very high quality in terms of fluency and grammaticality, yet they struggle with factual consistency (e.g., Cao et al., 2020;Maynez et al., 2020). Hence, there has been a surge in work that focuses on improving the factuality of generated summaries. These works focus either on the evaluation of summarization (e.g., Wang et al., 2020;Durmus et al., 2020), or on the modeling procedures themselves, for example by explicitly incorporating graph-based meaning representations in the modeling process (Ribeiro et al., 2022). In this work we use BART and T5 as the summarization backbone of our baselines, but we are excited to explore graph-based methods in future work.",
            "score": 0.6156724274615288,
            "section_title": "Related Work",
            "char_start_offset": 5082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1337
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 110,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 110,
                    "end": 129,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 166,
                    "end": 188,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 188,
                    "end": 204,
                    "matchedPaperCorpusId": "238259960"
                },
                {
                    "start": 524,
                    "end": 543,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 615,
                    "end": 635,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 777,
                    "end": 794,
                    "matchedPaperCorpusId": "224706057"
                },
                {
                    "start": 794,
                    "end": 814,
                    "matchedPaperCorpusId": "218487034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "253383981",
            "title": "Computing and Exploiting Document Structure to Improve Unsupervised Extractive Summarization of Legal Case Decisions",
            "text": "Supervised Extractive Summarization Using Discourse Information Graph-based methods have been exploited for extractive summarization tasks to better model the inter-sentence relations based on document structure. Xu et al. (2020) applied a GCN layer to aggregate information from the document's discourse graph based on RST trees and dependencies. More recently, HiStruct+ (Ruan et al., 2022) and HEGEL (Zhang et al., 2020a) started to incorporate the hierarchical structure and topic structure of scientific articles into supervised model training, respectively. However, HiSruct+ relied on the relatively fixed and explicit document structure of scientific articles 2 , while HEGEL relied on a large training set to identify the topic distributions. Our work uses an unsupervised extractive summarization approach in a lower-resource setting, as well as studies the effects of computing different types of document structures. We leave the exploration of the aforementioned supervised approaches on legal domain texts for future work.\n\nUnsupervised Extractive Summarization Traditional extractive summarization methods are mostly unsupervised (Radev et al., 2000;Yin and Pei, 2015;Hirao et al., 2013), where a large portion apply the graph-based algorithms (Salton et al., 1997;Steinberger and Jezek, 2004;Erkan and Radev, 2004) or are based on term frequencies such as n-gram overlaps (Nenkova et al., 2005) to rank the sentences' importance. More recently, pretrained transformer-based models (Devlin et al., 2019;Lewis et al., 2020;Zhang et al., 2020b) have provided better sentence representations. For instance, Zheng and Lapata (2019) built directed unsupervised graph-based models on news articles using BERT-based sentence representations and achieved comparable performance to supervised models on multiple benchmarks. Dong et al. (2021) augmented the document graph of Zheng and Lapata (2019) with sentence position and section hierarchy to reflect the document structure of scientific articles. Different from these two works which are based on assumptions of news and scientific article structures, our method uses reweighting to better utilize the document structure of legal cases.\n\nExtractive Summarization of Legal Texts Despite the success of supervised",
            "score": 0.6145072797618788,
            "section_title": "Related Work",
            "char_start_offset": 5948,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 424,
                    "matchedPaperCorpusId": "252780923"
                },
                {
                    "start": 1537,
                    "end": 1557,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1619,
                    "end": 1642,
                    "matchedPaperCorpusId": "182952892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "231824260",
            "title": "Generating a Citation Summary Based on Cited Sentences and the Implied Citation Emotions",
            "text": "LexRank is a content selection algorithm based on a network graph, which has been proven to work well in multi-document summarization [39]. In the network graph, the nodes are the cited sentences, and the edge between two nodes shows the lexical similarity. Once the network is built for each cluster, the LexRank algorithm will perform a random walk to find the most central cited sentence in the network, which is then extracted to be the candidate summary sentence. \n\nFollowing this, the extracted cited sentences in the candidate set are ranked based on their MMR scores. MMR is used to examine the diversity of candidate sets and extract sentences with high differences to generate the final summary.",
            "score": 0.6143621980014871,
            "section_title": "3) EXTRACTING SUMMARY SENTENCES",
            "char_start_offset": 19073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 705
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 138,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "19090115",
            "title": "Exploring Graph Bushy Paths to Improve Statistical Multilingual Automatic Text Summarization",
            "text": "They use a modified version of PageRank [5] in order to score sentences. LexRank incorporates tf \u2212 idf into similarity calculation, while LexRank uses simple cosine similarity. When multiple documents are summarized, it is preferable to include temporal information to the score favoring recent documents [21]. To capture the impact of documents on sentences in multi-document ATS, sentence to document dimension can be used [22]. The author incorporates sentence position into the score to distinguish sentences from each other when they belong to the same document. Another work aiming to fuse statistical features with graphbased approach is iSpreadRank [23]. The method is based on activation theory [18] explaining the cognitive process of human comprehension. Each sentence is scored using some features (centroid, position, and first-sentence overlap), then these scores are spread from a sentence to its neighbors iteratively until equilibrium. The ranking in iterative graph-based ATS may never converge. Also, the ranking convergence depends on the initial scores and the damping factor.",
            "score": 0.6143357681454709,
            "section_title": "Related Works",
            "char_start_offset": 5511,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1097
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 43,
                    "matchedPaperCorpusId": "7587743"
                },
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "6610898"
                },
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "17446655"
                },
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "18748899"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "16689087",
            "title": "Large-Margin Learning of Submodular Summarization Methods",
            "text": "Work on extractive summarization spans a large range of approaches. Starting with unsupervised methods, one of the widely known approaches is MMR [12]. It uses a greedy approach for selection and considers the trade-off between relevance and redundancy. Later it was extended [13] to support multi-document settings by incorporating additional information available in this case. Good results can be achieved by reformulating this as a knapsack packing problem and solving it using dynamic programing [14]. \n\nA popular stohastic graph-based summarization method is LexRank [15]. It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank [16] is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank [17]. Sentence extraction can also be implemented using other graph based scoring approaches [18] such as HITS [19] and positional power functions. \n\nGraph based methods can also be paired with clustering such as in CollabSum [20]. This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities. Another clustering based algorithm [21] is diversity based extension of MMR that finds diversity by clustering and then proceeds to reduce redundancy by selecting a representative for each cluster. \n\nThe manually tuned sentence pairwise model [22,23] we took inspiration from is based on budgeted submodular optimization. A summary is produced by maximizing an objective function that includes coverage and redundancy terms. Coverage is defined as the sum of sentence similarities between the selected summary and the rest of the sentences, while redundancy is the sum of pairwise intra-summary sentence similarities. Another approach based on submodularity [24] is relying on extracting important keyphrases from citation sentences for a given paper and using them to build the summary. \n\nIn the supervised setting, a lot of early methods [25] made independent binary decisions whether to include a particular sentence in the summary or not. This ignores dependencies between sentences and can result in high redundancy.",
            "score": 0.614037234100515,
            "section_title": "Related work",
            "char_start_offset": 3465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2058
                },
                {
                    "start": 2061,
                    "end": 2213
                },
                {
                    "start": 2214,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "8294822"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "2798452"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 946,
                    "end": 950,
                    "matchedPaperCorpusId": "2100529"
                },
                {
                    "start": 964,
                    "end": 968,
                    "matchedPaperCorpusId": "216000619"
                },
                {
                    "start": 1079,
                    "end": 1083,
                    "matchedPaperCorpusId": "10228847"
                },
                {
                    "start": 1306,
                    "end": 1310,
                    "matchedPaperCorpusId": "6460414"
                },
                {
                    "start": 1514,
                    "end": 1518,
                    "matchedPaperCorpusId": "1803710"
                },
                {
                    "start": 1929,
                    "end": 1933,
                    "matchedPaperCorpusId": "6494467"
                },
                {
                    "start": 2111,
                    "end": 2115,
                    "matchedPaperCorpusId": "5775833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "237353135",
            "title": "Multiplex Graph Neural Network for Extractive Text Summarization",
            "text": "Numerous documents from a variety of sources are uploaded to the Internet or database everyday, such as news articles (Hermann et al., 2015), scientific papers (Qazvinian and Radev, 2008) and electronic health records (Jing et al., 2019). How to effectively digest the overwhelming information has always been a fundamental question in natural language processing (Nenkova and McKeown, 2011). This question has sparked the research interests in the task of extractive text summarization, which aims to generate a short summary of a document by extracting the most representative sentences from it.\n\nMost of the recent methods (Cheng and Lapata, 2016;Narayan et al., 2018;Luo et al., 2019;Wang et al., 2020a;Mendes et al., 2019; formulate the task of extractive text summarization as a sequence labeling task, where the labels indicate whether a sentence should be included in the summary. To extract sentence features, existing approaches generally use Recurrent Neural Networks (RNN) (Yasunaga et al., 2017;Nallapati et al., 2017;Mendes et al., 2019;Luo et al., 2019;Cheng and Lapata, 2016), Convolutional Neural Networks (CNN) (Cheng and Lapata, 2016;Luo et al., 2019;Narayan et al., 2018) or Transformers (Zhong et al., 2019;Liu and Lapata, 2019a). Endeavors have been made to develop models to capture various sentence-level relations. Early studies, such as LexRank  and TextRank (Mihalcea and Tarau, 2004), built similarity graphs among sentences and leverage PageRank (Page et al., 1999) to score them. Later, graph neural networks e.g., Graph Convolutional Networks (GCN) (Kipf and Welling, 2016) have been adopted on various inter-sentential graphs, such as the approximate discourse graph (Yasunaga et al., 2017), the discourse graph (Xu et al., 2020) and the bipartite graph between sentences and words (Wang et al., 2020a;Jia et al., 2020).\n\nAlbeit the effectiveness of the existing methods, there are still two under-explored problems",
            "score": 0.6134366942625601,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 140,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 218,
                    "end": 237,
                    "matchedPaperCorpusId": "196199713"
                },
                {
                    "start": 626,
                    "end": 650,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 650,
                    "end": 671,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 671,
                    "end": 688,
                    "matchedPaperCorpusId": "202775562"
                },
                {
                    "start": 688,
                    "end": 707,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 707,
                    "end": 727,
                    "matchedPaperCorpusId": "102352252"
                },
                {
                    "start": 985,
                    "end": 1008,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1008,
                    "end": 1031,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1031,
                    "end": 1051,
                    "matchedPaperCorpusId": "102352252"
                },
                {
                    "start": 1051,
                    "end": 1068,
                    "matchedPaperCorpusId": "202775562"
                },
                {
                    "start": 1068,
                    "end": 1091,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 1129,
                    "end": 1153,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 1153,
                    "end": 1170,
                    "matchedPaperCorpusId": "202775562"
                },
                {
                    "start": 1170,
                    "end": 1191,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 1208,
                    "end": 1228,
                    "matchedPaperCorpusId": "195833318"
                },
                {
                    "start": 1228,
                    "end": 1250,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1385,
                    "end": 1411,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1699,
                    "end": 1722,
                    "matchedPaperCorpusId": "6532096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57568359375
        },
        {
            "corpus_id": "221340611",
            "title": "MultiGBS: A multi-layer graph approach to biomedical summarization",
            "text": "\uf0b7 Topic-based approaches: In these methods, the summaries are created by identifying the topic.The topic is the primary concern of the document that is described in five different ways: (1) Topic signatures, (2) Enhanced topic signatures, (3) Thematic signatures, (4) Modeling the documents content structure, and (5)Templates [22].The algorithms in this category are rather sophisticated and require considerable skills to be used [23].\n\n\uf0b7 Machine learning-based approaches: These methods are based on the well-known machine learning algorithms such as classification [24], Hidden Markov Models [25],\n\nBayesian methods, neural networks [22], Support Vector Regression (SVR) and Least Angle Regression [26].SummaRuNNer is an extractive summarizer that uses a bidirectional recurrent neural network (RNN) for sentence representation and sentence selection [27].Cheng and Lapata introduced NN-SE as an extractive summarizer.In this model, sentences are represented with convolutional neural networks (CNN) network, and sentences are selected with NN-SE [28].PriorSum uses the gold standard summaries for training.It merges a multi-layer CNN network with statistical features such as sentence position and average term frequency [29].A limitation of the existing methods in this category is that the need for an extensive training corpus [4].\n\n\uf0b7 Discourse-based approaches: In this group, the summarization methods consider linguistic knowledge [30] [31].Afnan et al. [32] employ the coherence and cohesion of a document.Other approaches try to combine statistical and linguistic techniques.These algorithms have a moderate performance and do not provide significant improvements [4].\n\n\uf0b7 Graph-based approaches: Graph-based summarization methods represent the document as a graph with nodes, which show different parts such as terms, phrases, concepts, or sentences, and edges, which describe the similarity relation between them [8].There are various measures for calculating the similarity between the text units, such as cosine similarity [33], the longest common subsequences [34], the number of common words [35].\n\nLexRank is a well-known multi-document summarization system.",
            "score": 0.6132828925867739,
            "section_title": "Related Work",
            "char_start_offset": 5522,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 437
                },
                {
                    "start": 439,
                    "end": 601
                },
                {
                    "start": 603,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1231
                },
                {
                    "start": 1231,
                    "end": 1339
                },
                {
                    "start": 1341,
                    "end": 1452
                },
                {
                    "start": 1452,
                    "end": 1518
                },
                {
                    "start": 1518,
                    "end": 1588
                },
                {
                    "start": 1588,
                    "end": 1681
                },
                {
                    "start": 1683,
                    "end": 2115
                },
                {
                    "start": 2117,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "16459894"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "11168734"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "5775833"
                },
                {
                    "start": 596,
                    "end": 600,
                    "matchedPaperCorpusId": "2767184"
                },
                {
                    "start": 637,
                    "end": 641,
                    "matchedPaperCorpusId": "11168734"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 1226,
                    "end": 1230,
                    "matchedPaperCorpusId": "6311642"
                },
                {
                    "start": 1335,
                    "end": 1338,
                    "matchedPaperCorpusId": "16459894"
                },
                {
                    "start": 1442,
                    "end": 1446,
                    "matchedPaperCorpusId": "60514661"
                },
                {
                    "start": 1447,
                    "end": 1451,
                    "matchedPaperCorpusId": "14048633"
                },
                {
                    "start": 1465,
                    "end": 1469,
                    "matchedPaperCorpusId": "464430"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "16459894"
                },
                {
                    "start": 2039,
                    "end": 2043,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 2077,
                    "end": 2081,
                    "matchedPaperCorpusId": "16105394"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "235097525",
            "title": "Unsupervised document summarization using pre-trained sentence embeddings and graph centrality",
            "text": "Automatic text summarization is a very old and important task in Natural Language Processing (NLP) that has received continued attention since the creation of the field in the late 50's (Luhn, 1958), mainly because of the ever-increasing size of collections of text. The objective of the task is, given a document, to produce a shorter text with maximum information content, fluency and coherence. The summarization task can be classified into extractive and abstractive. Extractive summarization means that the summary is composed exclusively of passages present in the original document and abstractive summarization means that there can be words in the summary that did not appear in the original document. \n\nSince the creation of the first neural language models (Bengio et al., 2003), vector representations of text that encode meaning (called embeddings) have played a significant role in NLP. They allow the application of statistical and geometrical methods to words, sentences and documents ( (Pennington et al., 2014), (Mikolov et al., 2013), (Reimers and Gurevych, 2019)), leading to stateof-the-art performance on several NLP tasks like Information Retrieval, Question Answering or Paraphrase Identification. Among these neural language models, very deep pre-trained neural language models, like BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), and GPT-3 (Brown et al., 2020) have shown impressive performance in tasks like language modelling and text generation or benchmarks like GLUE (Wang et al., 2018). \n\nAn important variation of extractive summarization that goes back as far as the late 90's (Salton et al., 1994(Salton et al., , 1997) ) utilizes graphs, where the nodes represent text units and the links represent some measure of semantic similarity. These early graphbased summarization techniques involved creating a graph where the nodes were the sentences or paragraphs of a document and two nodes were connected if the corresponding text units had a similar vocabulary. After creating the document graph, the system created a summary by starting at the first paragraph and following random walks defined by different algorithms that tried to cover as much of the graph as possible.",
            "score": 0.6125524835617523,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1524
                },
                {
                    "start": 1527,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 198,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 767,
                    "end": 788,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 1002,
                    "end": 1027,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 1029,
                    "end": 1051,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 1339,
                    "end": 1360,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1504,
                    "end": 1523,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1617,
                    "end": 1637,
                    "matchedPaperCorpusId": "32296317"
                },
                {
                    "start": 1637,
                    "end": 1662,
                    "matchedPaperCorpusId": "12250431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60546875
        },
        {
            "corpus_id": "170079112",
            "title": "Hierarchical Transformers for Multi-Document Summarization",
            "text": "Aside from the difficulties in obtaining training data, a major obstacle to the application of end-to-end models to multi-document summarization is the sheer size and number of source documents which can be very large. As a result, it is practically infeasible (given memory limitations of current hardware) to train a model which encodes all of them into vectors and subsequently generates a summary from them. Liu et al. (2018) propose a two-stage architecture, where an extractive model first selects a subset of salient passages, and subsequently an abstractive model generates the summary while conditioning on the extracted subset. The selected passages are concatenated into a flat sequence and the Transformer (Vaswani et al., 2017), an architecture well-suited to language modeling over long sequences, is used to arXiv:1905.13164v1 [cs.CL] 30 May 2019 decode the summary. \n\nAlthough the model of Liu et al. (2018) takes an important first step towards abstractive multidocument summarization, it still considers the multiple input documents as a concatenated flat sequence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light (Radev, 2000). The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004;Christensen et al., 2013;Wan, 2008;Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. \n\nIn this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner.",
            "score": 0.6125524835617523,
            "section_title": "Introduction",
            "char_start_offset": 1905,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 412,
                    "end": 429,
                    "matchedPaperCorpusId": "39871772"
                },
                {
                    "start": 718,
                    "end": 740,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 906,
                    "end": 923,
                    "matchedPaperCorpusId": "39871772"
                },
                {
                    "start": 1355,
                    "end": 1368,
                    "matchedPaperCorpusId": "10103200"
                },
                {
                    "start": 1615,
                    "end": 1638,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1663,
                    "end": 1673,
                    "matchedPaperCorpusId": "17446655"
                },
                {
                    "start": 1673,
                    "end": 1698,
                    "matchedPaperCorpusId": "8437725"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "265607988",
            "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
            "text": "Another approach is to construct a sentence-level fully connected graph. Zhong et al. (Zhong et al., 2019) and Liu et al. (Liu and Lapata, 2019) used transformer (Vaswani et al., 2017) encoders to determine how sentences interact with each other. Wang et al. (Wang et al., 2020) introduced an heterogeneous graph neural network for extractive summarization which used additional semantic units (words) as intermediate nodes to construct relationships between sentences. \n\nAbstractive summarizers focus heavily on form, with the goal of producing a generalized summary, which tends to necessitate complex languagegenerating models. These models are typically based on sequence-to-sequence (seq2seq) architectures, in which a source document is seen as one sequence whereas its summary as another. The majority of previous research on neural abstractive summarization depended on large-scale, high-quality datasets of supervised document-summarization pairings (See et al., 2017). Recently, state-ofthe-art solutions on abstractive summarization are built upon the transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) models. These attention-based abstractive models are being used in different fields like clinical note summariza-tion (Kanwal and Rizzo, 2022), scientific document summarization (Zhang et al., 2022), and layabstract generation (Yu et al., 2020). \n\nIn this paper, addressing the above-mentioned issues, we have built a standalone summarization model which can generate both extractive and abstractive summaries from scientific documents incorporating the citation network. Analyzing the citation network, citing statements from the citing articles are accumulated with the original text document to incorporate the research community's observation on that particular cited manuscript. These summaries are the abstracts of the original papers with additional information reflecting the research community's view. After that, we run the Long-Former (Beltagy et al., 2020) encoder to generate sentence and word representations and train extractive and abstractive summarizers together.",
            "score": 0.6121539079399394,
            "section_title": "Introduction",
            "char_start_offset": 4141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 144,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 162,
                    "end": 184,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 247,
                    "end": 278,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 959,
                    "end": 977,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1075,
                    "end": 1097,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1107,
                    "end": 1128,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1247,
                    "end": 1271,
                    "matchedPaperCorpusId": "233296914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.640625
        },
        {
            "corpus_id": "235187330",
            "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph",
            "text": "contains sparse relations between partially extracted phrases, which cannot reflect the global structure and rich relations of the overall sequence.\n\nFor better modeling the long-distance relations and global structure of a long sequence, we propose to apply a phrase-level unified semantic graph to facilitate content selection and organization. Based on fine-grained phrases extracted from dependency parsing, our graph is suitable for information aggregation with the help of coreference resolution that substantially compresses the input and benefits content selection. Furthermore, relations between phrases play an important role in organizing the salient content when generating summaries. For example, in Figure 1 the phrases \"Albert Einstein\", \"the great prize\" and \"explanation of the of the photoelectric\" which distribute in different sentences are easily aggregated through their semantic relations to compose the final summary sentence.\n\nWe further propose a graph-based encoderdecoder model based on the unified semantic graph. The graph-encoder effectively encodes long sequences by explicitly modeling the relations between phrases and capturing the global structure based on the semantic graph. Besides, several graph augmentation methods are also applied during graph encoding to tap the potential semantic relations. For the decoding procedure, the graph decoder incorporates the graph structure by graph propagate attention to guide the summary generation process, which can help select salient content and organize them into a coherent summary.\n\nWe conduct extensive experiments on both the long-document summarization dataset BIG-PATENT and MDS dataset WikiSUM to validate the effectiveness of our model. Experiment results demonstrate that our graph-based model significantly improves the performance of both longdocument and multi-document summarization over several strong baselines. Our main contributions are summarized as follows:\n\n\u2022 We present the unified semantic graph which aggregates co-referent phrases distributed in context for better modeling the longdistance relations and global structure in longdocument summarization and MDS.\n\n\u2022 We propose a graph-based encoder-decoder model to improve both the document representation and summary generation process of the Seq2Seq architecture by leveraging the graph structure.\n\n\u2022 Automatic and human evaluation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model.\n\n2 Related Works",
            "score": 0.6115623177734175,
            "section_title": "Introduction",
            "char_start_offset": 1964,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "216552978",
            "title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization",
            "text": "Extractive Document Summarization With the development of neural networks, great progress has been made in extractive document summarization. Most of them focus on the encoderdecoder framework and use recurrent neural networks (Cheng and Lapata, 2016;Nallapati et al., 2017;Zhou et al., 2018) or Transformer encoders (Zhong et al., 2019b;Wang et al., 2019a) for the sentential encoding. Recently, pre-trained language models are also applied in summarization for contextual word representations (Zhong et al., 2019a;Liu and Lapata, 2019b;Xu et al., 2019;Zhong et al., 2020). \n\nAnother intuitive structure for extractive summarization is the graph, which can better utilize the statistical or linguistic information between sentences. Early works focus on document graphs constructed with the content similarity among sentences, like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Some recent works aim to incorporate a relational priori into the encoder by graph neural networks (GNNs) (Yasunaga et al., 2017;Xu et al., 2019). Methodologically, these works only use one type of nodes, which formulate each document as a homogeneous graph. \n\nHeterogeneous Graph for NLP Graph neural networks and their associated learning methods (i.e. message passing (Gilmer et al., 2017), selfattention (Velickovic et al., 2017)) are originally designed for the homogeneous graph where the whole graph shares the same type of nodes. However, the graph in the real-world application usually comes with multiple types of nodes (Shi et al., 2016), namely the heterogeneous graph. To model these structures, recent works have made preliminary exploration. Tu et al. (2019) introduced a heterogeneous graph neural network to encode documents, entities and candidates together for multihop reading comprehension. Linmei et al. (2019) focused on semi-supervised short text classification and constructed a topic-entity heterogeneous neural graph.",
            "score": 0.6113945055430992,
            "section_title": "Related Work",
            "char_start_offset": 4387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 251,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 251,
                    "end": 274,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 274,
                    "end": 292,
                    "matchedPaperCorpusId": "49656757"
                },
                {
                    "start": 317,
                    "end": 338,
                    "matchedPaperCorpusId": "203593998"
                },
                {
                    "start": 495,
                    "end": 516,
                    "matchedPaperCorpusId": "195833318"
                },
                {
                    "start": 516,
                    "end": 538,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 554,
                    "end": 573,
                    "matchedPaperCorpusId": "215828313"
                },
                {
                    "start": 841,
                    "end": 864,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 878,
                    "end": 904,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1277,
                    "end": 1298,
                    "matchedPaperCorpusId": "9665943"
                },
                {
                    "start": 1536,
                    "end": 1554,
                    "matchedPaperCorpusId": "7885409"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "49690971",
            "title": "Text Summarization Using Morphological Filtering of Intuitionistic Fuzzy Hypergraph",
            "text": "The Google brain team has developed and open sourced the tensor flow model (TST, 2016) for text summarization for generating news headlines on an annotated English giga word, where tensor flow is an open source library for numerical computation using data flow graphs. Interesting parts of the document are extracted using some metric (tf-idf) to create summary. There are many other graph based summarization methods, out of which five methods like HITS, positional power function, page rank methods, undirected graphs and weighted methods are compared (Mihalcea, 2004) and HITS and page rank seems to provide a better performance. Weighted directed graphs (Borhan et al., 2014) are also created by taking in to consideration the distortion measure. There, an edge is formed only if the distortion (semantic distance between node) is below a predefined threshold. In multigraph method (Fatima et al., 2015), there are more number of edges between two nodes (sentences). The number of edges equals the number of common words in both sentences. Results of this method are being compared against many online summarizers available and they have shown good performance. Lexical centrality (Erkan and Radev, 2004) is being used in LexRank method, where the sentences similar to many other sentences are found to be central to a topic. Given the similarity of each sentence to other sentences, the overall centrality of a sentence is calculated. The system has shown better results when compared to human summaries. On creation of a graph with multiple documents, sentence selection is done with segmented bushy path (Ribaldo et al., 2012) and depth first path method. Redundancy removal is being done at the end.",
            "score": 0.6110727139259069,
            "section_title": "Graph based Methods",
            "char_start_offset": 1783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1707
                }
            ],
            "ref_mentions": [
                {
                    "start": 554,
                    "end": 570,
                    "matchedPaperCorpusId": "2100529"
                },
                {
                    "start": 658,
                    "end": 679,
                    "matchedPaperCorpusId": "9634655"
                },
                {
                    "start": 886,
                    "end": 907,
                    "matchedPaperCorpusId": "17391861"
                },
                {
                    "start": 1185,
                    "end": 1207,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1611,
                    "end": 1633,
                    "matchedPaperCorpusId": "17235059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "9319303",
            "title": "Abstractive Multi-document Summarization with Semantic Information Extraction",
            "text": "In order to evaluate the performance of our system, we use two datasets that have been used in recent multi-document summarization shared tasks: DUC2005 and DUC2007. Each task has a gold standard dataset consisting of document clusters and reference summaries. In our experiments, DUC2005 was used for training and parameter tuning, and DUC2007 was used for testing. Based on the tuning set, the parameter \u03bb is set as 10 and \u03b4 is set as 0.7 after tuning. \n\nOur system is compared with one state-of-theart graph-based extractive approach MultiMR (Wan and Xiao, 2009) and one abstractive approach TTG (Genest and Lapalme, 2011). In addition, we have implemented another baseline RankBSU which uses the graph-based ranking methods on the BSUs network to rank BSUs and select the top ranked BSUs to generate sentences.",
            "score": 0.6107010616975335,
            "section_title": "Dataset and Experimental Settings",
            "char_start_offset": 12430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 814
                }
            ],
            "ref_mentions": [
                {
                    "start": 545,
                    "end": 565,
                    "matchedPaperCorpusId": "58509157"
                },
                {
                    "start": 599,
                    "end": 625,
                    "matchedPaperCorpusId": "4942873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "250012667",
            "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
            "text": "In earlier studies, there are mainly two algorithms, TextRank [23] and LexRank [9], which build document graphs based on sentence similarity and extract summaries without supervision. Thereafter, Wei et al. [34] suggested constructing a document graph that contains words, topics, and sentences, and utilize Markov chains to learn the graph and generate summaries. Recently, GNN networks have attracted much research in text summarization tasks [11,36,37]. And these text summarization models which are under the GNN framework only constructed document graphs based on words or sentences. On the contrary, we introduce sentence nodes and topic information nodes to construct document graph at the same time, which can not only solve the joint updating of sentence context representation and topic information but also reduce the problem of semantic fragmentation. In addition, since the traditional GAT is designed for homogeneous graphs and is not suitable for this model, it is necessary to modify the traditional GAT to design a new heterogeneous document graph.",
            "score": 0.6102885706458493,
            "section_title": "Graph-based Summarization",
            "char_start_offset": 8025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 82,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 449,
                    "end": 452,
                    "matchedPaperCorpusId": "219036690"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "239768911",
            "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
            "text": "The graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. Erkan and Radev (2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2020) both use discourse graphs to generate concise and informative summaries. Li et al. (2016) and Li and Zhuge (2019) propose to utilize event relation graph to represent documents for MDS. However, most existing graph-based summarization methods only consider the graph structure of source document. They neglect that summary is also a graph and its graph structure can reflect the quality of a summary. For example, in a similarity graph, if selected sentences are lexical similar, the summary is probably redundant. And in a discourse graph, if selected sentences have strong discourse connections, the summary tend to be coherent.\n\nWe argue that the graph structure of summary is equally important as the source document. Document graph helps to extract salient sentences, while summary graph helps to evluate the quality of summary. Based on this thought, we propose a novel MDS framework SgSum which transforms summarization into the problem of sub-graph selection. SgSum captures relation of sentences both in whole graph structure (source documents) and sub-graph structures (candidate summaries). Moreover, in our framework, summary is viewed as a whole unit in the form of sub-graph. Thus, SgSum can generate more coherent and informative results than traditional sentence-level extractors. Figure 1 shows the overview of our framework. Firstly, source documents are transformed into a relation graph by well-known graph construction methods such as similarity graph and discourse graph. Sentences are the basic information units and represented as nodes in the graph. And relations between sentences are represented as edges. For example, a similarity graph can be built based on cosine similarities between tf-idf representations of sentences. Let G denotes a graph representation matrix of the input documents, where G[i] [j] indicates the tf-idf weights between sentence S i and S j .",
            "score": 0.6100012653846988,
            "section_title": "Summarization as Sub-graph Selection",
            "char_start_offset": 4318,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 197,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 299,
                    "end": 325,
                    "matchedPaperCorpusId": "337730"
                },
                {
                    "start": 340,
                    "end": 357,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 431,
                    "end": 447,
                    "matchedPaperCorpusId": "6788641"
                },
                {
                    "start": 452,
                    "end": 471,
                    "matchedPaperCorpusId": "196170254"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "220045815",
            "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
            "text": "We use ROUGE (Lin, 2004) to evaluate the produced summary in our experiments. Following previous work, we report ROUGE F11 on Multi-News dataset. We compare our model with several typical baselines and several baselines proposed in the latest years. Lead-3 is an extractive baseline which concatenates the first-3 sentences of each source document as a summary. LexRank (Erkan and Radev, 2004) Model R-1 R-2 R-SU4 Lead-3 39.41 11.77 14.51 LexRank (Erkan and Radev, 2004) 38.27 12.70 13.20 TextRank (Mihalcea and Tarau, 2004) 38.44 13.10 13.50 MMR (Carbonell and Goldstein, 1998) 38.77 11.98 12.91 HIBERT (Zhang et al., 2019) 43.86 14.62 18.34 PGN (See et al., 2017) 41.85 12.91 16.46 CopyTransformer (Gehrmann et al., 2018) 43.57 14.03 17.37 Hi-MAP(Fabbri et al., 2019) 43.47 14.89 17.41 HF (Liu and Lapata, 2019) 43 is an unsupervised graph based method for computing relative importance in extractive summarization. \n\nTextRank (Mihalcea and Tarau, 2004) is also an unsupervised algorithm while sentence importance scores are computed based on eigenvector centrality within weighted-graphs for extractive sentence summarization. MMR (Carbonell and Goldstein, 1998) extracts sentences with a ranked list of the candidate sentences based on the relevance and redundancy. HIBERT (Zhang et al., 2019) first encodes each sentence using the sentence Transformer encoder, and then encode the whole document using the document Transformer encoder. \n\nIt is a single document summarization model and cannot handle the hierarchical relationship of documents. We migrate it to multi-document summarization by concatenating multiple source documents into a long sequence. These extractive methods are set to give an output of 300 tokens.",
            "score": 0.6093969226137494,
            "section_title": "Metrics and Baselines",
            "char_start_offset": 17412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1725
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 24,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 370,
                    "end": 393,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 447,
                    "end": 470,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 498,
                    "end": 524,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 547,
                    "end": 578,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 604,
                    "end": 624,
                    "matchedPaperCorpusId": "155100086"
                },
                {
                    "start": 647,
                    "end": 665,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 700,
                    "end": 723,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 791,
                    "end": 813,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 929,
                    "end": 955,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1134,
                    "end": 1165,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 1277,
                    "end": 1297,
                    "matchedPaperCorpusId": "155100086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "250012667",
            "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
            "text": "In this study, we integrate the global semantic structure of the text by proposing a novel graph-based topic-aware abstract text summarization model GTA-Sum and investigate the abstract text summarization problem. In particular, neural topic models, BERT, and graph neural networks are combined for summarization. Extensive experiments are conducted on two real-world datasets to compare GTASum with several methods. It turns out that GTASum outperforms the vast majority of classical models, with performance approaching industry-leading methods. In future work, we will continue to explore incorporating more types of high-level semantic units into the model to improve the performance and robustness of the model.",
            "score": 0.6089985488630222,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 47429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 716
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "259251722",
            "title": "Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents",
            "text": "We benchmark our proposed model against 20 baselines -six extractive text summarization, eight abstractive text summarization, two video-based abstractive text summarization, and four multimodalinput-based abstractive text summarization baselines. We briefly elaborate on them below. \n\nText-only extractive summarization: (i) Lead-2: The top 2 sentences of the source documents are marked as the generated summary and evaluated against the target summary. (ii) LexRank: It [13] represents the source document sentence as nodes of a graph and edges as a similarity measure. The edge weights are computed using the eigenvector centrality and the token frequency. (iii) Tex-tRank: Similar to LexRank, it [38] also represents the source document as a fully-connected graph. All edge weights are given a unit weight, and later a derived version of PageRank re-ranks the edge weights. (iv) MMR (Maximal Marginal Loss): The redundancy between the sentences is computed by mapping the query to the sentence [6]. The relevant sentences are kept in a cluster and filtered based on the similarity ranking. (v) ICSISumm: The coverage of the sentence in the final summary is optimized using the linear optimization framework [18]. Given a summary length bound, integer linear programming (ILP) solvers try to maximize the global topic coverage. (vi) BERTExtrative: The task of summarization is transformed into a binary classification problem [35]. The sentences are classified into two classes representing whether the sentence is a part of the final summary or not. \n\nText-only abstractive summarization: (i) Seq2Seq: It [41] uses the standard RNN network for both encoder and decoder with a global attention mechanism. (ii) Pointer Generator (PG): It [46] extends the Seq2Seq network with the addition of the Pointing mechanism. The Pointing mechanism allows the network to either generate tokens from the vocabulary or directly copy from the source document. (iii) CopyTransformer: It [15] uses the standard Transformer network. Similar to PG, a random attention head acts as a pointing mechanism.",
            "score": 0.6089584213859985,
            "section_title": "BASELINES",
            "char_start_offset": 19259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1554
                },
                {
                    "start": 1557,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 473,
                    "end": 477,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 1610,
                    "end": 1614,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1741,
                    "end": 1745,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1976,
                    "end": 1980,
                    "matchedPaperCorpusId": "52144157"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.416748046875
        },
        {
            "corpus_id": "250301954",
            "title": "Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization",
            "text": "We evaluate the quality of the summarization based on ROUGE (Lin 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) between generated or extracted summaries and gold summaries to assess informativeness. The longest common subsequence (ROUGE-L) on sentencelevel is reported for evaluating fluency. \n\nEvaluation w/o pre-training In our original model, no pre-trained models are involved so we remove any pretrained models in the baseline models for fair comparison in this section. We show the results of our proposed model against recently released summarization models on three news datasets in Table 3. We classify all the baselines into two groups: non-graph-based models and graph-based models. From the results, we can see that graph-based methods generally show comparable performance with the nongraph-based ones when GNN models are incorporated, as the case for Gated GNN and SemSUM. Furthermore, Synap-Sum outperforms the existing popular graph-based models and the listed non-graph-based models without pre-training in abstractive and extractive settings. SynapSum achieves the improvement by converting each sentence into a hierarchical graph with finer granularity and generating representations for words, constituents, and sentences simultaneously with rich syntactic and semantic meanings. However, the graphs used in other models tend only to capture the relationship between words and sentences, which leads to the loss of semantic meanings of the phrases as a whole. Therefore, our model can better mimic the human way of conducting summarization hierarchically, from words to phrases to sentences and documents, finally. \n\nEvaluation w/ pre-training The most recent summarization models are typically instantiated with some contextualized pre-trained models like BERT (Devlin et al. 2019). Consequently, it is essential to demonstrate the potential performance improvement of our proposed model with the help of pre-trained models. We use two popular recent pre-trained models: BART (Lewis et al. 2020) and MatchSum (Zhong et al. 2020) for abstractive and extensive summarization tasks respectively. More specifically, we firstly pre-train the model, following the setting used in PEGASUS (Zhang et al. 2020).",
            "score": 0.6084801292500599,
            "section_title": "Automatic Evaluation",
            "char_start_offset": 18075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 69,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 1801,
                    "end": 1820,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 2049,
                    "end": 2068,
                    "matchedPaperCorpusId": "215828313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "216163834",
            "title": "Comprehensive Document Summarization with Refined Self-Matching Mechanism",
            "text": "Automatic summarization systems have been made great progress in many applications, such as headline generation [1], single or multi-document summarization [2,3], opinion mining [4], text categorization, etc. The system aims to shorten the input and retain the salient information from the source document. Practical needs for such systems grow with the continuous increasing text sources in various fields. Text summarization methods would be divided into two categories: abstractive and extractive. The extractive methods select salient informative sentences from the source document as a summary, while the abstractive methods can generate words or sentences that are not present in the source document. The abstractive summarization is more difficult as it has to deal with factual or grammatical errors, semantic incoherence, as well as problems with the obtaining of explicit textual paraphrases and generalizations. Extractive methods relieve these problems by identifying important sentences from the document, therefore summary generated by extractive methods are generally better than that generated by abstractive methods in terms of grammaticality and factuality. However, those methods may encounter problems like the lack of core information and incomprehensive generalization. With the advantages of simpler calculation and higher generation efficiency, numerous empirical comparisons in recent years have shown that the state-of-the-art extractive methods usually have better performance than the abstractive ones [5].\n\nClassical document extractive summarizer relies on sophisticated feature engineering that mainly based on the statistical properties of the document, such as word probability, term frequency-inverse document frequency (TF-IDF) weights, sentence position, sentence length, etc. [6]. Graph-based methods, such as Lexrank [7], and TextRank [8], use graph weights to measure the sentence importance. In recent years, some neural network-based methods [9][10][11][12] have been proposed and applied to news datasets. Deep learning models with thousands of parameters require large annotated datasets. In the summarization field, Chen et al. [13] overcame this difficulty by creating news stories datasets from Central News Networks (CNN) and Daily Mail, which consist of 280 K documents and human writing summaries.\n\nDeep learning models would learn hidden features of text owing",
            "score": 0.60810907348303,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "1770102"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 159,
                    "end": 161,
                    "matchedPaperCorpusId": "150295060"
                },
                {
                    "start": 178,
                    "end": 181,
                    "matchedPaperCorpusId": "8753815"
                },
                {
                    "start": 1530,
                    "end": 1533,
                    "matchedPaperCorpusId": "51870490"
                },
                {
                    "start": 1813,
                    "end": 1816,
                    "matchedPaperCorpusId": "11944261"
                },
                {
                    "start": 1855,
                    "end": 1858,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1873,
                    "end": 1876,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1983,
                    "end": 1986,
                    "matchedPaperCorpusId": "49656757"
                },
                {
                    "start": 1986,
                    "end": 1990,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1990,
                    "end": 1994,
                    "matchedPaperCorpusId": "4999752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "145993667",
            "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis",
            "text": "\u2022 Alguliyev et al. ( 2013) is an optimization-based multi-document summarization model. It uses sentence-to-document collection, summary-to-document collection and sentence-tosentence relations to select salient sentences and reduce redundancy in the summary. \n\n\u2022 Kumar et al. ( 2014) is a topic-focused multi-document summarization strategy based on cross document relations and fuzzy reasoning. The approach follows three phases; extracting news components in the documents using Wordnet thresasure, named entity recognition and Gazateer lists, establishing cross document relations to identify relevant sentences, and finally the application of fuzzy reasoning to assign final sentence scores. \n\n\u2022 Sankarasubramaniam et al. ( 2014) is a graph-based summarization system leveraged with Wikipedia concepts. The study maps document sentences to Wikipedia concepts for the construction of a bipartite sentence-concept graphs. The summarizer then ranks sentences based on the ranking of corresponding concept nodes. It is noteworthy that this is one of the closest studies to our work in terms of the implementation, e.g., the use of concept graphs and Wikpedia as external knowledge. \n\n\u2022 Oliveira et al. ( 2016) is a generic single and multi document summarizer based on eighteen of the most widely used sentence scoring techniques including TextRank, TF-IDF, graph-based similarity, named-entities, sentence centrality and word co-occurrence. The scoring methods are used to compute the sentence importance in a document/cluster. This comparative study found that the strategy of combining features can lead to improved results. \n\n\u2022 Ren et al. ( 2016) is a redundancy-aware summarization system which considers the importance of the sentences and the redundancy in the summary simultaneously instead of modelling them as two separate processes. Particularly, this approach first evaluates the importance of each sentence and then selects sentences to generate a summary based on both the importance scores and redundancy among sentences. \n\n\u2022 Sun & Zhuge (2018) is a SDS approach which extracts semantic link network from documents using graph representations of different language granularities (words, sentences, paragraphs and sections) as nodes and semantic links between the nodes.",
            "score": 0.6047576449556373,
            "section_title": "Comparison with Benchmark Methods and Related Works",
            "char_start_offset": 42316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 259
                },
                {
                    "start": 262,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1628
                },
                {
                    "start": 1631,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2037
                },
                {
                    "start": 2040,
                    "end": 2285
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87646484375
        },
        {
            "corpus_id": "227905191",
            "title": "Hindi History Note Generation with Unsupervised Extractive Summarization",
            "text": "The basic idea behind the task of extractive summarization is that individual sentences in the source document are scored and ranked to extract the top n sentences as a summary.In this work, four unsupervised methods are investigated to score and rank the input document sentences.The methods used here are KLSum (Aria and Vanderwende, 2009), Luhn Summarization (Luhn, 1958), TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004).The summaries generated by these methods are evaluated and compared against each other intrinsically and extrinsically.TextRank and LexRank are graph based approaches.KLSum utilizes a probabilistic approach.Luhn uses a naive ranking algorithm based on word significance.",
            "score": 0.6027169413250837,
            "section_title": "Methodology",
            "char_start_offset": 4423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 718
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 340,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 424,
                    "end": 447,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480712890625
        },
        {
            "corpus_id": "18206432",
            "title": "PerSum: Novel Systems for Document Summarization in Persian",
            "text": "Following the general framework for graph-based summarization that we described in the previous section, it can be observed that there are essentially two free parameters: network type, and centrality measure. Network type can have important implications in a summarization system. Parsumist, for example, constructed a complete graph of sentences then pruned the nodes that were deemed too similar to their neighbors. LexRank, on the other hand, went ahead with a complete graph representation and no pruning whatsoever. \n\nWe, too, followed the same in Sections 3 and 4. Sentence similarity can also be an important aspect of network construction, because they allow us to weight the edges differently. While LexRank employs simple cosine similarity, TextRank used a modified form of Dice Similarity. \n\nIn this part of the study, we constructed sentence networks for both single and multi-document summarization. Sentences were extracted from the Pasokh corpus using the normalizer and tokenizer developed by Mojgan Seraji (Hal\u00e1csy et al., 2007;Seraji, 2013;Seraji, 2015), in the same spirit as Section 3.1. Our networks are undirected and weighted complete graphs, where edge weights are given by cosine similarity between two sentences. \n\nNote that the network structure and choice of similarity measure will have an impact on the final result, but since our goal in this part of the study was to compare centrality measures, and not to tweak the network structure, we went ahead with the standard LexRank construction. \n\nWe experimented with three different vector representations of sentencestf, tfidf, and binary (presence/absence of words). Vector elements are all unique words in the Pasokh corpus, and idf was computed on the Hamshahri corpus. Furthermore, we constructed sentence vectors in two waysremoving all stop words, and keeping them. 15 This gives rise to six types of sentence networks. \n\nOnce the sentence networks have been constructed, the next step is to select the most central sentences in the network. Note that the use of centrality measures in document summarization is not new, but it is new in Persian. We used the following seven centrality measures: 16 \n\n1. Strength: sum of the weights of the edges incident to a node (also called \"weighted degree\"). \n\n15",
            "score": 0.6018497084580696,
            "section_title": "Centrality-Based Systems",
            "char_start_offset": 50792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1905
                },
                {
                    "start": 1908,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2184
                },
                {
                    "start": 2187,
                    "end": 2283
                },
                {
                    "start": 2286,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 1024,
                    "end": 1046,
                    "matchedPaperCorpusId": "58492779"
                },
                {
                    "start": 1046,
                    "end": 1059,
                    "matchedPaperCorpusId": "108424280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46240234375
        },
        {
            "corpus_id": "198315717",
            "title": "Exploring Methods for Predicting Important Utterances Contributing to Meeting Summarization",
            "text": "Text summarization involves two approaches: Extractive [33] and abstractive summarization. The basic idea of extractive summarization is to distinguish between the informative and uninformative dialogue units in meetings, and to concatenate the informative ones to produce a summary. There are two ways to identify informative sentences. One option is a vector space model in which sentences are represented as word vectors that are commonly weighted based on tf-idf. The cosine similarity between two sentences is used for judging relevance and redundancy [34]. As a more sophisticated approach, latent semantic analysis (LSA) is applied to project sentence representation in the LSA space [35]. Summarizations of text-based e-mail conversations and discussions have employed this approach [36,37]. In more recent studies, the deep learning approach, including embedding representation, was employed in text summarization, and achieved better performance [33,[38][39][40]. The second option for extractive summarization is a feature-based approach, where supervised machine learning techniques are exploited to train a classifier, which judges each sentence as informative or not informative. Many extractive summarization studies have employed this approach. Recent studies in text summarization employed neural network approaches to learn feature representations, and achieved comparable performance to the models using hand-crafted features [41]. In contrast, abstractive summarization generates summaries rather than selecting sentences and ordering them. Wang and Cardie [42] introduced a template-based approach. In this approach, human-authored summaries were clustered and represented using word-graph models, and the ranked graph paths were used as templates to produce a summary. Singla et al. [43] proposed an automatic template selection method using cosine similarity on different levels of language representation. Murray [44] formulated the graph-based summary generation task as the Markov Decision Process (MDP), and proposed a model that learned a policy for selecting words in the word-graph. As a neural network approach, Zhao et al. [45] proposed a hierarchical encoder based on recurrent networks to learn the high-level semantic representation of meeting conversations.",
            "score": 0.6013258641594703,
            "section_title": "Text, Speech, and Meeting Summarization",
            "char_start_offset": 8705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 59,
                    "matchedPaperCorpusId": "6354619"
                },
                {
                    "start": 557,
                    "end": 561,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 691,
                    "end": 695,
                    "matchedPaperCorpusId": "7003631"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "26813678"
                },
                {
                    "start": 795,
                    "end": 798,
                    "matchedPaperCorpusId": "5867869"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "6354619"
                },
                {
                    "start": 960,
                    "end": 964,
                    "matchedPaperCorpusId": "35618061"
                },
                {
                    "start": 964,
                    "end": 968,
                    "matchedPaperCorpusId": "10675728"
                },
                {
                    "start": 968,
                    "end": 972,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 1577,
                    "end": 1581,
                    "matchedPaperCorpusId": "1030812"
                },
                {
                    "start": 1805,
                    "end": 1809,
                    "matchedPaperCorpusId": "38009271"
                },
                {
                    "start": 1937,
                    "end": 1941,
                    "matchedPaperCorpusId": "18210174"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "The primary objective of text summarization is to condense a document\u2019s length while preserving its essential content. Extractive summarization methods are commonly used due to their effectiveness and straightforward presentation. However, a significant challenge lies in segmenting documents into distinct concepts and understanding how sentences interact, especially in complex materials such as scientific articles. This process entails identifying relationships between sentences and determining the most significant and informative content within extensive text collections. Traditional techniques often utilize pre-trained models like BERT, known for their ability to capture word context. Nonetheless, these models have limitations, including constrained input lengths and the computational intensity of self-attention mechanisms, which hinder their effectiveness in processing large-scale scientific texts. To address these challenges, we propose a computationally efficient Heterogeneous Graph Neural Network (HGNN) for the extractive summarization of lengthy scientific texts. This framework combines GloVe embeddings with Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory (BiLSTM) encoders. GloVe offers simple yet effective word embeddings, CNNs focus on capturing local word structures, and BiLSTMs identify long-range dependencies, allowing for flexible encoding of extensive texts. For global context and topic modeling, we utilize an enhanced version of Latent Dirichlet Allocation (LDA) to retain essential document attributes. In this model, words, sentences, and topics are represented as nodes in a heterogeneous graph, with TF-IDF values illustrating the relationships between edges. The graph is processed using a Graph Attention Network (GAT), which refines node representations by integrating both local and global information. This study represents the first instance of combining LDA with CNN and BiLSTM encoders in a Graph Attention-based model for summarizing scientific texts. Experimental results demonstrate that the proposed framework outperforms both BERT-based and non-BERT approaches on publicly available datasets from arXiv and PubMed. Our model achieves a ROUGE-1 score of 46.31, a ROUGE-2 score of 19.98, and a ROUGE-L score of 40.21 on the arXiv dataset. It performs even better on the PubMed dataset, attaining a ROUGE-1 score of 48.85,",
            "score": 0.5994047837018404,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "264146402",
            "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
            "text": "Although Graph Neural Networks (GNN) [62,114,145,158] are primarily used in domains where data naturally exhibits graph structures, such as social networks, molecular structures, and knowledge graphs, they can also be adapted for NLP tasks. In NLP, GNNs are typically used to model relationships between words, sentences, or documents by representing them as nodes in a graph, with edges representing the relationships between these nodes. A GNN model learns to propagate information through the graph by iteratively updating the node representations based on the information from their neighbors. The core building blocks of GNNs are graph convolutional layers, which are designed to aggregate information from neighboring nodes and update the node features. \n\nJing et al. [75] present a novel Multiplex Graph Convolutional Network (Multi-GCN) approach for extractive summarization. Multi-GCN learns node embedding of different relations among sentences and words separately and combines them to produce a final embedding. This approach helps to mitigate the over-smoothing and vanishing gradient problems of the original GCN. \n\nA heterogeneous GNN, HETERSUMGRAPH [179] is introduced for extractive document summarization. This network includes nodes of different granularity levels apart from sentences, which act as intermediaries and enrich cross-sentence relations. This approach allows different sentences to interact considering overlapping word information. Moreover, the graph network can accommodate additional node types, such as document nodes for multi-document summarization. Doan et al. [40] propose a method for long document summarization by applying Heterogeneous Graph Neural Networks (HeterGNN) and introducing a homogeneous graph structure (HomoGNN). The HomoGNN focuses on sentence-level nodes to create a graph structure, enriching inter-sentence connections. Simultaneously, the HeterGNN explores the complex relationships between words and sentences, tackling intra-sentence connections. Both networks are constructed and updated using a Graph Attention Network model. In the HomoGNN, a BERT model is used for the initial encoding of sentences, while the HeterGNN uses a combination of CNN and BiLSTM for node feature extraction.",
            "score": 0.5988787298076608,
            "section_title": "Graph neural networks",
            "char_start_offset": 38428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "207238980"
                },
                {
                    "start": 41,
                    "end": 45,
                    "matchedPaperCorpusId": "17486263"
                },
                {
                    "start": 45,
                    "end": 49,
                    "matchedPaperCorpusId": "3051291"
                },
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "206756462"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "237353135"
                },
                {
                    "start": 1165,
                    "end": 1170,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1602,
                    "end": 1606,
                    "matchedPaperCorpusId": "252819335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "221293184",
            "title": "Query Understanding via Intent Description Generation",
            "text": "We also apply extractive summarization models to extract a sentence from the relevant documents as the intent description. \n\n\u2022 LSA [59] applys Singular Value Decomposition (SVD) to pick a representative sentence. \u2022 TextRank [39] is a graph-based method inspired by the PageRank algorithm. \n\n\u2022 LexRank [19] is also a graph-based method inspired by the PageRank algorithm. The difference with TextRank is to use different methods to calculate the similarity between two sentences.",
            "score": 0.5973936409412766,
            "section_title": "Extractive Models.",
            "char_start_offset": 30297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 125,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 478
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 135,
                    "matchedPaperCorpusId": "28867878"
                },
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 301,
                    "end": 305,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "233295991",
            "title": "Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs",
            "text": "Document Summarization Compared to extractive document summarization (Gupta and Lehal, 2010;Narayan et al., 2018;Liu and Lapata, 2019), abstractive document summarization is generally considered more challenging and has received more attention. Various methods have been designed to tackle abstractive document summarization like sequence-to-sequence models (Rush et al., 2015), pointer generators (See et al., 2017), reinforcement learning methods (Paulus et al., 2018;Huang et al., 2020) and pre-trained models (Lewis et al., 2020;. To generate faithful abstractive document summaries (Maynez et al., 2020), graphbased models were introduced recently such as extracting entity types (Fernandes et al., 2018;, leveraging knowledge graphs (Huang et al., 2020;Zhu et al., 2020a) or designing extra fact correction modules . Inspired by these graph-based methods, we also construct action graphs for generating more factual conversation summaries.\n\nConversation Summarization Extractive dialogue summarization (Murray et al., 2005) has been studied extensively via statistical machine learning methods such as skip-chain CRFs (Galley, 2006), SVM with LDA models (Wang and Cardie, 2013), and multi-sentence compression algorithms (Shang et al., 2018). Such methods struggled with generating succinct, fluent, and natural summaries, especially when the key information needs to be aggregated from multiple first-person point-of-view utterances (Song et al., 2020). Abstractive conversation summarization overcomes these issues by designing hierarchical models Zhu et al., 2020b), incorporating commonsense knowledge (Feng et al., 2020), or leveraging conversational structures like dialogue acts (Goo and Chen, 2018), key point sequences (Liu et al., 2019a), topic segments (Liu et al., 2019b; and Figure 2: Model architecture. Each utterance is encoded via transformer encoder; discourse relation graphs and action graphs are encoded through Graph Attention Networks (a). The multi-granularity decoder then generates summaries based on all levels of encoded information including utterances, action graphs, and discourse",
            "score": 0.5966759376777101,
            "section_title": "Related Work",
            "char_start_offset": 4865,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 92,
                    "matchedPaperCorpusId": "10157083"
                },
                {
                    "start": 92,
                    "end": 113,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 113,
                    "end": 134,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 358,
                    "end": 377,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 398,
                    "end": 416,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 449,
                    "end": 470,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 470,
                    "end": 489,
                    "matchedPaperCorpusId": "218487279"
                },
                {
                    "start": 513,
                    "end": 533,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 587,
                    "end": 608,
                    "matchedPaperCorpusId": "218487034"
                },
                {
                    "start": 685,
                    "end": 709,
                    "matchedPaperCorpusId": "53216170"
                },
                {
                    "start": 739,
                    "end": 759,
                    "matchedPaperCorpusId": "218487279"
                },
                {
                    "start": 759,
                    "end": 777,
                    "matchedPaperCorpusId": "213004108"
                },
                {
                    "start": 1008,
                    "end": 1029,
                    "matchedPaperCorpusId": "5776046"
                },
                {
                    "start": 1124,
                    "end": 1138,
                    "matchedPaperCorpusId": "5879376"
                },
                {
                    "start": 1160,
                    "end": 1183,
                    "matchedPaperCorpusId": "1030812"
                },
                {
                    "start": 1227,
                    "end": 1247,
                    "matchedPaperCorpusId": "44243501"
                },
                {
                    "start": 1556,
                    "end": 1574,
                    "matchedPaperCorpusId": "221749138"
                },
                {
                    "start": 1692,
                    "end": 1712,
                    "matchedPaperCorpusId": "52290192"
                },
                {
                    "start": 1734,
                    "end": 1753,
                    "matchedPaperCorpusId": "196184158"
                },
                {
                    "start": 1770,
                    "end": 1789,
                    "matchedPaperCorpusId": "203641742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "252780923",
            "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
            "text": "in (Xu et al., 2019), approximate discourse graph in (Yasunaga et al., 2017), topicsentence graph in (Cui and Hu, 2021) and worddocument heterogeneous graph in (Wang et al., 2020). However, the usability of these approaches is limited by the following two aspects: (1) These methods only model the pairwise interaction between sentences, while sentence interactions could be triadic, tetradic, or of a higher-order in natural language (Ding et al., 2020). How to capture high-order cross-sentence relations for extractive summarization is still an open question.\n\n(2) These graph-based approaches rely on either semantic or discourses structure cross-sentence relation but are incapable of fusing sentence interactions from different perspectives. Sentences within a document could have various types of interactions, such as embedding similarity, keywords coreference, topical modeling from the semantic perspective, and section or rhetorical structure from the discourse perspective. Capturing multi-type crosssentence relations could benefit sentence representation learning and sentence salience modeling. Figure 1 is an illustration showing different types of sentence interactions provide different connectivity for document graph construction, which covers both local and global context information.\n\nTo address the above issues, we propose HEGEL (HypErGraph transformer for Extractive Long document summarization), a graph-based model designed for summarizing long documents with rich discourse information. To better model high-order cross-sentence relations, we represent a document as a hypergraph, a generalization of graph structure, in which an edge can join any number of vertices. We then introduce three types of hyperedges that model sentence relations from different perspectives, including section structure, latent topic, and keywords coreference, respectively. We also propose hypergraph transformer layers to update and learn effective sentence embeddings on hypergraphs. We validate HEGEL by conducting extensive experiments and analyses on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL. We highlight our contributions as follows:\n\n(i) We propose a hypergraph neural model, HEGEL, for long document summarization. To the best of our knowledge, we are the first to model high-order cross-sentence relations with hypergraphs",
            "score": 0.5958390622028804,
            "section_title": "Introduction",
            "char_start_offset": 2030,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 119,
                    "matchedPaperCorpusId": "239050558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "216648281",
            "title": "Text Mining Business Policy Documents: Applied Data Science in Finance",
            "text": "Automatic summarization (Kanapala et al., 2019;Larson, 2012) defines two groups of summarizing algorithms, namely extractive and abstractive algorithms. These algorithms differ in the approach that they employ for constructing summary representations. Extractive algorithms construct the summaries by using the most important sentences of the textual document and concatenating them into a consisted summary. Contrarily, abstractive summaries may not always draw on the same concepts as the ones that the original text contains. It usually reuses the main phrases of the document and constructs them in a manner that would convey the message. In addition to this, these two algorithms are further categorized based on their appliance, which can be either a multi-document or singledocument summarization algorithm. As the name implies, a multi-document summarization algorithm generates a single summary from the entire corpus of documents, whereas a single-document summarization algorithm generates an individual summary for each document. These factors have scoped our algorithm selection options to an extractive, single-document summarization algorithm. This decision, in turn, has led us to use the TextRank approach.\n\nTextRank was introduced in (Mihalcea & Tarau, 2004) and it has its roots embedded in the PageRank algorithm of (Page & Brin, 1997), utilizing the same logic. As such, TextRank is a graphbased approach, that uses the knowledge drawn from the entire text to construct a graph representation, on which graph the PageRank formula is applied to determine the most important vertices. Adhering to the same methodology, the content of each policy is first split into sentences. Similar to IE, this is done with the NLTK sentence segmenter. This segregation enables to construct a graph representation of the policy where each individual sentence represents a vertex (node) in the graph. To add an edge between vertices, the algorithm relies on the \"recommendation\" concept. This concept is built under the assumption that a given sentence recommends another sentence to read, based on their resemblance. TextRank is still considered highly effective, and even in recent years several improvements over the original TextRank have been proposed, e.g. to better mine users' interests expressed in microblogs and to improve multi-document summarization in online argumentation (Ni",
            "score": 0.5945200242771927,
            "section_title": "Automatic Summarization",
            "char_start_offset": 20401,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 47,
                    "matchedPaperCorpusId": "26448969"
                },
                {
                    "start": 47,
                    "end": 60,
                    "matchedPaperCorpusId": "47179013"
                },
                {
                    "start": 1336,
                    "end": 1355,
                    "matchedPaperCorpusId": "3770261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.697265625
        },
        {
            "corpus_id": "259129368",
            "title": "AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization",
            "text": "Similar to general text summarization, opinion summarization can be categorized into abstractive and extractive methods. Mirroring the developmental trajectory of general summarization, extractive methods initially gained widespread use in this domain due to the high cost of creating golden summaries for datasets, particularly for review datasets where such summaries are not mandatory [22]. Early works in this field predominantly treated the task as a sentence or phrase selection problem, employing either ranking or classification approaches. For instance, Wei et al. prioritize sentences that closely corresponded to the query [23], while Erkan et al. employed a stochastic graph-based method to rank sentences by calculating their importance based on eigenvector centrality within a graph representation of the sentences [24]. Presently, extractive methods remain popular for opinion summarization [1,11,25,26]. \n\nAbstractive methods for opinion summarization began to emerge around 2010, with Ganesan et al. introducing a graph-based algorithm for generating abstractive summaries [27]. Although this approach produced abstractive summaries, it selected words, phrases, or sentences from the original text, rendering it more akin to an extractive method. In recent years, a growing number of studies have begun to leverage machine learning and deep learning techniques for this task. Notably, Chu et al. presented an unsupervised neural model for multi-document summarization, proposing an end-to-end architecture featuring an auto-encoder. This approach decoded the mean of the input reviews' representations into a coherent summary review without relying on any review-specific attributes [22].",
            "score": 0.5943728170653828,
            "section_title": "Opinion Summarization",
            "char_start_offset": 9591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1705
                }
            ],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 392,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "2814261"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 906,
                    "end": 909,
                    "matchedPaperCorpusId": "237434243"
                },
                {
                    "start": 909,
                    "end": 912,
                    "matchedPaperCorpusId": "33846824"
                },
                {
                    "start": 912,
                    "end": 915,
                    "matchedPaperCorpusId": "230102949"
                },
                {
                    "start": 915,
                    "end": 918,
                    "matchedPaperCorpusId": "233306955"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 1700,
                    "end": 1704,
                    "matchedPaperCorpusId": "59413781"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76806640625
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "The study presented by [10] investigates an unsupervised, discourseaware hierarchical and ranking graph-based model, meticulously tailored for comprehensive scientific documents. It takes advantage of the internal relations and the relations between different sentences to improve document comprehension. Nonetheless, this methodology's dependence on external tools can precipitate complications, including the production of outputs with semantic fragmentation [11]. \n\nThe research conducted by [9] established a word-sentence document graph grounded on word appearance. Nonetheless, this statistical method of graph construction is limited in its ability to encapsulate relationships at the semantic level. The research conducted by [6] presented a Heterogeneous Transformer-based model designed specifically for extractive summarization of large texts that contains novel concepts of multi-scale sparse attention with explicit categorization of tokens (words), entities, and sentences as separate nodes. The authors of [7] augmented the heterogeneous graph structure with the introduction of passage nodes, alongside the existing word and sentence nodes. This enhancement aimed to improve the overall sentence representation for long document summarization. The researchers in [12] focused on the extractive summarization of scholarly articles by proposing an innovative approach. This approach synergizes the strengths of BERT with BiGRU (Bidirectional Gated Recurrent Units), a recurrent neural network adept at capturing sequential dependencies within the text to extract salient information effectively. \n\nAnother pivotal aspect of summarization involves the modeling of global information, which significantly influences the selection of sentences [13]. The authors of [3], employed BERT for contextual sentence representations and simultaneously conducted training with latent topics to capture global context, making use of Neural Topic Model (NTM) for this objective. It should be noted that the pre-trained model boosts the summarization process but suffers from limitations such as a fixed maximum input length and the computational overhead associated with the quadratic complexity of their self-attention mechanism, making them less scalable and inefficient for processing extensive scientific texts. The authors in [8] presented a graph structure augmented with latent topical information, where topics are delineated utilizing clustering techniques such as K-means and Gaussian Mixture Models (GMM).",
            "score": 0.5935265803503436,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2314
                },
                {
                    "start": 2315,
                    "end": 2515
                }
            ],
            "ref_mentions": [
                {
                    "start": 1021,
                    "end": 1024,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 1279,
                    "end": 1283,
                    "matchedPaperCorpusId": "261471547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67236328125
        },
        {
            "corpus_id": "241394948",
            "title": "Multi-Document Abstractive Summarization using Recursive Neural Network",
            "text": "While looking at the source documents we come across two types, homogeneous multi documents usually cover the same file type and context whereas heterogeneous cover multiple domains or topics. There are various homogenous multi documents available such as Document Understanding Conference (DUC) datasets, Text Analysis Conference (TAC) datasets and Minimum Data Set (MDS). Heterogeneous multi-document datasets are very few such as the heterogeneous multi-genre corpus called heterogenous MDS \"hMDS\" and subsequently auto-hMDS which are generated from summaries on Wikipedia and crowd-sourcing. \n\nIn terms of the different methods of text summarization, using neural networks have benefitted in extractive methods for handling semantics as well as redundancy compared to other traditional methods but lack in coherence compared to abstractive methods. Among abstractive summarization there are different approaches such as linguistic based approaches, semantic graph based approaches and hybrid extractive/abstractive approaches. Linguistic based approaches make use of syntactic representations and tree structures but lack abstraction to semantic meanings. Semantic graph based approaches focus on semantic role labeling to determine abstraction of input to core meaning to form graphs to filter out redundancy followed by text generator to build summaries as discussed in previous papers. Hybrid approaches make use of extractive methods to obtain an output summary to be fed to a text generator to build non-key words and phrases to further improve coherence and readability of sentences. One type of neural network is the recursive neural network which makes use of the same weights recursively on a structured input to predict the output. One subclass of the recursive neural network is the recurrent neural network which has a linear chain structure whereas a recursive model has a hierarchical structure. Recurrent neural networks also work on linear progression of time and make use of previous time steps and states during the current time step thus making this model lucrative for text generation. Use of recurrent neural networks with an attention mechanism further helps with long text dependencies demonstrated by other researchers. Semantic information approach is taken for abstractive summaries for more multi documents with semantic data, more specifically the graph based approaches.",
            "score": 0.5934934144724944,
            "section_title": "II. LITERATURE SURVEY",
            "char_start_offset": 2602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2403
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "102353862",
            "title": "Guiding Extractive Summarization with Question-Answering Rewards",
            "text": "Comparison systems We compare our method with a number of extractive and abstractive systems that have reported results on the CNN/DM datasets. We consider non-neural approaches that extract sentences from the source article to form a summary. These include LexRank (Radev et al., 2004), SumBasic (Vanderwende et al., 2007), and KLSum (Haghighi and Vanderwende, 2009). Such methods treat sentences as bags of words, and then select sentences containing topically important words. We further include the Lead-3 baseline that extracts the first 3 sentences from any given article. The method has been shown to be a strong baseline for summarizing news articles. \n\nNeural extractive approaches focus on learning vector representations for sentences and words, then performing extraction based on the learned representations. Cheng et al. (2016) describe a neural network method composed of a hierarchical document encoder and an attention-based extractor. The system has two variants: NN-WE extracts words from the source article and NN-SE extracts sentences. SummaRuNNer (Nallapati et al., 2017) presents an autoregressive sequence labeling method based on recurrent neural networks. It selects summary sentences based on their content, salience, position, and novelty representations. \n\nAbstractive summarization methods are not directly comparable to our approach, but we choose to include three systems that report results respectively for CNN and DM datasets. Distraction-M3 (Chen et al., 2016b) trains the summarization system to distract its attention to traverse different regions of the source article. Graph attention (Tan et al., 2017) introduces a graph-based attention mechanism to enhance the encoderdecoder framework. PointerGen+Cov. (See et al., 2017) allows the system to not only copy words from the source text but also generate summary words by selecting them from a vocabulary. Abstractive methods can thus introduce new words to the summary that are not present in the source article. However, system summaries may change the meaning of the original texts due to this flexibility.",
            "score": 0.5931146003685487,
            "section_title": "Experimental Results",
            "char_start_offset": 25236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 323,
                    "matchedPaperCorpusId": "10496513"
                },
                {
                    "start": 335,
                    "end": 367,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 822,
                    "end": 841,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 1069,
                    "end": 1093,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1477,
                    "end": 1496,
                    "matchedPaperCorpusId": "12755643"
                },
                {
                    "start": 1625,
                    "end": 1643,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 1746,
                    "end": 1764,
                    "matchedPaperCorpusId": "8314118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67333984375
        },
        {
            "corpus_id": "247519084",
            "title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization",
            "text": "Multi-Document Summarization is the task of generating a summary from a cluster of related documents. State-of-the-art approaches to multi-document summarization are primarily either graph-based (Liao et al., 2018;Li et al., 2020;Pasunuru et al., 2021), leveraging graph neural networks to connect information between the documents, or hierarchical (Liu and Lapata, 2019a;Fabbri et al., 2019;Jin et al., 2020), building intermediate representations of individual documents and then aggregating information across. While effective, these models either require domain-specific additional information e.g. Abstract Meaning Representation (Liao et al., 2018), or discourse graphs (Christensen et al., 2013;Li et al., 2020), or use dataset-specific, customized architectures, making it difficult to leverage pretrained language models. Simultaneously, recent pretrained language models (typically encoder-decoder transformers) * Work mainly done during an internship at AI2. 1 The code and pre-trained models can be found at https: //github.com/allenai/PRIMER  have shown the advantages of pretraining and transfer learning for generation and summarization (Raffel et al., 2020;Lewis et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). Yet, existing pretrained models either use single-document pretraining objectives or use encoder-only models that do not work for generation tasks like summarization (e.g., CDLM, Caciularu et al., 2021).\n\nTherefore, we argue that these pretrained models are not necessarily the best fit for multi-document summarization. Alternatively, we propose a simple pretraining approach for multi-document summarization, reducing the need for dataset-specific architectures and large fine-tuning labeled data (See Figure 1 to compare with other pretrained models). Our method is designed to teach the model to identify and aggregate salient information across a \"cluster\" of related documents during pretraining. Specifically, our approach uses the Gap Sentence Generation objective (GSG) (Zhang et al., 2020)",
            "score": 0.5931096552416951,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "229182876",
            "title": "CERC: an interactive content extraction, recognition, and construction tool for clinical and biomedical text",
            "text": "For comparison purposes, topic-based and graphbased extraction summarization techniques were also included during the evaluation process: i topicDist: This method evaluates the relevance of a sentence term/concept frequency based on the overlap with the most frequent terms/concepts in the entire text [5,21,35]. j LexRank: LexRank is a graph based extractive summarization approach that uses the cosine similarity matrix to determine similarity between sentences and uses eigenvector centrality to extract relevant sentences [37]. A network of sentences is generated where each sentence corresponds to a node, and the edges represent the cosine similarity between pairs of sentences. The LexRank algorithm implemented in the MEAD toolkit was used for evaluation [38].",
            "score": 0.5924709180324774,
            "section_title": "Extractive summarization: MINTS: a multi-stage algorithm for sentence extraction and ranking",
            "char_start_offset": 14337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 768
                }
            ],
            "ref_mentions": [
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "2094180"
                },
                {
                    "start": 305,
                    "end": 308,
                    "matchedPaperCorpusId": "14968839"
                },
                {
                    "start": 308,
                    "end": 311,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "2962090"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60009765625
        },
        {
            "corpus_id": "117613639",
            "title": "Diverse Decoding for Abstractive Document Summarization",
            "text": "In order to verify the effectiveness of the proposed method, we compared our method with the results of state-of-the-art neural summarization methods reported in recent papers. Extractive models include a sentence ranking model called REFRESH [45], and SummaRuNNer [31], which is a recurrent neural network based sequence model. In addition, lead-3 is a strong extractive baseline that uses the first three sentences as the summary. Abstractive models include the original Pointer Generator Network (PGN) [36] and the Graph-Abs model [16] which applys the graph-based attention mechanism to seq2seq neural model.",
            "score": 0.5921170915615499,
            "section_title": "Baseline Methods",
            "char_start_offset": 24122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 612
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "26698484"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "224818792",
            "title": "Better Highlighting: Creating Sub-Sentence Summary Highlights",
            "text": "We compare our method with strong extractive and abstractive summarization systems for multidocument summarization, results are shown in Tables 3 and 5. DPP (Kulesza and Taskar, 2012) and variant DPP-BERT (Cho et al., 2019b) use determinantal point processes to extract whole sentences from a set of documents. SumBasic (Vanderwende et al., 2007) is an extractive approach leveraging the  fact that frequently occurring words are more likely to be included in the summary. KL-Sum (Haghighi and Vanderwende, 2009) is a greedy approach that iteratively adds sentences to the summary to minimize KL divergence. LexRank (Erkan and Radev, 2004) is a graph-based approach estimating sentence importance based on eigenvector centrality. All of these methods extract whole sentences rather than segments from a set of documents. We further consider abstractive summarization methods. Opinosis (Ganesan et al., 2010) creates a word co-occurrence graph and searches for a graph path to generate an abstract. PointerGen (See et al., 2017) learns to reuse source words or predict new words. The documents are concatenated to serve as input. CopyTrans (Gehrmann et al., 2018) uses a 4layer Transformer for the encoder and decoder. Hi-MAP (Fabbri et al., 2019) introduces an end-to-end hierarchical attention model to generate abstracts from multi-document inputs. \n\nWe explore two variants of our proposed method, called HL-XLNetSegs and HL-TreeSegs, focusing on highlighting summary segments. The former utilizes XLNet to extract a set of partially-overlapping segments from a sentence; the latter decomposes a sentence constituent parse tree into subtrees and collect text segments governed by the subtrees. An illustration is shown in Figure 3. Constituent parse trees are obtained using the Stanford parser (Manning et al., 2014). In both cases, the segments are passed to DPP, which identifies a set of important and non-redundant segments as highlights.",
            "score": 0.5920870718797637,
            "section_title": "Summarization Results",
            "char_start_offset": 23800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 224,
                    "matchedPaperCorpusId": "204901346"
                },
                {
                    "start": 320,
                    "end": 346,
                    "matchedPaperCorpusId": "10496513"
                },
                {
                    "start": 480,
                    "end": 511,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 616,
                    "end": 638,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 885,
                    "end": 907,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 1009,
                    "end": 1027,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1139,
                    "end": 1162,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1225,
                    "end": 1246,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1798,
                    "end": 1820,
                    "matchedPaperCorpusId": "14068874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.873046875
        },
        {
            "corpus_id": "227905057",
            "title": "Neural Abstractive Multi-Document Summarization: Hierarchical or Flat Structure?",
            "text": "Neural multi-document summarization Regarding to extractive models, neural networks are the most widely-used approach to model in-and cross-document knowledge with the objective to minimize the distance between the selected sentence set and the gold summary (Cao et al., 2017;Li et al., 2017;Ma et al., 2016;Nallapati et al., 2016;Yasunaga et al., 2017). One representative study (Yasunaga et al., 2017) is to construct a graph of the document cluster based on the similarities between sentences. Graph Neural Network (GNN) (Kipf and Welling, 2016) is then employed to select salient sentences. Argued by Liu and Lapata (2019), self-attention is a better mechanism to learn the latent dependency among documents than GNNs. As for abstractive models, studies tend to extract important paragraphs from different documents followed by a abstractive seq2seq model to generate summaries (Liu et al., 2018b;Liu and Lapata, 2019;Fabbri et al., 2019). Additionally, Chu and Liu (2019) adopts an auto-encoder model to conduct MDS in an unsupervised way. \n\nHierarchical neural network Hierarchical neural document models are applied in various fields of NLP such as document auto-encoder (Li et al., 2015) or text classification (Yang et al., 2016). In the area of abstractive summarization, Li et al. (2018) extends a hierarchical RNN encoderdecoder (Lin et al., 2015) with the hybrid sentenceword attention. Instead of trainable attention machanisms, Fabbri et al. (2019) hires a hierarchical RNN with Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) to represent the relationship between sentences. Liu and Lapata (2019) proposes a hierarchical Transformer by incorporating a global self-attention to represent cross-document relationships. Moreover, Zhang et al. (2019) constructs a hierarchical BERT (Devlin et al., 2018) to learn the context relationships among sentences by using other sentences to generate the masked sentence.",
            "score": 0.5920762726095364,
            "section_title": "Related work",
            "char_start_offset": 4276,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 276,
                    "matchedPaperCorpusId": "14651945"
                },
                {
                    "start": 276,
                    "end": 292,
                    "matchedPaperCorpusId": "29562039"
                },
                {
                    "start": 292,
                    "end": 308,
                    "matchedPaperCorpusId": "5845797"
                },
                {
                    "start": 331,
                    "end": 353,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 380,
                    "end": 403,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 605,
                    "end": 626,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 901,
                    "end": 922,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 922,
                    "end": 942,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 958,
                    "end": 976,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 1178,
                    "end": 1195,
                    "matchedPaperCorpusId": "207468"
                },
                {
                    "start": 1219,
                    "end": 1238,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 1282,
                    "end": 1298,
                    "matchedPaperCorpusId": "1412297"
                },
                {
                    "start": 1341,
                    "end": 1359,
                    "matchedPaperCorpusId": "17014226"
                },
                {
                    "start": 1443,
                    "end": 1463,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1527,
                    "end": 1558,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 1608,
                    "end": 1629,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1760,
                    "end": 1779,
                    "matchedPaperCorpusId": "155100086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66552734375
        },
        {
            "corpus_id": "5457260",
            "title": "Improved Affinity Graph Based Multi-Document Summarization",
            "text": "Automated multi-document summarization has drawn much attention in recent years. Multidocument summary is usually used to provide concise topic description about a cluster of documents and facilitate the users to browse the document cluster. A particular challenge for multi-document summarization is that the information stored in different documents inevitably overlaps with each other, and hence we need effective summarization methods to merge information stored in different documents, and if possible, contrast their differences. \n\nA variety of multi-document summarization methods have been developed recently. In this study, we focus on extractive summarization, which involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting t e sentences with highest scores. \n\nMEAD is an implementation of the centroid-based method (Radev et al., 2004) that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TF*IDF, etc. NeATS (Lin and Hovy, 2002) selects important content using entence position, term frequency, topic signature and term clustering, and then uses MMR (Goldstein et al., 1999) to remove redundancy. XDoX (Hardy et al., 1998) identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Harabagiu and Lacatusu (2005) investigate different topic representations and extraction methods. \n\nGraph-based methods have been proposed to rank sentences or passages. Websumm (Mani and Bloedorn, 2000) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization. \n\nIn this study, we extend the above graph-based works by proposing an integrated framework for considering both information richness and information novelty of a sentence based on sentence affinity graph.",
            "score": 0.591998654942947,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2033
                },
                {
                    "start": 2036,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 879,
                    "end": 898,
                    "matchedPaperCorpusId": "6354619"
                },
                {
                    "start": 1573,
                    "end": 1598,
                    "matchedPaperCorpusId": "9177142"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "225219530",
            "title": "Incorporating External Knowledge into Unsupervised Graph Model for Document Summarization",
            "text": "Document summarization has been widely studied for many years. Extractive summarization methods choose a subset of sentences from the original text to summarize a document. These sentences are scored and rearranged by many machine learning or deep learning algorithms. Before we use these ranking algorithms, sentences should be embedded into vectors to determine their similarity. A commonly used sentence embedding method is td-idf [10], which computes both the frequency and the inverse document frequency of words. Recently proposed skip-thought vectors [11] and BERT [5] are also useful sentence embedding methods, and they are widely used in deep learning summarization models. Many popular unsupervised models for document summarization are based on graph algorithms such as Textrank [8] and Centroid [9]. In the Textrank algorithm, the score of a sentence is calculated by Pagerank, which is a popular graph-based ranking algorithm in many areas. Lexrank [15] uses a stochastic graph-based method to compute the relationship of sentences, and this method is essentially identical to TextRank. To further improve the performance of these unsupervised graph algorithms, the original Textrank algorithm is often modified for the target task. For example, Mallick et al. [16] modify the inverse sentence frequency-cosine similarity by giving different weights to different words. Although these graph models perform well in document summarization tasks, the relationship between sentences are not considered in these models, and these models are likely to select similar sentences as summaries. In addition to the graph-based algorithms, unsupervised text cluster methods such as K-means [17] and K-medoids [18] are also used in document summarization tasks, and the sentences selected by these methods are independent of each other. In this paper, both the popular Textrank graph model and the K-means cluster algorithm are used in our approach. We take the relative position of sentences into account in order to improve the performance of Textrank, and select sentences that are independent of each other as summaries with the help of K-means cluster algorithms. \n\nDeep learning models are also widely used in document summarization tasks.",
            "score": 0.5919810386386027,
            "section_title": "Related Work",
            "char_start_offset": 6325,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2169
                },
                {
                    "start": 2172,
                    "end": 2246
                }
            ],
            "ref_mentions": [
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "14638345"
                },
                {
                    "start": 558,
                    "end": 562,
                    "matchedPaperCorpusId": "9126867"
                },
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "6354619"
                },
                {
                    "start": 963,
                    "end": 967,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1275,
                    "end": 1279,
                    "matchedPaperCorpusId": "69927855"
                },
                {
                    "start": 1692,
                    "end": 1696,
                    "matchedPaperCorpusId": "8433147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "264561617",
            "title": "Advanced product review summarization in e-commerce marketplaces: elevating beyond tf-idf and lexrank method",
            "text": "LexRank [20], a method for extractive summarization, stands as a pivotal element of this research, facilitating the systematic selection of sentences to create coherent product review summaries. In this section, we delve deeper into the mechanics of LexRank and its relevance in the e-commerce domain : \n\n\u2022 Graph-Based Summarization: LexRank employs a graph-based approach to summarization, utilizing the principles of centrality and similarity to identify the most significant sentences within the corpus of product reviews. Each sentence is represented as a node in the graph, and edges are established based on the similarity of sentences. \n\n\u2022 Eigenvalue Centrality (4): The core principle of LexRank is the concept of Eigenvector Centrality. \n\nEigenvector centrality (5) assigns importance scores to each sentence, considering not only the similarity of a sentence to others but also the importance of the sentences it is connected to. This ensures that important sentences are both similar to other sentences and well-connected within the network. \n\n\u2022 Fully Connected Graph: LexRank assumes a fully connected, undirected graph in which each sentence is a vertex and sentence similarity serves as the edges. The edges are weighted based on a similarity metric, typically idf-modified-cosine similarity, which quantifies the resemblance between sentences. \n\n\u2022 Similarity Measure: The similarity measure between sentences is crucial to LexRank's functioning. The idf-modified-cosine similarity is a common choice, comparing sentence vectors and incorporating inverse document frequency (IDF) to emphasize important terms that are rare in the corpus. The formula for idf-modified-cosine measures the overlap of terms between sentences and is central to the process (3). \n\n\u2022 Threshold-Based Sentence Selection: LexRank employs threshold values to determine which sentences are included in the final summary. By setting specific similarity thresholds, the method controls the length and detail of the generated summary. Higher thresholds result in more concise summaries, whereas lower thresholds yield more extensive summaries. \n\n\u2022 Degree Centrality: In addition to Eigenvector Centrality, LexRank uses degree centrality as another dimension for assessing the significance of sentences. Degree centrality quantifies the relationships between sentences and can help identify central sentences that may be pivotal in the summary.",
            "score": 0.5918419724368259,
            "section_title": "LexRank: An Extractive Summarization Method",
            "char_start_offset": 16518,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 302
                },
                {
                    "start": 305,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2127
                },
                {
                    "start": 2130,
                    "end": 2286
                },
                {
                    "start": 2287,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 12,
                    "matchedPaperCorpusId": "257408703"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69384765625
        },
        {
            "corpus_id": "202719258",
            "title": "Specificity-Based Sentence Ordering for Multi-Document Extractive Risk Summarization",
            "text": "Approaches to automatic text summarization fall into either the abstractive or extractive categories. Abstractive approaches seek to identify relevant phrases and sentences. The summary is a rewriting of those extracts; with recent approaches making use of graphs (Tan et al., 2017;Dohare et al., 2018) or neural networks (Chopra et al., 2016;Paulus et al., 2018). Extractive approaches attempt to: identify relevant text extractions in single and multi-document source material; rank the extracts to find the most informative; and combine the selected extracts into a summarized discourse. \n\nFinding and ranking relevant extracts is based on queries (Rahman and Borah, 2015), document word frequencies (Conroy et al., 2006;Gupta et al., 2007), probabilities (Vanderwende et al., 2007), tf-idf weighting (Erkan and Radev, 2004b;Fung and Ngai, 2006), topic modeling (Lin and Hovy, 2000), sentence clustering (McKeown et al., 1999;Siddharthan et al., 2004), graph-based methods (Erkan and Radev, 2004a,b;Mihalcea and Tarau, 2004), and neural networks (Filippova et al., 2015;Nallapati et al., 2017). Our extraction method (Section 3.2) is based on entity-keyword matching in multiple documents with subsequent ranking of token distances between entities and risk keywords. \n\nOnce extracts are selected for inclusion, techniques are applied to improve the overall quality of the summary. Improvements on the sentence level include sentence compression (Turner and Charniak, 2005;Galley and McKeown, 2007) and fusion (Jing and McKeown, 2000;Barzilay and McKeown, 2005).",
            "score": 0.5915714344464191,
            "section_title": "Automatic Summarization",
            "char_start_offset": 5412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1565
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 282,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 282,
                    "end": 302,
                    "matchedPaperCorpusId": "51867906"
                },
                {
                    "start": 322,
                    "end": 343,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 651,
                    "end": 675,
                    "matchedPaperCorpusId": "11691282"
                },
                {
                    "start": 703,
                    "end": 724,
                    "matchedPaperCorpusId": "7096616"
                },
                {
                    "start": 724,
                    "end": 743,
                    "matchedPaperCorpusId": "9777196"
                },
                {
                    "start": 759,
                    "end": 785,
                    "matchedPaperCorpusId": "10496513"
                },
                {
                    "start": 804,
                    "end": 828,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 828,
                    "end": 848,
                    "matchedPaperCorpusId": "15544040"
                },
                {
                    "start": 865,
                    "end": 885,
                    "matchedPaperCorpusId": "8598694"
                },
                {
                    "start": 907,
                    "end": 929,
                    "matchedPaperCorpusId": "8115414"
                },
                {
                    "start": 929,
                    "end": 954,
                    "matchedPaperCorpusId": "9670358"
                },
                {
                    "start": 1002,
                    "end": 1027,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1049,
                    "end": 1073,
                    "matchedPaperCorpusId": "1992250"
                },
                {
                    "start": 1073,
                    "end": 1096,
                    "matchedPaperCorpusId": "6405271"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "33696521",
            "title": "A Review on Automatic Text Summarization Approaches",
            "text": "Graph theory is simply used to model the connections or links that exist between objects. Generally, a graph can be denoted in the form of G = (V, E), where V represents the graph's vertex or node and E is the edge between each vertex. In the context of text documents, vertex represents sentences and an edge is the weight between two sentences. Using this approach, documents can therefore be represented as a graph where each sentence becomes the vertex and the weight between each vertex corresponds to the similarity between the two sentences. \n\nAs in most literature concerning graph based approach, the most widely used similarity measure is the cosine similarity measure (Erkan and Radev, 2004). An edge then exists if the similarity weight is above some predefined threshold. Figure 5 shows an example graph for multi document. Once the graph is constructed for a set of documents, important sentences will then be identified; it follows the idea that a sentence is considered important if it is strongly connected to many other sentences. \n\nThis approach differs from the cluster based approach where sentences are ranked based on its closeness to cluster centroid. Two well-known graph based ranking algorithms is the HITS algorithm (Kleinberg, 1999) and the Google's PageRank (Brin and Page, 2012). Both methods have been traditionally used in Web-link analysis and social networks. Lexrank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are two successful graph-based ranking systems that implement these algorithms. \n\nFurther studies have been carried to make improvement through modification in the ranking algorithm. Wan and Yang (2006) assigned different weights to intra-document links and inter-document links. \n\nThey gave more priority to sentence with high interdocument links. In later work by Hariharan and Srinivasan (2009), they approached the graph based method differently i.e., by discounting the already selected sentence by removing it from further consideration when they rank the remaining sentences in the document. \n\nApart from sentence level information, Wan (2008) and Wei et al. (2010) devised a document-sensitive graph model to explore document impact on the graphbased summarization, by incorporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process.",
            "score": 0.5912989591546349,
            "section_title": "Graph Based Method",
            "char_start_offset": 26533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2065
                },
                {
                    "start": 2068,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 679,
                    "end": 701,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1244,
                    "end": 1261,
                    "matchedPaperCorpusId": "216000619"
                },
                {
                    "start": 1288,
                    "end": 1309,
                    "matchedPaperCorpusId": "911040"
                },
                {
                    "start": 1403,
                    "end": 1426,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1650,
                    "end": 1669,
                    "matchedPaperCorpusId": "5457260"
                },
                {
                    "start": 1833,
                    "end": 1864,
                    "matchedPaperCorpusId": "62241768"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "1390285",
            "title": "Query-based summarization using MDL principle",
            "text": "Multiple works about QS have been published in recent years. Daum\u00e9 III and Marcu (2006) presented BayeSum, a model for sentence extraction in QS. BayeSum is based on the concepts of three models: language model, Bayesian statistical model, and graphical model. Mohamed and Rajasekaran (2006) proposed an approach for QS based on document graphs, which are directed graphs of concepts or entity nodes and relations between them. The work in (Bosma, 2005) introduced a graph search algorithm that looks for relevant sentences in the discourse structure represented as a graph. The author used Rhetorical Structure Theory for creating a graph representation of a text document -a weighted graph with nodes standing for sentences and weighted edges representing a distance between sentences. Conroy et al. (2005) presented the CLASSY summarizer that used a hidden Markov model based on signature terms and query terms for sentence selection within a document, and a pivoted question answering algorithm for redundancy removal. Liu et al. (2012) proposed QS with multi-document input using unsupervised deep learning. Schilder and Kondadadi (2008) presented FastSum -a fast query-based multi-document summarizer based solely on word-frequency features of clusters, documents, and topics, where summary sentences are ranked by a regression support vector machine. Tang et al. (2009) proposed two strategies to incorporate the query information into a probabilistic model. Park et al. (2006) introduced a method that uses non-negative matrix factorization to extract query-relevant sentences. Some works deal with domain-specific data (Chen and Verma, 2006) and use domain-specific terms when measuring the distance between sentences and a query. Zhou et al. (2006) describes a query-based multi-document summarizer based on basic elements, a head-modifier-relation triple representation of document content. Recently, many works integrate topic modeling into their summarization models. For example, Li and Li (2014) extend the standard graph ranking algorithm by proposing a two-layer (sentence layer and topic layer) graphbased semi-supervised learning approach based on topic modeling techniques.",
            "score": 0.5911013613992928,
            "section_title": "Related Work",
            "char_start_offset": 1717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 87,
                    "matchedPaperCorpusId": "6241932"
                },
                {
                    "start": 261,
                    "end": 291,
                    "matchedPaperCorpusId": "10381145"
                },
                {
                    "start": 1023,
                    "end": 1040,
                    "matchedPaperCorpusId": "17722928"
                },
                {
                    "start": 1113,
                    "end": 1142,
                    "matchedPaperCorpusId": "22109805"
                },
                {
                    "start": 1358,
                    "end": 1376,
                    "matchedPaperCorpusId": "6491726"
                },
                {
                    "start": 1466,
                    "end": 1484,
                    "matchedPaperCorpusId": "206604820"
                },
                {
                    "start": 1628,
                    "end": 1650,
                    "matchedPaperCorpusId": "13210237"
                },
                {
                    "start": 1740,
                    "end": 1758,
                    "matchedPaperCorpusId": "6408277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "271050481",
            "title": "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs",
            "text": "There are some past attempts that have been made to improve coherence in extractive summarization.Christensen et al. (2013) proposed a G-FLOW, a joint model for selection and ordering sentences that balances coherence for multi-document extractive summarization.After that, Parveen and Strube (2015) proposed a graph-based method for extractive single-document summarization that considers importance, non-redundancy, and local coherence simultaneously.In addition, Kurisinkel and Varma (2015) introduced A multi-document summarization method that ensures content coverage, sentence ordering, topical coherence, topical order, and inter-sentence structural relationships using a Local Coherent Unit (LCU).Following this, J Kurisinkel et al. (2016) proposed scoringbased function to identify the discourse structure which provides the context for the creation of a sentence for generating comprehensible summaries.Furthermore, Wu and Hu (2018) utilized reinforcement learning to extract a coherent summary, and Abdolahi and Zahedi (2019) enhanced coherence in extractive document summarization through a greedy approach and word vectors.In addition, Jie et al. (2023b) introduced two strategies, including pre-trained converting models (model-based) and converting matrices (MAT-based) that merge sentence representations to improve coherence.With the emergence of LLMs, Zhang et al. (2023b) attempted to analyze the performance of GPT-3 with different prompting for generating coherent summaries.Differing from these existing efforts, we approach the concept of coherence within summaries through the lens of user-specific intent.",
            "score": 0.5906453144120465,
            "section_title": "B Related Work",
            "char_start_offset": 16913,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 262
                },
                {
                    "start": 262,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 913
                },
                {
                    "start": 913,
                    "end": 1136
                },
                {
                    "start": 1136,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 299,
                    "matchedPaperCorpusId": "14547015"
                },
                {
                    "start": 723,
                    "end": 747,
                    "matchedPaperCorpusId": "18715648"
                },
                {
                    "start": 926,
                    "end": 942,
                    "matchedPaperCorpusId": "4999752"
                },
                {
                    "start": 1010,
                    "end": 1036,
                    "matchedPaperCorpusId": "146011057"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "865952",
            "title": "Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity",
            "text": "In pair-wise passage similarity-based summarization, passage centrality is defined by the similarity between each passage and every other passage. The work presented by Erkan and Radev (2004), as well as the work developed by Mihalcea and Tarau (2005), are examples of this approach. Erkan and Radev (2004) propose three graph-based approaches to pair-wise passage similarity-based summarization with similar performance: degree centrality, LexRank, and continuous LexRank. Degree centrality is based on the degree of a vertex. Pair-wise sentence similarity is used to build a graph representation of the input source: vertices are sentences and edges connect vertices which corresponding sentences are similar above a given threshold. Sentences similar to a large number of other sentences are considered the most central (relevant) ones. Degree centrality is similar to the model we propose. However, in the model we propose, we introduce the concept of support set to allow the use of a different threshold for each sentence. This improves the representation of each sentence, leading to the creation of better summaries. \n\nLexRank, based on Google's PageRank (Brin & Page, 1998), builds on degree centrality (degree) by making the centrality of a sentence s be influenced by similar sentences, the adjacent ones in the graph representation (Eq. 5). \n\nThe ranking model is similar to PageRank except in what concerns the similarity (adjacency) graph, that, in this case, is undirected (Eq. 6, d is a damping factor and N the number of sentences). \n\nContinuous LexRank is a weighted version of LexRank (it uses Eq. 7 instead of Eq. 5). \n\nsim(s, t) Mihalcea and Tarau (2005), in addition to Google's PageRank, also explore the HITS algorithm (Kleinberg, 1999) to perform graph-based extractive text summarization: again, documents are represented as networks of sentences and these networks are used to globally determine the importance of each sentence. As it happens in the models proposed by Erkan and Radev, sentences are vertices (V ) and edges (w) between vertices are established by passage similarity.",
            "score": 0.5905841547324158,
            "section_title": "Pair-wise Passage Similarity-based Centrality",
            "char_start_offset": 11636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1549
                },
                {
                    "start": 1552,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 191,
                    "matchedPaperCorpusId": "2962090"
                },
                {
                    "start": 226,
                    "end": 251,
                    "matchedPaperCorpusId": "8878897"
                },
                {
                    "start": 284,
                    "end": 306,
                    "matchedPaperCorpusId": "2962090"
                },
                {
                    "start": 1650,
                    "end": 1675,
                    "matchedPaperCorpusId": "8878897"
                },
                {
                    "start": 1743,
                    "end": 1760,
                    "matchedPaperCorpusId": "216000619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "269663378",
            "title": "Keyword Acquisition for Language Composition Based on TextRank Automatic Summarization Approach",
            "text": "The model utilized pre-trained language model reinforcement sequences and structural encoders to extract key information through a network of structural graphs and graph transformers to effectively guide the decoding process.Tests on a legal opinion news corpus revealed that the model outperforms other baselines in terms of ROUGE and BERT scores, and its effectiveness was proven by manual evaluation [8]. A. Zagar and M. Robnik-Sikonja presented a cross-language AS approach to summarizing Slovenian news articles using a pre-trained English summary model.To address decoder limitations, additional language models were introduced for target language text evaluation.The cross-language model was demonstrated to be qualitatively similar to the target language-specific model through automatic and manual evaluation, but occasionally misleading or absurd content appeared [9].E. Inan proposed an entity-based text summarization method that recognizes named entities and constructs dependency graphs from a pre-trained language model.A reconciliation centrality algorithm was applied to summarize the entity ordering, outperforming the unsupervised learning baseline and approaching the state-of-the-art end-to-end model [10].\n\nIn summary, the recent literature in the field of automatic text processing, especially in keyword extraction and summary generation, has demonstrated several notable advances.Researchers have developed different approaches in order to accommodate multiple languages and text formats.For example, Dewi and Widiastuti developed a model containing multiple summarization techniques specifically for Indonesian text to accommodate long texts and complex documents.In the widely studied TextRank algorithm, Qiu and Zheng enhanced its performance in keyword extraction through tolerance rough sets, while Hernawan et al. improved the accuracy of the algorithm in sentence importance assessment using BM25.Huang and Xie improved the accuracy of keyword extraction for patented text by combining the TextRank algorithm with a priori knowledge networks.Further, Aliakbarpour et al. combined a convolutional neural network and a long and short-term memory network while incorporating an attention mechanism to enhance the quality of text summarization.The elemental graph augmented abstract summarization model proposed by Huang et al. on the other hand, demonstrates superiority in handling legal opinion news.",
            "score": 0.5897585048530164,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 878
                },
                {
                    "start": 878,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1227
                },
                {
                    "start": 1229,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1513
                },
                {
                    "start": 1513,
                    "end": 1690
                },
                {
                    "start": 1690,
                    "end": 1929
                },
                {
                    "start": 1929,
                    "end": 2074
                },
                {
                    "start": 2074,
                    "end": 2272
                },
                {
                    "start": 2272,
                    "end": 2431
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "237621017"
                },
                {
                    "start": 874,
                    "end": 877,
                    "matchedPaperCorpusId": "237396364"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "225187980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482666015625
        },
        {
            "corpus_id": "220045815",
            "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
            "text": "Document summarization aims at producing a fluent, condensed summary for given documents. Single document summarization has shown promising results with sequence-to-sequence models that encode a source document and then decode it into a summary (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u00b8elikyilmaz et al., 2018). Multi-document summarization requires producing a summary from a cluster of thematically related documents, where the given documents complement and overlap each other. Multi-document summarization involves identifying important information and filtering out redundant information from multiple input sources. \n\nThere are two primary methodologies for multidocument summarization: extractive and abstractive. Extractive methods directly select important sentences from the original, which are relatively simple. Cao et al. (2015) rank sentences with a recursive neural network. Yasunaga et al. (2017) employ a Graph Convolutional Network (GCN) to incorporate sentence relation graphs to improve the performance for the extractive summarization. Abstractive methods can generate new words and new sentences, but it is technically more difficult than extractive methods. Some works on multidocument summarization simply concatenate multiple source documents into a long flat sequence and model multi-document summarization as a long sequence-to-sequence task (Liu et al., 2018;Fabbri et al., 2019). However, these approaches don't take the hierarchical structure of document clusters into account, while the too-long input often leads to the degradation in document summarization (Cohan et al., 2018;Liu and Lapata, 2019). Recently, hierarchical frameworks have shown their effectiveness on multi-document summarization (Zhang et al., 2018;Liu and Lapata, 2019). These approaches usually use multiple encoders to model hierarchical relationships in the discourse structure, but other methods to incorporate the structural semantic knowledge have not been explored. The combination of extractive and abstractive has been explored in single document summarization. Chen and Bansal (2018) use the extracted sentences as the input of the abstractive summarization.",
            "score": 0.5895722388548582,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 263,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 263,
                    "end": 283,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 283,
                    "end": 305,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 305,
                    "end": 332,
                    "matchedPaperCorpusId": "4406182"
                },
                {
                    "start": 846,
                    "end": 863,
                    "matchedPaperCorpusId": "10675728"
                },
                {
                    "start": 912,
                    "end": 934,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1409,
                    "end": 1429,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1612,
                    "end": 1632,
                    "matchedPaperCorpusId": "4894594"
                },
                {
                    "start": 1632,
                    "end": 1653,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1772,
                    "end": 1793,
                    "matchedPaperCorpusId": "170079112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "256627286",
            "title": "An entity-guided text summarization framework with relational heterogeneous graph neural network",
            "text": "It is still a challenge to effectively connect structural knowledge in text with knowledge in KG to improve summarization. \n\nFor the cross-sentence relations issue, modeling these relations is crucial to extracting summaryworth sentences which can be further summarized to abstractive summaries. Early traditional work such as LexRank [13] and TextRank [14] uses inter-sentence cosine similarity to build text graphs. Recent progresses are based on GNN [15] to capture long-distance dependency through modeling crosssentence relations as graph structures. Various types of graph structures can be built, and different variations of GNNs are proposed to calculate the graphs. In particular for summarization, graphs constructed from text often contains real-value weighted multi-type edges. A recent study constructs a sentence-word bipartite graph with a single type of weighted edge, and a heterogeneous GNN is proposed to calculate node encodings [2]. GNN is also used to model intra-and inter-sentence relations for dialogue summarization [16]. Other work relies on discourse structures to build summarization graphs [17,18]. For graphs with unweighted multi-type edges, the relational GNN (R-GNN) [19] is proposed to calculate the graphs. Edge types and edge weights are important information for GNN-based graph calculations, and Entities in text are informative semantic units for graph construction. Current studies for summarization do not make full use of entities, edge types, and edge weights for construction and calculation of graphs to model cross-sentence relations. \n\nAs an attempt for tackling the knowledge issue and the cross-sentence relation issue in combination for summarization, this paper proposes an entity-guided summarization framework by leveraging entities mentioned in text to connect KG and GNN, and by making use of edge weights and edge types in GNN for calculations of graphs with weighted multi-type edges. \n\nFirstly, entities mentioned in text are used to build and calculate a sentence-entity graph with weighted multi-type edges to model sentence relations for summarization. Three edge types, i.e., sentence-entity edges, entity-entity edges, and sentence-entity edges are introduced to link sentences and entities. These edges have different weights.",
            "score": 0.5895490936851405,
            "section_title": "Introduction",
            "char_start_offset": 1992,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 125,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1942
                },
                {
                    "start": 1945,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2255
                },
                {
                    "start": 2256,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 339,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 949,
                    "end": 952,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1042,
                    "end": 1046,
                    "matchedPaperCorpusId": "227231217"
                },
                {
                    "start": 1120,
                    "end": 1124,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1124,
                    "end": 1127,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "5458500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7763671875
        },
        {
            "corpus_id": "258833725",
            "title": "Enhancing Coherence of Extractive Summarization with Multitask Learning",
            "text": "Extractive summarization is to extract a set of text units to formulate a short summary for a given long article. Traditional extractive summarization methods like Textrank and Lexrank (Mihalcea and Tarau, 2004;Erkan and Radev, 2004) compute the importance of each sentence based on its centrality in the whole article. By using neural networks, extractive summarization can be considered as a sequence labelling problem, where the model scores each sentence for ranking and selecting (Cheng and Lapata, 2016). In the era of transformer, pretrained language models are widely adopted to further improve the extractive summarization. One of the most popular ones is BertSum (Liu, 2019), which uses BERT (Devlin et al., 2018) as the text encoder, and fine-tunes the [CLS] token at the beginning of each sentence for scoring (Liu, 2019). Graph-based neural networks (Wang et al., 2020;Jing et al., 2021) are also used to model the inter-sentence or sentence-word relationships, where different levels of granularity can be modelled as semantic nodes in a heterogeneous graph. Kwon et al. (2021) further considers nested tree structures of documents to explicitly represent sentence information. Gu et al. (2022) introduces a reinforcement-learning-based extractive summarizer enriched with information on the current extraction history at each step.",
            "score": 0.5889149876852046,
            "section_title": "Extractive Summarization",
            "char_start_offset": 3375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1346
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 211,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 211,
                    "end": 233,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 485,
                    "end": 509,
                    "matchedPaperCorpusId": "1499080"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6748046875
        },
        {
            "corpus_id": "272831675",
            "title": "GLARE: Guided LexRank for Advanced Retrieval in Legal Analysis",
            "text": "This section establishes the theoretical foundation for the unsupervised model used in this study to classify special appeals. Section 2.1 provides an overview of text summarization techniques, categorizing them into three main approaches. It highlights the risks of hallucination in abstractive methods and details extractive techniques like topic-based and centrality-based summarization, with emphasis on the BERTopic model and the LexRank algorithm. Section 2.2 provides an overview of Lexrank, a graph-based algorithm that served as a model for the summarization algorithm we developed in this project.",
            "score": 0.5887858536891998,
            "section_title": "Background",
            "char_start_offset": 10125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 607
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60693359375
        },
        {
            "corpus_id": "236460044",
            "title": "Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation",
            "text": "Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents to guide the summary generation process. \n\nWhile the multi-document summarization task aims to extract information shared by multiple documents, related work generation aims to compare and introduce the cited works in logic order.",
            "score": 0.5877960865087538,
            "section_title": "Related Work",
            "char_start_offset": 6577,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "170079112",
            "title": "Hierarchical Transformers for Multi-Document Summarization",
            "text": "Previous solutions include model transfer (Zhang et al., 2018;Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives (Ma et al., 2016;Chu and Liu, 2018). Liu et al. (2018) propose a methodology for constructing large-scale summarization datasets and a two-stage model which first extracts salient information from source documents and then uses a decoder-only architecture (that can attend to very long sequences) to generate the summary. We follow their setup in viewing multi-document summarization as a supervised machine learning prob- lem and for this purpose assume access to large, labeled datasets (i.e., source documents-summary pairs). In contrast to their approach, we use a learning-based ranker and our abstractive model can hierarchically encode the input documents, with the ability to learn latent relations across documents and additionally incorporate information encoded in well-known graph representations.",
            "score": 0.5874564889763031,
            "section_title": "Related Work",
            "char_start_offset": 6846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1093
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 62,
                    "matchedPaperCorpusId": "53223447"
                },
                {
                    "start": 62,
                    "end": 85,
                    "matchedPaperCorpusId": "52051402"
                },
                {
                    "start": 284,
                    "end": 301,
                    "matchedPaperCorpusId": "5845797"
                },
                {
                    "start": 321,
                    "end": 338,
                    "matchedPaperCorpusId": "39871772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "262592258",
            "title": "A hybrid approach for text summarization using semantic latent Dirichlet allocation and sentence concept mapping with transformer",
            "text": "The author used three methods: the first used a graph and a supervised model separately and then combined them to assign a score to the sentence. The second method evaluated the importance of sentences by using the graph model as an independent feature of the supervised model. The third model used a priori value to the graph model to score the sentences using a supervised approach. \n\nMulti-document summarization using deep learning architecture as a hybrid model [19] generates comprehensive summaries from news articles on specific topics. The architecture performed better than the traditional extractive model when evaluated using DUC 2004 data. Extracting the gist of documents is possible by using information such as titles, image captions, and side headings [20]. The author has proposed a single-document summarizer framework with a hierarchical document encoder with attention to side information. The extractive summarization framework with side information generates a better summary with fluency. Another framework matches extracted summary with the original document in semantic space [21] and models sentence relationships. It also provides a deep analysis of the gap between summary-level and sentence-level extractors based on the features of a dataset. \n\nOne of the main driving forces in recent development in abstractive text summarization is the availability of new neural architectures and new strategies in training. However, there is a need to address issues such as a proper model and data analysis tool and understanding the failure model of summarization. SummVis [22], an open-source tool, allows us to visualize, generate a summary, and analyze the summarization models and the evaluation metrics used. Topic modeling has been recently used in text summarization to identify hidden topics in the document [23]. Latent Dirichlet allocation (LDA) performs better than latent semantic analysis (LSA) if the number of features increases in the sentences. A hybrid approach for text summarization [24] proposed a novel sentence scoring method for extractive summarization. The sentence scoring parameter significantly improves the performance of the model. The researchers presented a single-document text summarization technique based on sentence similarity and document context [25]. Their approach utilized undirected graph-based scoring to evaluate sentences and determine which ones should be included in the summary.",
            "score": 0.5869933797929268,
            "section_title": "LITERATURE REVIEW",
            "char_start_offset": 8520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2312
                },
                {
                    "start": 2313,
                    "end": 2449
                }
            ],
            "ref_mentions": [
                {
                    "start": 1102,
                    "end": 1106,
                    "matchedPaperCorpusId": "215828313"
                },
                {
                    "start": 1594,
                    "end": 1598,
                    "matchedPaperCorpusId": "233241131"
                },
                {
                    "start": 2024,
                    "end": 2028,
                    "matchedPaperCorpusId": "237974591"
                },
                {
                    "start": 2307,
                    "end": 2311,
                    "matchedPaperCorpusId": "23433138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "16812271",
            "title": "Complex Question Answering: Unsupervised Learning Approaches and Experiments",
            "text": "Researchers all over the world working on query-based summarization are trying different directions to see which methods provide the best results. \n\nThere are a number of sentence retrieval systems based on IR (Information Retrieval) techniques. These systems typically don't use a lot of linguistic information, but they still deserve special attention. Murdock and Croft (2005) propose a translation model specifically for monolingual data, and show that it significantly improves sentence retrieval over query likelihood. Translation models train on a parallel corpus and they used a corpus of question/answer pairs. Losada (2005) presents a comparison between multiple-Bernoulli models and multinomial models in the context of a sentence retrieval task and shows that a multivariate Bernoulli model can really outperform popular multinomial models for retrieving relevant sentences. Losada and Fern\u00e1ndez (2007) propose a novel sentence retrieval method based on extracting highly frequent terms from top retrieved documents. Their results reinforce the idea that top retrieved data is a valuable source to enhance retrieval systems. This is specially true for short queries because there are usually few query-sentence matching terms. They argue that this method improves significantly the precision at top ranks when handling poorly specified information needs. \n\nThe LexRank method addressed by Erkan and Radev (2004) was very successful in generic multi-document summarization. A topic-sensitive LexRank is proposed by Otterbacher, Erkan, and Radev (2005). As in LexRank, the set of sentences in a document cluster is represented as a graph where nodes are sentences, and links between the nodes are induced by a similarity relation between the sentences. The system then ranks the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question. \n\nConcepts of coherence and cohesion enable us to capture the theme of the text. Coherence represents the overall structure of a multi-sentence text in terms of macro-level relations between clauses or sentences (Halliday & Hasan, 1976). Cohesion, as defined by Halliday and Hasan (1976), is the property of holding text together as one single grammat-ical unit based on relations (i.e.",
            "score": 0.5867845562681864,
            "section_title": "Related Work",
            "char_start_offset": 9065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 149,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1964
                },
                {
                    "start": 1967,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 379,
                    "matchedPaperCorpusId": "7188219"
                },
                {
                    "start": 620,
                    "end": 633,
                    "matchedPaperCorpusId": "18849071"
                },
                {
                    "start": 887,
                    "end": 914,
                    "matchedPaperCorpusId": "14219081"
                },
                {
                    "start": 1401,
                    "end": 1423,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1539,
                    "end": 1562,
                    "matchedPaperCorpusId": "10274362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "250572619",
            "title": "Enhancing Biomedical Scientific Reviews Summarization with Graph-based Factual Evidence Extracted from Papers",
            "text": "Abstractive Document Summarization. Summarizing text implies compressing the input document into a shorter version, retaining salient information, and discarding redundant or unnecessary attributes. An abstractive summarizer is asked to generate new sentences, rather than simply selecting the core ones, thus imitating a paraphrasing process closer to human-like interpretation. \n\nNeural models have achieved unprecedented results in recent years, mainly thanks to encoderdecoder frameworks. In a nutshell, an encoder maps the source tokens into a sequence of continuous representations, while a decoder generates the summary step-by-step. Remarkably, transformer-based architectures and self-supervised pre-training techniques have been responsible for a profound impetus in abstractive summarization (Liu and Lapata, 2019;Dong et al., 2019;Rothe et al., 2020;Zhang et al., 2019;Qi et al., 2020;Lewis et al., 2020)-even in low-resource (Moro and Ragazzi, 2022) and multi-document settings (Moro et al., 2022), promoting the creation of large unlabeled corpora. \n\nHowever, according to large-scale human evaluations (Maynez et al., 2020), nowadays text generators are highly prone to hallucinate content that is unfaithful to the input document. For this reason, latest contributions (Pasunuru and Bansal, 2018;Arumae and Liu, 2019;Huang et al., 2020a) tend to include reinforcement learning modules to improve informativeness and consistency. \n\nGraph-enhanced Summarization. Graphs are one of the most effective forms for introducing external knowledge into summarization models, allowing different quality improvements (e.g., coherence, factuality, low redundancy, long-range dependencies, informativeness, semantic coverage) depending on how they are constructed. \n\nParticularly, graph structures have long been used for extractive summarization. In this sense, early approaches, such as TextRank (Mihalcea and Tarau, 2004), propose to build a connectivity network with inter-sentence cosine similarity and document-level relations (Wan, 2008).",
            "score": 0.5865277855338714,
            "section_title": "RELATED WORK",
            "char_start_offset": 4508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 803,
                    "end": 825,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 825,
                    "end": 843,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 843,
                    "end": 862,
                    "matchedPaperCorpusId": "198967997"
                },
                {
                    "start": 881,
                    "end": 897,
                    "matchedPaperCorpusId": "210164665"
                },
                {
                    "start": 897,
                    "end": 916,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1117,
                    "end": 1138,
                    "matchedPaperCorpusId": "218487034"
                },
                {
                    "start": 1285,
                    "end": 1312,
                    "matchedPaperCorpusId": "4940548"
                },
                {
                    "start": 1333,
                    "end": 1353,
                    "matchedPaperCorpusId": "218487279"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "219690773",
            "title": "Movie Review Summarization Using Supervised Learning and Graph-Based Ranking Algorithm",
            "text": "e final summary is produced based on sentences containing the relevant keywords. A statistical method was proposed in [37] for news article summarization. e sentences in the news documents are scored based on different features such length of sentence, first sentence of news article, title of news article, proper nouns, and term frequency. \n\nComputational Intelligence and Neuroscience e top scored sentences are selected to produce a summary. e authors in [38] presented a pattern-based method for news article summarization. \n\nIn recent years, various graph-based methods have attracted more attention and effectively attempted for text summarization. ese methods utilize PageRank algorithm [39] and its variants to give rank/score to graph nodes, which represent sentences or passages. e authors in [40] proposed a connectivity graph, which assumes that nodes only carry significant information if they are connected to many other nodes. e authors in [41] introduced a Lex-PageRank approach that is based on eigenvector centrality, which constructs a sentence connectivity matrix and utilizes similar algorithm like PageRank to find the significant sentences for summary. A similar algorithm to PageRank was also proposed in [42] which finds salient sentences for summary generation. \n\ne authors in [24] presented a graph-based approach, which integrates surface features with text content and investigates subtopic features in multiple documents to incorporate them into the graph-based ranking algorithm. A multidocument summarization approach based on affinity graph [43] exploits similar algorithm to PageRank and computes sentence scores in the affinity graph based on information richness. e authors in [44] demonstrated a document-sensitive graph model for multidocument generic summarization and highlighted the impact of global document set information at sentence level. A weighted graph model for generic multidocument summarization introduced in [45] combines sentence ranking and sentence clustering methods. e authors in [46] presented a graphbased method for multidocument summarization of Vietnamese documents and employed traditional PageRank algorithm to rank the important sentences. e authors in [47] demonstrated an event graph-based approach for multidocument extractive summarization. However, the approach requires the construction of hand crafted rules for argument extraction, which is a time consuming process and may limit its application to a specific domain.",
            "score": 0.5864611784724577,
            "section_title": "Related Work",
            "char_start_offset": 14204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2312
                },
                {
                    "start": 2313,
                    "end": 2493
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 122,
                    "matchedPaperCorpusId": "14486205"
                },
                {
                    "start": 459,
                    "end": 463,
                    "matchedPaperCorpusId": "3780782"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "9177142"
                },
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "8878897"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "22342754"
                },
                {
                    "start": 1575,
                    "end": 1579,
                    "matchedPaperCorpusId": "5457260"
                },
                {
                    "start": 1714,
                    "end": 1718,
                    "matchedPaperCorpusId": "35133176"
                },
                {
                    "start": 1963,
                    "end": 1967,
                    "matchedPaperCorpusId": "39384736"
                },
                {
                    "start": 2040,
                    "end": 2044,
                    "matchedPaperCorpusId": "26333220"
                },
                {
                    "start": 2221,
                    "end": 2225,
                    "matchedPaperCorpusId": "45592507"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "271791043",
            "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
            "text": "The rapid expansion of online content and big data has precipitated an urgent need for efficient summarization techniques to swiftly comprehend vast textual documents without compromising their original integrity. Current approaches in Extractive Text Summarization (ETS) leverage the modeling of inter-sentence relationships, a task of paramount importance in producing coherent summaries. This study introduces an innovative model that integrates Graph Attention Networks (GATs) with Transformer-based Bidirectional Encoder Representations from Transformers (BERT) and Latent Dirichlet Allocation (LDA), further enhanced by Term Frequency-Inverse Document Frequency (TF-IDF) values, to improve sentence selection by capturing comprehensive topical information. Our approach constructs a graph with nodes representing sentences, words, and topics, thereby elevating the interconnectivity and enabling a more refined understanding of text structures. This model is stretched to Multi-Document Summarization (MDS) from Single-Document Summarization, offering significant improvements over existing models such as THGS-GMM and Topic-GraphSum, as demonstrated by empirical evaluations on benchmark news datasets like Cable News Network (CNN)/Daily Mail (DM) and Multi-News. The results consistently demonstrate superior performance, showcasing the model\u2019s robustness in handling complex summarization tasks across single and multi-document contexts. This research not only advances the integration of BERT and LDA within a GATs but also emphasizes our model\u2019s capacity to effectively manage global information and adapt to diverse summarization challenges.",
            "score": 0.5864447625859983,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "266362657",
            "title": "Shaping Political Discourse using multi-source News Summarization",
            "text": "There has been a lot of recent research interest in multi-document summarization, primarily owing to the success of deep learning in the single document case. In [10] the authors propose a document-level reconstruction framework named DocRebuild, which reconstructs the documents with summary sentences through a neural document model and selects summary sentences to minimize the reconstruction error. They also apply two strategies, sentence filtering and beamsearch, to improve the performance of their method. \n\n[19] proposes a neural multi-document summarization system that incorporates sentence relation graphs. They employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. The authors then use a greedy heuristic to extract salient sentences that avoid redundancy. \n\nMore recently, [8] uses a transformer based model to generate summaries, drawing from the recent success of transformers in natural language understanding and generation tasks. The authors use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, they introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder-decoder architectures used in sequence transduction. They show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. [7] extends methods from single document summarization to multiple documents. Even more recently, [4] converts the multiple source documents into a knowledge graph before feeding it into a summarization module. \n\nArgument mining [11,5] is an area within natural language understanding that focuses on mining arguments from opinionated text. It is an advanced form of human language understanding by the machine. When sufficient explicit discourse markers are present in the language utterances, the argumentation can be interpreted by the machine with an acceptable degree of accuracy.",
            "score": 0.5864135123734042,
            "section_title": "Related Work",
            "char_start_offset": 3310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "5845797"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "153313213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "8862807",
            "title": "Selecting Sentences for Answering Complex Questions",
            "text": "In LexRank (Erkan and Radev, 2004), the concept of graph-based centrality is used to rank a set of sentences, in producing generic multi-document summaries. A similarity graph is produced for the sentences in the document collection. In the graph, each node represents a sentence. The edges between the nodes measure the cosine similarity between the respective pair of sentences. The degree of a given node is an indication of how much important the sentence is. Once the similarity graph is constructed, the sentences are then ranked according to their eigenvector centrality. To apply LexRank to queryfocused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005). We followed a similar approach in order to calculate this feature. The score of a sentence is determined by a mixture model of the relevance of the sentence to the query and the similarity of the sentence to other high-scoring sentences.",
            "score": 0.586261823627226,
            "section_title": "Graph-based Similarity Measure",
            "char_start_offset": 14857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 938
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 34,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 673,
                    "end": 699,
                    "matchedPaperCorpusId": "10274362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7568359375
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "The fundamental concept behind employing graph representation lies in leveraging the linguistic information contained within sentences. As a result, Graph Neural Networks (GNNs) have gained widespread adoption due to their ability to effectively discern cross-sentence relationships by leveraging the principles of discourse graphs [7], [10]. Recent research trends have shifted towards representing documents through heterogeneous graphs, which include different types of nodes. This approach aims to harness the influence of additional semantic units, such as words and sentences, as demonstrated by the researchers in [5] and [9]. \n\nThe authors in studies [3], [8] included latent topic nodes in heterogeneous graph structures to capture global context. They employed NTM and K-means along with Gaussian Mixture Models (GMM) respectively [3], [8]. Following a similar approach, the researchers of [7] expanded the heterogeneous graph structure by adding passage nodes, complementing the existing nodes of words and sentences. The work done by [28] utilized the Transformer-Gated Attention (TGA) to efficiently capture sentence representations through diverse types of text graphs at multiple levels. The authors [29] advanced methodologies, including hypergraph transformer layers, are employed to iteratively update and cultivate strong sentence representation. \n\nExisting graph-based summarization models have several shortcomings that hinder their effectiveness in capturing document structures. The heterogeneous graph methods utilize diverse node types [5], [9] but often fail to establish strong interconnections, leading to sub-optimal information propagation. Passage nodes, for instance, facilitates hierarchical representations [7] but fail to model fine-grained dependencies between semantic units. \n\nFurther, TGA [28] and hypergraph-based approaches [29], such as attention-based models, can produce high quality sentence representations. However, their high computational complexity makes them inefficient for large-scale summarization tasks.",
            "score": 0.5852317798216494,
            "section_title": "B. NEURAL GRAPH BASED SUMMARIZATION",
            "char_start_offset": 12105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1812
                },
                {
                    "start": 1815,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 332,
                    "end": 335,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 621,
                    "end": 624,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 664,
                    "end": 667,
                    "matchedPaperCorpusId": "252670538"
                },
                {
                    "start": 846,
                    "end": 849,
                    "matchedPaperCorpusId": "252670538"
                },
                {
                    "start": 900,
                    "end": 903,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "247249484"
                },
                {
                    "start": 1561,
                    "end": 1564,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1741,
                    "end": 1744,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "247249484"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "506350",
            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
            "text": "Researchers have also tried to integrate machine learning into summarization as more features have been proposed and more training data have become available (Kupiec, Pedersen, & Chen, 1995;Lin, 1999;Osborne, 2002;Daum\u00e9 III & Marcu, 2004). \n\nOur summarization approach in this paper is to assess the centrality of each sentence in a cluster and extract the most important ones to include in the summary. We investigate different ways of defining the lexical centrality principle in multi-document summarization, which measures centrality in terms of lexical properties of the sentences. \n\nIn Section 2, we present centroid-based summarization, a well-known method for judging sentence centrality. Then we introduce three new measures for centrality, Degree, LexRank with threshold, and continuous LexRank, inspired from the \"prestige\" concept in social networks. We propose a graph representation of a document cluster, where vertices represent the sentences and edges are defined in terms of the similarity relation between pairs of sentences. This representation enables us to make use of several centrality heuristics defined on graphs. We compare our new methods with centroid-based summarization using a featurebased generic summarization toolkit, MEAD, and show that our new features outperform Centroid in most of the cases. Test data for our experiments are taken from 2003 and 2004 summarization evaluations of Document Understanding Conferences (DUC) to compare our system with other state-of-the-art summarization systems and human performance as well.",
            "score": 0.585025165242841,
            "section_title": "Introduction",
            "char_start_offset": 3867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1563
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 190,
                    "matchedPaperCorpusId": "5775833"
                },
                {
                    "start": 190,
                    "end": 200,
                    "matchedPaperCorpusId": "2010014"
                },
                {
                    "start": 200,
                    "end": 214,
                    "matchedPaperCorpusId": "7353825"
                },
                {
                    "start": 214,
                    "end": 238,
                    "matchedPaperCorpusId": "6773914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87255859375
        },
        {
            "corpus_id": "239768911",
            "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
            "text": "In this section, we introduce our graph-based multidocument encoder. It takes a multi-document set as input and represents all sentences by graph structure. It has three main components: (1) Hierarchical Transformer which processes each document independently and outputs the sentence representations.\n\n(2) Graph encoding layer which updates sentence representations by modeling the graph structure of documents. (3) Graph pooling layer which helps to generate an overall representation of source documents. Figure 2 illustrates the overall architecture of SgSum.\n\nHierarchical Transformer Most previous works (Cao et al., 2017;Jin et al., 2020;Wang et al., 2017) did not consider the multi-document structure. They simply concatenate all documents together and treat the MDS as a special SDS with longer input.  preprocess the multi-document input by truncating lead sentences averagely from each document, then concatenating them together as the MDS input. These preprocessing methods are simple ways to help the model encode multi-document inputs. But they do not make full use of the source document structures. Lead sentences extracted from each document might be similar with each other and result in redundant and incoherent problems. In this paper, we encode source documents by a Hierarchical Transformer, which consists of several sharedweight single Transformers (Vaswani et al., 2017) that process each document independently. Each Transformer takes a tokenized document as input and outputs its sentence representations. This architecture enables our model to process much longer input. Graph Encoding To effectively capture the relations between sentences in source documents, we incorporate explicit graph representations of documents into the neural encoding process via a graph-informed attention mechanism similar to Li et al. (2020). Each sentence can collect information from other related sentences to capture global information from the whole input. The graphinformed attention mechanism extends the vanilla self-attention mechanism to consider the pairwise relations in explicit graph representations as:\n\nwhere e ij denotes the origin self-attention weights between sentences S i and S j , \u03b1 ij denotes the adjusted weights by graph structure. The key point of the graph-based self-attention is the additional pairwise relation bias R ij , which is computed as a Gaussian bias of the weights of graph representation matrix G:\n\nwhere \u03c3 denotes the",
            "score": 0.5849366355946768,
            "section_title": "Graph-based Multi-document Encoder",
            "char_start_offset": 7814,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 610,
                    "end": 628,
                    "matchedPaperCorpusId": "14651945"
                },
                {
                    "start": 628,
                    "end": 645,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 645,
                    "end": 663,
                    "matchedPaperCorpusId": "11296298"
                },
                {
                    "start": 1374,
                    "end": 1396,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1835,
                    "end": 1851,
                    "matchedPaperCorpusId": "218718706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81787109375
        },
        {
            "corpus_id": "55819965",
            "title": "An Enhanced Approach for Using Data Visualization for Sentiment Analysis and Auto Summarization Data",
            "text": "This works great with controlled vocabulary seen in e-commerce website such as amazon.com. Many studies apply graph based ranking for single or multi document summarization and they select the top-K sentences as the summaries of the input document(s). Example includes TextRank, LexRank. Similar to Google's PageRank algorithm or Kleinberg's HITS algorithm, these methods first build a graph based on the similarity relationships among the sentences in a document. MEAD and LexRank methods have shown good results for single or multi documents summarization giving some pages of concrete articles. The position of the sentences and the similarity of the sentences to the title of the resource are among features that are used. Three default features that come with the MEAD distribution are Centroid, Position, and Length. A centroid is a set of words that are statistically important to a group of documents While extractive summarization is mainly concerned with what the summary content should be, usually relying solely on extraction of sentences, abstractive summarization puts strong emphasis on the form, aiming to produce a grammatical summary, which usually requires advanced language generation techniques. In a paradigm more tuned to information retrieval (IR), one can also consider topic-driven summarization, that assumes that the summary content depends on the preference of the user and can be accessed via a query, making the final summary focused on a particular topic (Das, and Martins,2007). (Radev et al., 2002), extraction is the procedure of identifying important sections of the text and producing them verbatim; abstraction aims to produce important material in a new way; fusion combines extracted parts coherently; and compression aims to throw out unimportant sections of the text The semantic -pragmatic gap (Bach, 2004) is one of the main issues in computational linguistic research and query retrieval. The issue consists of being able to create an algorithm or a system that is able to understand the message that a certain phrase or text is trying to communicate. One of the ways to test if this is achieved by a given algorithm is to propose a query system and analyze the results in relation with the original query and decide if they are valid.",
            "score": 0.5838624034520192,
            "section_title": "Related Work",
            "char_start_offset": 11929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "221340611",
            "title": "MultiGBS: A multi-layer graph approach to biomedical summarization",
            "text": "LexRank is a well-known multi-document summarization system.It creates a weighted graph based on the predefined threshold and finds the essential sentences using a random walk on the graph [33].\n\nIn this paper, we focus on biomedical summarization methods.There are a variety of acronyms, abbreviations, synonyms, or hypernyms in the biomedical domain, which the general text summarization methods do not take into consideration.Some domain-independent summarization systems used frequent itemset mining [36]- [38].Other methods used UMLS as a domain of knowledge resources and extract the concepts and vocabularies [39]- [43].BioChain [44] is a typical single document summarization that uses UMLS and creates concept chains that were ranked based on concept frequency.Another method created a simple graph from the input text based on UMLS concepts [45], which measured the similarity between nodes based on \"is-a\" relationship and clustered the graph based on Genetic Graph-based Clustering (GGC).Nasr et al.\n\n[43] created a simple graph and used n-gram based on frequent set mining to create edges.Most existing graph-based methods of biomedical text summarization cannot handle more than one relationship between the text elements simultaneously, thus providing limited accuracy.\n\nThe proposed method is a novel biomedical summarization method that uses multi-layer graphs instead of simple graphs, making it possible to handle several types of relationships between the sentences.To this aim, MultiGBS employs different tools such as MetaMap, OGER, and SemRep to extract the concepts and relations from the biomedical domain, which are introduced in Section 4.",
            "score": 0.5819370355565865,
            "section_title": "Related Work",
            "char_start_offset": 7639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 60,
                    "end": 194
                },
                {
                    "start": 196,
                    "end": 256
                },
                {
                    "start": 256,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 627
                },
                {
                    "start": 627,
                    "end": 770
                },
                {
                    "start": 770,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1011
                },
                {
                    "start": 1013,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1284
                },
                {
                    "start": 1286,
                    "end": 1486
                },
                {
                    "start": 1486,
                    "end": 1666
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 193,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "3780782"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "266379486"
                },
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "49217427"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "14815610"
                },
                {
                    "start": 851,
                    "end": 855,
                    "matchedPaperCorpusId": "14665965"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "260760995",
            "title": "TCMBank: bridges between the largest herbal medicines, chemical ingredients, target proteins, and associated diseases with intelligence text mining",
            "text": "For the automatic summary extraction task, the proposed biased LexRank based on the feature fusion model is divided into two stages: (1) we construct a multi-layer perceptron (MLP) classi\ue103er to determine whether a sentence is selected as a summary and use this classi\ue103er to compute prior probability score of all sentences that are chosen as summaries. (2) To construct the graph, each sentence in the text is represented as a node, and the prior score is taken as the initial score of the node. We score each node using biased LexRank and select high-scoring sentences to generate summaries. The details of the biased LexRank based on the feature fusion framework are shown in Fig. 3. Speci\ue103cally, in the \ue103rst phase, we approach the summarization generation task as a binary classi\ue103cation task. Sentences selected as summaries in the text in the public dataset are considered positive samples while the remaining sentences are regarded as negative samples. Then we extracted 6 features in the text and used MLP as a classi\ue103er. The 6 features are TF-ISF, named entity, numeric attributes, parts of speech (POS), position and length of sentences. Finally, the multilayer perceptron calculates the prior score of sentences being chosen as summaries in new text, and this score is used as the initial weight of the node in the next stage of the graph. \n\nIn the second stage, \ue103rst we consider sentences as nodes, the correlation between sentences as edges, and the whole article is represented as a graph. The initial scores of the nodes are the prior scores outputted by the classi\ue103er in the previous stage. Cosine similarity is commonly employed to measure the relationship between nodes, as shown in eqn (1). \n\nwhere \u1e7dsi and \u1e7dsj represent the vectors of two sentences, respectively, and j j represents the module of the vector. Then, the weight of each node is calculated by Markov random walk, as shown in eqn (2).",
            "score": 0.5819058362757761,
            "section_title": "Biased LexRank based on feature fusion for automatic summarization extraction",
            "char_start_offset": 15977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1707
                },
                {
                    "start": 1710,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1914
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2391357421875
        },
        {
            "corpus_id": "225075639",
            "title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
            "text": "In addition to the lead baseline and extractive oracle, we also include two commonly used unsupervised extractive summarization models, LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004), as baselines. \n\nFor supervised abstractive models, we test state-of-the-art multi-document summarization models HiMAP (Fabbri et al., 2019) and Both deal with multi-documents using a fusion mechanism, which performs the transformation of the documents in the vector space. \n\nHiMAP adapts a pointer-generator model (See et al., 2017) with maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998;Lebanoff et al., 2018) to compute weights over multi-document inputs. \n\nHier-Summ (Liu and Lapata, 2019a) uses a passage ranker that selects the most important document as the input to the hierarchical transformer-based generation model. \n\nIn addition, we apply existing state-ofthe-art single-document summarization models, including Pointer-Generator (See et al., 2017), BART (Lewis et al., 2019) and BertABS (Liu and Lapata, 2019b), for the task of multidocument summarization by simply concatenating the input references. Pointer-Generator incorporates attention over source texts as a copy mechanism to aid the generation. BART is a sequence-to-sequence model with an encoder that is pre-trained with the denosing autoencoder objective. BertABS uses a pretrained BERT (Devlin et al., 2019) as the encoder and trains a randomly initialized transformer decoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBert) pretrained on scientific articles (Beltagy et al., 2019).",
            "score": 0.5793429511990849,
            "section_title": "Models",
            "char_start_offset": 9270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 848
                },
                {
                    "start": 851,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1631
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 167,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 181,
                    "end": 207,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 523,
                    "end": 541,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 580,
                    "end": 611,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 611,
                    "end": 633,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 693,
                    "end": 715,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 964,
                    "end": 982,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1022,
                    "end": 1045,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1384,
                    "end": 1405,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64208984375
        },
        {
            "corpus_id": "269157041",
            "title": "Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization",
            "text": "To validate the effectiveness of our proposed DIR framework, we choose the following three types of baselines for comparison.\n\n1) Extractive methods: TextRank [22] and LexRank [8]: are two graph-based unspervised extractive models.HeterSumGraph [33]: a graph-based model based on heterogeneous graph, which incorporates semantic nodes of various levels of granularity.\n\n2) Abstractive methods: Pointer-Generator [29]: a seq2seq abstractive model based on Recurrent Neural Networks (RNN).MGSum [10]: a multi-document summarization model that utilizes a multigranularity interaction network.GraphSum [14]: a graph-based model that utilizes graphs to enhance both the encoding and decoding processes.BertABS [17]: an abstractive model that utilizes BERT [7] as the encoder and uses a randomly-initialized decoder.BART [13]: a transformer-based pretrained text generation model.PRIMERA [38]: a pretrained encoder-decoder model designed for multi-document summarization.TAG [5]: an abstractive model for MDSS, which leverages keyphrases graph and contrastive learning.UR 3 WG [31]: an MDSS model that uses LLM to retrieve query and generate summary.KGSum [36]: a Transformer-based MDSS model that utilizes knowledge graph for encoding and decoding.EDITSum [34]: the current State-of-the-art model that leverages sentence-level planning for MDSS.TransS2S [36]: a Transformerbased model with randomly-initialized encoder and decoder.\n\nTable 1: The performances of different models on MDSS across three datasets.The number of selected summary candidates  = 3. \"Oracle\" and \"Pred\" mean we take the oracle ranking and the predicted ranking (obtained by \u00a74.1) of candidates as input when testing, respectively.\"-\" means the result is inaccessible.\" \u2020\" and \" \u2020 \u2020\" indicate statistically significantly better than the corresponding base model with Paired t-test  < 0.01 and  < 0.05.",
            "score": 0.5781404549907294,
            "section_title": "Baselines.",
            "char_start_offset": 24225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 127,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 368
                },
                {
                    "start": 370,
                    "end": 487
                },
                {
                    "start": 487,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 810
                },
                {
                    "start": 810,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 965
                },
                {
                    "start": 965,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1243
                },
                {
                    "start": 1243,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1426
                },
                {
                    "start": 1428,
                    "end": 1504
                },
                {
                    "start": 1504,
                    "end": 1699
                },
                {
                    "start": 1699,
                    "end": 1736
                },
                {
                    "start": 1736,
                    "end": 1869
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 493,
                    "end": 497,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 969,
                    "end": 972,
                    "matchedPaperCorpusId": "249097545"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "266176619"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "252185277"
                },
                {
                    "start": 1251,
                    "end": 1255,
                    "matchedPaperCorpusId": "258966106"
                },
                {
                    "start": 1349,
                    "end": 1353,
                    "matchedPaperCorpusId": "252185277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "218718706",
            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
            "text": "Most previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been 1 Codes and results are in: https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2020-GraphSum further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences.",
            "score": 0.5781379954910373,
            "section_title": "Graph-based MDS",
            "char_start_offset": 3840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1448
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 315,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 413,
                    "end": 438,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 520,
                    "end": 530,
                    "matchedPaperCorpusId": "17446655"
                },
                {
                    "start": 945,
                    "end": 965,
                    "matchedPaperCorpusId": "58509157"
                },
                {
                    "start": 965,
                    "end": 982,
                    "matchedPaperCorpusId": "14383376"
                },
                {
                    "start": 999,
                    "end": 1021,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1282,
                    "end": 1299,
                    "matchedPaperCorpusId": "199466313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "273346012",
            "title": "A Mixed-Language Multi-Document News Summarization Dataset and a Graphs-Based Extract-Generate Model",
            "text": "To benchmark the MLMD-news dataset, in addition to our proposed graph-based extractivegenerative method, we also used the following baselines, which can be categorized into Extractthen-translate, Translate-then-extract, Abstractive models, LLM, and Extract-then-abstract. \n\nExtract-then-translate: First, summaries are extracted from the source document cluster using classic extractive models such as Centroid (Radev et al., 2004), LexRank (Erkan and Radev, 2004), LexRank (Erkan and Radev, 2004), MMR (Carbonell andGoldstein, 1998), andTextRank (Mihalcea andTarau, 2004), and then translated into the target language. \n\nTranslate-then-extract: First, the documents in the source document cluster are translated into the target language, and then summaries are extracted using classic extractive models such as Centroid, LexRank, MMR, and TextRank. \n\nAbstractive models: Use mT5 (Xue, 2020) and mBART (Tang et al., 2020), which have multilanguage understanding and generation capabilities, to directly generate summaries from the source document cluster. If the input exceeds the model's capacity, the excess parts will be truncated. \n\nLLM: Use models such as GPT-3.5-turbo-16k1 , GPT-4.0-turbo-32k 1 , Gemini-1.5-pro2 , and Claude-2.13 , which have multi-language and long input capabilities, to directly generate summaries from the source document cluster. \n\nExtract-then-abstract: First, use classic extractive models such as Centroid and TextRank to extract summaries from the source document cluster, and then generate the target summary using generative models like mT5 and mBART.",
            "score": 0.5775132855225649,
            "section_title": "Baselines",
            "char_start_offset": 18200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1587
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 464,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 474,
                    "end": 497,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 503,
                    "end": 517,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 517,
                    "end": 538,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 560,
                    "end": 572,
                    "matchedPaperCorpusId": "577937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "254408844",
            "title": "Scientific Paper Extractive Summarization Enhanced by Citation Graphs",
            "text": "Figure 1 demonstrates this intuition. A good paper abstract of the query paper is more rel-evant to the directly cited paper 1, 2, and indirectly cited paper 3 by sharing domain-specific terms, and not relevant to paper 4 and 5 that are far from it in the citation graph. \n\nBased on this observation, we first propose a Multi-granularity Unsupervised Summarization (MUS) as a light solution to the task without the requirement of rich supervision information. MUS first finetunes a pre-trained encoder model on the citation graph to obtain better sentence and document representations by solving a link prediction task. Then, MUS transforms the citation graph into a multi-granularity graph. Sentences are then extracted considering the multi-granularity positionaware centrality. Experimental results demonstrate that citation graph information can enhance the summarization even by this simple unsupervised framework. \n\nMotivated by this, we further propose a Graphbased Supervised Summarization model (GSS) to achieve more accurate results for the scenario where large-scale labeled data is feasible. Firstly, a graph neural network encoder based on a pretrained language model is employed to obtain sentence representations for the target document and document representations for related papers in the citation graph. Secondly, we propose a gated sentence encoder that polishes the sentence representations based on their relatedness to the document gist. Then, a graph information fusion module is utilized to incorporate the information from reference papers to the polished sentence representations. Finally, a multi-task framework is applied to the model, which jointly assigns selection weights to extract abstracts and predicts whether there exists an edge between two nodes. Under the supervision setting, the graph information gives stronger guidance in two ways. In one way, we employ the link prediction task on the graph to obtain better document representation, and in the other way, the document representation is used to polish sentence representations. Results show that our model significantly surpasses the prior state-of-the-art model by on public benchmark dataset. \n\nOur contributions can be summarized as follows: \n\n\u2022 Our work demonstrates the effectiveness of citation graph modeling in scientific paper extractive summarization.",
            "score": 0.5769365147927847,
            "section_title": "Introduction",
            "char_start_offset": 2145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2189
                },
                {
                    "start": 2192,
                    "end": 2239
                },
                {
                    "start": 2242,
                    "end": 2356
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "269762702",
            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
            "text": "Along with the prosperity of knowledge-aware research in the natural language processing field, more and more document summarization models attempted to incorporate knowledge graphs to enhance the quality of generated summaries.SDS with KG Gunel et al. [6]  MDS with KG Zhou et al. [26] presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. [18] presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News [5] and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model [13] with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. [18] utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
            "score": 0.5761256505528667,
            "section_title": "STATE OF THE ART",
            "char_start_offset": 6022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 879
                },
                {
                    "start": 879,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1466
                },
                {
                    "start": 1466,
                    "end": 1729
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "204735695"
                },
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "235097309"
                },
                {
                    "start": 1112,
                    "end": 1115,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1210,
                    "end": 1214,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1525,
                    "end": 1529,
                    "matchedPaperCorpusId": "235097309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "236477806",
            "title": "A Graph-based Approach at Passage Level to Investigate the Cohesiveness of Documents",
            "text": "Previously, graphs have been used to represent text for ad-hoc information retrieval tasks (Blanco and Lioma, 2012;Thammasut and Sornil, 2006). The formulation of the weighting schemes to rank documents and summarize text by using graphs has also been studied in recent years (Blanco and Lioma, 2012;Rousseau and Vazirgiannis, 2013;Erkan and Radev, 2004;Tan et al., 2017). Graph-based approaches like PageRank (Page et al., 1999) and HIT S (Kleinberg, 1999) have been widely employed for ranking the top web pages, analysis of social networks, as well as for ad-hoc document retrieval (Kurland and Lee, 2010;Kurland and Lee, 2006) purposes. For passage retrieval, Li et al. (Li and Chen, 2010) proposed a graph-based ranking model that measures the relationship between passages and uses it to re-rank the passage results in Question Answering (QA) task (Dang et al., 2007). They constructed the graph after the initial standard retrieval against a query, and then re-ranked the returned passages based on a similarity of different passages/vertices. Furthermore, Otterbacher et al. (Otterbacher et al., 2009) used a variation of a graph-based ranking model called LexRank (Erkan and Radev, 2004) to rank a set of sentences for the generation of a document summary. They applied this approach in the context of passage retrieval for the QA task. They calculated the tf-idf score of all the sentences in the documents and used it as an edge score to build a graph. Similarly, Dkaki (Dkaki et al., 2007) presented a model based on graph comparison for passage retrieval task. Their graph model considered the sentence dependencies by following the Hyperlink-Induced Topic Search (HITS) algorithm (Kleinberg, 1999) or PageRank (Blondel et al., 2004). However, they did not consider the explicit links between the documents by using hyperlinks or citations, etc. Instead of using the implicit inter-document relationship based on the cosine similarity, they have utilized the approach the identify the linkage between units/sentences based on related terms that are shared among themselves.",
            "score": 0.5755456004763737,
            "section_title": "Graph based Passage Retrieval",
            "char_start_offset": 7859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 142,
                    "matchedPaperCorpusId": "17447007"
                },
                {
                    "start": 300,
                    "end": 332,
                    "matchedPaperCorpusId": "207206731"
                },
                {
                    "start": 332,
                    "end": 354,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 354,
                    "end": 371,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 440,
                    "end": 457,
                    "matchedPaperCorpusId": "216000619"
                },
                {
                    "start": 585,
                    "end": 608,
                    "matchedPaperCorpusId": "53245989"
                },
                {
                    "start": 608,
                    "end": 630,
                    "matchedPaperCorpusId": "2741865"
                },
                {
                    "start": 674,
                    "end": 693,
                    "matchedPaperCorpusId": "16744963"
                },
                {
                    "start": 854,
                    "end": 873,
                    "matchedPaperCorpusId": "262787428"
                },
                {
                    "start": 1083,
                    "end": 1108,
                    "matchedPaperCorpusId": "17636912"
                },
                {
                    "start": 1173,
                    "end": 1196,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1481,
                    "end": 1501,
                    "matchedPaperCorpusId": "7580498"
                },
                {
                    "start": 1694,
                    "end": 1711,
                    "matchedPaperCorpusId": "216000619"
                },
                {
                    "start": 1724,
                    "end": 1746,
                    "matchedPaperCorpusId": "2494252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "202780836",
            "title": "Subtopic-driven Multi-Document Summarization",
            "text": "Extractive methods select sentences from documents to form a summary. A typical framework is based on a graph, where sentences are vertices and similarities between sentences are edge weights, e.g., TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004). They apply a random walk to explore the relationships among sentences and then produce a sentence ranking. MEAD (Radev et al., 2000) is a centroid-based method, which ranks sentences based on a set of features, including centroid value, positional value, first sentence overlap and redundancy. However, the graph-based models do not perform well when the sentence graph is not well connected. Wan and Yang (2008) claim that the document set is usually composed of a few themes, which are represented by sets of sentences. They apply conditional Markov Random Walk and HITS on clusters, separately. Alternatively, Haghighi and Vanderwende (2009) adopt a hierarchical Latent Dirichlet Allocation based model to discover the multiple themes within a document set. Similarly, Gong et al. (2010) use the theme structure to define the representation for each sentence. However, their solutions only consider statistical knowledge. Semantic and contextual information among sentences are neglected. \n\nFollowing the idea of themes, Banerjee et al. (2015) suggest that the sentences in the most important document of the set are relevant to the sentences in the other documents. Hence, they cluster sentences based on those in the most important document. However, there may not always exist a most important document in the set. Further, the relationships among documents are not necessarily to be conclusive. In fact, there are many kinds of relationships among documents (e.g., similar, complementary, or evolutionary). Liu et al. (2015) adopt the idea of reconstruction. They apply a two-level sparse representation model and reconstruct the document by extracted sentences with constraints. Similarly, Ma et al. (2016) try to minimize the reconstruction error between selected sentences and the document set with a neural model. Cao et al. (2017) make use of multi-task learning by incorporating classification task with summarization to train better sentence representations for reconstruction.",
            "score": 0.5755097119047391,
            "section_title": "Extractive Methods",
            "char_start_offset": 4406,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 234,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 384,
                    "end": 404,
                    "matchedPaperCorpusId": "1320"
                },
                {
                    "start": 665,
                    "end": 684,
                    "matchedPaperCorpusId": "9849366"
                },
                {
                    "start": 885,
                    "end": 916,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 1044,
                    "end": 1062,
                    "matchedPaperCorpusId": "14644011"
                },
                {
                    "start": 1296,
                    "end": 1318,
                    "matchedPaperCorpusId": "15795297"
                },
                {
                    "start": 1786,
                    "end": 1803,
                    "matchedPaperCorpusId": "16517338"
                },
                {
                    "start": 1970,
                    "end": 1986,
                    "matchedPaperCorpusId": "5845797"
                },
                {
                    "start": 2097,
                    "end": 2114,
                    "matchedPaperCorpusId": "14651945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.841796875
        },
        {
            "corpus_id": "266599825",
            "title": "Summarization of Investment Reports Using Pre-trained Model",
            "text": "Related research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain [6]. Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization [7]. Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset [8]. Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing [10]. They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries [11]. Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries [12]. Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words [13]. Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature [14]. \n\nAs related work of extractive summarization, there is research by Cui et al. [15]. They proposed extractive summarization that can summarize long-form documents without content loss. Xu et al. proposed the neural network framework for extractive and compressive summarization [16]. \n\nAs related work of abstractive summarization, there is research by Nallapati et al. [17].",
            "score": 0.5752808470056238,
            "section_title": "VII. RELATED WORKS",
            "char_start_offset": 9491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 841,
                    "end": 845,
                    "matchedPaperCorpusId": "52011473"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1383,
                    "end": 1387,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1521,
                    "end": 1525,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1697,
                    "end": 1701,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "235097475"
                },
                {
                    "start": 1981,
                    "end": 1985,
                    "matchedPaperCorpusId": "59599804"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7802734375
        },
        {
            "corpus_id": "271791043",
            "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
            "text": "This research evaluation strategy is focused on rigorously testing the proposed model's effectiveness in extractive summarization against contemporary state-of-the-art models. The performance assessment utilizes the CNN/DM and Multi-News datasets, which are standards in the field for SDS and MDS tasks, respectively. This deliberate choice confirms a broad and challenging test environment, reflecting real-world applications and the complexity of integrating multiple document contexts. \n\nThrough systematic comparisons, proposed model is tested against well-known benchmarks and recent advances in neural graph-based models. These assessments are not just limited to performance metrics like ROUGE scores but also extend to an examination of how well each model assimilates and processes linguistic and thematic information across different document types. This research examines the performance in relation to various node configurations and graph structures used in competing models, providing a comprehensive view of the proposed model's abilities. Furthermore, the assessment comprises a detailed review of the implementation settings, such as vocabulary size, tokenization standards, and hyperparameters like learning rates and dropout settings. These particulars underscore the tailored approach of proposed model in handling the refinements of extractive summarization, highlighting its innovative integration of advanced neural network techniques and its adaptability to both SDS and MDS summarization challenges. This laborious evaluation framework ensures conclusions are well-founded and validate clear advancements over existing methodologies.",
            "score": 0.5751999148217423,
            "section_title": "Performance Evaluation",
            "char_start_offset": 29267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1658
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7294921875
        },
        {
            "corpus_id": "265607988",
            "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
            "text": "Extractive text summarization models classify sentences in a document using labels that indicate whether or not a sentence ought to be included in the summary. Originally, these models were designed based on the encoder-decoder architecture using RNNs (Nallapati et al., 2017). Since transformer and BERT-based models provide a more enriched sentence encoding, they have become the foundation for the majority of extractive summarizer models in recent years. Liu and Lapata (2019) fine-tuned BERT with stacked layers of transformer to obtain the sentence vectors and then used a sigmoid classifier for identifying the sentences that would be included in the summary. Zhang et al. (2019) fine-tuned an hierarchical transformer (HI-BERT) for the extractive summarization task. Another prominent approach for extractive summarization is using graph representations which can preserve sentence-level correlations. Later, the graph convolutional network (GCN) (Welling and Kipf, 2016) has been espoused for building different inter-sentence correlation graphs (Yasunaga et al., 2017) for this task. Wang et al. (2020) built an heterogeneous graph neural network for extractive summarization (HeterSumGraph) which takes into account additional semantic units at the word level for building the sentence-level correlation graph.",
            "score": 0.574940975787581,
            "section_title": "Extractive Text Summarization",
            "char_start_offset": 7599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1321
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 276,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 459,
                    "end": 480,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 667,
                    "end": 686,
                    "matchedPaperCorpusId": "155100086"
                },
                {
                    "start": 1055,
                    "end": 1078,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1094,
                    "end": 1112,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7021484375
        },
        {
            "corpus_id": "236460044",
            "title": "Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation",
            "text": "We discuss the related work on related work generation and multi-document summarization. \n\nRelated Work Generation. Most of the previous related work section generation methods are extractive. For example, Hoang and Kan (2010) take in a set of keywords arranged in a hierarchical fashion to drive the creation of an extractive related work. Later, (Hu and Wan, 2014) first exploits a Probabilistic Latent Semantic Analysis (PLSA) model to split the sentence set of multiple reference papers into different topic-biased parts, and then applies regression models to learn the importance of the sentences. Finally, it employs an optimization framework to generate the related work section. Chen and Zhuge (2019) propose to first construct a minimum Steiner tree of the keywords. Then the summary is generated by extracting the sentences from the papers that cite the reference papers of the paper being written to cover the Steiner tree. \n\nHowever, abstractive approaches on related work generation have met with limited success. Apart from the lack of sufficient training data, neural models also face the challenge of identifying the logic relationship between multiple input documents. \n\nMulti-document Summarization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013;Parveen and Strube, 2014;Ma et al., 2016;Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents to guide the summary generation process.",
            "score": 0.5746459295646589,
            "section_title": "Related Work",
            "char_start_offset": 4553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 91,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 934
                },
                {
                    "start": 937,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 366,
                    "matchedPaperCorpusId": "1948673"
                },
                {
                    "start": 687,
                    "end": 708,
                    "matchedPaperCorpusId": "57764609"
                },
                {
                    "start": 1355,
                    "end": 1379,
                    "matchedPaperCorpusId": "8294822"
                },
                {
                    "start": 1449,
                    "end": 1475,
                    "matchedPaperCorpusId": "337730"
                },
                {
                    "start": 1475,
                    "end": 1500,
                    "matchedPaperCorpusId": "8437725"
                },
                {
                    "start": 1500,
                    "end": 1516,
                    "matchedPaperCorpusId": "5845797"
                },
                {
                    "start": 1549,
                    "end": 1567,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1805,
                    "end": 1821,
                    "matchedPaperCorpusId": "225075639"
                },
                {
                    "start": 1915,
                    "end": 1932,
                    "matchedPaperCorpusId": "220045815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "220936463",
            "title": "Experiments in Extractive Summarization: Integer Linear Programming, Term/Sentence Scoring, and Title-driven Models",
            "text": "Summarization of articles like news, research papers, blog posts etc. has always been a difficult task for both humans as well as machines. Automated summarizers have come a long way since the 1950s with current systems using supervised learning methods like the use of submodular optimization models (Sipos et al., 2012a) for determining maximum information coverage with minimum redundancy, theme-based summarization using rank-based clustering (Yang et al., 2014). Unsupervised approaches for text document summarization used in previous research include greedy selection algorithms like Maximal Marginal Relevance (Carbonell and Goldstein, 1998) as well as dynamic programming algorithms (McDonald, 2007), which view document summarization as a 'knapsack problem.' specifically for the domain of legal texts (Polsley et al., 2016). PKUSUMSUM is a general purpose summarizer like DocSumm; however, it is for the Java programming language (Zhang et al., 2016). \n\nRecently, SummIT (Feigenblat et al., 2017) is work done by researchers to apply summarization as a direct tool for human users. Humans must input a query to search for documents that they are seeking. However, SummIT's goal is more in application of summarization than improving summarization itself. \n\nThere are also plenty of online tools that provide automatic summarization. Doing a simple Google search shows the prevalence of such tools. 12 These tools fail to provide an avenue for further research, e.g., there is no method for bulk/batch summarization. The tools also do not provide a way to use them through scripts or program calls, which further hinders efficient evaluation. NewsSumm lends itself naturally to bulk and unattended testing. Graph-based Models. A stochastic graph-based approach LexRank is proposed in Erkan and Radev (2004) for multidocument summarization, which builds the graph based on similarity of sentences, which are the nodes. The summary is generated by using the concept of sentence similarity.",
            "score": 0.5744918660992224,
            "section_title": "Related Work",
            "char_start_offset": 2474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1265
                },
                {
                    "start": 1268,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 1997
                }
            ],
            "ref_mentions": [
                {
                    "start": 301,
                    "end": 322,
                    "matchedPaperCorpusId": "12718597"
                },
                {
                    "start": 447,
                    "end": 466,
                    "matchedPaperCorpusId": "5792920"
                },
                {
                    "start": 618,
                    "end": 649,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 692,
                    "end": 708,
                    "matchedPaperCorpusId": "2798452"
                },
                {
                    "start": 812,
                    "end": 834,
                    "matchedPaperCorpusId": "17614070"
                },
                {
                    "start": 941,
                    "end": 961,
                    "matchedPaperCorpusId": "8069082"
                },
                {
                    "start": 982,
                    "end": 1007,
                    "matchedPaperCorpusId": "20698889"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.728515625
        },
        {
            "corpus_id": "232307817",
            "title": "Nutri-bullets: Summarizing Health Studies by Composing Segments",
            "text": "Multi-document Summarization Approaches in neural sequence-to-sequence learning (Rush, Chopra, and Weston 2015;Cheng and Lapata 2016;See, Liu, and Manning 2017) for document summarization have shown promise and have been adapted successfully for multi-document summarization (Zhang, Tan, and Wan 2018;Lebanoff, Song, and Liu 2018;Baumel, Eyal, and Elhadad 2018;Amplayo and Lapata 2019;Fabbri et al. 2019). Trained on large amounts of data, these methods have improved upon traditional extractive (Carbonell and Goldstein 1998;Radev and McKeown 1998;Haghighi and Vanderwende 2009) and abstractive approaches (Barzilay, McKeown, and Elhadad 1999;McKeown and Radev 1995;Ganesan, Zhai, and Han 2010). De-spite producing fluent text, these techniques also tend to generate false information which is not faithful to the original inputs (Puduppully, Dong, and Lapata 2019;Kry\u015bci\u0144ski et al. 2019). Side-information, such as citations in scientific domains (Qazvinian and Radev 2008;Qazvinian et al. 2013) or semantic representations (Liu et al. 2015), can be used to improve this (Sharma et al. 2019;Wenbo et al. 2019;Puduppully, Dong, and Lapata 2019;Koncel-Kedziorski et al. 2019a). However, such methods struggle in low resource scenarios. In this work, we are interested in producing faithful and fluent text in a technical domain where few parallel examples are available.\n\nText Fusion Traditionally, sentence fusion approaches (Barzilay and McKeown 2005) aid the concatenation of different text fragments for summarization. Recent language modeling approaches like Devlin et al. (2018); Stern et al. (2019) can also be extended for completion and fusion of partial text. These models have more flexibility than those trained on text fusion datasets (Narayan et al. 2017;Geva et al. 2019) that can combine two fragments only. In this work, we modify the Blank Language Model (Shen et al. 2020) to combine fragments coming from different source documents",
            "score": 0.5744522880147217,
            "section_title": "Related Work",
            "char_start_offset": 4677,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 111,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 111,
                    "end": 133,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 133,
                    "end": 160,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 275,
                    "end": 301,
                    "matchedPaperCorpusId": "53223447"
                },
                {
                    "start": 496,
                    "end": 526,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 526,
                    "end": 549,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 549,
                    "end": 579,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 607,
                    "end": 644,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 644,
                    "end": 667,
                    "matchedPaperCorpusId": "2446679"
                },
                {
                    "start": 667,
                    "end": 695,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 975,
                    "end": 997,
                    "matchedPaperCorpusId": "324527"
                },
                {
                    "start": 1026,
                    "end": 1043,
                    "matchedPaperCorpusId": "5001921"
                },
                {
                    "start": 1073,
                    "end": 1093,
                    "matchedPaperCorpusId": "202537639"
                },
                {
                    "start": 1426,
                    "end": 1453,
                    "matchedPaperCorpusId": "16188305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36865234375
        },
        {
            "corpus_id": "256826757",
            "title": "PDSum: Prototype-driven Continuous Summarization of Evolving Multi-document Sets Stream",
            "text": "For the new task EMDS, we prepared strong baselines by adopting a centroid-based model [13,38] with a pretrained language model: DocCent and SentCent with document-and sentence-based centers, respectively, and their incremental versions IncDocCent, and IncSentCent. We also compared three popular unsupervised algorithms for multi-document summarization: the graph-based model Lexrank [9], the state-of-the-art extractive model Summpip [57], and the state-of-the-art abstractive model PRIMERA [47]. We fed each document set in a context to them so that they can only infer the temporally correlated documents to update their set summaries. See Appendix A.2 for details.",
            "score": 0.5742796031425351,
            "section_title": "Compared Algorithms.",
            "char_start_offset": 22425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 669
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 91,
                    "matchedPaperCorpusId": "1564849"
                },
                {
                    "start": 91,
                    "end": 94,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 493,
                    "end": 497,
                    "matchedPaperCorpusId": "247519084"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "232404911",
            "title": "Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization",
            "text": "Extractive document summarization aims to extract relevant sentences from the original documents covering salient information. Remarkable success in single document summarization has been achieved in recent years with deep neural network (Nallapati et al., 2016a;Cheng and Lapata, 2016;See et al., 2017;Nallapati et al., 2016b;Narayan et al., 2018;Liu and Lapata, 2019). The success of these high-capacity supervised models heavily relies on large-scale annotated corpora containing hundreds of thousands of document-summary pairs. However, document summary reference writing and annotation can be both time-consuming and laborintensive, making it extremely hard to obtain highquality large-scale corpora. As a result, practical unsupervised summarization approaches requiring no annotated document-summary training data at all have received renewed attention recently (Chu and Liu, 2019;Zheng and Lapata, 2019). \n\nMost of the existing unsupervised summarization methods are graph-based extractors. They utilize graph structure to model the relationship between sentences and then extract sentences to form a summary (Mihalcea and Tarau, 2004;Erkan and Radev, 2004;Wan and Yang, 2008;Zheng and Lapata, 2019). Specifically, a document (or a cluster of documents) is represented as a graph, with nodes representing sentences and edge weights representing sentence similarity. In order to decide which sentence to include in the summary, two critical graph properties, centrality and centroid, are widely used to score sentences (nodes). Centrality-based approaches (Mihalcea and Tarau, 2004;Zheng and Lapata, 2019) rely on edge connection information of the graph and rank sentences based on node centrality. On the other hand, centroid-based approaches (Radev et al., 2004;Rossiello et al., 2017) rely on node representation information, representing the whole document as a centroid of all sentence vectors and ranking sentences by their cosine similarity to the centroid vector. How to effectively utilize both node representation and edge information of the graph has not been investigated. Moreover, previous approaches are all sentencelevel extractors, which extract sentences one by one based on salience scores.",
            "score": 0.5736808099663054,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 869,
                    "end": 888,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 1117,
                    "end": 1143,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1143,
                    "end": 1165,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1165,
                    "end": 1184,
                    "matchedPaperCorpusId": "9849366"
                },
                {
                    "start": 1563,
                    "end": 1589,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1772,
                    "end": 1795,
                    "matchedPaperCorpusId": "2346086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "247292760",
            "title": "A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization",
            "text": "Multi-document summarization (MDS) aims to consolidate salient points of information across a set of documents into a concise summary. The main requirement for the summary is that it adequately represent the document set, with low redundancy and high coverage across all documents, while at the same time being readable and fluent. Combined with this, is the need to develop techniques that can handle the significant memory complexity required to tackle MDS. Recently, the re-lease of dedicated datasets (Fabbri et al., 2019;Gholipour Ghalandari et al., 2020), and intelligently designed Transformer models (Liu et al., 2018;Liu and Lapata, 2019;Beltagy et al., 2020), have helped drive advancements in multi-document summarization, generally improving the accuracy and fluency of the predicted summaries. However, aspects such as the requirement to cover as much salient information from the input documents as possible, whilst still maintaining low repetition and low redundancy, have certainly been less explored to date (Nayeem et al., 2018;Mao et al., 2020).\n\nWithin the sphere of contemporary neural MDS models, two main lines of investigation can be identified: graph-based approaches (Li et al., 2020;Pasunuru et al., 2021), and concatenation approaches (Liu et al., 2018;Zhang et al., 2020a). The former are approaches that rely on the construction of graphs to capture the inter-and intra-document relations. While powerful, they need to elicit the relations explicitly. The latter instead assume that all the input documents within a document set can be simply concatenated, possibly with document separators and tags, such that the relations can be \"discovered\" by the model. Like ordinary summarization, also MDS comes in two remarkably different styles: extractive, where the generated summaries consist of verbatim sentences from the original input documents (Nallapati et al., 2017), and abstractive, where the model is instead encouraged to generate a paraphrased understanding of the input documents. The intrinsic appeal of abstractive summaries and the advent of sequenceto-sequence models have increasingly shifted the trend toward abstractive summar",
            "score": 0.5736719631781552,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "219616694",
            "title": "Modern Multi-Document Text Summarization Techniques",
            "text": "Representation [76] presented a graph-based approach and their work depends on the most significant works on diagram-based strategies for sentence positioning; TextRank and LexRank. Graph based strategies for sentence ranking have demonstrated to be effective for both single-archive and multi-report outlines. Such methodologies don't include any complex phonetic handling of the content other than recognizing its sentences and words. They likewise have the upside of being completely solo and rely just upon the content to be outlined without the requirement for any preparation information. The document cluster is pre-processed, sentence similarity is quantitatively acquired and the sentences are ranked before being selected to extract the summary. Meanwhile in 2019 some researchers [74] came up with a Dual Pattern improved rendition model which focuses on the queries rather than focusing on the features like the graph-based approach. Their proposed approach employs a model that enhances the patterns and generates renditions that are semantically rich and fittingly selective for the data. They also incorporated a relevance model that identifies the relevance for the query with respect to the sentences based on the patterns. With these renditions based on the patterns, their approach is successful in the amalgamation of multiple metrics for indication into a single integrated model for highly efficient multi-document summarization. \n\nThe tables 1,2,3 and 4 present a more feature specific comparison between the various algorithms.",
            "score": 0.5732402990063934,
            "section_title": "D. Graph Based Vs Dual Pattern Enhanced",
            "char_start_offset": 52722,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1451
                },
                {
                    "start": 1454,
                    "end": 1551
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "59529812"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "49614912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "8862807",
            "title": "Selecting Sentences for Answering Complex Questions",
            "text": "Researchers all over the world working on querybased summarization are trying different directions to see which methods provide the best results. The LexRank method addressed in (Erkan and Radev, 2004) was very successful in generic multi-document summarization. A topic-sensitive LexRank is proposed in (Otterbacher et al., 2005). As in LexRank, the set of sentences in a document cluster is represented as a graph, where nodes are sentences and links between the nodes are induced by a similarity relation between the sentences. Then the system ranked the sentences according to a random walk model defined in terms of both the intersentence similarities and the similarities of the sentences to the topic description or question. \n\nThe summarization methods based on lexical chain first extract the nouns, compound nouns and named entities as candidate words (Li et al., 2007). Then using WordNet, the systems find the semantic similarity between the nouns and compound nouns. After that, lexical chains are built in two steps: 1) Building single document strong chains while disambiguating the senses of the words and, 2) building multi-chain by merging the strongest chains of the single documents into one chain. The systems rank sentences using a formula that involves a) the lexical chain, b) keywords from query and c) named entities. (Harabagiu et al., 2006) introduce a new paradigm for processing complex questions that relies on a combination of (a) question decompositions; (b) factoid QA techniques; and (c) Multi-Document Summarization (MDS) techniques. The question decomposition procedure operates on a Marcov chain, by following a random walk with mixture model on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations. Decomposed questions are then submitted to a state-of-the-art QA system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a MDS system. They show that question decompositions using this method can significantly enhance the relevance and comprehensiveness of summary-length answers to complex questions.",
            "score": 0.5728566376346408,
            "section_title": "Related Work",
            "char_start_offset": 3190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 201,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 304,
                    "end": 330,
                    "matchedPaperCorpusId": "10274362"
                },
                {
                    "start": 862,
                    "end": 879,
                    "matchedPaperCorpusId": "18108824"
                },
                {
                    "start": 1344,
                    "end": 1368,
                    "matchedPaperCorpusId": "12003166"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "266361920",
            "title": "Coreference Graph Guidance for Mind-Map Generation",
            "text": "A mind-map is a diagram with a hierarchical structure, which can disclose the logical structure of a document (Buzan and Buzan 2006). Of the two kinds of mind-map, SSM is similar to extractive text summarization (Zhong et al. 2020), which involves selecting and combining essential sentences or phrases from a given document to create a concise summary. Mind-map, by contrast, aims at not only the general idea of a document but also the relation of succession between paragraphs. \n\nThere are a number of works that use sentence-based graphs to generate text summarization. A previous study LexRank (Erkan and Radev 2004) employs a graph-based approach to compute an adjacency matrix for sentence representation. This method relies on intra-sentence cosine similarity. However, generating a meaningful mind-map from this graph representation becomes challenging, as specific sentence pairs with semantic relations may possess zero lexical similarity. Additionally, several extractive summarization studies have utilized graph techniques. For instance, to enhance the ranking of sentences within a document, several methods have been proposed, including the utilization of bipartite graphs for sentence and entity nodes (Parveen and Strube 2015), and weighted graphs featuring topic nodes (Parveen, Ramsl, and Strube 2015). Recently, Wang et al. (2020) utilize a heterogeneous graph for the purpose of capturing the relations between words and sentences. Liu, Hughes, and Yang (2021) construct sentence graphs based on both the similarities and relative distances in the neighborhood of each sentence. However, these attempts of involving graph knowledge can hardly acquire governing relations between sentences, and thus can not reveal the logical structure of a document. \n\nGraph contrastive learning (GCL) is a self-supervised learning algorithm for graph data. It learns to capture meaningful patterns and relationships within the graph by encouraging similar nodes or edges to be close in the embedding space while pushing dissimilar ones apart. Sun et al. (2022) construct heterogeneous graphs from texts and expand the heterogeneous graph neural network model (HGAT) with simple neighbor contrastive learning. The negative samples are created by corrupting the edges of the graphs.",
            "score": 0.5727672749884785,
            "section_title": "Related Works",
            "char_start_offset": 4446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2215
                },
                {
                    "start": 2216,
                    "end": 2287
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 231,
                    "matchedPaperCorpusId": "215828313"
                },
                {
                    "start": 599,
                    "end": 620,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1288,
                    "end": 1321,
                    "matchedPaperCorpusId": "269225"
                },
                {
                    "start": 1333,
                    "end": 1351,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1454,
                    "end": 1482,
                    "matchedPaperCorpusId": "235792514"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "258461571",
            "title": "DiffuSum: Generation Enhanced Extractive Summarization with Diffusion",
            "text": "Recent advances in deep neural networks have dramatically boosted the progress in extractive summarization systems. Existing extractive summarization systems span an extensive range of approaches. Most works formulate the task as a sequence classification problem and use sequential neural models with different encoders like recurrent neural networks (Cheng and Lapata, 2016;Nallapati et al., 2016) and pre-trained language models (Egonmwan and Chali, 2019;Liu and Lapata, 2019;Zhang et al., 2023). Another group of work formulates extractive summarization as a node classification problem and applies graph neural networks to model inter-sentence dependencies (Xu et al., 2019;Zhang and Zhang, 2020;Wang et al., 2020;Zhang et al., 2022a). These formulations are sentence-level methods that make individual predictions for each sentence. Recently, Zhong et al. (2020) observed that a summary consisting of sentences with the highest scores is not necessarily the best. As a result, summary-level formulation like text matching (Zhong et al., 2020;An et al., 2023) and reinforcement learning (Narayan et al., 2018b;Bae et al., 2019) are proposed. Our proposed framework DiffuSum is also a novel summary-level extractive system with generation augmentation. Instead of sequentially labeling sentences, DiffuSum directly generates the desired summary sentence representations with diffusion models and extracts sentences by representation matching.",
            "score": 0.5723545192417537,
            "section_title": "Extractive Summarization",
            "char_start_offset": 4659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1446
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 458,
                    "matchedPaperCorpusId": "208246040"
                },
                {
                    "start": 679,
                    "end": 701,
                    "matchedPaperCorpusId": "226262336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58935546875
        },
        {
            "corpus_id": "182953152",
            "title": "This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation",
            "text": "To benchmark our method, we use several methods from the summarization field, including some recent state-of-the-art systems, because the email subject line can be viewed as a short summary of the email content. They can be clustered into two groups. \n\n(1) Unsupervised extractive or/and abstractive summarization. LEAD-2 directly uses the first two sentences as the subject line. We choose lead-2 to include both the greeting and the first sentence of main content. TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) are two graph-based ranking models to extract the most salient sentence as the subject line. Shang et al. ( 2018) use a graph-based framework to extract topics and then generate a single abstractive sentence for each topic under a budget constraint. \n\n(2) Neural summarization using encoderdecoder networks with attention mechanisms. (Sutskever et al., 2014;Bahdanau et al., 2015). The Pointer-Generator Network from See et al. \n\n(2017) augments the standard encoder-decoder network by adding the ability to copy words from the source text and using the coverage loss to avoid repetitive generation. Paulus et al. (2018) propose neural intra-attention models with a mixed objec-    (Narayan et al., 2018a). It is unclear how they perform to generate email subject lines of extremely abstractive summarization. We train these models on our dataset.",
            "score": 0.5723460391782225,
            "section_title": "Baselines",
            "char_start_offset": 14791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1386
                }
            ],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 502,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 515,
                    "end": 538,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 873,
                    "end": 897,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 897,
                    "end": 919,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 1221,
                    "end": 1244,
                    "matchedPaperCorpusId": "215768182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54296875
        },
        {
            "corpus_id": "247958026",
            "title": "Automatic Text Summarization Methods: A Comprehensive Review",
            "text": "For the text summarization task, (Wang et al., 2019) extended the traditional definition of the domain from categories to data sources. Then used, a multi-domain summary dataset to see how the distance between different domains affects neural summarization model performance. Traditional applications have a major flaw: they use high-dimensional, sparse data, making it impossible to gather relevant information. Word embedding is a neural network technique that produces a considerably smaller word representation than the classic Bag-of-Words (BOW) method. (Alami et al., 2019) has created a text summarization system based on word embeddings, and it showed that the Word2Vec representation outperforms the classic BOW representation. Another summarization approach using word embeddings was given by (Mohd et al., 2020). This study also used Word2Vec as a distributional semantic model that captures the semantics. \n\nCurrent state-of-art systems produce generic summaries that are unrelated to the preferences and expectations of their users. CTRLsum (He, Kryscinski, et al., 2020), a unique framework for controlled summarizing, is presented to address that limitation. This system permits users to interact with the summary system via textual input in a collection of key phrases or descriptive prompts to influence several features of generated summaries. The majority of recent neural network summarization algorithms are either selection-based extraction or generation-based abstraction. (Xu & Durrett, 2020) introduced a neural model based on joint extraction and syntactic compression for single-document summarization. The model selects phrases from the document, identifies plausible compressions based on constituent parses, and rates those compressions using a neural model to construct the final summary. Four algorithms were proposed by (El-Kassas et al., 2020). The first algorithm uses the input document to create a new text graph model representation. The second and third algorithms look for sentences to include in the candidate summary in the built text graph. The fourth algorithm selects the most important sentences when the resulting candidate summary exceeds a userspecified limit. Automatic text summarization is an arduous effort for under-resourced languages like Hindi, and it is still an unsolved topic.",
            "score": 0.5722987349882526,
            "section_title": "Extractive text summarization",
            "char_start_offset": 36414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2209
                },
                {
                    "start": 2210,
                    "end": 2336
                }
            ],
            "ref_mentions": [
                {
                    "start": 559,
                    "end": 579,
                    "matchedPaperCorpusId": "67892357"
                },
                {
                    "start": 1054,
                    "end": 1084,
                    "matchedPaperCorpusId": "227745074"
                },
                {
                    "start": 1496,
                    "end": 1515,
                    "matchedPaperCorpusId": "59599804"
                },
                {
                    "start": 1853,
                    "end": 1877,
                    "matchedPaperCorpusId": "224973389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.397705078125
        },
        {
            "corpus_id": "264817506",
            "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders",
            "text": "In this paper, we introduce a pre-training process that optimizes summary-worthy representations for extractive summarization. Our approach employs graph pre-training autoencoders to learn intrasentential and inter-sentential features on sentenceword bipartite graphs, resulting in pre-trained embeddings useful for extractive summarization. Our model is easily incorporated into existing unsupervised summarization models and outperforms salient BERT-based and RoBERTa-based summarization methods with predominant ROUGE-1/2/L score gains. Future work involves exploring the potential of our pre-trained sentential representations for other unsupervised extractive summarization tasks and text-mining applications.",
            "score": 0.572013140579936,
            "section_title": "Conclusion",
            "char_start_offset": 22334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 714
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "52205000",
            "title": "Learning to Summarize Radiology Findings",
            "text": "Early Summarization Systems. Early work on summarization systems mainly focused on extractive approaches, where the summaries are generated by scoring and selecting sentences from the input. Luhn (1958) proposed to represent the input by topic words and score each sentence by the amount of topic words it contains. Kupiec et al. (1995) scored sentences with a feature-based statistical classifier. Steinberger and Jezek (2004) applied latent semantic analysis to cluster the topics and then select sentences that cover the most topics. Meanwhile, various graph-based methods, such as the LexRank (Mihalcea and Tarau, 2004) and the TextRank algorithm (Erkan and Radev, 2004), were applied to model sentence dependency by representing sentences as vertices and similarities as edges. Sentences are then scored and selected via modeling of the graph properties. \n\nNeural Summarization Systems. Summarization systems based on neural network models enable abstractive summarization, where new words and phrases are generated to form the summaries. Rush et al. (2015) first applied an attention-based neural encoder and a neural language model decoder to this task. Nallapati et al. (2016) used recurrent neural networks for both the encoder and the decoder. To address the limitation that neural models with a fixed vocabulary cannot handle outof-vocabulary words, a pointer-generator model was proposed which uses an attention mechanism that copies elements directly from the input (Nallapati et al., 2016;Merity et al., 2017;See et al., 2017). See et al. (2017) further proposed a coverage mechanism to address the repetition problem in the generated summaries. Paulus et al. (2018) applied reinforcement learning to summarization and more recently, Chen and Bansal (2018) obtained improved result with a model that first selects sentences and then rewrites them. \n\nSummarization of Radiology Reports. Most prior work that attempts to \"summarize\" radiology reports focused on classifying and extracting information from the report text (Friedman et al., 1995;Hripcsak et al., 1998;Elkins et al., 2000;Hripcsak et al., 2002).",
            "score": 0.571834619322873,
            "section_title": "Related Work",
            "char_start_offset": 4553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 202,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 316,
                    "end": 336,
                    "matchedPaperCorpusId": "5775833"
                },
                {
                    "start": 399,
                    "end": 427,
                    "matchedPaperCorpusId": "14961089"
                },
                {
                    "start": 597,
                    "end": 623,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 651,
                    "end": 674,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1044,
                    "end": 1062,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 1161,
                    "end": 1184,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1479,
                    "end": 1503,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1503,
                    "end": 1523,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 1523,
                    "end": 1540,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1542,
                    "end": 1559,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1660,
                    "end": 1680,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 1748,
                    "end": 1770,
                    "matchedPaperCorpusId": "44129061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "216868126",
            "title": "Few-Shot Learning for Abstractive Multi-Document Opinion Summarization",
            "text": "LexRank (Erkan and Radev, 2004) is an unsupervised extractive graph based algorithm selecting sentences based on graph centrality. Sentences represent nodes in a graph whose edges have weights denoting similarity computed with tf-idf. \n\nCopycat is the state-of-the-art unsupervised abstractive summarizer (Bra\u017einskas et al., 2019) that uses continues latent representations to model review groups and individual review semantics. It has an implicit mechanism for novelty reduction and a copy mechanism applied over source reviews. \n\nMeanSum is an unsupervised abstractive summarization model (Chu and Liu, 2019) that treats a summary as a discrete latent state of an autoencoder. The model is trained in a multi-task fashion with two objectives, one for prediction of reviews and the other one for summary-reviews alignment in the semantic space using the cosine similarity. \n\nAs is common in the summarization literature, we also employed a number of simple summarization baselines. First, the clustroid review was computed for each group of reviews as follows. We took each review from a group and computed ROUGE-L with respect to all other reviews. The review with the highest ROUGE score was selected as the clustroid review. Second, we sampled a random review from each group to be used the summary. Third, we constructed the summary by selecting the leading sentences from each review of a group.",
            "score": 0.5707352982092548,
            "section_title": "Baselines",
            "char_start_offset": 16385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1402
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 30,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 592,
                    "end": 610,
                    "matchedPaperCorpusId": "59413781"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "259203165",
            "title": "One model to rule them all: ranking Slovene summarizers",
            "text": "Early approaches to text summarization relied on statistical frequencies of words, sentence position, and sentences containing keywords [12]. These approaches aimed to extract important sentences or phrases from a text and generate a summary by concatenating them. Abstractive methods involved deleting less important words from the text to create a summary [6].\n\nGraph-based methods have been another popular approach to text summarization. In this approach, the document is represented as a graph, where sentences are nodes, and edges represent the relationships between them. The graph is then used to generate a summary by selecting the most important sentences. This method has been explored in several works [10], [3].\n\nWith the advent of neural networks, there has been an increasing interest in developing abstractive summarization techniques. Early neural abstractive systems used methods such as LSTM and other recurrent neural networks [14], [11]. However, transformer-based architectures have emerged as state-of-the-art models for abstractive text summarization [18], [9]. These models use self-attention mechanisms to selectively focus on important parts of the text and can generate more fluent and coherent summaries compared to earlier methods.\n\nWhile several approaches have been proposed for text summarization, many of them are designed to handle specific genres or types of text. In this work, our goal is to build a summarization system that can handle every type of text and genre with every possible property that can appear in the real world. This includes texts of varying lengths, topics, styles, and summaries that capture the most important information in the text. Achieving this goal requires developing a robust and adaptable model that can learn to summarize texts of diverse types and produce high-quality summaries.",
            "score": 0.5700261573357426,
            "section_title": "Related work",
            "char_start_offset": 4021,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "7793213"
                },
                {
                    "start": 714,
                    "end": 718,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 720,
                    "end": 723,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 953,
                    "end": 957,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1081,
                    "end": 1084,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "865952",
            "title": "Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity",
            "text": "As it happens in the models proposed by Erkan and Radev, sentences are vertices (V ) and edges (w) between vertices are established by passage similarity. The TextRank (Mihalcea & Tarau, 2004)-how the model based on PageRank was designated and the main contribution-formalization is similar to Continuous LexRank (see Eq. 8), although Mihalcea and Tarau also explore directed graphs in the representation of the text 12 . For summarization, the best results were obtained using a backward directed graph: the orientation of the edges from a vertex representing a sentence is to vertices representing previous sentences in the input source. \n\nPassage similarity is based on content overlap 3 and is defined in Eq. 9. Given two sets P p 1 , p 2 , ..., p n and Q q 1 , q 2 , ..., q n , each corresponding to a passage, similarity consists in the cardinality of the intersection over the sum of the logarithms of the cardinality of each set. \n\nA similar graph-based approach is described by Antiqueira et al. (2009). This work uses complex networks to perform extractive text summarization. Documents are also represented as networks, where the sentences are the nodes and the connections between nodes are established between sentences sharing common meaningful nouns.",
            "score": 0.5699981417620017,
            "section_title": "Pair-wise Passage Similarity-based Centrality",
            "char_start_offset": 13592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1265
                }
            ],
            "ref_mentions": [
                {
                    "start": 987,
                    "end": 1011,
                    "matchedPaperCorpusId": "17064982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "248780330",
            "title": "Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature",
            "text": "At the edge of our knowledge, this is the first work that applies a probability marginalization method for multi-document summarization. We believe this work can inspire novel research towards endto-end multi-model collaboration instead of solutions with a single large model addressing the entire task. According to the divide et impera pattern, each model learns a specific sub-task, creating a more efficient and transparent cooperating solution. Tasks such as related work generation or text generation from multi-sourced inputs can get the most from our method, improving pre-existing solutions to discriminate helpful knowledge from noise. \n\nFurther possible directions to deal with multiinputs are the following: i) extracting relevant snippets from documents with term weighting techniques (Domeniconi et al., 2015) or semantic relations with unsupervised methods (Domeniconi et al., 2016b(Domeniconi et al., , 2017) ) to better model interpretable representations based on knowledge graph learning techniques (Frisoni and Moro, 2020;Chen et al., 2021a,b) or event extraction methods (Frisoni et al., 2021); ii) training models to write and read crossdocument information with self-supervised representation learning methods (Domeniconi et al., 2014c) and memory-based neural layers (Moro et al., 2018;Cui and Hu, 2021).",
            "score": 0.5697358927211043,
            "section_title": "Future works",
            "char_start_offset": 19583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 1328
                }
            ],
            "ref_mentions": [
                {
                    "start": 798,
                    "end": 823,
                    "matchedPaperCorpusId": "51873494"
                },
                {
                    "start": 872,
                    "end": 897,
                    "matchedPaperCorpusId": "28667777"
                },
                {
                    "start": 897,
                    "end": 926,
                    "matchedPaperCorpusId": "35329921"
                },
                {
                    "start": 1018,
                    "end": 1042,
                    "matchedPaperCorpusId": "236460045"
                },
                {
                    "start": 1092,
                    "end": 1114,
                    "matchedPaperCorpusId": "244678529"
                },
                {
                    "start": 1233,
                    "end": 1259,
                    "matchedPaperCorpusId": "7363908"
                },
                {
                    "start": 1310,
                    "end": 1327,
                    "matchedPaperCorpusId": "235097475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4443359375
        },
        {
            "corpus_id": "18711007",
            "title": "Multi-Document Summarization via Discriminative Summary Reranking",
            "text": "Multi-document summarization methods can be extractionbased or abstraction-based, and we focus on extractive summarization methods in this paper. Extractive summarization methods usually produce a summary by selecting some original sentences in the document set. Sentences can be scored by employing rule based methods to simply combine a few feature weights, e.g., the centroidbased method (Radev et al. 2004) and NeATS (Lin and Hovy 2002). Machine learning techniques have been used for better combining various sentence features (Ouyang et al. 2007;Shen et al. 2007;Schilder and Kondadadi 2008;Wong et al. 2008). Many advanced methods have been proposed for extractive summarization in recent years, which are based on various techniques: budgeted median method (Takamura and Okumura 2009), A* search algorithm (Aker et al. 2010), minimum dominating set (Shen and Li 2010), matrix factorization (Wang et al. 2008), topic model (Wang et al. 2009) Integer Linear Programming (McDonald 2007;Gillick et al. 2008;Gillick and Favre 2009;Li et al. 2013), and submodular function (Lin and Bilmes 2010;Li et al. 2012). Graph-based methods have also been proposed for various summarization tasks, such as LexRank (Erkan and Radev 2004), TextRank (Mihalcea and Tarau 2005) and ClusterCMRW (Wan and Yang 2008). Furthermore, ensemble methods have also been used for sentence ranking. For example, Wang and Li (2010) propose a weighted consensus method to aggregate different sentence ranking results by different summarization methods. \n\nHowever, all the above studies focus on sentence scoring and ranking, and none of them has attempted to rank summaries directly. Different from previous studies, we rank summaries directly in this study. The advantage of ranking summaries is that we can optimize the summarization performance directly based on the characteristics of the summaries. \n\nAutomatic summary evaluation is partially related to this work.",
            "score": 0.5689181310332037,
            "section_title": "Related Work",
            "char_start_offset": 3339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1876
                },
                {
                    "start": 1879,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 440,
                    "matchedPaperCorpusId": "8448956"
                },
                {
                    "start": 532,
                    "end": 552,
                    "matchedPaperCorpusId": "14707480"
                },
                {
                    "start": 552,
                    "end": 569,
                    "matchedPaperCorpusId": "126818"
                },
                {
                    "start": 569,
                    "end": 597,
                    "matchedPaperCorpusId": "22109805"
                },
                {
                    "start": 765,
                    "end": 792,
                    "matchedPaperCorpusId": "9849366"
                },
                {
                    "start": 857,
                    "end": 875,
                    "matchedPaperCorpusId": "1671916"
                },
                {
                    "start": 930,
                    "end": 947,
                    "matchedPaperCorpusId": "18517541"
                },
                {
                    "start": 991,
                    "end": 1011,
                    "matchedPaperCorpusId": "18752763"
                },
                {
                    "start": 1011,
                    "end": 1034,
                    "matchedPaperCorpusId": "3050993"
                },
                {
                    "start": 1034,
                    "end": 1049,
                    "matchedPaperCorpusId": "12090675"
                },
                {
                    "start": 1075,
                    "end": 1096,
                    "matchedPaperCorpusId": "1803710"
                },
                {
                    "start": 1096,
                    "end": 1111,
                    "matchedPaperCorpusId": "15990379"
                },
                {
                    "start": 1206,
                    "end": 1228,
                    "matchedPaperCorpusId": "10418456"
                },
                {
                    "start": 1387,
                    "end": 1405,
                    "matchedPaperCorpusId": "2642018"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "258865347",
            "title": "Is Summary Useful or Not? An Extrinsic Human Evaluation of Text Summaries on Downstream Tasks",
            "text": "We need to select a few publicly available systems to generate summaries on the three datasets and then studying the usefulness of the summaries. As neural abstractive summarization methods with pretraining have achieved great success in recent years, we mainly focus on these summarization models. A total of six representative neural models are chosen as the abstractive systems, including: \n\n\u2022 BART [20]: a sequence-to-sequence model trained as a denoising autoencoder, which is applicable to various natural language generation tasks. It is fine-tuned on CNN/DailyMail. \u2022 Pegasus [45]: a model pre-trained with self-supervised gapsentence-generation objective designed for abstractive summarization. We use the version fine-tuned on CNN/DailyMail. \u2022 BRIO [24]: a model with a new training paradigm that assigns candidate outputs probability mass according to their quality using contrastive learning. It is also fine-tuned on CNN/DailyMail. \u2022 T5 [36]: a text-to-text transfer learning framework that is pretrained with several unsupervised and supervised objectives, including summarization. \u2022 T0 [39]: a prompt-based model, which is fine-tuned on standard summarization datasets including CNN/DailyMail. \u2022 GPT3 [3]: a prompt-based language model that achieves strong performance in the few-shot setting. In this work, we use OpenAI's text-davinci-002 [32]. We also include two simple extractive systems for comparison: \n\n\u2022 Lead-n: Lead-3 is a simple but commonly used summarization baseline that selects the first three sentences of an article as the summary. we modify the Lead-3 setting and refer to it as the Lead-n model. Lead-n selects the first several sentences that are closest to the summary length we set. \n\n\u2022 Lexrank [7]: a graph-based text summarization model that calculates the importance of sentences by determining the cosine similarity between them, and the sentences with highest scores are selected as the summary.",
            "score": 0.5681415867530094,
            "section_title": "Representative Summarization Systems",
            "char_start_offset": 12108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1423
                },
                {
                    "start": 1426,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 588,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1216,
                    "end": 1219,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1733,
                    "end": 1736,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408447265625
        },
        {
            "corpus_id": "238259708",
            "title": "LawSum: A weakly supervised approach for Indian Legal Document Summarization",
            "text": "In [1], a systematic comparative study is performed on 17000+ Indian supreme court judgements, using algorithms such as LexRank [24], LSA [25], DSDR [26], LetSum [10], Casesummarizer [19], Graphical Model [2], Neural Extractive Summarizer etc. \n\nAll the approaches discussed above are extractive in nature, where most important sentences are identified and concatenated to form a summary. For, single-document extractive summarization, sentence representations of the documents are created and ranks sentences using machine learning algorithms. For example, SVM and Naive Bayes [27] and Hidden Markov Model [28]. As the amount of labeled dataset increased, deep learning models started being used. In [29], CNN and LSTM are were used for creating sentence representations. In [30], both CNN and LSTM are used for ranking the sentences. In [31], LSTM and GRU were used in a hierarchical manner for generating sentence representations. With onset state-of-the-art pre-trained language models several systems that used transformer like BERT [32], Roberta [33] etc, were proposed for summarization [34,35]. Recently, Google has also released [36] for summarization of Long Documents. \n\nIn case Unsupervised Extractive Summarisation, TextRank and LexRank were the prominent works. But, lately due increase in amount of text data published daily, and less labelled data many new approaches are coming up. The work by [37] proposes a novel approach for unsupervised sentence summarization by mapping the Information Bottleneck principle to a conditional language modelling objective. In [38], two language models are used, and shows that by using a product-ofexperts criteria is enough for maintaining continuous contextual matching while maintaining output fluency. In [39], graph-based ranking algorithm is modified by computing node centrality in two ways, employing BERT [32] and graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document.",
            "score": 0.5680411357120253,
            "section_title": "Related Work",
            "char_start_offset": 10042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 6,
                    "matchedPaperCorpusId": "104292894"
                },
                {
                    "start": 128,
                    "end": 132,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "14961089"
                },
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "16471324"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "19807072"
                },
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "17614070"
                },
                {
                    "start": 205,
                    "end": 208,
                    "matchedPaperCorpusId": "16399382"
                },
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "18517541"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "2767184"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "6311642"
                },
                {
                    "start": 839,
                    "end": 843,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1580,
                    "end": 1584,
                    "matchedPaperCorpusId": "196201849"
                },
                {
                    "start": 1763,
                    "end": 1767,
                    "matchedPaperCorpusId": "182952892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427978515625
        },
        {
            "corpus_id": "262592258",
            "title": "A hybrid approach for text summarization using semantic latent Dirichlet allocation and sentence concept mapping with transformer",
            "text": "Their approach utilized undirected graph-based scoring to evaluate sentences and determine which ones should be included in the summary. Extractive text summarization [26] considers sentence length, position, cue phrases, and cohesion when selecting sentences for summarization. In recent years, the use of neural networks for text summarization has become widespread, as these models can learn sentence patterns. \n\nMostly used deep learning method is the recursive neural network (RNN) [27]. Long short-term memory (LSTM), gated recurrent units (GRU), and transformers were other approaches for solving gradient disappearance. The extractive method has given results since it can easily combine many techniques and improve performance. Using content attention, two-phase multi-document summarization [28] extracts subtopics from documents. The summary was formulated using different sub-topics as an optimization problem of minimizing the sentence distance. Huang et al. [29] employed Hepos, a novel encoder-decoder attention, to extract features from original documents. They also introduced a new dataset called GovReport, which includes lengthy documents and their corresponding summaries. The evaluation model for text summarization has its shortcoming in using neural networks [30]. The author has tried to overcome the shortcomings of evaluation metrics of text summarization in five dimensions. He re-evaluated the metrics using neural networks and benchmarked metrics using recent text summarization models. Sentence regression [31] identifies essential features that represent the sentences. The sentence relation such as contextual sentence relations, query sentence relations, and title sentence relations are used to extract basic sentences from the documents. \n\nTraining a neural network for text summarization has some difficulty processing large documents. We can replace phrases with general terms in semantic content generalization. Some used the pointer generator network [32], copying the original content and combining the semantic content generalization. There are many attention-based mechanisms to generate an article summary; one such method is attentive encoder-based summarization (AES) combined with unidirectional recurrent neural network (Uni-AES) [33].",
            "score": 0.567984908632859,
            "section_title": "LITERATURE REVIEW",
            "char_start_offset": 10833,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "57036582"
                },
                {
                    "start": 487,
                    "end": 491,
                    "matchedPaperCorpusId": "238027281"
                },
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "243638031"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "233033613"
                },
                {
                    "start": 1283,
                    "end": 1287,
                    "matchedPaperCorpusId": "220768873"
                },
                {
                    "start": 1537,
                    "end": 1541,
                    "matchedPaperCorpusId": "14001298"
                },
                {
                    "start": 1991,
                    "end": 1995,
                    "matchedPaperCorpusId": "231713675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58154296875
        },
        {
            "corpus_id": "259376708",
            "title": "Two-step Text Summarization for Long-form Biographical Narrative Genre",
            "text": "We briefly survey existing methods that propose multi-stage text summarization systems evaluated on datasets from a broad range of domains. \n\nLing and Rush (2017) introduced a coarse-tofine attention model that reads a document hierarchically, using coarse attention to select top-level blocks of text and fine attention to read the tokens of the chosen blocks. Their proposed summarizer scales linearly with the number of top-level chunks and effectively handles long sequences. However, their model performance lagged behind the standard instantiation baseline of the attention function on ROUGE similarity metrics. Xu and Lapata (2020) proposed a coarse-to-fine modeling framework for extractive summarization applied to query focused multi-document. Their system incorporates a relevance estimator for retrieving textual segments-such as sentences or longer passages associated with a query-an evidence estimator which further isolates segments likely to contain answers to the query, and a centrality estimator which finally selects which segments to include in the summary. Our extractive summary component is resemblant in spirit to their centrality estimator, however we use a Sentence Transformers (SBERT; Reimers and Gurevych, 2019) model to generate contextual sentence embeddings that follows producing a sentence similarity matrix for computing graph centrality based ranking. Pilault et al. (2020) explored Transformer language models and proposed an extract-thensummarize computational pipeline for long documents. Their model consists of an extractive element comprising a hierarchical neural encoder that outputs sentence representations of either a pointer to input sentences or to the result of sentence classification; and a Transformer language model conditioned on the extracted sentences as well as on either a part of or the entire input document to generate the summary. Their system was shown to outperform several baselines on similarity metrics, however, a discussion on factual correctness and consistency analyses of experimental results appears relatively sparse. Gidiotis and Tsoumakas (2020) proposed a divide-and-conquer method by splitting the input into multiple segments, summarizing them separately, and combining the summary pieces. Basing on smaller source and target summary pairs that are focused on a specific aspect of the text, results in better alignment and considerable reduction of computation complexity.",
            "score": 0.5672175545819644,
            "section_title": "Related Work",
            "char_start_offset": 6604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2271
                },
                {
                    "start": 2272,
                    "end": 2454
                }
            ],
            "ref_mentions": [
                {
                    "start": 1215,
                    "end": 1242,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1390,
                    "end": 1411,
                    "matchedPaperCorpusId": "202541012"
                },
                {
                    "start": 2095,
                    "end": 2124,
                    "matchedPaperCorpusId": "219425367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61376953125
        },
        {
            "corpus_id": "248965506",
            "title": "Proposition-Level Clustering for Multi-Document Summarization",
            "text": "We compare our method to several strong abstractive baselines: Opinosis (Ganesan et al., 2010) generates abstracts from salient paths in a word cooccurrence graph; Extract+Rewrite  selects sentences using LexRank and generates for each sentence a title-like summary; PG (See et al., 2017) runs a Pointer-Generator model that includes a sequence-to-sequence network with a copy-mechanism; PG-MMR (Lebanoff et al., 2018) selects representative sentences with MMR (Carbonell and Goldstein, 1998) and fuses them with a PG-based model; Hi-MAP (Fabbri et al., 2019) is a hierarchical version of the PG model that allows calculating sentence-level MMR scores; MDS-Joint-SDS (Jin and Wan, 2020) is a hierarchical encoder-decoder architecture that is trained with SDS and MDS datasets while preserving document boundaries.\n\nWe additionally compare to several strong extractive baselines: SumBasic (Vanderwende et al., 2007) extracts phrases with words that appear frequently in the documents; KLSumm (Haghighi and Vanderwende, 2009) extracts sentences that opti-mize KL-divergence; LexRank (Erkan and Radev, 2004) is a graph-based approach where vertices represent sentences, the edges stand for word overlap between sentences, and sentence importance is computed by eigenvector centrality; DPP-Caps-Comb (Cho et al., 2019) balances between salient sentence extraction and redundancy avoidance by optimizing determinantal point processes (DPP); HL-XLNetSegs and HL-TreeSegs (Cho et al., 2020) are two versions of a DPP-based span highlighting approach that heuristically extracts candidate spans by their probability to begin and end with an EOS token; RL-MMR (Mao et al., 2020) adapts a neural reinforcement learning single document summarization (SDS) approach (Chen and Bansal, 2018) to the multi-document setup and integrates Maximal Margin Relevance (MMR) to avoid redundancy.",
            "score": 0.5671444766261313,
            "section_title": "C Compared Methods",
            "char_start_offset": 33614,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 288,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 395,
                    "end": 418,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 1651,
                    "end": 1668,
                    "matchedPaperCorpusId": "222090788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "268713198",
            "title": "MultiSum: A Multi-Facet Approach for Extractive Social Summarization Utilizing Semantic and Sociological Relationships",
            "text": "Our methodology is contrasted with two distinct categories of methods. The first category exclusively employs text content information and encompasses the subsequent methods. The Centroid (Radev, Blair-Goldensohn, and Zhang 2001) identifies sentences of high relevance to a cluster of related documents using centroid-based features. The LSA (Gong and Liu 2001) applies singular value decomposition to the feature matrix and selects posts with superior singular values. The Lexrank (Erkan and Radev 2004) employs a PageRank-like algorithm on the constructed similarity graph to select prominent posts. The DSDR (He et al. 2012) perceives summarization as a reconstruction task and extracts summaries by minimizing reconstruction error. The MDS-Sparse (Wang et al. 2015) conducts multi-document summarization using a sparse coding technique and extracts summaries by reconstructing the source document. The Pac-Sum (Zheng and Lapata 2019) is a graph-based extractive model that uses BERT as sentence features and considers the relative position of posts and model documents as a directed graph. The Spectral (Liu et al. 2015) proposes a spectral-based hypothesis and identifies the importance of a sentence according to the spectral impact of posts. The MT-GNN (Doan, Nguyen, and Bui 2022) uses finer-grained semantic units in text posts to capture the complex relationship between words and posts, and constructs a heterogeneous graph neural network for extracting summaries. \n\nThe second category capitalizes on social relation and includes the subsequent models. The SNSR (He and Duan 2018) introduces a social regularization term into the sparse reconstruction framework to find salient tweets as summaries. The SCMGR (Liu et al. 2021) uses GCN on original social networks to extract socialized post representations and adopts a sparse reconstruction framework to select summary posts.",
            "score": 0.5665088568079215,
            "section_title": "Comparison Methods",
            "char_start_offset": 18103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 229,
                    "matchedPaperCorpusId": "12666673"
                },
                {
                    "start": 342,
                    "end": 361,
                    "matchedPaperCorpusId": "7003631"
                },
                {
                    "start": 482,
                    "end": 503,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 611,
                    "end": 626,
                    "matchedPaperCorpusId": "16471324"
                },
                {
                    "start": 751,
                    "end": 769,
                    "matchedPaperCorpusId": "16517338"
                },
                {
                    "start": 1107,
                    "end": 1124,
                    "matchedPaperCorpusId": "226262300"
                },
                {
                    "start": 1260,
                    "end": 1288,
                    "matchedPaperCorpusId": "252819335"
                },
                {
                    "start": 1721,
                    "end": 1737,
                    "matchedPaperCorpusId": "240230540"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.861328125
        },
        {
            "corpus_id": "221739210",
            "title": "Unsupervised Summarization by Jointly Extracting Sentences and Keywords",
            "text": "Previous investigations show that humanwritten summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Jing and McKeown, 2000;Cheung and Penn, 2013). Bing et al (2015) proposed a abstractive approach for multi-document summarization by extracting noun and verb phrases, and merging those phrases to construct new sentences. Rush et al (2015) were the first to apply neural networks to abstractive document summarization, achieving state-of-the-art performance on the sentence-level summarization. However, those approaches suffer from both the limited amount of training data and the higher complexity of machine learning models. \n\nOur approach is more close to the summarization models of (Wan et al., 2007;Cheng and Lapata, 2016;Wang et al., 2017) with the differences mainly in graph-based ranking algorithm, approximate sampling method, and distribution representation-based similarity. Wan et al (2007) proposed an iterative approach for simultaneous text summarization and keyword extraction. Wang et al (2017) formulated MDS as an affinitypreserving random walk and uses the \"global normalization\" to transform sentence affinity matrix into sentence transition matrix. They all use traditional tf-idf weighting method to compute the similarity between sentences and words, while we learn to embed words and sentences into a shared vector space and calculate the similarity based on their distances in such space. Besides, we prove that the iterative computation is not required for the joint task in graph-based ranking framework, present a sample algorithm for absorbing random walks. Cheng and Lapata (2016) developed a data-driven extractive framework for single-document summarization which includes a neural network-based hierarchical document encoder (a standard recurrent neural network following a convolutional layer) and a sentence (or word) extractor with attention mechanism. Their sentence or word extractor needs to be trained separately while ours can rank the salience of sentences and words in a unsupervised manner.",
            "score": 0.566072211245339,
            "section_title": "Related Work",
            "char_start_offset": 27387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 172,
                    "matchedPaperCorpusId": "800331"
                },
                {
                    "start": 172,
                    "end": 194,
                    "matchedPaperCorpusId": "15945816"
                },
                {
                    "start": 196,
                    "end": 213,
                    "matchedPaperCorpusId": "8377315"
                },
                {
                    "start": 370,
                    "end": 387,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 736,
                    "end": 754,
                    "matchedPaperCorpusId": "17483199"
                },
                {
                    "start": 754,
                    "end": 777,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 777,
                    "end": 795,
                    "matchedPaperCorpusId": "11296298"
                },
                {
                    "start": 937,
                    "end": 953,
                    "matchedPaperCorpusId": "17483199"
                },
                {
                    "start": 1045,
                    "end": 1062,
                    "matchedPaperCorpusId": "11296298"
                },
                {
                    "start": 1639,
                    "end": 1662,
                    "matchedPaperCorpusId": "1499080"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "189761982",
            "title": "Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking",
            "text": "For the comparisons, two unsupervised baseline models are employed. A graph-based unsupervised sentence extraction method, TextRank is employed (Mihalcea and Tarau, 2004), where sentence embeddings are used instead of bag-ofwords representations, based on (Rossiello et al., 2017). As an unsupervised word-level extractive approach, we employ Opinosis (Ganesan et al., 2010), which detects salient phrases in terms of their redundancy. Because we observe repetitive expressions in the dataset, Opinosis is added as a baseline. Both methods extract or generate a onesentence summary. Furthermore, a third, novel unsupervised baseline model MeanSum-single is introduced, which is an extended version of the unsupervised neural multi-document summarization model (Chu and Liu, 2018). While it decodes the mean of multiple document embeddings to generate the summary, MeanSum-single generates a singledocument summary by decoding the mean of the sentence embeddings in a document. It learns a language model through reconstruction of each sentence. By comparing with MeanSum-single, we verify that our model focuses on the main review points, and does not simply take the average of the entire document. \n\nAs supervised baselines, we employ vanilla neural sequence-to-sequence models for abstractive summarization (Hu et al., 2015), following previous studies (Ma et al., 2018;Wang and Ren, 2018). We denote the model as Seq-Seq and that with the attention mechanism as Seq-Seq-att. The encoder and decoder used are the same as those used in our model. difference between our models and the others are statistically significant (p < 0.05). Because the abstractive approach generates a concise summary by omitting trivial phrases, it can lead to a better performance than those of the extractive ones.",
            "score": 0.5660079040799603,
            "section_title": "Baseline",
            "char_start_offset": 14620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1796
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 280,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 352,
                    "end": 374,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 760,
                    "end": 779,
                    "matchedPaperCorpusId": "53112381"
                },
                {
                    "start": 1310,
                    "end": 1327,
                    "matchedPaperCorpusId": "11597846"
                },
                {
                    "start": 1356,
                    "end": 1373,
                    "matchedPaperCorpusId": "20636603"
                },
                {
                    "start": 1373,
                    "end": 1392,
                    "matchedPaperCorpusId": "53585063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56298828125
        },
        {
            "corpus_id": "238862768",
            "title": "Automatic Text Summarization Using Deep Reinforcement Learning and Beyond",
            "text": "Artificial intelligence technology is in a period of rapid development, and its application in various industries is becoming increasingly common. From medical diagnosis to social networks, and from intelligent education to news media, specific application cases of artificial intelligence can be seen everywhere. In people's daily lives, we are facing information overload, and how to deal with massive data information in limited time has become a problem. Using computers to understand natural language can filter useless and redundant information and only retain key information feedback to users. This specific application is called automatic text summarization. It uses a computer to summarize the whole text, helping users understand the core semantics of the original text directly by reading the abstract. Therefore, the machine learning model that automatically extracts summaries can quickly extract key information from massive texts, saving users valuable time. The emergence of automatic text summarization not only reduces information overload but also saves the high cost of manual text summarization. Automatic text summarization is mainly divided into two types of methods. The extractive summarization technique extracts several key sentences from the original text and then forms a single abstract. The abstractive summarization technique understands the semantics of the original text and summarizes and induces the subject matter using a human-readable language. At present, extractive summarization is relatively easy to implement, so it is more widely used. The abstractive summarization requires the ability of the computer to understand the original text, so it has higher technical requirements. \n\nTraditional extractive summarization techniques have two types: graph-based sorting methods and artificial feature-based methods. Based on the graph sorting method, each sentence in the document is used as a node of the graph, and calculate the similarity between sentences, and the value of the similarity is used as the weight of the edge to construct the graph model. Then, the PageRank algorithm [36] is used to solve each sentence score; finally, the highest scores are output as a summarization. The representative algorithms are TextRank [30] and LexRank [11]. The artificial feature-based method usually builds a model based on the length of each sentence, whether the sentence contains the title words. The representative algorithm is TextTeaser.",
            "score": 0.5653398835244078,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2292
                },
                {
                    "start": 2293,
                    "end": 2436
                },
                {
                    "start": 2437,
                    "end": 2480
                }
            ],
            "ref_mentions": [
                {
                    "start": 2287,
                    "end": 2291,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31884765625
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "This section presents a systematic evaluation of our proposed model. We compare its performance against state-of-the-art extractive and abstractive summarization models, including neural graph-based models that utilize various nodes. We also provide details of the long scientific medical documents dataset used in this research, along with the hyperparameters, implementation settings, and the impact of latent topics (global information) on the system's summaries.",
            "score": 0.56485617384305,
            "section_title": "IV. PERFORMANCE EVALUATION",
            "char_start_offset": 27929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 466
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57470703125
        },
        {
            "corpus_id": "10274390",
            "title": "Clustering cliques for graph-based summarization of the biomedical research literature",
            "text": "Most frequency-based methods provide extractive summaries composed of source sentences containing frequently occurring content units. Nenkova and Vanderwende [15] assessed the contribution of frequency of occurrence to summarization, which is considerable. Reeve et al. [16,17] further exploit domain ontologies to identify salient information.\n\nRecently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. [18][19][20]) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank [18] and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary.\n\nIn addition to text, biomedical data can also be represented as a graph, with nodes representing biological entities (e.g. genes or proteins) and edges associations between them. For example, protein-protein interactions can be successfully modeled by a graph. Based on the recognition of cohesive subgroups (such as cliques), gene or protein complexes can be extracted to help predict protein interactions or find gene-disease relations [22,23].",
            "score": 0.5645308172251217,
            "section_title": "Identifying salient source content",
            "char_start_offset": 5658,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "14968839"
                },
                {
                    "start": 1044,
                    "end": 1048,
                    "matchedPaperCorpusId": "2962090"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "268069298"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "13664986"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "2962090"
                },
                {
                    "start": 1869,
                    "end": 1873,
                    "matchedPaperCorpusId": "7883745"
                },
                {
                    "start": 1873,
                    "end": 1876,
                    "matchedPaperCorpusId": "303600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "11347066",
            "title": "Variations of the Similarity Function of TextRank for Automated Summarization",
            "text": "The field of automated summarization has attracted interest since the late 50's [14]. Traditional methods for text summarization analyze the frequency of words or sentences in the first paragraphs of the text to identify the most important arXiv:1602.03606v1 [cs.CL] 11 Feb 2016 lexical elements. The mainstream research in this field emphasizes extractive approaches to summarization using statistical methods [4]. Several statistical models have been developed based on training corpora to combine different heuristics using keywords, position and length of sentences, word frequency or titles [13]. \n\nOther methods are based in the representation of the text as a graph. The graph-based ranking approaches consider the intrinsic structure of the texts instead of treating texts as simple aggregations of terms. Thus it is able to capture and express richer information in determining important concepts [19]. \n\nThe selected text fragments to use in the graph construction can be phrases [6], sentences [14], or paragraphs [18]. Currently, many successful systems adopt the sentences considering the tradeoff between content richness and grammar correctness. According to these approach the most important sentences are the most connected ones in the graph and are used for building a final summary [2]. To identify relations between sentences (edges for the graph) there are several measures: overlapping words, cosine distance and query-sensitive similarity. Also, some authors have proposed combinations of the previous with supervised learning functions [19]. \n\nThese algorithms use different information retrieval techniques to determine the most important sentences (vertices) and build the summary [23]. The Tex-tRank algorithm developed by Mihalcea and Tarau [17] and the LexRank algorithm by Erkan and Radev [7] are based in ranking the lexical units of the text (sentences or words) using variations of PageRank [20]. Other graph-based ranking algorithms such as HITS [11] or Positional Function [10] may be also applied.",
            "score": 0.5643938846247306,
            "section_title": "Previous work",
            "char_start_offset": 1180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 596,
                    "end": 600,
                    "matchedPaperCorpusId": "5519987"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "27361296"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "486465"
                },
                {
                    "start": 1005,
                    "end": 1009,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 1025,
                    "end": 1029,
                    "matchedPaperCorpusId": "12968935"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "16188305"
                },
                {
                    "start": 1560,
                    "end": 1564,
                    "matchedPaperCorpusId": "27361296"
                },
                {
                    "start": 1707,
                    "end": 1711,
                    "matchedPaperCorpusId": "12250431"
                },
                {
                    "start": 1819,
                    "end": 1822,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1924,
                    "end": 1928,
                    "matchedPaperCorpusId": "1508503"
                },
                {
                    "start": 1980,
                    "end": 1984,
                    "matchedPaperCorpusId": "216000619"
                },
                {
                    "start": 2008,
                    "end": 2012,
                    "matchedPaperCorpusId": "118150259"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "218718706",
            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
            "text": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
            "score": 0.5642473185802919,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "269813542",
            "title": "Revolutionizing Content Digestion: Unleashing the Power of Bidirectional and Auto-Regressive Transformers in AI-Powered Automatic Text Summarization",
            "text": "Automatic text summarization has been a subject of extensive research in natural language processing (NLP) and information retrieval. Over the years, researchers have explored various approaches, including statistical methods, graph-based algorithms, and more recently, deep learning techniques, to address the challenges posed by the ever-increasing volume of textual data. In this literature review, we examine key developments in the field, focusing particularly on the role of bidirectional and auto-regressive transformers in AI-powered automatic text summarization. \n\nEarly approaches to text summarization relied heavily on statistical methods, such as frequency-based extraction and sentence scoring algorithms. These methods often suffered from limitations in capturing semantic relationships and producing coherent summaries. Graph-based algorithms, such as TextRank and LexRank, introduced the concept of representing documents as graphs and using graph algorithms to identify important sentences or phrases. While these methods showed improvement over purely statistical approaches, they still struggled with capturing complex linguistic patterns and producing abstractive summaries. \n\nThe emergence of deep learning techniques, particularly transformer-based models, has revolutionized the field of automatic text summarization. Bidirectional transformers, exemplified by models like BERT (Bidirectional Encoder Representations from Transformers), have gained prominence for their ability to capture contextual information from both preceding and following words. This bidirectional understanding allows them to grasp nuanced relationships within a text, leading to more accurate and informative summaries. BERT-based approaches typically involve fine-tuning pre-trained models on specific summarization tasks, leveraging their rich contextual representations to generate summaries. On the other hand, auto-regressive transformers, such as GPT (Generative Pre-trained Transformer), take a different approach by generating output tokens sequentially, considering the previously generated tokens. \n\nWhile initially designed for language generation tasks, GPT-based models have been adapted for text summarization, producing coherent and fluent summaries by iteratively predicting the next token in the sequence. \n\nThe convergence of bidirectional and auto-regressive techniques has led to the development of hybrid models, such as BART (Bidirectional and Auto-Regressive Transformers), which combine the strengths of both approaches.",
            "score": 0.5641987341107082,
            "section_title": "II. Literature Review",
            "char_start_offset": 2009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2107
                },
                {
                    "start": 2110,
                    "end": 2322
                },
                {
                    "start": 2325,
                    "end": 2544
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.662109375
        },
        {
            "corpus_id": "52053741",
            "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization",
            "text": "Popular methods for multi-document summarization have been extractive.Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002;Zajic et al., 2007;Gillick and Favre, 2009;Galanis and Androutsopoulos, 2010;Berg-Kirkpatrick et al., 2011;Li et al., 2013;Thadani and McKeown, 2013;Wang et al., 2013;Yogatama et al., 2015;Filippova et al., 2015;Durrett et al., 2016).In recent years neural networks have been exploited to learn word/sentence representations for single-and multi-document summarization (Cheng and Lapata, 2016;Cao et al., 2017;Isonuma et al., 2017;Yasunaga et al., 2017;Narayan et al., 2018).These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion.\n\nPrior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999;Carenini and Cheung, 2008;Ganesan et al., 2010;Gerani et al., 2014;Fabbrizio et al., 2014;Pighin et al., 2014;Bing et al., 2015;Liu et al., 2015;Liao et al., 2018).These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization.Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets.",
            "score": 0.5640353789152688,
            "section_title": "Related Work",
            "char_start_offset": 5566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 70,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 899
                },
                {
                    "start": 901,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1310
                },
                {
                    "start": 1310,
                    "end": 1422
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 206,
                    "matchedPaperCorpusId": "189898"
                },
                {
                    "start": 283,
                    "end": 313,
                    "matchedPaperCorpusId": "15467396"
                },
                {
                    "start": 313,
                    "end": 329,
                    "matchedPaperCorpusId": "8928513"
                },
                {
                    "start": 329,
                    "end": 355,
                    "matchedPaperCorpusId": "13458891"
                },
                {
                    "start": 355,
                    "end": 373,
                    "matchedPaperCorpusId": "1260503"
                },
                {
                    "start": 373,
                    "end": 395,
                    "matchedPaperCorpusId": "12194143"
                },
                {
                    "start": 395,
                    "end": 418,
                    "matchedPaperCorpusId": "1992250"
                },
                {
                    "start": 418,
                    "end": 439,
                    "matchedPaperCorpusId": "5125975"
                },
                {
                    "start": 575,
                    "end": 599,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 599,
                    "end": 616,
                    "matchedPaperCorpusId": "14651945"
                },
                {
                    "start": 616,
                    "end": 637,
                    "matchedPaperCorpusId": "35618061"
                },
                {
                    "start": 637,
                    "end": 659,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 659,
                    "end": 680,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 973,
                    "end": 996,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 996,
                    "end": 1022,
                    "matchedPaperCorpusId": "8660413"
                },
                {
                    "start": 1022,
                    "end": 1043,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 1043,
                    "end": 1063,
                    "matchedPaperCorpusId": "2767900"
                },
                {
                    "start": 1063,
                    "end": 1086,
                    "matchedPaperCorpusId": "12682781"
                },
                {
                    "start": 1086,
                    "end": 1106,
                    "matchedPaperCorpusId": "14959"
                },
                {
                    "start": 1106,
                    "end": 1124,
                    "matchedPaperCorpusId": "8377315"
                },
                {
                    "start": 1124,
                    "end": 1141,
                    "matchedPaperCorpusId": "5001921"
                },
                {
                    "start": 1141,
                    "end": 1159,
                    "matchedPaperCorpusId": "49210924"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.496337890625
        },
        {
            "corpus_id": "15009101",
            "title": "Abstractive Summarization of Spoken and Written Conversations Based on Phrasal Queries",
            "text": "We compare our approach with the following baselines: \n\n1) Cosine-1st: we rank the utterances in the chat log based on the cosine similarity between the utterance and query. Then, we select the first uttrance as the summary; \n\n2) Cosine-all: we rank the utterances in the chat log based on the cosine similarity between the utterance and query and then select the utterances with a cosine similarity greater than 0; \n\n3) TextRank: a widely used graph-based ranking model for single-document sentence extraction that works by building a graph of all sentences in a document and use similarity as edges to compute the salience of sentences in the graph (Mihalcea and Tarau, 2004); 4) LexRank: another popular graph-based content selection algorithm for multi-document summarization (Erkan and Radev, 2004); 5) Biased LexRank: is a state-of-the-art queryfocused summarization that uses LexRank algorithm in order to recursively retrieve additional passages that are similar to the query, as well as to the other nodes in the graph (Otterbacher et al., 2009). \n\nMoreover, we compare our abstractive system with the first part of our framework (utterance extraction in Figure 1), which can be presented as an extractive query-based summarization system (our extractive system). We also show the results of the version we use in our pipeline (our pipeline extractive system). The only difference between the two versions is the length of the generated summaries. In our pipeline we aim at higher recall, since we later filter sentences and aggregate them to generate new abstract sentences. In contrast, in the stand alone version (extractive system) we limit the number of retrieved sentences to the desired length of the summary. We also compare the results of our full system (i.e., with tuning) with a non-optimized version when the ranking coefficients are distributed equally (\u03b1 = \u03b2 = \u03b3 = 0.33). For parameters estimation, we tune all parameters (utterance selection and path ranking) exhaustively with 0.1 intervals using our development set.",
            "score": 0.5639738567255421,
            "section_title": "Baselines",
            "char_start_offset": 20566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 56,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 224
                },
                {
                    "start": 227,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 1055
                },
                {
                    "start": 1058,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2043
                }
            ],
            "ref_mentions": [
                {
                    "start": 780,
                    "end": 803,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1028,
                    "end": 1054,
                    "matchedPaperCorpusId": "17636912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "220483436",
            "title": "Neural Abstractive Summarization with Structural Attention",
            "text": "In this paper, we explore one such dataset dealing with community question answering (CQA) summarization. \n\nMulti-document summarization (MDS) is a well-studied problem with possible applications in summarizing tweets [Cao et al., 2016], news from varying sources [Yasunaga et al., 2017], reviews in e-commerce services [Ganesan et al., 2010], and so on. CQA services such as Quora, Yahoo! Answers, Stack Overflow help in curating information which may often be difficult to obtain directly from the existing resources on the web. However, a majority of question threads on these services harbour unstructured, often repeating or contradictory answers. Additionally, the answers vary in length from being a few words to a thousand words long. A large number of users with very different writing styles contribute to form these knowledge bases, which results in a diverse and challenging corpora. We define the task of CQA summarization as follows -given a list of such question threads, summarize the popular opinion reflected in them. This can be viewed as a multi-document summarization task [Chowdhury and Chakraborty, 2019]. \n\nWe envision a solution to this problem by incorporating structural information present in language in the summarization models. Recent studies have shown promise in this direction. For example, [Fernandes et al., 2018] add graph neural networks (GNNs) [Li et al., 2015] to the seq2se en-coder model for neural abstractive summarization. [Liu et al., 2019] model documents as multi-root dependency trees, each node representing a sentence; and pick the induced tree roots to be summary sentences in an extractive setting. Recently, [Song et al., 2018] attempt to preserve important structural dependency relations obtained from syntactic parse trees of the input text in the summary by using a structure infused copy mechanism. However, obtaining labeled data to explicitly model such dependency relations is expensive. Here, we attempt to mitigate this issue by proposing a structural encoder for summarization based on the prior work on structure attention networks [Liu and Lapata, 2018] which implicitly incorporates structural information within end-to-end training.",
            "score": 0.5635361020085428,
            "section_title": "Introduction",
            "char_start_offset": 1900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 108,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 236,
                    "matchedPaperCorpusId": "53288119"
                },
                {
                    "start": 1468,
                    "end": 1486,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 1662,
                    "end": 1681,
                    "matchedPaperCorpusId": "1828713"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5341796875
        },
        {
            "corpus_id": "258865905",
            "title": "Neural Summarization of Electronic Health Records",
            "text": "In this section, we briefly review extractive and abstractive approaches that have been used in non-medical documents. It includes several models central to this paper. Recent works on text summarization algorithms can be broadly classified based on the type of summary generated -extractive and abstractive. Extractive summarization involves taking a subset of phrases and sentences from the input documents and concatenating them to form a summary. On the other hand, abstractive summarization algorithms produce summaries based on their own vocabulary and the concepts they associate with the input document, not necessarily using the exact words of a source article. One of the models in the extractive summarization category is the Luhn summarizer [10]. It selects sentences based on the maximum number of significant words present in a particular sentence. The significance of words is determined through Term Frequency -Inverse Document Frequency, also known as TF-IDF [11]. A common baseline model for extractive summarization is LEAD-3, which is another extractive summarization solution that takes the first three sentences of the input document and sets them as the document's summary. Another sub-category of extractive summary algorithms is topic-based approaches. For example, the model designed by Harabagiu et al. [12] represents topic themes based on events that frequently occur over a set of documentation. They illustrate five ways of determining such frequencies -topic signatures, enhanced topic signatures, thematic signatures, modeling documents' content structure, and templates. There are also graph-based [13,14] approaches that use text representation in a graph where words or sentences are represented as nodes and semantically-related text elements are connected through edges. Finally, discourse-based approaches [15] integrate linguistic knowledge to represent the connections within and between sentences. \n\nA state-of-the-art model for abstractive text summarization is the Bidirectional and Auto-Regressive Transformer (BART) [16]. As the name suggests, BART employs a standard Transformer-based neural machine translation architecture with a bidirectional encoder and a left-to-right decoder.",
            "score": 0.5634936649377588,
            "section_title": "General Text Summarization",
            "char_start_offset": 3099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1939
                },
                {
                    "start": 1942,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "45793141"
                },
                {
                    "start": 1330,
                    "end": 1334,
                    "matchedPaperCorpusId": "14336759"
                },
                {
                    "start": 1632,
                    "end": 1636,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "15709889"
                },
                {
                    "start": 1845,
                    "end": 1849,
                    "matchedPaperCorpusId": "60514661"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "49557764",
            "title": "Modeling, comprehending and summarizing textual content by graphs",
            "text": "The PageRank centrality function is given by: \n\nwhere B u is the set containing all neighborhood of u and N v the number of neighborhoods of v. However, this strategy is normally employed with no restrictions that ensure an empty or a minimal intersection between sentences [10,28,6,27,2]. This lack of restrictions would increase the overall redundancy on these approaches. \n\nLexRank [10], which is a popular general-purpose extractive summarizer, relies on a graph representation of the document(s) building a complete graph, where each sentence from the document set becomes a node, and each edge weight is defined by the value of the cosine similarity between the sentences. Then the centrality index of each node is computed producing a ranking of vertices based on their importance, which indicates the ranking of the most relevant sentences to compose the summary. This well-known strategy is used as the basis for many recent unsupervised approaches [26,22,27,25]. Nevertheless, these approaches do not take into consideration the repetition problem that causes redundancy of words. Neither they present a conceptual model to meet the cross-domain summarization demands. Thus, next Section describes an unsupervised cross-domain summarization model and a post-processing algorithm to reduce repetition and improve the coverage of summaries.",
            "score": 0.5633891566079551,
            "section_title": "Background",
            "char_start_offset": 5914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 48,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1348
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "31010713"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "9409714"
                },
                {
                    "start": 286,
                    "end": 288,
                    "matchedPaperCorpusId": "25569070"
                },
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 958,
                    "end": 962,
                    "matchedPaperCorpusId": "195351652"
                },
                {
                    "start": 962,
                    "end": 965,
                    "matchedPaperCorpusId": "51615597"
                },
                {
                    "start": 965,
                    "end": 968,
                    "matchedPaperCorpusId": "9409714"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "26294259"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "218470557",
            "title": "HipoRank: Incorporating Hierarchical and Positional Information into Graph-based Unsupervised Long Document Extractive Summarization",
            "text": "We compare our approach with previous unsupervised and supervised models in extractive summarization. In addition, we also compare it with recent neural abstractive approaches for completeness. \n\nFor unsupervised extractive summarization models, we compare with SumBasic (Vanderwende et al., 2007), LSA (Steinberger and Jezek, 2004), LexRank (Erkan and Radev, 2004) and PACSUM (Zheng and Lapata, 2019). For supervised neural extractive summarization models, we compare with a vanilla encoder-decoder model (Cheng and Lapata, 2016), SummaRuNNer (Nallapati et al., 2017), GlobalLocalCont (Xiao and Carenini, 2019), Sent-CLF and Sent-PTR (Subramanian et al., 2019). We also compare with neural abstractive summarization models as reported in Xiao and Carenini (2019): Attn-Seq2Seq (Nallapati et al., 2016), Pntr-Gen-Seq2Seq (See et al., 2017) and Discourseaware (Cohan et al., 2018). In addition, we report the lead baseline that selects the first k tokens as a summary (k = 203, = 220 for PubMed and arXiv respectively). Lastly, we report baselines for an Oracle summarizer, which extracts gold standard summaries (Nallapati et al., 2017).",
            "score": 0.5621332330537236,
            "section_title": "Baselines",
            "char_start_offset": 14346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1137
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 297,
                    "matchedPaperCorpusId": "10496513"
                },
                {
                    "start": 303,
                    "end": 332,
                    "matchedPaperCorpusId": "14961089"
                },
                {
                    "start": 342,
                    "end": 365,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 377,
                    "end": 401,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 506,
                    "end": 530,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 544,
                    "end": 568,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 586,
                    "end": 611,
                    "matchedPaperCorpusId": "201697450"
                },
                {
                    "start": 739,
                    "end": 763,
                    "matchedPaperCorpusId": "201697450"
                },
                {
                    "start": 778,
                    "end": 802,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 821,
                    "end": 839,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 859,
                    "end": 879,
                    "matchedPaperCorpusId": "4894594"
                },
                {
                    "start": 1112,
                    "end": 1136,
                    "matchedPaperCorpusId": "6405271"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "218502581",
            "title": "An unsupervised semantic sentence ranking scheme for text documents",
            "text": "Unsupervised summarization methods exploit relations between words, as related words \"promote\" each other. For example, TextRank [4] and LexRank [22] each models a document as a sentence graph based on word relations, but they use only syntactic features. PageRank [23] is used to score words. \n\nOther methods incorporate additional information for achieving a higher accuracy. UniformLink [24], for example, constructs a sentence graph on a set of similar documents, where a sentence is scored based on both of the in-document score and crossdocument score. URank [25], on the other hand, uses a unified graph-based framework to study both single-document and multi-document summarization. \n\nThe quality of a summary may be improved using max-margin methods [26] or integer-linear programming (ILP) [27,?]. Among the previous algorithms, CP 3 [28] offers the highest ROUGE-1, ROUGE-2, and ROUGE-SU4 scores over DUC-02. It uses a bipartite graph to represent a document, and a different algorithm, Hyperlink-Induced Topic Search (HITS) [29], is used to score sentences. CP 3 treats the summarization problem as an ILP problem, which maximizes the sentence importance, non-redundancy, and coherence simultaneously. However, since solving ILP is NP-hard, obtaining an exact solution to an ILP problem is intractable. \n\nIt is worth noting that a recent unsupervised method [30], while not related to the problem studied in this paper, may provide new ideas. On a collec-tion of consumer reviews on a particular product, the method generates an abstractive sentence as the main review point, and ranks sentences by the number of descendants in a discourse tree rooted on the main review point. It would be interesting to investigate if this method can be modified to rank sentences of a given document according to their relative importance. \n\nEarly unsupervised methods have two common downsides: \n\n1. They don't promote diversity.",
            "score": 0.5619057295832535,
            "section_title": "Unsupervised methods",
            "char_start_offset": 8441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1928
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 149,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "207178257"
                },
                {
                    "start": 565,
                    "end": 569,
                    "matchedPaperCorpusId": "17224077"
                },
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "1792271"
                },
                {
                    "start": 844,
                    "end": 848,
                    "matchedPaperCorpusId": "2917544"
                },
                {
                    "start": 1036,
                    "end": 1040,
                    "matchedPaperCorpusId": "216000619"
                },
                {
                    "start": 1370,
                    "end": 1374,
                    "matchedPaperCorpusId": "189761982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "264146402",
            "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
            "text": "Extractive and abstractive summarization are two primary approaches to text summarization. Extractive summarization aims to identify and select the most pertinent sentences or phrases from the original text [108], whereas abstractive summarization creates novel sentences that rephrase and consolidate the key concepts of the source document [42]. \n\nFig. 1. The generated summary presented is the output of a \"unilm-base-cased\" model [11] that has been fine-tuned, whereas the extractive summary provided is the output of a \"distilbertbase-uncased\" model [157] that has also undergone fine-tuning. Both models were trained on the CNN/Daily Mail dataset. [38] The underlying assumption of extractive summarization is that the original text contains sentences that are sufficiently informative and well-formed to be included in the summary directly. Extractive summarization typically relies on techniques such as scoring sentences based on their relevance, importance, or position within the source text. A graph-based ranking model for text processing, called TextRank [115], is inspired by the PageRank algorithm used in web search. The model represents the input as a graph, where nodes are sentences or words, and edges represent the similarity between nodes. The algorithm iteratively scores nodes based on their connections, with higher-scoring nodes considered more important. Furthermore, LexRank [46], which is based on the concept of eigenvector centrality in a graph representation of the original text, is another unsupervised method for extractive summarization. On the other hand, Maximal Marginal Relevance (MMR) [23] addresses the problem of redundancy of extractive summaries by selecting sentences that are both relevant to the query and diverse from the already-selected sentences in the summary. The MMR algorithm iteratively selects sentences based on the combination of query relevance and novelty, considering the content of previously chosen sentences. \n\nInstead of merely selecting existing sentences, abstractive summarization creates new sentences that convey the key ideas in a more natural and fluid manner. This approach requires a deeper understanding of the text and more advanced natural language generation capabilities.",
            "score": 0.5619043397039953,
            "section_title": "Summarization Method: Extractive vs Abstractive",
            "char_start_offset": 6742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2253
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 212,
                    "matchedPaperCorpusId": "927278"
                },
                {
                    "start": 342,
                    "end": 346,
                    "matchedPaperCorpusId": "1729177"
                },
                {
                    "start": 1069,
                    "end": 1074,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1627,
                    "end": 1631,
                    "matchedPaperCorpusId": "4508623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6689453125
        },
        {
            "corpus_id": "254563804",
            "title": "Implementing Deep Learning-Based Approaches for Article Summarization in Indian Languages",
            "text": "Text summarization research dates back to 1958 when the first article on the topic [7] was published. Since then, numerous rule-based and deep learning-based techniques have been presented. Rule-based approaches include work done by Baxendale [21], which selects sentences for a summary based on word position and heading of the article, and that by Oliviera in 2016 [22], which used scoring criteria such as lexical similarity, sentence centrality, text rank, and so on for text summarization. \n\nResearch on deep-learning approaches for text summarization picked up the pace when encoder-decoder [23] and attention-based architectures [24] were proposed. Yu [25] suggested methods for creating one-sentence summaries of news stories that use recurrent neural network models like LSTM [26] and GRU [27], as well as with/without attention. In recent years, fine-tuning pre-trained models using domain-specific datasets has been the dominant paradigm in text summarization research. Pre-trained models which implement the BART [28], T5 [15], etc. architectures have been proposed, which are available in the Hugging Face library. Recent research includes the implementation of an importance-based ordering approach implemented by Zhao et al., a cascade approach to abstractive summarization with content selection and fusion proposed by Lebanoff et al [29]., and usage of prompt-based models such as GPT-3 [30], PaLM [31], T0 [32], etc. Many times, articles considered for summarization can be multidocument in nature. Wang et al. [33] suggested a task-specific architecture for multi-document summarization by combining numerous texts into a single graph. Zhong et al. [34] implemented a semantic-based framework for the same. \n\nIn the case of Hindi and Gujarati, there has been relatively little research on text summarization. K. Vimal Kumar et al. [35] suggested a graph-based method for summarising text in Hindi. Gulati et al. [36] developed a unique fuzzy inference method for summarising multi-source Hindi literature.",
            "score": 0.5617519595786183,
            "section_title": "Related Work",
            "char_start_offset": 5585,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 86,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "21241987"
                },
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "36306586"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1025,
                    "end": 1029,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1034,
                    "end": 1038,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1350,
                    "end": 1354,
                    "matchedPaperCorpusId": "222208724"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1424,
                    "end": 1428,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 1529,
                    "end": 1533,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1668,
                    "end": 1672,
                    "matchedPaperCorpusId": "215828313"
                },
                {
                    "start": 1850,
                    "end": 1854,
                    "matchedPaperCorpusId": "62473359"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52734375
        },
        {
            "corpus_id": "233189577",
            "title": "MultiHumES: Multilingual Humanitarian Dataset for Extractive Summarization",
            "text": "LEAD4 LEAD-n is an algorithm that selects the first n sentences from a document as its summary. It is a simple but strong baseline for extractive summarization models, created based on the assumption that the first sentences in a document are the most informative ones. In our experiments, we use n = 4 as it shows the best performance on the validation set. \n\nText Rank Text Rank is a graph-based model that ranks text units from most relevant to least relevant by using text units as vertices and the similarity between text units as edges. Given this ranking and a fixed length of the desired output summary, the model produces a summary in an unsupervised manner (Mihalcea and Tarau, 2004). \n\nNeuSum NeuSum is a neural extractive summarization method that employs a hierarchical document encoder to produce sentence representations. It also uses a sentence extractor to iteratively extract sentences from sentence representation extracts according to their overall contribution to the performance of the current summary (Qingyu et al., 2018). \n\nThe hierarchical document encoder is composed of two Recurrent Neural Networks (RNNs). The first network encodes each word in a sentence (sentence encoder), while the second network encodes each sentence given its context in the document (document encoder). Bidirectional Gated Recurrent Units (BiGRU) (Cho et al., 2014) are used for both RNNs.",
            "score": 0.5615171568475994,
            "section_title": "Baseline Models",
            "char_start_offset": 9034,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1393
                }
            ],
            "ref_mentions": [
                {
                    "start": 1024,
                    "end": 1045,
                    "matchedPaperCorpusId": "49656757"
                },
                {
                    "start": 1351,
                    "end": 1369,
                    "matchedPaperCorpusId": "5590763"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "52198214",
            "title": "Closed-Book Training to Improve Summarization Encoder Memory",
            "text": "Extractive and Abstractive Summarization: Early models for automatic text summarization were usually extractive (Jing and McKeown, 2000;Knight and Marcu, 2002;Clarke and Lapata, 2008;Filippova et al., 2015). For abstractive summarization, different early non-neural approaches were applied, based on graphs (Giannakopoulos, 2009;Ganesan et al., 2010), discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014;Wang et al., 2013), and a combination of linguistic compression and topic detection (Zajic et al., 2004). Recent neuralnetwork models have tackled abstractive summarization using methods such as hierarchical encoders and attention, coverage, and distraction (Rush et al., 2015;Chopra et al., 2016;Nallapati et al., 2016;Chen et al., 2016;Takase et al., 2016) as well as various initial large-scale, shortlength summarization datasets like DUC-2004 andGigaword. Nallapati et al. (2016) adapted the CNN/Daily Mail (Hermann et al., 2015) dataset for long-text summarization, and provided an abstractive baseline using attentional sequence-tosequence model. Pointer Network for Summarization: Pointer networks (Vinyals et al., 2015) are useful for summarization models because summaries often need to copy/contain a large number of words that have appeared in the source text. This provides the advantages of both extractive and abstractive ap-proaches, and usually includes a gating function to model the distribution for the extended vocabulary including the pre-set vocabulary and words from the source text (Zeng et al., 2016;Nallapati et al., 2016;Gu et al., 2016b;Gulcehre et al., 2016;Miao and Blunsom, 2016). See et al. (2017) used a soft gate to control model's behavior of copying versus generating. They further applied coverage mechanism and achieved the state-of-the-art results on CNN/Daily Mail dataset.",
            "score": 0.5614023567770473,
            "section_title": "Related Work",
            "char_start_offset": 9599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 136,
                    "matchedPaperCorpusId": "800331"
                },
                {
                    "start": 136,
                    "end": 159,
                    "matchedPaperCorpusId": "7793213"
                },
                {
                    "start": 159,
                    "end": 183,
                    "matchedPaperCorpusId": "3004447"
                },
                {
                    "start": 183,
                    "end": 206,
                    "matchedPaperCorpusId": "1992250"
                },
                {
                    "start": 329,
                    "end": 350,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 368,
                    "end": 389,
                    "matchedPaperCorpusId": "2767900"
                },
                {
                    "start": 413,
                    "end": 436,
                    "matchedPaperCorpusId": "9375250"
                },
                {
                    "start": 436,
                    "end": 454,
                    "matchedPaperCorpusId": "1260503"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "62613480"
                },
                {
                    "start": 694,
                    "end": 713,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 713,
                    "end": 733,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 733,
                    "end": 756,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 756,
                    "end": 774,
                    "matchedPaperCorpusId": "14288483"
                },
                {
                    "start": 774,
                    "end": 794,
                    "matchedPaperCorpusId": "5450302"
                },
                {
                    "start": 948,
                    "end": 970,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 1142,
                    "end": 1164,
                    "matchedPaperCorpusId": "5692837"
                },
                {
                    "start": 1562,
                    "end": 1585,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1585,
                    "end": 1602,
                    "matchedPaperCorpusId": "8174613"
                },
                {
                    "start": 1602,
                    "end": 1624,
                    "matchedPaperCorpusId": "969555"
                },
                {
                    "start": 1624,
                    "end": 1647,
                    "matchedPaperCorpusId": "10480989"
                },
                {
                    "start": 1649,
                    "end": 1666,
                    "matchedPaperCorpusId": "8314118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380859375
        },
        {
            "corpus_id": "182712867",
            "title": "Learning with fuzzy hypergraphs: A topical approach to query-oriented text summarization",
            "text": "We compare our MRC algorithm to four state-of-the-art graph or hypergraph-based summarizers. Unless stated otherwise, lexical similarity denotes the cosine similarity between tfisf representations of sentences as defined in [4]. Topic-sensitive LexRank (TS-LexRank) defines a graph in which an edge connects two sentences if they have nonzero lexical similarity [27]. Sentence scores are obtained through a query-biased PageRank algorithm: the score r i of sentence i is \n\nsim(i, q) j sim(j, q) \n\n+ (1 \u2212 \u03c9 1 ) j =i sim(i, j)r j l =i sim(i, l) (37) in which \u03c9 1 \u2208]0, 1[ is a parameter whose value is set to 0.95, as in [27]. \n\nThe second method [37], based on Hubs and Authorities algorithm, first discovers sentence clusters by applying agglomerative clustering to tfisf representations of sentences. A bipartite graph is then formed in which sentences and clusters represent vertices and edges have weights corresponding to their lexical similarities. HITS algorithm is then applied to rank both sentences (considered as authorities) and clusters (considered as hubs) based on the iterative formulas r i = l sim(i, l)q l , q l = i sim(i, l)r i (38) where r i is the score of i-th sentence and q l is the score of l-th cluster. To produce queryoriented summaries, we restrict the sentence set to the top 10% of sentences relevant to the query, as suggested in [39]. \n\nHyperSum is a hypergraph-based text summarizer [39]. It first applies DBSCAN algorithm to detect clusters of lexically similar sentences. A hypergraph is built in which each cluster defines a hyperedge connecting sentences of the cluster. Sentence scores are obtained by applying a semi-supervised learning algorithm in which query relevance scores are propagated across the hypergraph. \n\nHERF builds on a similar principle but it includes an initial topic modelling step in which topics are extracted from sentences using a topic model [41].",
            "score": 0.5611772823299417,
            "section_title": "Comparison with other graph-based summarization algorithms",
            "char_start_offset": 70847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "10686299"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "10274362"
                },
                {
                    "start": 543,
                    "end": 547,
                    "matchedPaperCorpusId": "9849366"
                },
                {
                    "start": 618,
                    "end": 622,
                    "matchedPaperCorpusId": "10274362"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "9849366"
                },
                {
                    "start": 1360,
                    "end": 1364,
                    "matchedPaperCorpusId": "27105029"
                },
                {
                    "start": 1415,
                    "end": 1419,
                    "matchedPaperCorpusId": "27105029"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.640625
        },
        {
            "corpus_id": "49656757",
            "title": "Neural Document Summarization by Jointly Learning to Score and Select Sentences",
            "text": "Traditional approaches to automatic text summarization focus on identifying important content, usually at sentence level (Nenkova and McKeown, 2011). With the identified important sentences, a summarization system can extract them to form an output summary. In recent years, extractive methods for summarization have proven effective in many systems (Carbonell and Goldstein, 1998;Mihalcea and Tarau, 2004;McDonald, 2007;Cao et al., 2015a). In previous works that use extractive methods, text summarization is decomposed into two subtasks, i.e., sentence scoring and sentence selection. \n\nSentence scoring aims to assign an importance score to each sentence, and has been broadly studied in many previous works. Feature-based methods are popular and have proven effective, such as word probability, TF*IDF weights, sentence position and sentence length features (Luhn, 1958;Hovy and Lin, 1998;Ren et al., 2017). Graph-based methods such as TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) measure sentence importance using weighted-graphs. In recent years, neural network has also been applied to sentence modeling and scoring (Cao et al., 2015a;Ren et al., 2017). \n\nFor the second step, sentence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary. Integer Linear Programming based methods (McDonald, 2007) treat sentence selection as an optimization problem under some constraints such as summary length. Submodular functions (Lin and Bilmes, 2011) have also been applied to solving the optimization problem of finding the optimal subset of sentences in a document. Ren et al. (2016) train two neural networks with handcrafted features. One is used to rank sentences, and the other one is used to model redundancy during sentence selection. \n\nIn this paper, we present a neural extractive document summarization (NEUSUM) framework which jointly learns to score and select sentences.",
            "score": 0.561100828805145,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 148,
                    "matchedPaperCorpusId": "47179013"
                },
                {
                    "start": 350,
                    "end": 381,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 381,
                    "end": 406,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 406,
                    "end": 421,
                    "matchedPaperCorpusId": "2798452"
                },
                {
                    "start": 421,
                    "end": 439,
                    "matchedPaperCorpusId": "10675728"
                },
                {
                    "start": 862,
                    "end": 874,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 893,
                    "end": 910,
                    "matchedPaperCorpusId": "4500716"
                },
                {
                    "start": 949,
                    "end": 975,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 988,
                    "end": 1010,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1150,
                    "end": 1169,
                    "matchedPaperCorpusId": "10675728"
                },
                {
                    "start": 1169,
                    "end": 1186,
                    "matchedPaperCorpusId": "4500716"
                },
                {
                    "start": 1326,
                    "end": 1357,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 1535,
                    "end": 1551,
                    "matchedPaperCorpusId": "2798452"
                },
                {
                    "start": 1672,
                    "end": 1694,
                    "matchedPaperCorpusId": "320371"
                },
                {
                    "start": 1812,
                    "end": 1829,
                    "matchedPaperCorpusId": "17851936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "253762013",
            "title": "SAPGraph: Structure-aware Extractive Summarization for Scientific Papers with Heterogeneous Graph",
            "text": "Although some of the above works value the function of facet structure, the majority of them rely on manual selection, which lacks universality and may also result in the loss of supporting information. In contrast, graph-based models are more flexible and can build connections between long-span texts. \n\nEarly works like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) predict sentence centrality of a document graph. Recently, more well-designed graph-based methods consider the structure information, such as PACSUM (Zheng and Lapata, 2019), HIPO-RANK (Dong et al., 2021), FAR (Liang et al., 2021), etc. To rank sentences, they fuse together such information as hierarchical structure, sentence position, and sentence similarity. GNN (Kipf and Welling, 2016;Veli\u010dkovi\u0107 et al., 2018) can learn nodes representation with neural networks. Heterogeneous graph methods (Huang and Kurohashi, 2021;Wang et al., 2020a;Yasunaga et al., 2017) can consider more diverse information with multitype nodes and edges. In graph-based works, HET-ERSUMGRAPH (HSG) (Wang et al., 2020a) is comparable to our SAPGraph, but SAPGraph takes into account the structure of facets and knowledge in the paper, making it a better graph prior to paper summarizing.",
            "score": 0.5605058922254641,
            "section_title": "Related work",
            "char_start_offset": 7258,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1262
                }
            ],
            "ref_mentions": [
                {
                    "start": 331,
                    "end": 354,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 368,
                    "end": 394,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 544,
                    "end": 568,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 580,
                    "end": 599,
                    "matchedPaperCorpusId": "231592914"
                },
                {
                    "start": 605,
                    "end": 625,
                    "matchedPaperCorpusId": "236477774"
                },
                {
                    "start": 892,
                    "end": 919,
                    "matchedPaperCorpusId": "233189611"
                },
                {
                    "start": 919,
                    "end": 938,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1074,
                    "end": 1093,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "258866124",
            "title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
            "text": "We use LEAD, LexRank (Erkan and Radev, 2004), ext-oracle, Big Bird (Zaheer et al., 2020), and FiD (Izacard and Grave, 2021) as the baseline methods. \n\nLEAD-k selects the first k sentences from each input document and concatenates them as a summary. LexRank is a graph-based unsupervised extractive method that considers a graph in which the sentences are nodes, and the similarities between the sentences are edges. It calculates the importance of sentences using PageRank algorithm (PAGE, 1998) and extracts the top l sentences with high importance as a summary. Ext-oracle greedily selects l sentences that maximize the ROUGE-2 scores between the selected sentences and the target summary. Its results show the upper bound of an extractive system on SciReviewGen. We set k = 1 and l = 5 such that the average summary length is the same as that of the abstractive models. \n\nBig Bird simplifies the self-attention computation in the Transformer using the sparse attention mechanism, supporting longer inputs of up to approximately 16K tokens. In our experiments, we use the model that was fine-tuned for summarization on arXiv dataset (Cohan et al., 2018) 1 . We further fine-tuned it on SciReviewGen. FiD is a Transformer encoder-decoder model that allows multiple documents to be input. As shown in the upper part of Figure 2, FiD separately encodes multiple documents and concatenates their hidden states. The hidden states are then input into the decoder together, which enables multiple documents to be simultaneously processed while capturing the relations among documents. In our experiment, we initialized the weights of FiD using the BART-Large model (Lewis et al., 2020) fine-tuned for summarization on the CNN/Daily Mail dataset2 , and further fine-tuned it on SciReviewGen. \n\nNote that we also evaluated the performance of GPT-3 model davinci (Brown et al., 2020) on SciReviewGen with the prompt \"Summarize the above scientific papers focusing on the title and chapter title.\"",
            "score": 0.5600954019314079,
            "section_title": "Baseline Methods",
            "char_start_offset": 16231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1785
                },
                {
                    "start": 1788,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 44,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 67,
                    "end": 88,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 98,
                    "end": 123,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 483,
                    "end": 495,
                    "matchedPaperCorpusId": "1508503"
                },
                {
                    "start": 1135,
                    "end": 1155,
                    "matchedPaperCorpusId": "4894594"
                },
                {
                    "start": 1660,
                    "end": 1680,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66259765625
        },
        {
            "corpus_id": "220633461",
            "title": "SummPip: Unsupervised Multi-Document Summarization with Sentence Graph Compression",
            "text": "Non-neural approaches tend to make the use of domain expertise in developing summarization systems [2]. In particular, current unsupervised approaches focus on building graphs to blend sentences from different documents and leveraging correlations among documents to extract the most representative sentences [6]. Yet, they are limited in incorporating finer linguistic information. \n\nGiven that documents in MDS come from different sources, they are redundant and repetitive in expressing ideas, while summaries contain only a few key points. We believe semantic clusters can be formed according to the distances between sentences, and then each cluster can be compressed into a single sentence representing salient content. In this paper, we propose SummPip: an unsupervised multi-document summarization approach based on sentence graph compression. To our best knowledge, we are the first to apply sentence compression to MDS. Our main contributions are: \n\n\u2022 SummPip is the first unsupervised summarization method which constructs sentence graphs by incorporating both linguistic knowledge and deep neural representations. It assumes that a summary sentence can be created by compressing a within-graph cluster. \n\n\u2022 We perform both automatic evaluation and human evaluation on two MDS benchmark datasets. SummPip shows significantly better performance than other unsupervised approaches and is compared favourably to the latest supervised neural models. \n\n2 SummPip: Automatic pipeline for Multi-Document Summarization \n\nOur pipeline consists of four major steps: i) Conduct document processing. ii) Build a structured sentence graph, where the nodes correspond to the sentences generated at step one and the edges are drawn based on both the lexical and the deep semantic relations between sentences. iii) Apply graph clustering to get within-graph partitions. iv) Generate summary texts from the extracted sub-graphs. Figure 1 illustrates our pipeline for unsupervised MDS. We specify each step in the following subsections3 .",
            "score": 0.5600480584991983,
            "section_title": "Introduction",
            "char_start_offset": 2233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1521
                },
                {
                    "start": 1524,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2031
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "201666437"
                },
                {
                    "start": 309,
                    "end": 312,
                    "matchedPaperCorpusId": "337730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "53083054",
            "title": "Content Selection in Deep Learning Models of Summarization",
            "text": "The introduction of the CNN-DailyMail corpus by Hermann et al. (2015) allowed for the application of large-scale training of deep learning models for summarization. Cheng and Lapata (2016) developed a sentence extractive model that uses a word level CNN to encode sentences and a sentence level sequence-to-sequence model to predict which sentences to include in the summary. Subsequently, Nallapati et al. (2017) proposed a different model using word-level bidirectional RNNs along with a sentence level bidirectional RNN for predicting which sentences should be extracted. Their sentence extractor creates representations of the whole document and computes separate scores for salience, novelty, and location. These works represent the state-of-the-art for deep learningbased extractive summarization and we analyze them further in this paper. \n\nOther recent neural network approaches include, Yasunaga et al. (2017), who learn a graphconvolutional network (GCN) for multi-document summarization. They do not closely examine the choice of sentence encoder, which is one of the focuses of the present paper; rather, they study the best choice of graph structure for the GCN, which is orthogonal to this work. \n\nNon-neural network learning-based approaches have also been applied to summarization. Typically they involve learning n-gram feature weights in linear models along with other non-lexical word or structural features (Berg-Kirkpatrick et al., 2011;Sipos et al., 2012;Durrett et al., 2016). In this paper, we study representation learning in neural networks that can capture more complex word level feature interactions and whose dense representations are more compatible with current practices in NLP. \n\nThe previously mentioned works have focused on news summarization. To further understand the content selection process, we also explore other domains of summarization. In particular, we explore personal narrative summarization based on stories shared on Reddit (Ouyang et al., 2017), workplace meeting summarization (Carletta et al., 2005), and medical journal article summarization (Mishra et al., 2014). \n\nWhile most work on these summarization tasks often exploit domain-specific features (e.g.",
            "score": 0.5600386128135383,
            "section_title": "Related Work",
            "char_start_offset": 2920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2119
                },
                {
                    "start": 2122,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 69,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 390,
                    "end": 413,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1427,
                    "end": 1458,
                    "matchedPaperCorpusId": "15467396"
                },
                {
                    "start": 1458,
                    "end": 1477,
                    "matchedPaperCorpusId": "12718597"
                },
                {
                    "start": 1975,
                    "end": 1996,
                    "matchedPaperCorpusId": "6002018"
                },
                {
                    "start": 2030,
                    "end": 2053,
                    "matchedPaperCorpusId": "6118869"
                },
                {
                    "start": 2097,
                    "end": 2118,
                    "matchedPaperCorpusId": "22257046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6669921875
        },
        {
            "corpus_id": "271213718",
            "title": "Toward Regulatory Compliance: A few-shot Learning Approach to Extract Processing Activities",
            "text": "Extractive summarization extracts text segments from an input text, capturing essential information related to the key concepts mentioned in the input text [45], [46].Unlike abstractive summarization, which rephrases the information, extractive summarization preserves the original text, ensuring factual accuracy and clarity [47].This characteristic makes it particularly suitable for factual text like our work, where preserving precise details about processing activities is crucial.\n\nJadhav et al. introduced SWAP-NET, a sequence-tosequence model for extractive summarization [48].SWAP-NET is designed to identify salient sentences and keywords within a document, combining them to create an extractive summary.Similarly, Nallapati et al. developed a method using an RNN-based binary classifier to determine whether a sentence should be included in the summary [49].This method relies on the sentence's content, its significance within the document, its novelty compared to previously selected sentences, and additional positional features.Expanding on these approaches, Yadav et al. proposed a textual graph-based technique [50], where document sentences form the nodes of a graph, with edges representing associations between sentences.The summary is generated based on the sentence weight and the average weight of the textual graph.Diverging from the neural network and graph-based methodologies, Mishra et al. approached summarization as a question-answering task [51].They leveraged LLMs to generate pseudo-labels for dialogue, which were then used to fine-tune a chat summarization model, effectively transferring knowledge from a large LLM to a more specialized, smaller model.In contrast to these works, our method conceptualizes extractive summarization as an eventargument extraction task.By leveraging LLMs, we generate summaries of scenarios that are both more precise and less cluttered with irrelevant information.",
            "score": 0.5590430053724771,
            "section_title": "D. Extractive Summarization",
            "char_start_offset": 9794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 331
                },
                {
                    "start": 331,
                    "end": 486
                },
                {
                    "start": 488,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 870
                },
                {
                    "start": 870,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1478
                },
                {
                    "start": 1478,
                    "end": 1689
                },
                {
                    "start": 1689,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "21723747"
                },
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "198189946"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "51870490"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1129,
                    "end": 1133,
                    "matchedPaperCorpusId": "260185439"
                },
                {
                    "start": 1473,
                    "end": 1477,
                    "matchedPaperCorpusId": "266176105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "38851848",
            "title": "Automated text summarisation and evidence-based medicine: A survey of two domains",
            "text": "MMR was initially utilised for document retrieval and is given by the following formula: \n\nwhere, as explained by Carbonell and Goldstein [1998]: \"Q is a query; R = IR(C, Q, \u03b8), i.e., the ranked list of documents retrieved by an IR system given a document collection C and a relevance threshold \u03b8; S is the subset of documents in R already selected; R\\S is the set difference, i.e., the set of unselected documents in R; Sim 1 is the similarity metric used in document retrieval and relevance ranking between documents and a query; and Sim 2 can be the same or a different metric.\" \n\nGraph-based approaches have also been applied to text summarisation [Mani and Bloedorn 1997;Mani and Maybury 1999;Erkan and Radev 2004;Leskovec et al. 2005], with Mani and Bloedorn [1997] being the pioneers in this area. In their approach, the authors use nodes to represent words and edges between nodes represent relationships. The summaries generated can be topic driven, and there is no text in the summaries. Instead, the summary content is represented via as nodes and edges that represent contents and relations between them. When summarising a pair of documents, common nodes represent same words or synonyms, while difference nodes are those that are not common. Sentence selection from the graph is computed from the average activated weights of the covered words: for a sentence s, its score in terms of coverage of common nodes is given by the following formula: \n\nwhere c(s) = {w|w \u2208 Common s}. The score for differences is calculated similarly. The sentences with higher common and difference scores are selected for the final summary. Erkan and Radev [2004] presented the LexRank system for multi-document summarisation, which is a graph-based system using a connected, undirected graph to represent documents. A similar method, suitable for single-document summarisation only, was proposed by Mihalcea and Tarau [2004]. Other graph based approaches have been proposed, both in the medical domain [Reeve et al. 2007;Fiszman et al. 2004] and outside it [Litvak and Last 2008].",
            "score": 0.5584698788811178,
            "section_title": "3.3.3",
            "char_start_offset": 41068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 91,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 144,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 698,
                    "end": 719,
                    "matchedPaperCorpusId": "2962090"
                },
                {
                    "start": 719,
                    "end": 740,
                    "matchedPaperCorpusId": "16136677"
                },
                {
                    "start": 1634,
                    "end": 1656,
                    "matchedPaperCorpusId": "2962090"
                },
                {
                    "start": 1893,
                    "end": 1918,
                    "matchedPaperCorpusId": "14130036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "49210924",
            "title": "Abstract Meaning Representation for Multi-Document Summarization",
            "text": "Neural abstractive summarization has sparked great interest in recent years. These approaches focus primarily on short text summarization and single-document summarization (Rush et al., 2015;Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce word repetitions (See et al., 2017;Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016;Zhou et al., 2017;Tan et al., 2017), control the summary length (Kikuchi et al., 2016), reduce the occurrence of out-of-vocabulary tokens in summaries (See et al., 2017), improve the learning objective and search (Ranzato et al., 2016;Huang et al., 2017), and generate summaries that are true to the original inputs (Cao et al., 2018;Song et al., 2018). Training neural models generally requires large amounts of data; they are often acquired by pairing news articles with titles or human-written highlights. Nonetheless, obtaining parallel data for multi-document summarization is often costly. There is thus a need to investigate alternative approaches that are less data-thirsty. \n\nAbstractive summarization via natural language generation (NLG, Reiter and Dale, 2000;Gatt and Krahmer, 2018) is a promising line of work. The approaches often identify salient text units from source documents, arrange them in a compact form, such as domain-specific templates, and subsequently synthesize them into natural language texts (Barzilay et al., 1999;Genest and Lapalme, 2011;Oya et al., 2014;Gerani et al., 2014;Fabbrizio et al., 2014). A challenge faced by these approaches is that there lacks a principled means of content representation. This paper studies the feasibility of using AMR, a semantic formalism grounded in linguistic theory, for content representation. Within this framework, condensing source documents to summary AMR graphs and generating natural language sentences from summary graphs are both data-driven and not specifically designed for any domain.",
            "score": 0.5584697421195146,
            "section_title": "Related Work",
            "char_start_offset": 4611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 214,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 315,
                    "end": 333,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 333,
                    "end": 357,
                    "matchedPaperCorpusId": "1207251"
                },
                {
                    "start": 391,
                    "end": 410,
                    "matchedPaperCorpusId": "12755643"
                },
                {
                    "start": 410,
                    "end": 428,
                    "matchedPaperCorpusId": "1770102"
                },
                {
                    "start": 428,
                    "end": 445,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 474,
                    "end": 496,
                    "matchedPaperCorpusId": "11157751"
                },
                {
                    "start": 561,
                    "end": 579,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 623,
                    "end": 645,
                    "matchedPaperCorpusId": "7147309"
                },
                {
                    "start": 645,
                    "end": 664,
                    "matchedPaperCorpusId": "30601989"
                },
                {
                    "start": 726,
                    "end": 744,
                    "matchedPaperCorpusId": "19198109"
                },
                {
                    "start": 744,
                    "end": 762,
                    "matchedPaperCorpusId": "46936631"
                },
                {
                    "start": 1181,
                    "end": 1203,
                    "matchedPaperCorpusId": "16946362"
                },
                {
                    "start": 1434,
                    "end": 1457,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 1457,
                    "end": 1482,
                    "matchedPaperCorpusId": "4942873"
                },
                {
                    "start": 1482,
                    "end": 1499,
                    "matchedPaperCorpusId": "12299544"
                },
                {
                    "start": 1499,
                    "end": 1519,
                    "matchedPaperCorpusId": "2767900"
                },
                {
                    "start": 1519,
                    "end": 1542,
                    "matchedPaperCorpusId": "12682781"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "53295957",
            "title": "Abstractive Summarization of Reddit Posts with Multi-level Memory Networks",
            "text": "Our work can be uniquely positioned in the context of the following three topics. \n\nNeural Abstractive Summarization. Many deep neural network models have been proposed for abstractive summarization. One of the most dominant architectures is to employ RNN-based seq2seq models with attention mechanism such as (Rush et al., 2015;Chopra et al., 2016;Nallapati et al., 2016;Cohan et al., 2018;Hsu et al., 2018;Gehrmann et al., 2018). In addition, recent advances in deep network research have been promptly adopted for improving abstractive summarization. Some notable examples include the use of variational autoencoders (VAEs) (Miao and Blunsom, 2016;Li et al., 2017), graph-based attention (Tan et al., 2017), pointer-generator models (See et al., 2017), self-attention networks (Liu et al., 2018), reinforcement learning (Paulus et al., 2018;Pasunuru and Bansal, 2018), contextual agent attention (Celikyilmaz et al., 2018) and integration with extractive models (Hsu et al., 2018;Gehrmann et al., 2018). \n\nCompared to existing neural methods of abstractive summarization, our approach is novel to replace an RNN-based encoder with explicit multi-level convolutional memory. While RNNbased encoders always consider the whole sequence to represent each hidden state, our multilevel memory network exploits convolutions to control the extent of representation in multiple levels of sentences, paragraphs, and the whole text. \n\nSummarization Datasets. Most existing summarization datasets use formal documents as source text. News articles are exploited the most, including in DUC (Over et al., 2007), Gigaword (Napoles et al., 2012), CNN/DailyMail (Nallapati et al., 2016;Hermann et al., 2015), Newsroom (Grusky et al., 2018) and XSum (Narayan et al., 2018a) datasets. Cohan et al. (2018)  and PubMed.",
            "score": 0.5582973900374024,
            "section_title": "Related Work",
            "char_start_offset": 5442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 84,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1801
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 329,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 329,
                    "end": 349,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 349,
                    "end": 372,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 372,
                    "end": 391,
                    "matchedPaperCorpusId": "4894594"
                },
                {
                    "start": 391,
                    "end": 408,
                    "matchedPaperCorpusId": "21723747"
                },
                {
                    "start": 408,
                    "end": 430,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 627,
                    "end": 651,
                    "matchedPaperCorpusId": "10480989"
                },
                {
                    "start": 651,
                    "end": 667,
                    "matchedPaperCorpusId": "1508909"
                },
                {
                    "start": 691,
                    "end": 709,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 736,
                    "end": 754,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 844,
                    "end": 870,
                    "matchedPaperCorpusId": "4940548"
                },
                {
                    "start": 899,
                    "end": 925,
                    "matchedPaperCorpusId": "4406182"
                },
                {
                    "start": 965,
                    "end": 983,
                    "matchedPaperCorpusId": "21723747"
                },
                {
                    "start": 983,
                    "end": 1005,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1672,
                    "end": 1693,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 1704,
                    "end": 1725,
                    "matchedPaperCorpusId": "13752552"
                },
                {
                    "start": 1735,
                    "end": 1758,
                    "matchedPaperCorpusId": "215768182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.344970703125
        },
        {
            "corpus_id": "16812271",
            "title": "Complex Question Answering: Unsupervised Learning Approaches and Experiments",
            "text": "Erkan and Radev (2004) used the concept of graph-based centrality to rank a set of sentences for producing generic multi-document summaries. A similarity graph is produced for the sentences in the document collection. In the graph each node represents a sentence. The edges between nodes measure the cosine similarity between the respective pair of sentences. \n\nThe degree of a given node is an indication of how important the sentence is. Figure 5 shows an example of a similarity graph for 4 sentences. \n\nOnce the similarity graph is constructed, the sentences are ranked according to their eigenvector centrality. The LexRank performed well in the context of generic summarization. To apply LexRank to query-focused context a topic-sensitive version of LexRank is proposed by Otterbacher et al. (2005). We followed a similar approach in order to calculate this feature. The score of a sentence is determined by a mixture model of the relevance of the sentence to the query and the similarity of the sentence to other high-scoring sentences. We first stem out all the sentences in the collection and compute the word IDFs (Inverse Document Frequency) using the following formula: \n\nWhere N is the total number of sentences in the cluster, and sf w is the number of sentences that the word w appears in. \n\nWe also stem out the questions and remove the stop words. The relevance of a sentence s to the question q is computed by: rel(s|q) = w\u2208q log (tf w,s + 1) \u00d7 log (tf w,q + 1) \u00d7 idf w (28) \n\nWhere tf w,s and tf w,q are the number of times w appears in s and q, respectively.",
            "score": 0.5570149482161889,
            "section_title": "Graph-based Similarity Measure",
            "char_start_offset": 52619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1578
                }
            ],
            "ref_mentions": [
                {
                    "start": 779,
                    "end": 804,
                    "matchedPaperCorpusId": "10274362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "272831675",
            "title": "GLARE: Guided LexRank for Advanced Retrieval in Legal Analysis",
            "text": "We adopted an extractive summarization approach to avoid possible hallucinations. In this way, when working with terms and structures that are, in fact, in the original text, we aim to preserve their semantics and avoid losses in the next stage of the method. \n\nAs described in Section 2.2, LexRank represents text as a weighted graph, where nodes represent sentences and edges represent the similarity between sentences. The weights associated with nodes are related to the degree of centrality of each sentence. Considering the context of this work, when we send the text of a special appeal to LexRank, the algorithm outputs a summary, which is formed by the set of sentences that best represent the appeal. We should consider that a sentence not included in the summary might still be important for classifying the special appeal into a theme. LexRank can ignore a sentence because it calculates the centrality of each sentence without accounting for any external factors beyond the text itself. Otterbacher et al. (2009) present a version of the LexRank algorithm that considers bias based on a topic of interest. An approach is proposed to introduce bias that starts from the general LexRank equation and adds a priority term based on a weight function. According to the similarity that a given sentence in the text has with a previously defined topic, the sentence will receive a weight that will influence the composition of the final summary of the text. \n\nWe propose a change to the LexRank algorithm to guide the summarization task. With the proposed change, the decision for a sentence to compose the summary of the text is the result of a combination of two factors. One of the factors is internal to the text, that is, the degree of centrality of the sentence calculated in the original LexRank algorithm. The other factor is external to the text, calculated according to the similarity between each sentence to be summarized and the texts of the themes in this work. This way, in Guided LexRank, for each sentence s, the following steps are performed: \n\n1. Compute the degree centrality \u03b3 s in the same way as it is calculated in LexRank. \n\n2. Send each sentence s as input to the algorithm BM25 targeting each theme t \u2208 T . Store the highest score found when running BM25, \u03c3 st . 3. Calculate the combined score using two weighting factors (see Equation 1).",
            "score": 0.5568274473307072,
            "section_title": "Summarization Through Guided LexRank",
            "char_start_offset": 24826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 259
                },
                {
                    "start": 262,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2066
                },
                {
                    "start": 2069,
                    "end": 2153
                },
                {
                    "start": 2156,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2295
                },
                {
                    "start": 2296,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 1000,
                    "end": 1025,
                    "matchedPaperCorpusId": "17636912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.484619140625
        },
        {
            "paperId": "9b7c95c2ee916ba94127e4b18945727c2e218d08",
            "corpusId": 274470988,
            "title": "Extractive Multi-Document Summarization Using Transformer-Based Neural Networks",
            "venue": "International Conference on Signals and Electronic Systems",
            "year": 2024,
            "referenceCount": 37,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSES63445.2024.10763280?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSES63445.2024.10763280, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2262020475",
                    "name": "Sunilkumar Ketineni"
                },
                {
                    "authorId": "2292738433",
                    "name": "Sheela J"
                }
            ],
            "abstract": "The management and extraction of pertinent information from sizable text collections depends critically on multi-document summarization. Using Transformer-based neural networks, this research describes a technical method for extractive multi-document summarization. To capture the important information across many publications, we suggest a unique architecture that makes use of self-attention processes and contextualized word representations. To improve the summary generation process, the model is trained using a combination of supervised learning and reinforcement learning methods. We run thorough tests on benchmark datasets and evaluate our method against current state-of-the-art techniques. The outcomes show how well our suggested model performs in producing informative and cogent summaries from a variety of documents.",
            "corpus_id": "274470988",
            "text": "The management and extraction of pertinent information from sizable text collections depends critically on multi-document summarization. Using Transformer-based neural networks, this research describes a technical method for extractive multi-document summarization. To capture the important information across many publications, we suggest a unique architecture that makes use of self-attention processes and contextualized word representations. To improve the summary generation process, the model is trained using a combination of supervised learning and reinforcement learning methods. We run thorough tests on benchmark datasets and evaluate our method against current state-of-the-art techniques. The outcomes show how well our suggested model performs in producing informative and cogent summaries from a variety of documents.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.60888671875
        },
        {
            "paperId": "fe6725afcbe32559a9f71bcfc54aae4594851c0f",
            "corpusId": 271730061,
            "title": "Document Summarization Using Multi-Objective Weighted Graph Embedding Feature and Optimized Rank Based Deep Classifier",
            "venue": "2024 International Conference on Expert Clouds and Applications (ICOECA)",
            "year": 2024,
            "referenceCount": 28,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICOECA62351.2024.00041?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICOECA62351.2024.00041, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "70641465",
                    "name": "Samina Mulla"
                },
                {
                    "authorId": "3375862",
                    "name": "N. Shaikh"
                }
            ],
            "abstract": "The necessity for automatic text summarizing techniques is growing in the modern day. The primary goal of extractive multi-document summarizing techniques is to eliminate unnecessary information while simultaneously obtaining the essential content of a group of documents. Multi objective weighted graph embedding feature with an Optimized rank based deep Convolutional Neural Network-Bidirectional Long Short Term Memory classifier (MOWGEF+rank deep CNN-BiLSTM model) is developed by this study for multi-document text summarization. The process of text summarization begins with two stages preprocess document to prepare for input as a multi-document summary and extract multi-objective weighted graph embedding features from its content. The profound achievement of this paper is to regard weighing model as a weighting for semantic information of word, phrase and sentence relationships. This model also has a conventional entropy-based feature selection technique that aids in finding significant variables. For the optimized rank-based deep classifier model these attributes are utilized as the input which captures the document intricate details. The optimized rank-based deep classifier's novel layer assesses words according to the significance they have become in a document. This arranging takes place using a novel optimization method that generates sentence ranking, demonstrating a unique and effective approach to multi-document summarizing. Elytron Vespid-B optimization (EVO) algorithm essentially takes inspiration from the foraging behaviors of insects, fine-tune the classifier's parameters to optimize its performance. Based on the achievements the model attains high recall 1, recall 2 and recall L values of 0.77, 0.39 and 0.75 respectively for DUC 2002 database.",
            "corpus_id": "271730061",
            "text": "The necessity for automatic text summarizing techniques is growing in the modern day. The primary goal of extractive multi-document summarizing techniques is to eliminate unnecessary information while simultaneously obtaining the essential content of a group of documents. Multi objective weighted graph embedding feature with an Optimized rank based deep Convolutional Neural Network-Bidirectional Long Short Term Memory classifier (MOWGEF+rank deep CNN-BiLSTM model) is developed by this study for multi-document text summarization. The process of text summarization begins with two stages preprocess document to prepare for input as a multi-document summary and extract multi-objective weighted graph embedding features from its content. The profound achievement of this paper is to regard weighing model as a weighting for semantic information of word, phrase and sentence relationships. This model also has a conventional entropy-based feature selection technique that aids in finding significant variables. For the optimized rank-based deep classifier model these attributes are utilized as the input which captures the document intricate details. The optimized rank-based deep classifier's novel layer assesses words according to the significance they have become in a document. This arranging takes place using a novel optimization method that generates sentence ranking, demonstrating a unique and effective approach to multi-document summarizing. Elytron Vespid-B optimization (EVO) algorithm essentially takes inspiration from the foraging behaviors of insects, fine-tune the classifier's parameters to optimize its performance. Based on the achievements the model attains high recall 1, recall 2 and recall L values of 0.77, 0.39 and 0.75 respectively for DUC 2002 database.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.60595703125
        },
        {
            "paperId": "7f531adbc09f3ee20fccebc2a6890813b2e61425",
            "corpusId": 277902007,
            "title": "Graph-based and generative approaches to multi-document summarization",
            "venue": "Journal of Computer Science and Cybernetics",
            "year": 2024,
            "referenceCount": 50,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.15625/1813-9663/18353?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.15625/1813-9663/18353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2101316144",
                    "name": "Tam Doan Thanh"
                },
                {
                    "authorId": "2191810014",
                    "name": "Tan-Minh Nguyen"
                },
                {
                    "authorId": "2287676414",
                    "name": "Thai-Binh Nguyen"
                },
                {
                    "authorId": "2239461326",
                    "name": "Hoang-Trung Nguyen"
                },
                {
                    "authorId": "2239461322",
                    "name": "Hai-Long Nguyen"
                },
                {
                    "authorId": "2238664337",
                    "name": "Mai-Vu Tran"
                },
                {
                    "authorId": "2356087531",
                    "name": "Quang Thuy Ha"
                },
                {
                    "authorId": "2356095240",
                    "name": "Ha Thanh Nguyen"
                }
            ],
            "abstract": "Multi-document summarization is a challenging problem in the Natural Language Processing field that has drawn a lot of interest from the research community. In this paper, we propose a two-phase pipeline to tackle the Vietnamese abstractive multi-document summarization task. The initial phase of the pipeline involves an extractive summarization stage including two different systems. The first system employs a hybrid model based on the TextRank algorithm and a text correlation consideration mechanism. The second system is a modified version of SummPip - an unsupervised graph-based method for multi-document summarization. The second phase of the pipeline is abstractive summarization models. Particularly, generative models are applied to produce abstractive summaries from previous phase outputs. The proposed method achieves competitive results as we surpassed many strong research teams to finish the first rank in the AbMusu task - Vietnamese abstractive multi-document summarization, organized in the VLSP 2022 workshop.",
            "corpus_id": "277902007",
            "text": "Multi-document summarization is a challenging problem in the Natural Language Processing field that has drawn a lot of interest from the research community. In this paper, we propose a two-phase pipeline to tackle the Vietnamese abstractive multi-document summarization task. The initial phase of the pipeline involves an extractive summarization stage including two different systems. The first system employs a hybrid model based on the TextRank algorithm and a text correlation consideration mechanism. The second system is a modified version of SummPip - an unsupervised graph-based method for multi-document summarization. The second phase of the pipeline is abstractive summarization models. Particularly, generative models are applied to produce abstractive summaries from previous phase outputs. The proposed method achieves competitive results as we surpassed many strong research teams to finish the first rank in the AbMusu task - Vietnamese abstractive multi-document summarization, organized in the VLSP 2022 workshop.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7978515625
        },
        {
            "paperId": "11086b26e6fecaf4adef5d43e02d75272c0951d6",
            "corpusId": 255994102,
            "title": "A Graph and PhoBERT based Vietnamese Extractive and Abstractive Multi-Document Summarization Frame",
            "venue": "Conference on Research, Innovation and Vision for the Future in Computing & Communication Technologies",
            "year": 2022,
            "referenceCount": 18,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/RIVF55975.2022.10013828?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/RIVF55975.2022.10013828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2101316144",
                    "name": "Tam Doan Thanh"
                },
                {
                    "authorId": "2149703947",
                    "name": "Xuan-Bach Ngo"
                },
                {
                    "authorId": "2149703490",
                    "name": "Doan-Thinh Ngo"
                },
                {
                    "authorId": "39740617",
                    "name": "Mai-Vu Tran"
                },
                {
                    "authorId": "2016974",
                    "name": "Quang-Thuy Ha"
                }
            ],
            "abstract": "Although many methods of solving the Multi-Document Summarization (MDS) problem have been proposed, which belong to both extractive and abstractive summarization, models using only one of the two types of summarization still bring their own disadvantages. One of the good and potential approaches to the MDS problem is the combined approach of extractive and abstractive summarization. Currently, with many languages and especially Vietnamese, the studies that propose a combination of extractive and abstractive summarization are still very limited and have not been deeply exploited. In this paper, we propose a new MDS frame which contains two components in a pipeline architecture combining extractive and abstractive approaches for Vietnamese MDS. We use extractive approach in the first component to select the most important sentences in each document by constructing graphs with the edges representing sentences' relationship, nodes illustrating sentences of input documents. The selected sentences will be clustered to groups of sentences with similar meaning, then combined into documents corresponding to each group. The abstractive approach is used in second component, which uses the PhoBERT2PhoBERT model to generate final summary document. The results of the frame achieved a positive evaluation with the ROUGE-2 measure on two datasets ViMs and VN-MDS are 36.42 and 34.89 percent respectively.",
            "corpus_id": "255994102",
            "text": "Although many methods of solving the Multi-Document Summarization (MDS) problem have been proposed, which belong to both extractive and abstractive summarization, models using only one of the two types of summarization still bring their own disadvantages. One of the good and potential approaches to the MDS problem is the combined approach of extractive and abstractive summarization. Currently, with many languages and especially Vietnamese, the studies that propose a combination of extractive and abstractive summarization are still very limited and have not been deeply exploited. In this paper, we propose a new MDS frame which contains two components in a pipeline architecture combining extractive and abstractive approaches for Vietnamese MDS. We use extractive approach in the first component to select the most important sentences in each document by constructing graphs with the edges representing sentences' relationship, nodes illustrating sentences of input documents. The selected sentences will be clustered to groups of sentences with similar meaning, then combined into documents corresponding to each group. The abstractive approach is used in second component, which uses the PhoBERT2PhoBERT model to generate final summary document. The results of the frame achieved a positive evaluation with the ROUGE-2 measure on two datasets ViMs and VN-MDS are 36.42 and 34.89 percent respectively.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.4912109375
        },
        {
            "paperId": "180f7fca8222ab40b9a62f960075eb0a22bfe680",
            "corpusId": 279284116,
            "title": "From Extraction to Reasoning: A Systematic Review of Algorithms in Multi-Document Summarization and QA",
            "venue": "Statistics, Optimization &amp; Information Computing",
            "year": 2025,
            "referenceCount": 127,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.19139/soic-2310-5070-2398?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.19139/soic-2310-5070-2398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2366344084",
                    "name": "Emmanuel Efosa-Zuwa"
                },
                {
                    "authorId": "2131111376",
                    "name": "O. Oladipupo"
                },
                {
                    "authorId": "4459677",
                    "name": "Jelili O. Oyelade"
                }
            ],
            "abstract": "Multi-document summarization and question-answering (QA) have become pivotal tasks in Natural Language Processing (NLP), facilitating information extraction and decision-making across various domains. This systematic review explores the evolution of algorithms used in these tasks, providing a comprehensive taxonomy of traditional, modern, and emerging approaches. We examine the progression from early extractive methods, such as TFIDF and TextRank, to the advent of neural models like BERT, GPT, and T5 and the integration of retrieval-augmented generation (RAG) for QA. Hybrid models combining traditional techniques with neural approaches and graph-based methods are also discussed. Through a detailed analysis of algorithmic frameworks, we identify key strengths, weaknesses, and challenges in current methodologies. Additionally, the review highlights recent trends such as unified models, multimodal algorithms, and the application of reinforcement learning in summarization and QA tasks. We also explore the real-world relevance of these algorithms in sectors such as news, legal, medical, and education. The paper concludes by outlining open research directions, proposing new evaluation frameworks, and emphasizing the need for cross-task annotations and ethical considerations in future algorithmic development.",
            "corpus_id": "279284116",
            "text": "Multi-document summarization and question-answering (QA) have become pivotal tasks in Natural Language Processing (NLP), facilitating information extraction and decision-making across various domains. This systematic review explores the evolution of algorithms used in these tasks, providing a comprehensive taxonomy of traditional, modern, and emerging approaches. We examine the progression from early extractive methods, such as TFIDF and TextRank, to the advent of neural models like BERT, GPT, and T5 and the integration of retrieval-augmented generation (RAG) for QA. Hybrid models combining traditional techniques with neural approaches and graph-based methods are also discussed. Through a detailed analysis of algorithmic frameworks, we identify key strengths, weaknesses, and challenges in current methodologies. Additionally, the review highlights recent trends such as unified models, multimodal algorithms, and the application of reinforcement learning in summarization and QA tasks. We also explore the real-world relevance of these algorithms in sectors such as news, legal, medical, and education. The paper concludes by outlining open research directions, proposing new evaluation frameworks, and emphasizing the need for cross-task annotations and ethical considerations in future algorithmic development.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.59033203125
        },
        {
            "paperId": "eafb78056fe05aea600cf03818716d7c2761a651",
            "corpusId": 258231820,
            "title": "Grapharizer: A Graph-Based Technique for Extractive Multi-Document Summarization",
            "venue": "Electronics",
            "year": 2023,
            "referenceCount": 48,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/12/8/1895/pdf?version=1681740423",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics12081895?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics12081895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "70642020",
                    "name": "Zakia Jalil"
                },
                {
                    "authorId": "2128893061",
                    "name": "Muhammad Nasir"
                },
                {
                    "authorId": "2268000",
                    "name": "M. Alazab"
                },
                {
                    "authorId": "2152458353",
                    "name": "Jamal Nasir"
                },
                {
                    "authorId": "1742471",
                    "name": "Tehmina Amjad"
                },
                {
                    "authorId": "2078200186",
                    "name": "Abdullah Alqammaz"
                }
            ],
            "abstract": "In the age of big data, there is increasing growth of data on the Internet. It becomes frustrating for users to locate the desired data. Therefore, text summarization emerges as a solution to this problem. It summarizes and presents the users with the gist of the provided documents. However, summarizer systems face challenges, such as poor grammaticality, missing important information, and redundancy, particularly in multi-document summarization. This study involves the development of a graph-based extractive generic MDS technique, named Grapharizer (GRAPH-based summARIZER), focusing on resolving these challenges. Grapharizer addresses the grammaticality problems of the summary using lemmatization during pre-processing. Furthermore, synonym mapping, multi-word expression mapping, and anaphora and cataphora resolution, contribute positively to improving the grammaticality of the generated summary. Challenges, such as redundancy and proper coverage of all topics, are dealt with to achieve informativity and representativeness. Grapharizer is a novel approach which can also be used in combination with different machine learning models. The system was tested on DUC 2004 and Recent News Article datasets against various state-of-the-art techniques. Use of Grapharizer with machine learning increased accuracy by up to 23.05% compared with different baseline techniques on ROUGE scores. Expert evaluation of the proposed system indicated the accuracy to be more than 55%.",
            "corpus_id": "258231820",
            "text": "In the age of big data, there is increasing growth of data on the Internet. It becomes frustrating for users to locate the desired data. Therefore, text summarization emerges as a solution to this problem. It summarizes and presents the users with the gist of the provided documents. However, summarizer systems face challenges, such as poor grammaticality, missing important information, and redundancy, particularly in multi-document summarization. This study involves the development of a graph-based extractive generic MDS technique, named Grapharizer (GRAPH-based summARIZER), focusing on resolving these challenges. Grapharizer addresses the grammaticality problems of the summary using lemmatization during pre-processing. Furthermore, synonym mapping, multi-word expression mapping, and anaphora and cataphora resolution, contribute positively to improving the grammaticality of the generated summary. Challenges, such as redundancy and proper coverage of all topics, are dealt with to achieve informativity and representativeness. Grapharizer is a novel approach which can also be used in combination with different machine learning models. The system was tested on DUC 2004 and Recent News Article datasets against various state-of-the-art techniques. Use of Grapharizer with machine learning increased accuracy by up to 23.05% compared with different baseline techniques on ROUGE scores. Expert evaluation of the proposed system indicated the accuracy to be more than 55%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8779296875
        },
        {
            "paperId": "e28adeb4db46469df9f9bd653501871ddc5f4318",
            "corpusId": 250340193,
            "title": "MuchSUM: Multi-channel Graph Neural Network for Extractive Summarization",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022,
            "referenceCount": 37,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://eprints.whiterose.ac.uk/185779/6/MuchSUM__Multi_channel_Graph_Neural_Network_for_Extractive_Summarization%20%281%29.pdf",
                "status": "GREEN",
                "license": "public-domain",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3477495.3531906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3477495.3531906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "67081502",
                    "name": "Qianren Mao"
                },
                {
                    "authorId": "2149670456",
                    "name": "Hongdong Zhu"
                },
                {
                    "authorId": "40478976",
                    "name": "J. Liu"
                },
                {
                    "authorId": "2052296239",
                    "name": "Cheng Ji"
                },
                {
                    "authorId": null,
                    "name": "Hao Peng"
                },
                {
                    "authorId": "47785906",
                    "name": "Jianxin Li"
                },
                {
                    "authorId": "2108740151",
                    "name": "Lihong Wang"
                },
                {
                    "authorId": "11032852",
                    "name": "Z. Wang"
                }
            ],
            "abstract": "Recent studies of extractive text summarization have leveraged BERT for document encoding with breakthrough performance. However, when using a pre-trained BERT-based encoder, existing approaches for selecting representative sentences for text summarization are inadequate since the encoder is not explicitly trained for representing sentences. Simply providing the BERT-initialized sentences to cross-sentential graph-based neural networks (GNNs) to encode semantic features of the sentences is not ideal because doing so fail to integrate other summary-worthy features like sentence importance and positions. This paper presents MuchSUM, a better approach for extractive text summarization. MuchSUM is a multi-channel graph convolutional network designed to explicitly incorporate multiple salient summary-worthy features. Specifically, we introduce three specific graph channels to encode the node textual features, node centrality features, and node position features, respectively, under bipartite word-sentence heterogeneous graphs. Then, a cross-channel convolution operation is designed to distill the common graph representations shared by different channels. Finally, the sentence representations of each channel are fused for extractive summarization. We also investigate three weighted graphs in each channel to infuse edge features for graph-based summarization modeling. Experimental results demonstrate our model can achieve considerable performance compared with some BERT-initialized graph-based extractive summarization systems.",
            "corpus_id": "250340193",
            "text": "Recent studies of extractive text summarization have leveraged BERT for document encoding with breakthrough performance. However, when using a pre-trained BERT-based encoder, existing approaches for selecting representative sentences for text summarization are inadequate since the encoder is not explicitly trained for representing sentences. Simply providing the BERT-initialized sentences to cross-sentential graph-based neural networks (GNNs) to encode semantic features of the sentences is not ideal because doing so fail to integrate other summary-worthy features like sentence importance and positions. This paper presents MuchSUM, a better approach for extractive text summarization. MuchSUM is a multi-channel graph convolutional network designed to explicitly incorporate multiple salient summary-worthy features. Specifically, we introduce three specific graph channels to encode the node textual features, node centrality features, and node position features, respectively, under bipartite word-sentence heterogeneous graphs. Then, a cross-channel convolution operation is designed to distill the common graph representations shared by different channels. Finally, the sentence representations of each channel are fused for extractive summarization. We also investigate three weighted graphs in each channel to infuse edge features for graph-based summarization modeling. Experimental results demonstrate our model can achieve considerable performance compared with some BERT-initialized graph-based extractive summarization systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6298828125
        },
        {
            "paperId": "3a6f161a17bb031659dcd2167630e6909eefab92",
            "corpusId": 258378312,
            "title": "Enhancing Multi-Document Summarization with Cross-Document Graph-based Information Extraction",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "referenceCount": 37,
            "citationCount": 16,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.eacl-main.124.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.eacl-main.124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116461591",
                    "name": "Zixuan Zhang"
                },
                {
                    "authorId": "3053247",
                    "name": "Heba Elfardy"
                },
                {
                    "authorId": "40262269",
                    "name": "Markus Dreyer"
                },
                {
                    "authorId": "50044599",
                    "name": "Kevin Small"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                }
            ],
            "abstract": "Information extraction (IE) and summarization are closely related, both tasked with presenting a subset of the information contained in a natural language text. However, while IE extracts structural representations, summarization aims to abstract the most salient information into a generated text summary \u2013 thus potentially encountering the technical limitations of current text generation methods (e.g., hallucination). To mitigate this risk, this work uses structured IE graphs to enhance the abstractive summarization task. Specifically, we focus on improving Multi-Document Summarization (MDS) performance by using cross-document IE output, incorporating two novel components: (1) the use of auxiliary entity and event recognition systems to focus the summary generation model; (2) incorporating an alignment loss between IE nodes and their text spans to reduce inconsistencies between the IE graphs and text representations. Operationally, both the IE nodes and corresponding text spans are projected into the same embedding space and pairwise distance is minimized. Experimental results on multiple MDS benchmarks show that summaries generated by our model are more factually consistent with the source documents than baseline models while maintaining the same level of abstractiveness.",
            "corpus_id": "258378312",
            "text": "Information extraction (IE) and summarization are closely related, both tasked with presenting a subset of the information contained in a natural language text. However, while IE extracts structural representations, summarization aims to abstract the most salient information into a generated text summary \u2013 thus potentially encountering the technical limitations of current text generation methods (e.g., hallucination). To mitigate this risk, this work uses structured IE graphs to enhance the abstractive summarization task. Specifically, we focus on improving Multi-Document Summarization (MDS) performance by using cross-document IE output, incorporating two novel components: (1) the use of auxiliary entity and event recognition systems to focus the summary generation model; (2) incorporating an alignment loss between IE nodes and their text spans to reduce inconsistencies between the IE graphs and text representations. Operationally, both the IE nodes and corresponding text spans are projected into the same embedding space and pairwise distance is minimized. Experimental results on multiple MDS benchmarks show that summaries generated by our model are more factually consistent with the source documents than baseline models while maintaining the same level of abstractiveness.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.82861328125
        },
        {
            "paperId": "3deae031512b07fc5c7067756ddf3612efe6d737",
            "corpusId": 278345901,
            "title": "A Concise Multi-Document Extractive Summarization Approach on Product Reviews",
            "venue": "2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)",
            "year": 2024,
            "referenceCount": 15,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WI-IAT62293.2024.00034?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WI-IAT62293.2024.00034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2214765596",
                    "name": "Nathaniel Benham"
                },
                {
                    "authorId": "2112135742",
                    "name": "Siqi Gao"
                },
                {
                    "authorId": "2267793123",
                    "name": "Yiu-Kai Ng"
                }
            ],
            "abstract": "As the rate of web data creation and storage continues to rise, so does the need for effective multi-document summarization techniques. Computer-generated summaries that can effectively reflect the original data in a readable format saves the need for manual human labor. In the field of online product reviews specifically, a single product can have hundreds to thousands of reviews, and an average shopper is often to only read a handful of them. With effective summarization tools, these shoppers could be given a single generated summary that condenses the content of the review set into a single paragraph or two. Many efforts have been made to fulfill this need for multi-document summarization; however, most require complex data graphs, structures, or language models. Our research stresses simplicity in the extractive algorithm, making the process easier to understand and implement. In this paper, we propose four different versions of multi-document extractive summarizers based on KL-Divergence, TF-IDF, and Diversity scoring. These extractive summarizers are then measured against each other, along with several top-notch summarizers, in terms of effectiveness at expressing relevant content and linguistic quality. The results of these tests show a significant advantage for our summarizers, thus promoting our summarizers as a powerful-yet-simple process ready for use in product review summarization.",
            "corpus_id": "278345901",
            "text": "As the rate of web data creation and storage continues to rise, so does the need for effective multi-document summarization techniques. Computer-generated summaries that can effectively reflect the original data in a readable format saves the need for manual human labor. In the field of online product reviews specifically, a single product can have hundreds to thousands of reviews, and an average shopper is often to only read a handful of them. With effective summarization tools, these shoppers could be given a single generated summary that condenses the content of the review set into a single paragraph or two. Many efforts have been made to fulfill this need for multi-document summarization; however, most require complex data graphs, structures, or language models. Our research stresses simplicity in the extractive algorithm, making the process easier to understand and implement. In this paper, we propose four different versions of multi-document extractive summarizers based on KL-Divergence, TF-IDF, and Diversity scoring. These extractive summarizers are then measured against each other, along with several top-notch summarizers, in terms of effectiveness at expressing relevant content and linguistic quality. The results of these tests show a significant advantage for our summarizers, thus promoting our summarizers as a powerful-yet-simple process ready for use in product review summarization.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.4931640625
        }
    ],
    "quotes": {
        "cost": 0.29945399999999994,
        "quotes": [
            {
                "idx": 0,
                "key": "[10274390 | Zhang et al. | 2013 | Citations: 28]",
                "snippets": "Recently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. (Erkan et al., 2004)(Zhang et al., 2007)(\u00d6zg\u00fcr et al., 2008)) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank (Erkan et al., 2004) and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13664986 | Ozgur et al. | 2008 | Citations: 372]": "Motivation: Understanding the role of genetics in diseases is one of the most important aims of the biological sciences. The completion of the Human Genome Project has led to a rapid increase in the number of publications in this area. However, the coverage of curated databases that provide information manually extracted from the literature is limited. Another challenge is that determining disease-related genes requires laborious experiments. Therefore, predicting good candidate genes before experimental analysis will save time and effort. We introduce an automatic approach based on text mining and network analysis to predict gene-disease associations. We collected an initial set of known disease-related genes and built an interaction network by automatic literature mining based on dependency parsing and support vector machines. Our hypothesis is that the central genes in this disease-specific network are likely to be related to the disease. We used the degree, eigenvector, betweenness and closeness centrality metrics to rank the genes in the network. Results: The proposed approach can be used to extract known and to infer unknown gene-disease associations. We evaluated the approach for prostate cancer. Eigenvector and degree centrality achieved high accuracy. A total of 95% of the top 20 genes ranked by these methods are confirmed to be related to prostate cancer. On the other hand, betweenness and closeness centrality predicted more genes whose relation to the disease is currently unknown and are candidates for experimental study. Availability: A web-based system for browsing the disease-specific gene-interaction networks is available at: http://gin.ncibi.org Contact: radev@umich.edu"
                },
                "metadata": [
                    {
                        "section_title": "Identifying salient source content",
                        "pdf_hash": "",
                        "start": 346,
                        "end": 1429,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "2962090",
                            "268069298",
                            "13664986",
                            "2962090"
                        ],
                        "quote": "Recently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. (Erkan et al., 2004)(Zhang et al., 2007)(\u00d6zg\u00fcr et al., 2008)) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank (Erkan et al., 2004) and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[13237013 | Han et al. | 2016 | Citations: 25]",
                "snippets": "Another approach of unsupervised extract-based summarization (Wang et al., 2008)(Li, 2005) uses graph-based model. TextRank (Park et al., 2007) and LexRank (Dhillon, 2001) are first two graph-based models applied in text summarization, which use the PageRank-like algorithms to mark sentences. Then, other researchers have integrated the statistical and linguistic features to drive the sentence selection process, for example, the sentence position (Katragadda et al., 2009), term frequency (Lin et al., 1997), topic signature (Lin et al., 2000), lexical chains (Barzilay et al., 1997), and syntactic patterns (Baralis et al., 2013)(Haggag, 2013). Ko and Seo (Ko et al., 2008) composed two sentences nearby into a bigram. Those bigrams were supposed to be context information. First, they extracted the bigrams by using the sentence extraction model. Then they used another extraction module to extract sentences from them. The ClusterCMRW and ClusterHITS models calculated the sentences scores by considering the clusterlevel information in the graph-based ranking algorithm. Canhasi and Kononenko (Canhasi et al., 2013) improve matrix decomposition by employing the archetypal analysis for generic multidocument summarization. While coming to the document set, there must be more consideration about the document-level influence. But it did not consider the relationship between words and sentences. The DsR model (Wei et al., 2010) achieved it by using document-sensitive graph-based ranking model. But this method did not get a satisfied result. Yin et al. improved the summarization quality by adding extra information which came from the query-extraction scenario. Goyal et al. (Goyal et al., 2013) take Bernoulli model of randomness to index weights of sentences taking the context into consideration. The method proposed in [28] decomposed sentences by semantic role analysis, but while building the model, it did not use graphbased algorithms.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[11989149 | Katragadda et al. | 2009 | Citations: 42]": "In this paper, we describe a sentence position based summarizer that is built based on a sentence position policy, created from the evaluation testbed of recent summarization tasks at Document Understanding Conferences (DUC). We show that the summarizer thus built is able to outperform most systems participating in task focused summarization evaluations at Text Analysis Conferences (TAC) 2008. Our experiments also show that such a method would perform better at producing short summaries (upto 100 words) than longer summaries. Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task.",
                    "[2642018 | Wang et al. | 2008 | Citations: 343]": "Multi-document summarization aims to create a compressed summary while retaining the main characteristics of the original set of documents. Many approaches use statistics and machine learning techniques to extract sentences from documents. In this paper, we propose a new multi-document summarization framework based on sentence-level semantic analysis and symmetric non-negative matrix factorization. We first calculate sentence-sentence similarities using semantic analysis and construct the similarity matrix. Then symmetric matrix factorization, which has been shown to be equivalent to normalized spectral clustering, is used to group sentences into clusters. Finally, the most informative sentences are selected from each group to form the summary. Experimental results on DUC2005 and DUC2006 data sets demonstrate the improvement of our proposed framework over the implemented existing summarization systems. A further study on the factors that benefit the high performance is also conducted.",
                    "[503868 | Haggag | 2013 | Citations: 6]": "Text summarization is machine based generation of a shortened version of a text. The summary should be a non-redundant extract from the original text. Most researches of text summarization use sentence extraction instead of abstraction to produce a summary. Extraction is depending mainly on sentences that already contained in the original input, which makes it more accurate and more concise. When all input articles are surrounding a particular event, extracting similar sentences would result in producing a highly repetitive summary. In this paper, a novel model for text summarization is proposed based on removing the non-effective sentences in producing an extract from the text. The model utilizes semantic analysis by evaluating sentences similarity. This similarity is provided by evaluating individual words similarity as well as syntactic relationships between neighboring words. These relationships addressed throughout the model as syntactic patterns. Word senses and the correlating part of speech for the word within context are provided in the semantic processing of matched patterns. The introduction of syntactic patterns knowledge supports text reduction by mapping the matched patterns into summarized ones. In addition, syntactic patterns make use of sentence relatedness evaluation in defining which sentences to keep and which to drop. Experiments proved that the model presented throughout the paper is well performing in results evaluation of compression rate, accuracy, recall and other human criteria like correctness, novelty, fluency and usefulness.",
                    "[5519987 | Lin et al. | 1997 | Citations: 316]": "This paper addresses the problem of identifying likely topics of texts by their position in the text. It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure. This method can be used in applications such as information retrieval, routing, and text summarization.",
                    "[8598694 | Lin et al. | 2000 | Citations: 548]": "In order to produce a good summary, one has to identify the most relevant portions of a given text. We describe in this paper a method for automatically training topic signatures-sets of related words, with associated weights, organized around head topics and illustrate with signatures we created with 6,194 TREC collection texts over 4 selected topics. We describe the possible integration of topic signatures with outologies and its evaluaton on an automated text summarization system."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1746,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 92
                            },
                            {
                                "start": 93,
                                "end": 245
                            },
                            {
                                "start": 246,
                                "end": 502
                            },
                            {
                                "start": 503,
                                "end": 563
                            },
                            {
                                "start": 564,
                                "end": 618
                            },
                            {
                                "start": 619,
                                "end": 692
                            },
                            {
                                "start": 693,
                                "end": 765
                            },
                            {
                                "start": 766,
                                "end": 918
                            },
                            {
                                "start": 919,
                                "end": 1052
                            },
                            {
                                "start": 1053,
                                "end": 1155
                            },
                            {
                                "start": 1156,
                                "end": 1225
                            },
                            {
                                "start": 1226,
                                "end": 1311
                            },
                            {
                                "start": 1312,
                                "end": 1359
                            },
                            {
                                "start": 1360,
                                "end": 1480
                            },
                            {
                                "start": 1481,
                                "end": 1602
                            },
                            {
                                "start": 1603,
                                "end": 1746
                            }
                        ],
                        "ref_mentions": [
                            "2642018",
                            "1750917",
                            "6524990",
                            "11847258",
                            "11989149",
                            "5519987",
                            "8598694",
                            "1584325",
                            "15709889",
                            "503868",
                            "16153195",
                            "207057042",
                            "35133176",
                            "17333959"
                        ],
                        "quote": "Another approach of unsupervised extract-based summarization (Wang et al., 2008)(Li, 2005) uses graph-based model. TextRank (Park et al., 2007) and LexRank (Dhillon, 2001) are first two graph-based models applied in text summarization, which use the PageRank-like algorithms to mark sentences. Then, other researchers have integrated the statistical and linguistic features to drive the sentence selection process, for example, the sentence position (Katragadda et al., 2009), term frequency (Lin et al., 1997), topic signature (Lin et al., 2000), lexical chains (Barzilay et al., 1997), and syntactic patterns (Baralis et al., 2013)(Haggag, 2013). Ko and Seo (Ko et al., 2008) composed two sentences nearby into a bigram. Those bigrams were supposed to be context information. First, they extracted the bigrams by using the sentence extraction model. Then they used another extraction module to extract sentences from them. The ClusterCMRW and ClusterHITS models calculated the sentences scores by considering the clusterlevel information in the graph-based ranking algorithm. Canhasi and Kononenko (Canhasi et al., 2013) improve matrix decomposition by employing the archetypal analysis for generic multidocument summarization. While coming to the document set, there must be more consideration about the document-level influence. But it did not consider the relationship between words and sentences. The DsR model (Wei et al., 2010) achieved it by using document-sensitive graph-based ranking model. But this method did not get a satisfied result. Yin et al. improved the summarization quality by adding extra information which came from the query-extraction scenario. Goyal et al. (Goyal et al., 2013) take Bernoulli model of randomness to index weights of sentences taking the context into consideration. The method proposed in [28] decomposed sentences by semantic role analysis, but while building the model, it did not use graphbased algorithms."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[145993667 | Mohamed et al. | 2019 | Citations: 101]",
                "snippets": "In addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization (Erkan et al., 2004)15,(Canhasi, 2011)(Wei et al., 2010)(Wan, 2010)21,(Azadani et al., 2018)[23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev (Erkan et al., 2004) proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization (Canhasi, 2011)(Wei et al., 2010)(Wan, 2010). In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. (Wei et al., 2010) and Wan (Wan, 2010) considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts (Plaza et al., 2008)25,(Lloret et al., 2011)(Azadani et al., 2018). Of this, Azadani et al. (Azadani et al., 2018) and Plaza et al. (Plaza et al., 2008) have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[5674504 | Plaza et al. | 2008 | Citations: 53]": "One of the main problems in research on automatic summarization is the inaccurate semantic interpretation of the source. Using specific domain knowledge can considerably alleviate the problem. In this paper, we introduce an ontology-based extractive method for summarization. It is based on mapping the text to concepts and representing the document and its sentences as graphs. We have applied our approach to summarize biomedical literature, taking advantages of free resources as UMLS. Preliminary empirical results are presented and pending problems are identified."
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 236,
                        "end": 2204,
                        "sentence_offsets": [
                            {
                                "start": 236,
                                "end": 542
                            },
                            {
                                "start": 543,
                                "end": 652
                            },
                            {
                                "start": 653,
                                "end": 724
                            },
                            {
                                "start": 725,
                                "end": 849
                            },
                            {
                                "start": 850,
                                "end": 931
                            },
                            {
                                "start": 932,
                                "end": 1083
                            },
                            {
                                "start": 1084,
                                "end": 1333
                            },
                            {
                                "start": 1334,
                                "end": 1492
                            },
                            {
                                "start": 1493,
                                "end": 1669
                            },
                            {
                                "start": 1670,
                                "end": 1821
                            },
                            {
                                "start": 1822,
                                "end": 1987
                            },
                            {
                                "start": 1988,
                                "end": 2073
                            },
                            {
                                "start": 2074,
                                "end": 2204
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "16493063",
                            "35133176",
                            "17224077",
                            "49217427",
                            "506350",
                            "16493063",
                            "35133176",
                            "17224077",
                            "35133176",
                            "17224077",
                            "5674504",
                            "16459894",
                            "49217427",
                            "49217427",
                            "5674504"
                        ],
                        "quote": "In addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization (Erkan et al., 2004)15,(Canhasi, 2011)(Wei et al., 2010)(Wan, 2010)21,(Azadani et al., 2018)[23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev (Erkan et al., 2004) proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization (Canhasi, 2011)(Wei et al., 2010)(Wan, 2010). In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. (Wei et al., 2010) and Wan (Wan, 2010) considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts (Plaza et al., 2008)25,(Lloret et al., 2011)(Azadani et al., 2018). Of this, Azadani et al. (Azadani et al., 2018) and Plaza et al. (Plaza et al., 2008) have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28]."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[16689087 | Sipos et al. | 2011 | Citations: 6]",
                "snippets": "A popular stohastic graph-based summarization method is LexRank (Erkan et al., 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank [16] is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank [17]. Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. \n\nGraph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[2100529 | Mihalcea | 2004 | Citations: 520]": "This paper presents an innovative unsupervised method for automatic sentence extraction using graph-based ranking algorithms. We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 509,
                        "end": 1270,
                        "sentence_offsets": [
                            {
                                "start": 509,
                                "end": 578
                            },
                            {
                                "start": 579,
                                "end": 694
                            },
                            {
                                "start": 695,
                                "end": 858
                            },
                            {
                                "start": 859,
                                "end": 1000
                            },
                            {
                                "start": 1003,
                                "end": 1084
                            },
                            {
                                "start": 1085,
                                "end": 1270
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "2100529",
                            "216000619",
                            "10228847"
                        ],
                        "quote": "A popular stohastic graph-based summarization method is LexRank (Erkan et al., 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank [16] is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank [17]. Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. \n\nGraph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[170079112 | Liu et al. | 2019 | Citations: 298]",
                "snippets": "The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004;Christensen et al., 2013;Wan, 2008;Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them.\n\nMost previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004;Christensen et al., 2013;Wan, 2008;Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them.\n\nMost previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[182712867 | Lierde et al. | 2019 | Citations: 26]",
                "snippets": "The earliest graph-based summarizer, called LexRank (Erkan et al., 2004), defines edges as term co-occurrence relationships between sentences. Then, PageRank algorithm is applied to compute relevance scores of sentences. Adapting this idea for the task of query-focused summarization, topic sensitive LexRank (Otterbacher et al., 2005) introduces a query bias in probabilities of transition, which results in higher scores for sentences that are similar to the query. Sim-ilarly, (Wan, 2013) proposes a manifold ranking algorithm in which scores are popagated accross a graph including both sentences and the query as vertices. To remove redundancies in summaries, (Mei et al., 2010) proposes a new node ranking algorithm called DivRank, which tends to select dissimilar sentences. While early graph-based algorithms only involved sentences, a bipartite graph model is proposed in (Wan et al., 2008), involving both sentences and terms as vertices and it applies HITS algorithm to score sentences. (Wei et al., 2010) combines this idea with a PageRank-like method to score sentences, terms and documents simultaneously.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10274362 | Otterbacher et al. | 2005 | Citations: 179]": "We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. \"How many victims have been found?\") Judges found sentences providing an answer to each question. To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap. In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline."
                },
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 1023,
                        "end": 2055,
                        "sentence_offsets": [
                            {
                                "start": 1023,
                                "end": 1149
                            },
                            {
                                "start": 1150,
                                "end": 1227
                            },
                            {
                                "start": 1228,
                                "end": 1452
                            },
                            {
                                "start": 1453,
                                "end": 1605
                            },
                            {
                                "start": 1606,
                                "end": 1745
                            },
                            {
                                "start": 1746,
                                "end": 1947
                            },
                            {
                                "start": 1948,
                                "end": 2055
                            }
                        ],
                        "ref_mentions": [
                            "2962090",
                            "10274362",
                            "11840355",
                            "11977708",
                            "9849366",
                            "35133176"
                        ],
                        "quote": "The earliest graph-based summarizer, called LexRank (Erkan et al., 2004), defines edges as term co-occurrence relationships between sentences. Then, PageRank algorithm is applied to compute relevance scores of sentences. Adapting this idea for the task of query-focused summarization, topic sensitive LexRank (Otterbacher et al., 2005) introduces a query bias in probabilities of transition, which results in higher scores for sentences that are similar to the query. Sim-ilarly, (Wan, 2013) proposes a manifold ranking algorithm in which scores are popagated accross a graph including both sentences and the query as vertices. To remove redundancies in summaries, (Mei et al., 2010) proposes a new node ranking algorithm called DivRank, which tends to select dissimilar sentences. While early graph-based algorithms only involved sentences, a bipartite graph model is proposed in (Wan et al., 2008), involving both sentences and terms as vertices and it applies HITS algorithm to score sentences. (Wei et al., 2010) combines this idea with a PageRank-like method to score sentences, terms and documents simultaneously."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[202889056 | Antognini et al. | 2019 | Citations: 22]",
                "snippets": "Extractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. (Radev, 2000) introduced a cross-document structure theory, as a basis for multi-document summarization. (Erkan et al., 2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan et al., 2006)(Antiqueira et al., 2009). (Wan et al., 2008) pairs graph-based methods (e.g. random walk) with clustering. (Mei et al., 2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by (Christensen et al., 2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann et al., 1988).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[5457260 | Wan et al. | 2006 | Citations: 109]": "This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems.",
                    "[10103200 | Radev | 2000 | Citations: 248]": "We introduce CST (cross-document structure theory), a paradigm for multi-document analysis. CST takes into account the rhetorical structure of clusters of related textual documents. We present a taxonomy of cross-document relationships. We argue that CST can be the basis for multi-document summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering of facts."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 480,
                        "end": 1404,
                        "sentence_offsets": [
                            {
                                "start": 480,
                                "end": 570
                            },
                            {
                                "start": 571,
                                "end": 614
                            },
                            {
                                "start": 615,
                                "end": 718
                            },
                            {
                                "start": 719,
                                "end": 873
                            },
                            {
                                "start": 874,
                                "end": 990
                            },
                            {
                                "start": 991,
                                "end": 1042
                            },
                            {
                                "start": 1043,
                                "end": 1072
                            },
                            {
                                "start": 1073,
                                "end": 1194
                            },
                            {
                                "start": 1195,
                                "end": 1404
                            }
                        ],
                        "ref_mentions": [
                            "10103200",
                            "506350",
                            "5457260",
                            "17064982",
                            "9849366",
                            "11977708",
                            "337730",
                            "60514661"
                        ],
                        "quote": "Extractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. (Radev, 2000) introduced a cross-document structure theory, as a basis for multi-document summarization. (Erkan et al., 2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan et al., 2006)(Antiqueira et al., 2009). (Wan et al., 2008) pairs graph-based methods (e.g. random walk) with clustering. (Mei et al., 2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by (Christensen et al., 2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann et al., 1988)."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[216867290 | ElSahar et al. | 2020 | Citations: 48]",
                "snippets": "Graph-based methods, such as LexRank (Erkan & Radev, 2004) or TextRank (Mihalcea & Tarau, 2004;Zheng & Lapata, 2019), work by constructing a graph whose nodes are the sentences from the input documents and whose edges indicate a high word overlap between two sentences. Then, they use the PageRank algorithm to extract the sentences with the highest centrality.\n\nNon-neural abstractive methods (Ganesan et al., 2010;Nayeem et al., 2018) are also graph-based, but construct graphs whose nodes are word types and edges indicate the immediate precedence relationship between two instantiations of the word type in a sentence. The summary is extracted by finding salient paths in the graph.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Graph-based methods, such as LexRank (Erkan & Radev, 2004) or TextRank (Mihalcea & Tarau, 2004;Zheng & Lapata, 2019), work by constructing a graph whose nodes are the sentences from the input documents and whose edges indicate a high word overlap between two sentences. Then, they use the PageRank algorithm to extract the sentences with the highest centrality.\n\nNon-neural abstractive methods (Ganesan et al., 2010;Nayeem et al., 2018) are also graph-based, but construct graphs whose nodes are word types and edges indicate the immediate precedence relationship between two instantiations of the word type in a sentence. The summary is extracted by finding salient paths in the graph.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[218718706 | Li et al. | 2020 | Citations: 136]",
                "snippets": "Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models.\n\nMost previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models.\n\nMost previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[219036690 | Xu et al. | 2019 | Citations: 282]",
                "snippets": "Graph-based Summarization Graph approach has been explored in text summarization over decades. LexRank introduced a stochastic graphbased method for computing relative importance of textual units (Erkan et al., 2004). (Yasunaga et al., 2017) employed a GCN on the relation graphs with sentence embeddings obtained from RNN. (Tan et al., 2017) also proposed graphbased attention in abstractive summarization model. Fernandes et al. (2018) developed a framework to reason long-distance relationships for text summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[6532096 | Yasunaga et al. | 2017 | Citations: 258]": "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.",
                    "[26698484 | Tan et al. | 2017 | Citations: 298]": "Abstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods."
                },
                "metadata": [
                    {
                        "section_title": "Discourse & Summarization",
                        "pdf_hash": "",
                        "start": 694,
                        "end": 1216,
                        "sentence_offsets": [
                            {
                                "start": 694,
                                "end": 788
                            },
                            {
                                "start": 789,
                                "end": 914
                            },
                            {
                                "start": 915,
                                "end": 1108
                            },
                            {
                                "start": 1109,
                                "end": 1216
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "6532096",
                            "26698484"
                        ],
                        "quote": "Graph-based Summarization Graph approach has been explored in text summarization over decades. LexRank introduced a stochastic graphbased method for computing relative importance of textual units (Erkan et al., 2004). (Yasunaga et al., 2017) employed a GCN on the relation graphs with sentence embeddings obtained from RNN. (Tan et al., 2017) also proposed graphbased attention in abstractive summarization model. Fernandes et al. (2018) developed a framework to reason long-distance relationships for text summarization."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[219690773 | Khan et al. | 2020 | Citations: 16]",
                "snippets": "In recent years, various graph-based methods have attracted more attention and effectively attempted for text summarization. These methods utilize PageRank algorithm [39] and its variants to give rank/score to graph nodes, which represent sentences or passages. The authors in [40] proposed a connectivity graph, which assumes that nodes only carry significant information if they are connected to many other nodes. The authors in [41] introduced a Lex-PageRank approach that is based on eigenvector centrality, which constructs a sentence connectivity matrix and utilizes similar algorithm like PageRank to find the significant sentences for summary. A similar algorithm to PageRank was also proposed in [42] which finds salient sentences for summary generation.\n\nThe authors in [24] presented a graph-based approach, which integrates surface features with text content and investigates subtopic features in multiple documents to incorporate them into the graph-based ranking algorithm. A multidocument summarization approach based on affinity graph [43] exploits similar algorithm to PageRank and computes sentence scores in the affinity graph based on information richness. The authors in [44] demonstrated a document-sensitive graph model for multidocument generic summarization and highlighted the impact of global document set information at sentence level. A weighted graph model for generic multidocument summarization introduced in [45] combines sentence ranking and sentence clustering methods. The authors in [46] presented a graphbased method for multidocument summarization of Vietnamese documents and employed traditional PageRank algorithm to rank the important sentences. The authors in [47] demonstrated an event graph-based approach for multidocument extractive summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In recent years, various graph-based methods have attracted more attention and effectively attempted for text summarization. These methods utilize PageRank algorithm [39] and its variants to give rank/score to graph nodes, which represent sentences or passages. The authors in [40] proposed a connectivity graph, which assumes that nodes only carry significant information if they are connected to many other nodes. The authors in [41] introduced a Lex-PageRank approach that is based on eigenvector centrality, which constructs a sentence connectivity matrix and utilizes similar algorithm like PageRank to find the significant sentences for summary. A similar algorithm to PageRank was also proposed in [42] which finds salient sentences for summary generation.\n\nThe authors in [24] presented a graph-based approach, which integrates surface features with text content and investigates subtopic features in multiple documents to incorporate them into the graph-based ranking algorithm. A multidocument summarization approach based on affinity graph [43] exploits similar algorithm to PageRank and computes sentence scores in the affinity graph based on information richness. The authors in [44] demonstrated a document-sensitive graph model for multidocument generic summarization and highlighted the impact of global document set information at sentence level. A weighted graph model for generic multidocument summarization introduced in [45] combines sentence ranking and sentence clustering methods. The authors in [46] presented a graphbased method for multidocument summarization of Vietnamese documents and employed traditional PageRank algorithm to rank the important sentences. The authors in [47] demonstrated an event graph-based approach for multidocument extractive summarization.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[224818792 | Cho et al. | 2020 | Citations: 12]",
                "snippets": "LexRank (Erkan et al., 2004)) is a graph-based approach estimating sentence importance based on eigenvector centrality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                },
                "metadata": [
                    {
                        "section_title": "Summarization Results",
                        "pdf_hash": "",
                        "start": 608,
                        "end": 729,
                        "sentence_offsets": [
                            {
                                "start": 608,
                                "end": 729
                            }
                        ],
                        "ref_mentions": [
                            "506350"
                        ],
                        "quote": "LexRank (Erkan et al., 2004)) is a graph-based approach estimating sentence importance based on eigenvector centrality."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[232404911 | Zhang et al. | 2021 | Citations: 0]",
                "snippets": "Unsupervised graph summarization methods rely on graph connectivity (centrality) or node representations (centroid) (Radev et al., 2004;(Rossiello et al., 2017) to score and rank sentences. Popular centrality-based methods include TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and PACSUM (Zheng and Lapata, 2019). Details of centrality-based and centroid-based summarization will be discussed in Section 3. \n\nResearchers also explore supervised graph neural network (GNN) on the task (Yasunaga et al., 2017;Xu et al., 2019;Wang et al., 2020). Our paper follows this line of works on developing novel unsupervised graph models for single document summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[2346086 | Rossiello et al. | 2017 | Citations: 128]": "The textual similarity is a crucial aspect for many extractive text summarization methods. A bag-of-words representation does not allow to grasp the semantic relationships between concepts when comparing strongly related sentences with no words in common. To overcome this issue, in this paper we propose a centroid-based method for text summarization that exploits the compositional capabilities of word embeddings. The evaluations on multi-document and multilingual datasets prove the effectiveness of the continuous vector representation of words compared to the bag-of-words model. Despite its simplicity, our method achieves good performance even in comparison to more complex deep learning models. Our method is unsupervised and it can be adopted in other summarization tasks."
                },
                "metadata": [
                    {
                        "section_title": "Graph-based Summarization",
                        "pdf_hash": "",
                        "start": 344,
                        "end": 1026,
                        "sentence_offsets": [
                            {
                                "start": 344,
                                "end": 532
                            },
                            {
                                "start": 533,
                                "end": 679
                            },
                            {
                                "start": 680,
                                "end": 772
                            },
                            {
                                "start": 775,
                                "end": 908
                            },
                            {
                                "start": 909,
                                "end": 1026
                            }
                        ],
                        "ref_mentions": [
                            "2346086",
                            "577937",
                            "506350"
                        ],
                        "quote": "Unsupervised graph summarization methods rely on graph connectivity (centrality) or node representations (centroid) (Radev et al., 2004;(Rossiello et al., 2017) to score and rank sentences. Popular centrality-based methods include TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and PACSUM (Zheng and Lapata, 2019). Details of centrality-based and centroid-based summarization will be discussed in Section 3. \n\nResearchers also explore supervised graph neural network (GNN) on the task (Yasunaga et al., 2017;Xu et al., 2019;Wang et al., 2020). Our paper follows this line of works on developing novel unsupervised graph models for single document summarization."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[235097309 | Pasunuru et al. | 2021 | Citations: 73]",
                "snippets": "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[49210924 | Liao et al. | 2018 | Citations: 105]": "Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.",
                    "[5001921 | Liu et al. | 2018 | Citations: 302]": "We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-tograph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-totext generator. The framework is data-driven, trainable, and not specifically designed for a particular domain. Experiments on goldstandard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization",
                    "[6025826 | Mani et al. | 1997 | Citations: 252]": "We describe a new method for summarizing similarities and differences in a pair of related documents using a graph representation for text. Concepts denoted by words, phrases, and proper names in the document are represented positionally as nodes in the graph along with edges corresponding to semantic relations between items. Given a perspective in terms of which the pair of documents is to be summarized, the algorithm first uses a spreading activation technique to discover, in each document, nodes semantically related to the topic. The activated graphs of each document are then matched to yield a graph corresponding to similarities and differences between the pair, which is rendered in natural language. An evaluation of these techniques has been carried out."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1907,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 102
                            },
                            {
                                "start": 103,
                                "end": 251
                            },
                            {
                                "start": 252,
                                "end": 372
                            },
                            {
                                "start": 373,
                                "end": 547
                            },
                            {
                                "start": 548,
                                "end": 697
                            },
                            {
                                "start": 700,
                                "end": 883
                            },
                            {
                                "start": 884,
                                "end": 1100
                            },
                            {
                                "start": 1101,
                                "end": 1288
                            },
                            {
                                "start": 1289,
                                "end": 1387
                            },
                            {
                                "start": 1388,
                                "end": 1465
                            },
                            {
                                "start": 1466,
                                "end": 1613
                            },
                            {
                                "start": 1614,
                                "end": 1765
                            },
                            {
                                "start": 1768,
                                "end": 1907
                            }
                        ],
                        "ref_mentions": [
                            "6025826",
                            "10019526",
                            "6025826",
                            "10019526",
                            "506350",
                            "16587947",
                            "15709889",
                            "6788641",
                            "5001921",
                            "49210924"
                        ],
                        "quote": "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[235187219 | Hickmann et al. | 2021 | Citations: 1]",
                "snippets": "Graph-based approaches on graph neural networks and specifically using knowledge graphs have gained popularity [8,12,20,21] while recent models combine graph neural networks with a transformer architecture [8,12].\n\nFor extractive MDS, a graph-based representation can help to detect salient vertices, so that they are included in the generated summary to cover all important information. In abstractive MDS, salient vertices can guide the summarization process via attention mechanisms of the transformer architecture.\n\nDifferent textual units have been used for graph-based text summarization in the past, as discourse relations [8,20], sentences [3,21], or paragraphs [12].\n\nThe GraphSum model showed substantial improvements compared to strong MDS baselines such as LEAD-3 and LexRank [5].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Graph-based approaches on graph neural networks and specifically using knowledge graphs have gained popularity [8,12,20,21] while recent models combine graph neural networks with a transformer architecture [8,12].\n\nFor extractive MDS, a graph-based representation can help to detect salient vertices, so that they are included in the generated summary to cover all important information. In abstractive MDS, salient vertices can guide the summarization process via attention mechanisms of the transformer architecture.\n\nDifferent textual units have been used for graph-based text summarization in the past, as discourse relations [8,20], sentences [3,21], or paragraphs [12].\n\nThe GraphSum model showed substantial improvements compared to strong MDS baselines such as LEAD-3 and LexRank [5].",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[236478143 | Zhou et al. | 2021 | Citations: 28]",
                "snippets": "Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan et al., 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Wan, 2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, (Tan et al., 2017) propose a graph-based attention mechanism to identify salient sentences. (Yasunaga et al., 2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. (Fan et al., 2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. (huang et al., 2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. (Li et al., 2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, (Wang et al., 2020) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[6532096 | Yasunaga et al. | 2017 | Citations: 258]": "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.",
                    "[17446655 | Wan | 2008 | Citations: 82]": "The graph-based ranking algorithm has been recently exploited for multi-document summarization by making only use of the sentence-to-sentence relationships in the documents, under the assumption that all the sentences are indistinguishable. However, given a document set to be summarized, different documents are usually not equally important, and moreover, different sentences in a specific document are usually differently important. This paper aims to explore document impact on summarization performance. We propose a document-based graph model to incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Various methods are employed to evaluate the two factors. Experimental results on the DUC2001 and DUC2002 datasets demonstrate that the good effectiveness of the proposed model. Moreover, the results show the robustness of the proposed model.",
                    "[202785778 | Fan et al. | 2019 | Citations: 102]": "Query-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query, which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input, long-form question answering and multi-document summarization, feeding graph representations as input can achieve better performance than using retrieved text portions.",
                    "[216552978 | Wang et al. | 2020 | Citations: 282]": "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.",
                    "[218487279 | huang et al. | 2020 | Citations: 164]": "Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD. We propose the use of dual encoders\u2014a sequential document encoder and a graph-structured encoder\u2014to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.",
                    "[26698484 | Tan et al. | 2017 | Citations: 298]": "Abstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods."
                },
                "metadata": [
                    {
                        "section_title": "Graph-based Document Summarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1624,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 73
                            },
                            {
                                "start": 74,
                                "end": 174
                            },
                            {
                                "start": 175,
                                "end": 332
                            },
                            {
                                "start": 333,
                                "end": 474
                            },
                            {
                                "start": 475,
                                "end": 669
                            },
                            {
                                "start": 672,
                                "end": 813
                            },
                            {
                                "start": 814,
                                "end": 987
                            },
                            {
                                "start": 988,
                                "end": 1165
                            },
                            {
                                "start": 1166,
                                "end": 1339
                            },
                            {
                                "start": 1340,
                                "end": 1461
                            },
                            {
                                "start": 1462,
                                "end": 1624
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "17446655",
                            "26698484",
                            "6532096",
                            "202785778",
                            "218487279",
                            "218718706",
                            "216552978"
                        ],
                        "quote": "Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan et al., 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Wan, 2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, (Tan et al., 2017) propose a graph-based attention mechanism to identify salient sentences. (Yasunaga et al., 2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. (Fan et al., 2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. (huang et al., 2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. (Li et al., 2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, (Wang et al., 2020) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[239768911 | Chen et al. | 2021 | Citations: 22]",
                "snippets": "The graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. (Erkan et al., 2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2019) both use discourse graphs to generate concise and informative summaries. (Li et al., 2016) and (Li et al., 2021) propose to utilize event relation graph to represent documents for MDS....Most previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. (Erkan et al., 2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Christensen et al., 2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, (Yasunaga et al., 2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. (Li et al., 2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. (Wu et al., 2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[219036690 | Xu et al. | 2019 | Citations: 282]": "Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[6532096 | Yasunaga et al. | 2017 | Citations: 258]": "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.",
                    "[196170254 | Li et al. | 2021 | Citations: 30]": "The key to realize advanced document summarization is semantic representation of documents. This paper investigates the role of Semantic Link Network in representing and understanding documents for multi-document summarization. It proposes a novel abstractive multi-document summarization framework by first transforming documents into a Semantic Link Network of concepts and events and then transforming the Semantic Link Network into the summary of the documents based on the selection of important concepts and events while keeping semantics coherence. Experiments on benchmark datasets show that the proposed summarization approach significantly outperforms relevant state-of-the-art baselines and the Semantic Link Network plays an important role in representing and understanding documents.",
                    "[235187330 | Wu et al. | 2021 | Citations: 45]": "Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks."
                },
                "metadata": [
                    {
                        "section_title": "Summarization as Sub-graph Selection",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 545,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "506350",
                            "337730",
                            "219036690",
                            "6788641",
                            "196170254"
                        ],
                        "quote": "The graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. (Erkan et al., 2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2019) both use discourse graphs to generate concise and informative summaries. (Li et al., 2016) and (Li et al., 2021) propose to utilize event relation graph to represent documents for MDS"
                    },
                    {
                        "section_title": "Graph-based Summarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1328,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "506350",
                            "337730",
                            "6532096",
                            "218718706",
                            "235187330"
                        ],
                        "quote": ".Most previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. (Erkan et al., 2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Christensen et al., 2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, (Yasunaga et al., 2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. (Li et al., 2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. (Wu et al., 2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[240268841 | Krishnaveni et al. | 2022 | Citations: 0]",
                "snippets": "The earlier iterative graph algorithms are TextRank (Mihalcea & Tarau, 2004) and LexRank (Erkan et al., 2004). They can be applied to the summarization of a single or multiple documents in any language (Mihalcea et al., 2005). Even though they are the best graph ranking algorithms, they have high time complexity. Some recent graph based ranking research works are (Calvo et al., 2018), (Ye et al., 2018)(Ye et al., 2018)(Tixier et al., 2017)...The work of (Zheng et al., 2014) considers text summarization as a problem of finding the key paths composed of essential information and merges the common paths to remove redundancy. Han et al. (2016) propose the FrameNet-based semantic graph model. It uses FrameNet to calculate sentence similarity and assigns weights to both sentences and edges. After ranking, it selects summary sentences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[42957184 | Tixier et al. | 2017 | Citations: 28]": "We present a fully unsupervised, extractive text summarization system that leverages a submodularity framework introduced by past research. The framework allows summaries to be generated in a greedy way while preserving near-optimal performance guarantees. Our main contribution is the novel coverage reward term of the objective function optimized by the greedy algorithm. This component builds on the graph-of-words representation of text and the k-core decomposition algorithm to assign meaningful scores to words. We evaluate our approach on the AMI and ICSI meeting speech corpora, and on the DUC2001 news corpus. We reach state-of-the-art performance on all datasets. Results indicate that our method is particularly well-suited to the meeting domain."
                },
                "metadata": [
                    {
                        "section_title": "BACKGRoUNd",
                        "pdf_hash": "",
                        "start": 437,
                        "end": 875,
                        "sentence_offsets": [
                            {
                                "start": 437,
                                "end": 548
                            },
                            {
                                "start": 549,
                                "end": 665
                            },
                            {
                                "start": 666,
                                "end": 753
                            },
                            {
                                "start": 754,
                                "end": 876
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "8878897",
                            "44132297",
                            "125164049",
                            "125164049",
                            "42957184"
                        ],
                        "quote": "The earlier iterative graph algorithms are TextRank (Mihalcea & Tarau, 2004) and LexRank (Erkan et al., 2004). They can be applied to the summarization of a single or multiple documents in any language (Mihalcea et al., 2005). Even though they are the best graph ranking algorithms, they have high time complexity. Some recent graph based ranking research works are (Calvo et al., 2018), (Ye et al., 2018)(Ye et al., 2018)(Tixier et al., 2017)"
                    },
                    {
                        "section_title": "BACKGRoUNd",
                        "pdf_hash": "",
                        "start": 1471,
                        "end": 1866,
                        "sentence_offsets": [
                            {
                                "start": 1471,
                                "end": 1654
                            },
                            {
                                "start": 1655,
                                "end": 1721
                            },
                            {
                                "start": 1722,
                                "end": 1820
                            },
                            {
                                "start": 1821,
                                "end": 1865
                            }
                        ],
                        "ref_mentions": [
                            "35197081"
                        ],
                        "quote": "The work of (Zheng et al., 2014) considers text summarization as a problem of finding the key paths composed of essential information and merges the common paths to remove redundancy. Han et al. (2016) propose the FrameNet-based semantic graph model. It uses FrameNet to calculate sentence similarity and assigns weights to both sentences and edges. After ranking, it selects summary sentences."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[244510269 | Mridha et al. | 2021 | Citations: 88]",
                "snippets": "The graph-based method can be applied to both extractive and abstractive text summarization. This approach is an unsupervised learning method that rates the required sentences or terms using a graph. The purpose of the graphical process is to extract the most relevant sentences from a single text (Moratanch et al., 2017). \n\nGraph-based ranking algorithms determine the relevance of a vertex in a graph based on global information iteratively extracted from the entire chart. When it comes to text summarization, specific graph-based techniques are applied. 1) LexRank: LexRank is a probabilistic graph-based technique for calculating sentence significance based on the notion of eigenvector centrality in a graph representation of phrases for natural language processing. It is a connectivity matrix based on intra-sentence cosine similarity that is utilized as the adjacency matrix in a sentence graph representation (Erkan et al., 2004). (Otterbacher et al., 2009), (Wu et al., 2015), (Verma et al., 2016), (Li et al., 2016), (Samuel et al., 2016) proposed the LexRank algorithm for graph-based text summarization task. 2) Hyperlink-Induced Topic Search (HITS): Hyperlinkinduced topic search is a link analysis algorithm that determines the authority and hub values. The results for the search query are retrieved, and then the computation is performed only on this set of results. A hub value is the total of the scaled authority values of the pages it points to, and an authority value is the sum of the scaled authority values of the pages it points to (Litvak et al., 2008). Some articles focused on the HITS ranking algorithm for graphbased text summarization tasks, as can be seen in (Thakkar et al., 2010), (Mihalcea, 2004), (Bhandari et al., 2008), and (Zhang et al., 2005). 3) PageRank: The PageRank algorithm utilizes the inbound links of specified pages to measure their significance or quality to rank the search results. PageRank links more weight based on the importance of the page from which it originates [267]. Some articles proposed the PageRank algorithm for summarization (Elbarougy et al., 2020), (Mallick et al., 2018), (Wang et al., 2007), (Chongsuntornsri et al., 2006), (Sornil et al., 2006). 4) TextRank: TextRank is an unsupervised method for automatic text summarization to extract the most important keywords from a document.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[12222596 | Litvak et al. | 2008 | Citations: 311]": "In this paper, we introduce and compare between two novel approaches, supervised and unsupervised, for identifying the keywords to be used in extractive summarization of text documents. Both our approaches are based on the graph-based syntactic representation of text and web documents, which enhances the traditional vector-space model by taking into account some structural document features. In the supervised approach, we train classification algorithms on a summarized collection of documents with the purpose of inducing a keyword identification model. In the unsupervised approach, we run the HITS algorithm on document graphs under the assumption that the top-ranked nodes should represent the document keywords. Our experiments on a collection of benchmark summaries show that given a set of summarized training documents, the supervised classification provides the highest keyword identification accuracy, while the highest F-measure is reached with a simple degree-based ranking. In addition, it is sufficient to perform only the first iteration of HITS rather than running it to its convergence.",
                    "[2100529 | Mihalcea | 2004 | Citations: 520]": "This paper presents an innovative unsupervised method for automatic sentence extraction using graph-based ranking algorithms. We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks.",
                    "[38751984 | Samuel et al. | 2016 | Citations: 21]": "Summary generation is an important process in those conditions where the user needs to obtain the key features of the document without having to go through the whole document itself. The summarization process is of basically two types: 1 Single document Summarization and, 2 Multiple Document Summarization. But here the microblogging environment is taken into account which have a restriction on the number of characters contained within a post. Therefore, single document summarizers are not applicable to this condition. There are many features along which the summarization of the microblog post can be done for example, post's topic, it's posting time, happening of the event, etc. This paper proposes a method that includes the temporal features of the microblog posts to develop an extractive summary of the event from each and every post, which will further increase the quality of the summary created as it includes all the key features in the summary."
                },
                "metadata": [
                    {
                        "section_title": "E. EXTRACTIVE + ABSTRACTIVE 1) Graph Based Method",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2080,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 92
                            },
                            {
                                "start": 93,
                                "end": 199
                            },
                            {
                                "start": 200,
                                "end": 304
                            },
                            {
                                "start": 307,
                                "end": 457
                            },
                            {
                                "start": 458,
                                "end": 539
                            },
                            {
                                "start": 540,
                                "end": 754
                            },
                            {
                                "start": 755,
                                "end": 906
                            },
                            {
                                "start": 907,
                                "end": 1012
                            },
                            {
                                "start": 1013,
                                "end": 1159
                            },
                            {
                                "start": 1160,
                                "end": 1274
                            },
                            {
                                "start": 1275,
                                "end": 1455
                            },
                            {
                                "start": 1456,
                                "end": 1598
                            },
                            {
                                "start": 1599,
                                "end": 1749
                            },
                            {
                                "start": 1750,
                                "end": 1844
                            },
                            {
                                "start": 1845,
                                "end": 1943
                            },
                            {
                                "start": 1944,
                                "end": 2080
                            }
                        ],
                        "ref_mentions": [
                            "29980788",
                            "506350",
                            "17636912",
                            "40873863",
                            "63875451",
                            "17951450",
                            "38751984",
                            "12222596",
                            "21346422",
                            "2100529",
                            "5540065",
                            "18376399",
                            "213789058",
                            "69927855",
                            "33925572",
                            "14944858",
                            "15513453"
                        ],
                        "quote": "The graph-based method can be applied to both extractive and abstractive text summarization. This approach is an unsupervised learning method that rates the required sentences or terms using a graph. The purpose of the graphical process is to extract the most relevant sentences from a single text (Moratanch et al., 2017). \n\nGraph-based ranking algorithms determine the relevance of a vertex in a graph based on global information iteratively extracted from the entire chart. When it comes to text summarization, specific graph-based techniques are applied. 1) LexRank: LexRank is a probabilistic graph-based technique for calculating sentence significance based on the notion of eigenvector centrality in a graph representation of phrases for natural language processing. It is a connectivity matrix based on intra-sentence cosine similarity that is utilized as the adjacency matrix in a sentence graph representation (Erkan et al., 2004). (Otterbacher et al., 2009), (Wu et al., 2015), (Verma et al., 2016), (Li et al., 2016), (Samuel et al., 2016) proposed the LexRank algorithm for graph-based text summarization task. 2) Hyperlink-Induced Topic Search (HITS): Hyperlinkinduced topic search is a link analysis algorithm that determines the authority and hub values. The results for the search query are retrieved, and then the computation is performed only on this set of results. A hub value is the total of the scaled authority values of the pages it points to, and an authority value is the sum of the scaled authority values of the pages it points to (Litvak et al., 2008). Some articles focused on the HITS ranking algorithm for graphbased text summarization tasks, as can be seen in (Thakkar et al., 2010), (Mihalcea, 2004), (Bhandari et al., 2008), and (Zhang et al., 2005). 3) PageRank: The PageRank algorithm utilizes the inbound links of specified pages to measure their significance or quality to rank the search results. PageRank links more weight based on the importance of the page from which it originates [267]. Some articles proposed the PageRank algorithm for summarization (Elbarougy et al., 2020), (Mallick et al., 2018), (Wang et al., 2007), (Chongsuntornsri et al., 2006), (Sornil et al., 2006). 4) TextRank: TextRank is an unsupervised method for automatic text summarization to extract the most important keywords from a document."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[248571519 | Sankar et al. | 2022 | Citations: 0]",
                "snippets": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[10112929 | Li et al. | 2014 | Citations: 46]": "In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status \u2013 remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences. Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary. Compared with state-ofthe-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data."
                },
                "metadata": [
                    {
                        "section_title": "Multi Document Summarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1615,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 109
                            },
                            {
                                "start": 110,
                                "end": 276
                            },
                            {
                                "start": 277,
                                "end": 452
                            },
                            {
                                "start": 453,
                                "end": 619
                            },
                            {
                                "start": 620,
                                "end": 994
                            },
                            {
                                "start": 995,
                                "end": 1150
                            },
                            {
                                "start": 1151,
                                "end": 1363
                            },
                            {
                                "start": 1364,
                                "end": 1615
                            }
                        ],
                        "ref_mentions": [
                            "577937",
                            "506350",
                            "10112929",
                            "218718706",
                            "218718706"
                        ],
                        "quote": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[249097545 | Chen et al. | 2022 | Citations: 29]",
                "snippets": "Our extractive baselines include: LexRank (Erkan et al., 2004): a graph-based extractive model that computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences...EMS (Zhou et al., 2021): an entity-aware model for abstractive multi-document summarization with BERT encoder. It augments the classical Transformer based encoder-decoder framework with a graph consisting of paragraph nodes and entity nodes...RRG (Chen et al., 2021): an abstractive related work generator based on reference papers. It augments the RNN encoder with a relation-aware graph between multiple references. The relation graph and the document representation are refined iteratively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[236478143 | Zhou et al. | 2021 | Citations: 28]": ",",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[236460044 | Chen et al. | 2021 | Citations: 57]": "Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order. Most of existing related work generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion. Hence, in this paper, we propose a Relation-aware Related work Generator (RRG), which generates an abstractive related work from the given multiple scientific papers in the same research area. Concretely, we propose a relation-aware multi-document encoder that relates one document to another according to their content dependency in a relation graph. The relation graph and the document representation are interacted and polished iteratively, complementing each other in the training process. We also contribute two public datasets composed of related work sections and their corresponding papers. Extensive experiments on the two datasets show that the proposed model brings substantial improvements over several strong baselines. We hope that this work will promote advances in related work generation task."
                },
                "metadata": [
                    {
                        "section_title": "Baselines",
                        "pdf_hash": "",
                        "start": 314,
                        "end": 512,
                        "sentence_offsets": [
                            {
                                "start": 314,
                                "end": 513
                            }
                        ],
                        "ref_mentions": [
                            "506350"
                        ],
                        "quote": "Our extractive baselines include: LexRank (Erkan et al., 2004): a graph-based extractive model that computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences"
                    },
                    {
                        "section_title": "Baselines",
                        "pdf_hash": "",
                        "start": 1293,
                        "end": 1519,
                        "sentence_offsets": [
                            {
                                "start": 1293,
                                "end": 1388
                            },
                            {
                                "start": 1389,
                                "end": 1519
                            }
                        ],
                        "ref_mentions": [
                            "236478143"
                        ],
                        "quote": "EMS (Zhou et al., 2021): an entity-aware model for abstractive multi-document summarization with BERT encoder. It augments the classical Transformer based encoder-decoder framework with a graph consisting of paragraph nodes and entity nodes"
                    },
                    {
                        "section_title": "Baselines",
                        "pdf_hash": "",
                        "start": 1575,
                        "end": 1810,
                        "sentence_offsets": [
                            {
                                "start": 1575,
                                "end": 1648
                            },
                            {
                                "start": 1649,
                                "end": 1733
                            },
                            {
                                "start": 1734,
                                "end": 1809
                            }
                        ],
                        "ref_mentions": [
                            "236460044"
                        ],
                        "quote": "RRG (Chen et al., 2021): an abstractive related work generator based on reference papers. It augments the RNN encoder with a relation-aware graph between multiple references. The relation graph and the document representation are refined iteratively."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[250374734 | Chen et al. | 2022 | Citations: 2]",
                "snippets": "Existing multi-document summarization methods mainly focus on sentence-level summarization and can be categorized into extractive and abstractive methods. Most extractive methods are operated over graph-based representations of sentences or passages with edge weights computed by tf-idf (Erkan et al., 2004), discourse relations (Christensen et al., 2013) or sentence embeddings (Yasunaga et al., 2017b) and then a specific algorithm is further adopted for ranking text units for inclusion in the final summary. More recently, some extractive summarization works also utilize graph convolutional networks for salient sentences estimation (Kipf et al., 2016) and sentence ordering (Yin et al., 2019)...Intuitively, graph-based extractive methods are suitable to identify relationship of different sentences or documents and extract salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting. Hence, abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                },
                "metadata": [
                    {
                        "section_title": "Multi-document Summarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 698,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 154
                            },
                            {
                                "start": 155,
                                "end": 511
                            },
                            {
                                "start": 512,
                                "end": 699
                            }
                        ],
                        "ref_mentions": [
                            "506350"
                        ],
                        "quote": "Existing multi-document summarization methods mainly focus on sentence-level summarization and can be categorized into extractive and abstractive methods. Most extractive methods are operated over graph-based representations of sentences or passages with edge weights computed by tf-idf (Erkan et al., 2004), discourse relations (Christensen et al., 2013) or sentence embeddings (Yasunaga et al., 2017b) and then a specific algorithm is further adopted for ranking text units for inclusion in the final summary. More recently, some extractive summarization works also utilize graph convolutional networks for salient sentences estimation (Kipf et al., 2016) and sentence ordering (Yin et al., 2019)"
                    },
                    {
                        "section_title": "Multi-document Summarization",
                        "pdf_hash": "",
                        "start": 1107,
                        "end": 1462,
                        "sentence_offsets": [
                            {
                                "start": 1107,
                                "end": 1357
                            },
                            {
                                "start": 1358,
                                "end": 1461
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Intuitively, graph-based extractive methods are suitable to identify relationship of different sentences or documents and extract salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting. Hence, abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020)."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[252185277 | Wang et al. | 2022 | Citations: 15]",
                "snippets": "HDSG (Wang et al., 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "HDSG (Wang et al., 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[257219819 | Zesheng et al. | 2023 | Citations: 4]",
                "snippets": "In text summarization, Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Graph Neural Network",
                        "pdf_hash": "",
                        "start": 449,
                        "end": 1276,
                        "sentence_offsets": [
                            {
                                "start": 449,
                                "end": 645
                            },
                            {
                                "start": 646,
                                "end": 742
                            },
                            {
                                "start": 743,
                                "end": 880
                            },
                            {
                                "start": 881,
                                "end": 1043
                            },
                            {
                                "start": 1044,
                                "end": 1276
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In text summarization, Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[259129368 | Wang et al. | 2023 | Citations: 0]",
                "snippets": "\u2022 LexRank (Erkan et al., 2004) is an unsupervised graph-based summarization method. It employs a ranking algorithm to determine node centrality. In LexRank, sentences are treated as nodes to form a graph with weighted edges calculated using tf-idf. In our work, following the settings from (Angelidis et al., 2020), we also use BERT (Devlin et al., 2019) and SentiNeuron [34] vectors to calculate the adjacency matrices. \u2022 Opinosis (Ganesan et al., 2010) is a graph-based summarization framework that generates concise abstractive summaries of highly redundant opinions. It assumes no domain knowledge and leverages mostly the word order in the existing text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[227745131 | Angelidis et al. | 2020 | Citations: 111]": "Abstract We present the Quantized Transformer (QT), an unsupervised system for extractive opinion summarization. QT is inspired by Vector- Quantized Variational Autoencoders, which we repurpose for popularity-driven summarization. It uses a clustering interpretation of the quantized space and a novel extraction algorithm to discover popular opinions among hundreds of reviews, a significant step towards opinion summarization of practical scope. In addition, QT enables controllable summarization without further training, by utilizing properties of the quantized space to extract aspect-specific summaries. We also make publicly available Space, a large-scale evaluation benchmark for opinion summarizers, comprising general and aspect-specific summaries for 50 hotels. Experiments demonstrate the promise of our approach, which is validated by human studies where judges showed clear preference for our method over competitive baselines.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Baselines",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 588,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 67
                            },
                            {
                                "start": 68,
                                "end": 128
                            },
                            {
                                "start": 129,
                                "end": 232
                            },
                            {
                                "start": 233,
                                "end": 367
                            },
                            {
                                "start": 368,
                                "end": 499
                            },
                            {
                                "start": 500,
                                "end": 588
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "227745131",
                            "52967399",
                            "988010"
                        ],
                        "quote": "\u2022 LexRank (Erkan et al., 2004) is an unsupervised graph-based summarization method. It employs a ranking algorithm to determine node centrality. In LexRank, sentences are treated as nodes to form a graph with weighted edges calculated using tf-idf. In our work, following the settings from (Angelidis et al., 2020), we also use BERT (Devlin et al., 2019) and SentiNeuron [34] vectors to calculate the adjacency matrices. \u2022 Opinosis (Ganesan et al., 2010) is a graph-based summarization framework that generates concise abstractive summaries of highly redundant opinions. It assumes no domain knowledge and leverages mostly the word order in the existing text."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[259262067 | Nguyen et al. | 2023 | Citations: 0]",
                "snippets": "Graphs capturing relationships between textual units are of great benefit to MDS, which can help create more concise, informative, and coherent summaries from multiple sources documents. Furthermore, graphs can be easily constructed by representing sentences or paragraph as graph nodes and edges. LexRank (Erkan et al., 2004)) computes sentence importance based on a lexical similarity graph of sentences. Graph representations of documents such as discourse graph based on discourse relations (Christensen et al., 2013)...In this paper, we approach by switching back to the graph-based extractive problem, specifically, extracting summaries for multi-document by selecting sub-graphs based on the constructed graph. Inspired by study (Chen et al., 2021), we exploit more edge weights representing the relationship of sentences in the same document and train and improve the model for Vietnamese dataset.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 704,
                        "end": 1227,
                        "sentence_offsets": [
                            {
                                "start": 704,
                                "end": 890
                            },
                            {
                                "start": 891,
                                "end": 1001
                            },
                            {
                                "start": 1002,
                                "end": 1112
                            },
                            {
                                "start": 1113,
                                "end": 1228
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "337730"
                        ],
                        "quote": "Graphs capturing relationships between textual units are of great benefit to MDS, which can help create more concise, informative, and coherent summaries from multiple sources documents. Furthermore, graphs can be easily constructed by representing sentences or paragraph as graph nodes and edges. LexRank (Erkan et al., 2004)) computes sentence importance based on a lexical similarity graph of sentences. Graph representations of documents such as discourse graph based on discourse relations (Christensen et al., 2013)"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1812,
                        "end": 2194,
                        "sentence_offsets": [
                            {
                                "start": 1812,
                                "end": 2005
                            },
                            {
                                "start": 2006,
                                "end": 2193
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this paper, we approach by switching back to the graph-based extractive problem, specifically, extracting summaries for multi-document by selecting sub-graphs based on the constructed graph. Inspired by study (Chen et al., 2021), we exploit more edge weights representing the relationship of sentences in the same document and train and improve the model for Vietnamese dataset."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[264146402 | Wang et al. | 2023 | Citations: 4]",
                "snippets": "Traditional approaches usually employ extractive techniques both in the context of single-document and multi-document summarization. Graph-based methods can be applied to both single-document and multi-document summarization by representing the relationships between sentences in one document or several documents as a graph, with sentences as nodes and the edges as the similarity between the sentences. The systems (Erkan et al., 2004)(Lin et al., 2009)(Parveen et al., 2015)(Thakkar et al., 2010) then use algorithms like PageRank, HITS, or LexRank to identify the most important sentences in the graph, which are then extracted and combined to form the summary.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[269225 | Parveen et al. | 2015 | Citations: 117]": "We present an approach for extractive single-document summarization. Our approach is based on a weighted graphical representation of documents obtained by topic modeling. We optimize importance, coherence and non-redundancy simultaneously using ILP. We compare ROUGE scores of our system with state-of-the-art results on scientific articles from PLOS Medicine and on DUC 2002 data. Human judges evaluate the coherence of summaries generated by our system in comparision to two baselines. Our approach obtains competitive performance.",
                    "[8014853 | Lin et al. | 2009 | Citations: 90]": "We propose a novel approach for unsupervised extractive summarization. Our approach builds a semantic graph for the document to be summarized. Summary extraction is then formulated as optimizing submodular functions defined on the semantic graph. The optimization is theoretically guaranteed to be near-optimal under the framework of submodularity. Extensive experiments on the ICSI meeting summarization task on both human transcripts and automatic speech recognition (ASR) outputs show that the graph-based submodular selection approach consistently outperforms the maximum marginal relevance (MMR) approach, a concept-based approach using integer linear programming (ILP), and a recursive graph-based ranking algorithm using Google's PageRank."
                },
                "metadata": [
                    {
                        "section_title": "Source Document Quantity: Single-document vs",
                        "pdf_hash": "",
                        "start": 1061,
                        "end": 1659,
                        "sentence_offsets": [
                            {
                                "start": 1061,
                                "end": 1193
                            },
                            {
                                "start": 1194,
                                "end": 1465
                            },
                            {
                                "start": 1466,
                                "end": 1659
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "8014853",
                            "269225",
                            "21346422"
                        ],
                        "quote": "Traditional approaches usually employ extractive techniques both in the context of single-document and multi-document summarization. Graph-based methods can be applied to both single-document and multi-document summarization by representing the relationships between sentences in one document or several documents as a graph, with sentences as nodes and the edges as the similarity between the sentences. The systems (Erkan et al., 2004)(Lin et al., 2009)(Parveen et al., 2015)(Thakkar et al., 2010) then use algorithms like PageRank, HITS, or LexRank to identify the most important sentences in the graph, which are then extracted and combined to form the summary."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[264614849 | Ribeiro | 2022 | Citations: 60]",
                "snippets": "A growing body of work focuses on using graph-based representations for improving summarization. Whereas different approaches encode graphs into neural models for multi-document summarization (Fan et al., 2019;Li et al., 2020;Pasunuru et al., 2021;Chen et al., 2021), AMR structures have been shown to benefit both document representation and summary generation (Liu et al., 2015;Liao et al., 2018;Hardy and Vlachos, 2018;Dohare et al., 2018) and have the potential of improving controllability in summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[235097309 | Pasunuru et al. | 2021 | Citations: 73]": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.",
                    "[239768911 | Chen et al. | 2021 | Citations: 22]": "Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary, which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the coherence and conciseness of the whole summary. In this paper, we propose a novel MDS framework (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub-graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly outputs an integrate summary in the form of sub-graph which is more informative and coherent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architecture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks.",
                    "[202785778 | Fan et al. | 2019 | Citations: 102]": "Query-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query, which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input, long-form question answering and multi-document summarization, feeding graph representations as input can achieve better performance than using retrieved text portions.",
                    "[49210924 | Liao et al. | 2018 | Citations: 105]": "Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.",
                    "[5001921 | Liu et al. | 2018 | Citations: 302]": "We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-tograph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-totext generator. The framework is data-driven, trainable, and not specifically designed for a particular domain. Experiments on goldstandard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization",
                    "[51867906 | Dohare et al. | 2018 | Citations: 19]": "Automatic abstractive summary generation remains a significant open problem for natural language processing. In this work, we develop a novel pipeline for Semantic Abstractive Summarization (SAS). SAS, as introduced by Liu et. al. (2015) first generates an AMR graph of an input story, through which it extracts a summary graph and finally, creates summary sentences from this summary graph. Compared to earlier approaches, we develop a more comprehensive method to generate the story AMR graph using state-of-the-art co-reference resolution and Meta Nodes. Which we then use in a novel unsupervised algorithm based on how humans summarize a piece of text to extract the summary sub-graph. Our algorithm outperforms the state of the art SAS method by 1.7% F1 score in node prediction.",
                    "[52111191 | Hardy et al. | 2018 | Citations: 69]": "Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance on later parses is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 36,
                        "end": 548,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "202785778",
                            "218718706",
                            "235097309",
                            "239768911",
                            "5001921",
                            "49210924",
                            "52111191",
                            "51867906"
                        ],
                        "quote": "A growing body of work focuses on using graph-based representations for improving summarization. Whereas different approaches encode graphs into neural models for multi-document summarization (Fan et al., 2019;Li et al., 2020;Pasunuru et al., 2021;Chen et al., 2021), AMR structures have been shown to benefit both document representation and summary generation (Liu et al., 2015;Liao et al., 2018;Hardy and Vlachos, 2018;Dohare et al., 2018) and have the potential of improving controllability in summarization."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[264817506 | Mao et al. | 2023 | Citations: 2]",
                "snippets": "There are several simple unsupervised summarization extraction frameworks, including TextRank (Mihalcea et al., 2004) and LexRank (Erkan et al., 2004), as well as more robust graph-based ranking methods such as Pac-Sum (Zheng et al., 2019), FAR (Liang et al., 2021), DASG (Liu et al., 2021). Graph-based ranking methods take sentence representations as input, using the algorithm of graph-based sentence centrality ranking for sentence selection. We now introduce extractive summarization backbones. \n\n\u2022 TextRank and LexRank utilize PageRank to calculate node centrality based on a Markov chain model recursively. \n\n\u2022 PacSum (Zheng and Lapata, 2019) constructs graphs with directed edges. The rationale behind this approach is that the centrality of two nodes is influenced by their relative position in the document, as illustrated by Equation 15. \n\n\u2022 DASG (Liu et al., 2021) selects sentences for summarization based on the similarities and relative distances among neighbouring sentences. It incorporates a graph edge weighting scheme to Equation 15, using a coefficient that maps a pair of sentence indices to a value calculated by their relative distance. \n\n\u2022 FAR (Liang et al., 2021) modifies Equation 15by applying a facet-aware centrality-based ranking model to filter out insignificant sentences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[182952892 | Zheng et al. | 2019 | Citations: 171]": "Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.",
                    "[235792514 | Liu et al. | 2021 | Citations: 29]": "Supervised summarization has made significant improvements in recent years by leveraging cutting-edge deep learning technologies. However, the true success of supervised methods relies on the availability of large quantity of human-generated summaries of documents, which is highly costly and difficult to obtain in general. This paper proposes an unsupervised approach to extractive text summarization, which uses an automatically constructed sentence graph from each document to select salient sentences for summarization based on both the similarities and relative distances in the neighborhood of each sentences. We further generalize our approach from single-document summarization to a multi-document setting, by aggregating document-level graphs via proximity-based cross-document edges. In our experiments on benchmark datasets, the proposed approach achieved competitive or better results than previous state-of-the-art unsupervised extractive summarization methods in both single-document and multi-document settings, and the performance is competitive to strong supervised baselines.",
                    "[236477774 | Liang et al. | 2021 | Citations: 44]": "Unsupervised extractive summarization aims to extract salient sentences from documents without labeled corpus. Existing methods are mostly graph-based by computing sentence centrality. These methods usually tend to select sentences within the same facet, however, which often leads to the facet bias problem especially when the document has multiple facets (i.e. long-document and multi-documents). To address this problem, we proposed a novel facet-aware centrality-based ranking model. We let the model pay more attention to different facets by introducing a sentence-document weight. The weight is added to the sentence centrality score. We evaluate our method on a wide range of summarization tasks that include 8 representative benchmark datasets. Experimental re-sults show that our method consistently out-performs strong baselines especially in long-and multi-document scenarios and even performs comparably to some supervised models. Extensive analyses con\ufb01rm that the performance gains come from alleviating the facet bias problem."
                },
                "metadata": [
                    {
                        "section_title": "Backbones of Summarization Approaches",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1315,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 301
                            },
                            {
                                "start": 302,
                                "end": 456
                            },
                            {
                                "start": 457,
                                "end": 509
                            },
                            {
                                "start": 512,
                                "end": 623
                            },
                            {
                                "start": 626,
                                "end": 698
                            },
                            {
                                "start": 699,
                                "end": 858
                            },
                            {
                                "start": 861,
                                "end": 1001
                            },
                            {
                                "start": 1002,
                                "end": 1170
                            },
                            {
                                "start": 1173,
                                "end": 1315
                            }
                        ],
                        "ref_mentions": [
                            "577937",
                            "506350",
                            "182952892",
                            "236477774",
                            "235792514",
                            "235792514",
                            "236477774"
                        ],
                        "quote": "There are several simple unsupervised summarization extraction frameworks, including TextRank (Mihalcea et al., 2004) and LexRank (Erkan et al., 2004), as well as more robust graph-based ranking methods such as Pac-Sum (Zheng et al., 2019), FAR (Liang et al., 2021), DASG (Liu et al., 2021). Graph-based ranking methods take sentence representations as input, using the algorithm of graph-based sentence centrality ranking for sentence selection. We now introduce extractive summarization backbones. \n\n\u2022 TextRank and LexRank utilize PageRank to calculate node centrality based on a Markov chain model recursively. \n\n\u2022 PacSum (Zheng and Lapata, 2019) constructs graphs with directed edges. The rationale behind this approach is that the centrality of two nodes is influenced by their relative position in the document, as illustrated by Equation 15. \n\n\u2022 DASG (Liu et al., 2021) selects sentences for summarization based on the similarities and relative distances among neighbouring sentences. It incorporates a graph edge weighting scheme to Equation 15, using a coefficient that maps a pair of sentence indices to a value calculated by their relative distance. \n\n\u2022 FAR (Liang et al., 2021) modifies Equation 15by applying a facet-aware centrality-based ranking model to filter out insignificant sentences."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[269157041 | Wang et al. | 2024 | Citations: 2]",
                "snippets": "According to the paradigm adopted, previous studies can be categorized into graph-based (Chen et al., 2022)(Wang et al., 2022), flat-based (Moro et al., 2022)(Shi et al., 2023), and hierarchical-based methods (Chen et al., 2021)[30].Concretely, graph-based methods construct external graphs (e.g., entity-relation graph, AMR, TF-IDF) to assist document representation and cross-document relation modeling.In this regard, Chen et al. (Chen et al., 2022) leverage graphs to encode source articles, constructing keyphrase graphs and using a multi-level contrastive learning strategy to improve the accuracy of generated summaries.\n\nWang et al. (Wang et al., 2022) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249097545 | Chen et al. | 2022 | Citations: 29]": "The related work section is an important component of a scientific paper, which highlights the contribution of the target paper in the context of the reference papers. Authors can save their time and effort by using the automatically generated related work section as a draft to complete the final related work. Most of the existing related work section generation methods rely on extracting off-the-shelf sentences to make a comparative discussion about the target work and the reference papers. However, such sentences need to be written in advance and are hard to obtain in practice. Hence, in this paper, we propose an abstractive target-aware related work generator (TAG), which can generate related work sections consisting of new sentences. Concretely, we first propose a target-aware graph encoder, which models the relationships between reference papers and the target paper with target-centered attention mechanisms. In the decoding process, we propose a hierarchical decoder that attends to the nodes of different levels in the graph with keyphrases as semantic indicators. Finally, to generate a more informative related work, we propose multi-level contrastive optimization objectives, which aim to maximize the mutual information between the generated related work with the references and minimize that with non-references. Extensive experiments on two public scholar datasets show that the proposed model brings substantial improvements over several strong baselines in terms of automatic and tailored human evaluations.",
                    "[252185277 | Wang et al. | 2022 | Citations: 15]": "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.",
                    "[236460044 | Chen et al. | 2021 | Citations: 57]": "Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order. Most of existing related work generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion. Hence, in this paper, we propose a Relation-aware Related work Generator (RRG), which generates an abstractive related work from the given multiple scientific papers in the same research area. Concretely, we propose a relation-aware multi-document encoder that relates one document to another according to their content dependency in a relation graph. The relation graph and the document representation are interacted and polished iteratively, complementing each other in the training process. We also contribute two public datasets composed of related work sections and their corresponding papers. Extensive experiments on the two datasets show that the proposed model brings substantial improvements over several strong baselines. We hope that this work will promote advances in related work generation task.",
                    "[248780330 | Moro et al. | 2022 | Citations: 32]": "Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can be vital. Others leverage linear model approximations to apply multi-input concatenation, worsening the results because all information is considered, even if it is conflicting or noisy with respect to a shared background. Despite the importance and social impact of medicine, there are no ad-hoc solutions for multi-document summarization. For this reason, we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization. Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews. Moreover, we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method.",
                    "[266176619 | Shi et al. | 2023 | Citations: 12]": ","
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK 2.1 Multi-Document Scientific Summarization",
                        "pdf_hash": "",
                        "start": 113,
                        "end": 785,
                        "sentence_offsets": [
                            {
                                "start": 113,
                                "end": 267
                            },
                            {
                                "start": 267,
                                "end": 439
                            },
                            {
                                "start": 439,
                                "end": 645
                            },
                            {
                                "start": 647,
                                "end": 785
                            }
                        ],
                        "ref_mentions": [
                            "249097545",
                            "252185277",
                            "248780330",
                            "266176619",
                            "236460044",
                            "249097545",
                            "252185277"
                        ],
                        "quote": "According to the paradigm adopted, previous studies can be categorized into graph-based (Chen et al., 2022)(Wang et al., 2022), flat-based (Moro et al., 2022)(Shi et al., 2023), and hierarchical-based methods (Chen et al., 2021)[30].Concretely, graph-based methods construct external graphs (e.g., entity-relation graph, AMR, TF-IDF) to assist document representation and cross-document relation modeling.In this regard, Chen et al. (Chen et al., 2022) leverage graphs to encode source articles, constructing keyphrase graphs and using a multi-level contrastive learning strategy to improve the accuracy of generated summaries.\n\nWang et al. (Wang et al., 2022) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[269762702 | Qu | 2024 | Citations: 0]",
                "snippets": "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP...Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235097309 | Pasunuru et al. | 2021 | Citations: 73]": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.",
                    "[236478143 | Zhou et al. | 2021 | Citations: 28]": ",",
                    "[174799390 | Fabbri et al. | 2019 | Citations: 590]": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",
                    "[204960716 | Lewis et al. | 2019 | Citations: 10856]": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
                },
                "metadata": [
                    {
                        "section_title": "STATE OF THE ART",
                        "pdf_hash": "",
                        "start": 270,
                        "end": 878,
                        "sentence_offsets": [
                            {
                                "start": 228,
                                "end": 547
                            },
                            {
                                "start": 547,
                                "end": 740
                            },
                            {
                                "start": 740,
                                "end": 879
                            }
                        ],
                        "ref_mentions": [
                            "236478143"
                        ],
                        "quote": "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP"
                    },
                    {
                        "section_title": "STATE OF THE ART",
                        "pdf_hash": "",
                        "start": 879,
                        "end": 1729,
                        "sentence_offsets": [
                            {
                                "start": 879,
                                "end": 1129
                            },
                            {
                                "start": 1129,
                                "end": 1335
                            },
                            {
                                "start": 1335,
                                "end": 1466
                            },
                            {
                                "start": 1466,
                                "end": 1729
                            }
                        ],
                        "ref_mentions": [
                            "235097309",
                            "174799390",
                            "204960716",
                            "235097309"
                        ],
                        "quote": "Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[273784877 | Ragazzi et al. | 2025 | Citations: 0]",
                "snippets": "Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 155,
                        "end": 560,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[273963834 | Zhang et al. | 2025 | Citations: 0]",
                "snippets": "In this regard, LexRank (Erkan et al., 2004) and TextRank (Mihalcea and Tarau 2004) first introduced graphs to extractive text summarization in 2004. They compute sentence importance using a graph representation of sentences to extract salient textual units from documents as summarization. (Zou, 2020), Wang et al. (Wang et al. 2020) propose to extract salient textual units from documents as summarization using a heterogeneous graph consisting of semantic nodes at several granularity levels of documents. In 2022, Wang et al. (Wang et al., 2022)) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template to achieve state-of-the-art performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[252185277 | Wang et al. | 2022 | Citations: 15]": "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[216342059 | Zou | 2020 | Citations: 261]": "Knowledge graphs, representation of information as a semantic graph, have caused wide concern in both industrial and academic world. Their property of providing semantically structured information has brought important possible solutions for many tasks including question answering, recommendation and information retrieval, and is considered to offer great promise for building more intelligent machines by many researchers. Although knowledge graphs have already supported multiple \u201cBig Data\u201d applications in all sorts of commercial and scientific domains since Google coined this term in 2012, there was no previous study give a systemically review of the application of knowledge graphs. Therefore, unlike other related work which focuses on the construction techniques of knowledge graphs, this present paper aims at providing a first survey on these applications stemming from different domains. This paper also points out that while important advancements of applying knowledge graphs\u2019 great ability of providing semantically structured information into specific domains have been made in recent years, several aspects still remain to be explored."
                },
                "metadata": [
                    {
                        "section_title": "Graphs in MSDS Tasks",
                        "pdf_hash": "",
                        "start": 217,
                        "end": 925,
                        "sentence_offsets": [
                            {
                                "start": 217,
                                "end": 368
                            },
                            {
                                "start": 369,
                                "end": 509
                            },
                            {
                                "start": 510,
                                "end": 723
                            },
                            {
                                "start": 724,
                                "end": 925
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "216342059",
                            "252185277"
                        ],
                        "quote": "In this regard, LexRank (Erkan et al., 2004) and TextRank (Mihalcea and Tarau 2004) first introduced graphs to extractive text summarization in 2004. They compute sentence importance using a graph representation of sentences to extract salient textual units from documents as summarization. (Zou, 2020), Wang et al. (Wang et al. 2020) propose to extract salient textual units from documents as summarization using a heterogeneous graph consisting of semantic nodes at several granularity levels of documents. In 2022, Wang et al. (Wang et al., 2022)) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template to achieve state-of-the-art performance."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[3230186 | Nayeem et al. | 2017 | Citations: 39]",
                "snippets": "LexRank (Erkan et al., 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods of computing sentence importance for text summarization. The RegSum system (Hong and Nenkova, 2014) employs a supervised model for predicting word importance. Treating multidocument summarization as a submodular maximization problem has proven successful by (Lin et al., 2011). Unfortunately, none of the above systems care about the coherence of the final extracted summary. \n\nIn very recent works using neural network, (Cheng et al., 2016) proposed an attentional encoder-decoder and (Nallapati et al., 2016) used a simple recurrent network based sequence classifier to solve the problem of extractive summarization. However, they are limited to single document settings, where sentences are implicitly ordered according to the sentence position. (Parveen and Strube, 2015;Parveen et al., 2015) arXiv:1706.06542v1 [cs.CL] 12 Jun 2017 proposed graph-based techniques to tackle coherence, which is also limited to single document summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[1499080 | Cheng et al. | 2016 | Citations: 810]": "Traditional approaches to extractive summarization rely heavily on humanengineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs 1 . Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.",
                    "[6405271 | Nallapati et al. | 2016 | Citations: 1265]": "\n \n We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.\n \n"
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 186,
                        "end": 1234,
                        "sentence_offsets": [
                            {
                                "start": 186,
                                "end": 338
                            },
                            {
                                "start": 339,
                                "end": 440
                            },
                            {
                                "start": 441,
                                "end": 563
                            },
                            {
                                "start": 564,
                                "end": 661
                            },
                            {
                                "start": 664,
                                "end": 908
                            },
                            {
                                "start": 909,
                                "end": 1038
                            },
                            {
                                "start": 1039,
                                "end": 1105
                            },
                            {
                                "start": 1106,
                                "end": 1234
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "320371",
                            "1499080",
                            "6405271"
                        ],
                        "quote": "LexRank (Erkan et al., 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods of computing sentence importance for text summarization. The RegSum system (Hong and Nenkova, 2014) employs a supervised model for predicting word importance. Treating multidocument summarization as a submodular maximization problem has proven successful by (Lin et al., 2011). Unfortunately, none of the above systems care about the coherence of the final extracted summary. \n\nIn very recent works using neural network, (Cheng et al., 2016) proposed an attentional encoder-decoder and (Nallapati et al., 2016) used a simple recurrent network based sequence classifier to solve the problem of extractive summarization. However, they are limited to single document settings, where sentences are implicitly ordered according to the sentence position. (Parveen and Strube, 2015;Parveen et al., 2015) arXiv:1706.06542v1 [cs.CL] 12 Jun 2017 proposed graph-based techniques to tackle coherence, which is also limited to single document summarization."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[33696521 | Kumar et al. | 2016 | Citations: 72]",
                "snippets": "This approach differs from the cluster based approach where sentences are ranked based on its closeness to cluster centroid. Two well-known graph based ranking algorithms is the HITS algorithm (Kleinberg, 1999) and the Google's PageRank (Brin and Page, 2012). Both methods have been traditionally used in Web-link analysis and social networks. Lexrank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are two successful graph-based ranking systems that implement these algorithms.\n\nFurther studies have been carried to make improvement through modification in the ranking algorithm. Wan and Yang (2006) assigned different weights to intra-document links and inter-document links. \n\nThey gave more priority to sentence with high interdocument links. In later work by Hariharan and Srinivasan (2009), they approached the graph based method differently i.e., by discounting the already selected sentence by removing it from further consideration when they rank the remaining sentences in the document. \n\nApart from sentence level information, Wan (2008) and Wei et al. (2010) devised a document-sensitive graph model to explore document impact on the graphbased summarization, by incorporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[5457260 | Wan et al. | 2006 | Citations: 109]": "This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems."
                },
                "metadata": [
                    {
                        "section_title": "Graph Based Method",
                        "pdf_hash": "",
                        "start": 845,
                        "end": 2164,
                        "sentence_offsets": [
                            {
                                "start": 837,
                                "end": 1048
                            },
                            {
                                "start": 1051,
                                "end": 1175
                            },
                            {
                                "start": 1176,
                                "end": 1310
                            },
                            {
                                "start": 1311,
                                "end": 1394
                            },
                            {
                                "start": 1395,
                                "end": 1546
                            },
                            {
                                "start": 1549,
                                "end": 1649
                            },
                            {
                                "start": 1650,
                                "end": 1746
                            },
                            {
                                "start": 1749,
                                "end": 1815
                            },
                            {
                                "start": 1816,
                                "end": 2065
                            },
                            {
                                "start": 2068,
                                "end": 2371
                            }
                        ],
                        "ref_mentions": [
                            "216000619",
                            "911040",
                            "506350",
                            "5457260",
                            "62241768"
                        ],
                        "quote": "This approach differs from the cluster based approach where sentences are ranked based on its closeness to cluster centroid. Two well-known graph based ranking algorithms is the HITS algorithm (Kleinberg, 1999) and the Google's PageRank (Brin and Page, 2012). Both methods have been traditionally used in Web-link analysis and social networks. Lexrank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are two successful graph-based ranking systems that implement these algorithms.\n\nFurther studies have been carried to make improvement through modification in the ranking algorithm. Wan and Yang (2006) assigned different weights to intra-document links and inter-document links. \n\nThey gave more priority to sentence with high interdocument links. In later work by Hariharan and Srinivasan (2009), they approached the graph based method differently i.e., by discounting the already selected sentence by removing it from further consideration when they rank the remaining sentences in the document. \n\nApart from sentence level information, Wan (2008) and Wei et al. (2010) devised a document-sensitive graph model to explore document impact on the graphbased summarization, by incorporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[38851848 | Sarker et al. | 2017 | Citations: 5]",
                "snippets": "Graph-based approaches have also been applied to text summarisation [Mani and Bloedorn 1997;Mani and Maybury 1999;(Erkan et al., 2004)(Leskovec et al., 2005), with Mani and Bloedorn [1997] being the pioneers in this area. In their approach, the authors use nodes to represent words and edges between nodes represent relationships. The summaries generated can be topic driven, and there is no text in the summaries. Instead, the summary content is represented via as nodes and edges that represent contents and relations between them. When summarising a pair of documents, common nodes represent same words or synonyms, while difference nodes are those that are not common. Sentence selection from the graph is computed from the average activated weights of the covered words: for a sentence s, its score in terms of coverage of common nodes is given by the following formula: \n\nwhere c(s) = {w|w \u2208 Common s}. The score for differences is calculated similarly. The sentences with higher common and difference scores are selected for the final summary. (Erkan et al., 2004) presented the LexRank system for multi-document summarisation, which is a graph-based system using a connected, undirected graph to represent documents. A similar method, suitable for single-document summarisation only, was proposed by (Miike et al., 1994). Other graph based approaches have been proposed, both in the medical domain [Reeve et al. 2007;Fiszman et al. 2004] and outside it [Litvak and Last 2008].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "3.3.3",
                        "pdf_hash": "",
                        "start": 584,
                        "end": 2074,
                        "sentence_offsets": [
                            {
                                "start": 584,
                                "end": 804
                            },
                            {
                                "start": 805,
                                "end": 913
                            },
                            {
                                "start": 914,
                                "end": 997
                            },
                            {
                                "start": 998,
                                "end": 1116
                            },
                            {
                                "start": 1117,
                                "end": 1255
                            },
                            {
                                "start": 1256,
                                "end": 1458
                            },
                            {
                                "start": 1461,
                                "end": 1491
                            },
                            {
                                "start": 1492,
                                "end": 1542
                            },
                            {
                                "start": 1543,
                                "end": 1633
                            },
                            {
                                "start": 1634,
                                "end": 1809
                            },
                            {
                                "start": 1810,
                                "end": 1919
                            },
                            {
                                "start": 1920,
                                "end": 2074
                            }
                        ],
                        "ref_mentions": [
                            "2962090",
                            "16136677",
                            "2962090",
                            "14130036"
                        ],
                        "quote": "Graph-based approaches have also been applied to text summarisation [Mani and Bloedorn 1997;Mani and Maybury 1999;(Erkan et al., 2004)(Leskovec et al., 2005), with Mani and Bloedorn [1997] being the pioneers in this area. In their approach, the authors use nodes to represent words and edges between nodes represent relationships. The summaries generated can be topic driven, and there is no text in the summaries. Instead, the summary content is represented via as nodes and edges that represent contents and relations between them. When summarising a pair of documents, common nodes represent same words or synonyms, while difference nodes are those that are not common. Sentence selection from the graph is computed from the average activated weights of the covered words: for a sentence s, its score in terms of coverage of common nodes is given by the following formula: \n\nwhere c(s) = {w|w \u2208 Common s}. The score for differences is calculated similarly. The sentences with higher common and difference scores are selected for the final summary. (Erkan et al., 2004) presented the LexRank system for multi-document summarisation, which is a graph-based system using a connected, undirected graph to represent documents. A similar method, suitable for single-document summarisation only, was proposed by (Miike et al., 1994). Other graph based approaches have been proposed, both in the medical domain [Reeve et al. 2007;Fiszman et al. 2004] and outside it [Litvak and Last 2008]."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[506350 | Erkan et al. | 2004 | Citations: 3097]",
                "snippets": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[5457260 | Wan et al. | 2006 | Citations: 109]",
                "snippets": "Graph-based methods have been proposed to rank sentences or passages. Websumm (Mani et al., 1997) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1495,
                        "end": 2033,
                        "sentence_offsets": [
                            {
                                "start": 1495,
                                "end": 1564
                            },
                            {
                                "start": 1565,
                                "end": 1756
                            },
                            {
                                "start": 1757,
                                "end": 1889
                            },
                            {
                                "start": 1890,
                                "end": 2033
                            }
                        ],
                        "ref_mentions": [
                            "9177142"
                        ],
                        "quote": "Graph-based methods have been proposed to rank sentences or passages. Websumm (Mani et al., 1997) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[59599960 | Lierde et al. | 2019 | Citations: 55]",
                "snippets": "Extractive summarizers differ in the method used to identify relevant sentences, which leads to a classification of models as either feature-based or graph-based approaches...Since feature-based approaches generally require datasets with labelled sentences which are hard to produce [12], unsupervised graph-based methods have attracted growing interest in recent years.\n\nGraph-based summarizers represent the sentences of a corpus as the nodes of a graph with the edges modelling relationships of similarity between the sentences [1]. Then, graph-based algorithms are applied to identify relevant sentences. The models generally differ in the type of relationship captured by the graph or in the sentence selection approach. Most graph-based models define the edges connecting sentences based on the co-occurrence of terms in pairs of sentences [1,3,4]. Then, important sentences are identified either based on node ranking algorithms, or using a global optimization approach. Methods based on node ranking compute individual relevance scores for the sentences and build summaries with highly scored sentences. The earliest such summarizer, LexRank [1], applies the PageRank algorithm to compute sentence scores.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[10274362 | Otterbacher et al. | 2005 | Citations: 179]": "We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. \"How many victims have been found?\") Judges found sentences providing an answer to each question. To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap. In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline."
                },
                "metadata": [
                    {
                        "section_title": "Background and related work",
                        "pdf_hash": "",
                        "start": 926,
                        "end": 1098,
                        "sentence_offsets": [
                            {
                                "start": 926,
                                "end": 1099
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Extractive summarizers differ in the method used to identify relevant sentences, which leads to a classification of models as either feature-based or graph-based approaches"
                    },
                    {
                        "section_title": "Background and related work",
                        "pdf_hash": "",
                        "start": 1125,
                        "end": 2164,
                        "sentence_offsets": [
                            {
                                "start": 1100,
                                "end": 1267
                            },
                            {
                                "start": 1268,
                                "end": 1363
                            },
                            {
                                "start": 1364,
                                "end": 1559
                            },
                            {
                                "start": 1562,
                                "end": 1725
                            },
                            {
                                "start": 1726,
                                "end": 1798
                            },
                            {
                                "start": 1799,
                                "end": 1915
                            },
                            {
                                "start": 1916,
                                "end": 2044
                            },
                            {
                                "start": 2045,
                                "end": 2167
                            }
                        ],
                        "ref_mentions": [
                            "7992772",
                            "556431",
                            "506350",
                            "506350",
                            "10274362",
                            "27105029"
                        ],
                        "quote": "Since feature-based approaches generally require datasets with labelled sentences which are hard to produce [12], unsupervised graph-based methods have attracted growing interest in recent years.\n\nGraph-based summarizers represent the sentences of a corpus as the nodes of a graph with the edges modelling relationships of similarity between the sentences [1]. Then, graph-based algorithms are applied to identify relevant sentences. The models generally differ in the type of relationship captured by the graph or in the sentence selection approach. Most graph-based models define the edges connecting sentences based on the co-occurrence of terms in pairs of sentences [1,3,4]. Then, important sentences are identified either based on node ranking algorithms, or using a global optimization approach. Methods based on node ranking compute individual relevance scores for the sentences and build summaries with highly scored sentences. The earliest such summarizer, LexRank [1], applies the PageRank algorithm to compute sentence scores."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[63821954 | Canhasi | 2016 | Citations: 4]",
                "snippets": "Graph-based methods like TextRank [12] and PageRank (Erkan et al., 2004) model a document or a set of documents as a text similarity graph, constructed by taking sentences as vertices and the similarity between sentences as edge weights. They take into account the global information and recursively calculate the sentence significance from the entire text graph rather than simply relying on unconnected individual sentences. From an NLP perspective, extractive summarization embodies two criteria: sentence relevance and sentence redundancy. Graph-based sentence ranking algorithms successfully merge both of these criteria into a single framework, by utilizing the so-called graph-based lexical centrality principle. Graph-based ranking algorithms were also used in query-focused summarization when it became a popular research topic. For instance, a topic-sensitive version of LexRank is proposed by (Otterbacher et al., 2009). It integrates the relevance of a sentence to the query into LexRank to get a biased PageRank ranking.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 286,
                        "end": 1281,
                        "sentence_offsets": [
                            {
                                "start": 286,
                                "end": 507
                            },
                            {
                                "start": 508,
                                "end": 696
                            },
                            {
                                "start": 697,
                                "end": 813
                            },
                            {
                                "start": 814,
                                "end": 989
                            },
                            {
                                "start": 990,
                                "end": 1107
                            },
                            {
                                "start": 1108,
                                "end": 1179
                            },
                            {
                                "start": 1180,
                                "end": 1281
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "17636912"
                        ],
                        "quote": "Graph-based methods like TextRank [12] and PageRank (Erkan et al., 2004) model a document or a set of documents as a text similarity graph, constructed by taking sentences as vertices and the similarity between sentences as edge weights. They take into account the global information and recursively calculate the sentence significance from the entire text graph rather than simply relying on unconnected individual sentences. From an NLP perspective, extractive summarization embodies two criteria: sentence relevance and sentence redundancy. Graph-based sentence ranking algorithms successfully merge both of these criteria into a single framework, by utilizing the so-called graph-based lexical centrality principle. Graph-based ranking algorithms were also used in query-focused summarization when it became a popular research topic. For instance, a topic-sensitive version of LexRank is proposed by (Otterbacher et al., 2009). It integrates the relevance of a sentence to the query into LexRank to get a biased PageRank ranking."
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[6532096 | Yasunaga et al. | 2017 | Citations: 258]",
                "snippets": "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 903,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems."
                    }
                ]
            },
            {
                "idx": 41,
                "key": "[8437725 | Parveen et al. | 2014 | Citations: 16]",
                "snippets": "A graph-based representation of documents for summarization is adopted by various approaches. For instance, TextRank by Mihalcea and Tarau (2004) applies the PageRank algorithm (Brin and Page, 1998) to extract important sentences for single document summarization. This ranking algorithm proclaims the importance of a sentence by considering the global information which is computed recursively from the entire graph. Later, the graph is converted into a weighted graph in which the weights are calculated by measuring the similarity of sentences (Mihalcea, 2004). Similarly, in the LexRank approach (Erkan et al., 2004), documents are represented as a similarity graph in which the sentences are nodes and these sentences are then ranked according to centrality measures. The three centrality measures used are degree, LexRank with threshold and continuous LexRank. LexRank is a measure to calculate ranks using the similarity graph of sentences. It is also known as lexical PageRank. The summarization approach developed by (Gong et al., 2001) is also based on ranking sentences where important sentences are selected using a relevance measure and latent semantic analysis.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[2100529 | Mihalcea | 2004 | Citations: 520]": "This paper presents an innovative unsupervised method for automatic sentence extraction using graph-based ranking algorithms. We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1178,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 93
                            },
                            {
                                "start": 94,
                                "end": 264
                            },
                            {
                                "start": 265,
                                "end": 417
                            },
                            {
                                "start": 418,
                                "end": 564
                            },
                            {
                                "start": 565,
                                "end": 775
                            },
                            {
                                "start": 776,
                                "end": 869
                            },
                            {
                                "start": 870,
                                "end": 950
                            },
                            {
                                "start": 951,
                                "end": 988
                            },
                            {
                                "start": 989,
                                "end": 1178
                            }
                        ],
                        "ref_mentions": [
                            "2100529",
                            "506350",
                            "7003631"
                        ],
                        "quote": "A graph-based representation of documents for summarization is adopted by various approaches. For instance, TextRank by Mihalcea and Tarau (2004) applies the PageRank algorithm (Brin and Page, 1998) to extract important sentences for single document summarization. This ranking algorithm proclaims the importance of a sentence by considering the global information which is computed recursively from the entire graph. Later, the graph is converted into a weighted graph in which the weights are calculated by measuring the similarity of sentences (Mihalcea, 2004). Similarly, in the LexRank approach (Erkan et al., 2004), documents are represented as a similarity graph in which the sentences are nodes and these sentences are then ranked according to centrality measures. The three centrality measures used are degree, LexRank with threshold and continuous LexRank. LexRank is a measure to calculate ranks using the similarity graph of sentences. It is also known as lexical PageRank. The summarization approach developed by (Gong et al., 2001) is also based on ranking sentences where important sentences are selected using a relevance measure and latent semantic analysis."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.049158,
        "cot": "I need to organize these quotes into logical dimensions that will help answer the user's query about graph-based approaches in multi-document summarization, covering both classical extractive methods and recent neural models that incorporate graph representations.\n\nAfter analyzing the quotes, I'll create the following dimensions:\n\n1. Introduction/Background: This will provide a foundation for understanding graph-based approaches in multi-document summarization. While there aren't specific quotes dedicated solely to background information, I'll include this section as it's important for the summary structure.\n\n2. Classical Graph-Based Approaches: This dimension will cover traditional graph-based methods like LexRank, TextRank, and other PageRank variants. These are fundamental extractive methods that form the basis of graph-based summarization. This should be in synthesis format to explain how these approaches work and their significance.\n\n3. Graph Construction and Representation Methods: This dimension will focus on how graphs are constructed for text summarization - what elements become nodes, what relationships form edges, etc. This is essential information for understanding the implementation of graph-based approaches. A synthesis format will help explain the different representation methods coherently.\n\n4. Centrality and Ranking Algorithms: This dimension will explain the various algorithms used to determine importance in graph-based summarization approaches. This includes PageRank, HITS, and other centrality measures. A synthesis format will help explain these technical concepts clearly.\n\n5. Recent Neural Graph-Based Models: This dimension will cover the modern neural approaches that incorporate graph representations, including Graph Neural Networks, Graph Convolutional Networks, etc. This should be in synthesis format to explain how these modern approaches work.\n\n6. Applications and Effectiveness: This dimension will focus on how graph-based approaches have been applied to various summarization tasks and their effectiveness. This should be in synthesis format to provide a coherent analysis of applications.\n\nThis organization will provide a comprehensive overview of graph-based approaches in multi-document summarization, addressing both classical methods and recent neural models as requested by the user.",
        "plan": {
            "Introduction/Background to Graph-Based Summarization (synthesis)": [],
            "Classical Graph-Based Approaches (synthesis)": [
                0,
                1,
                2,
                3,
                5,
                6,
                7,
                11,
                13,
                17,
                18,
                19,
                26,
                33,
                34,
                35,
                36,
                37
            ],
            "Graph Construction and Representation Methods (synthesis)": [
                0,
                1,
                2,
                4,
                8,
                10,
                16,
                21,
                23,
                25,
                27,
                29,
                30,
                31
            ],
            "Centrality and Ranking Algorithms (synthesis)": [
                0,
                1,
                3,
                5,
                10,
                12,
                17,
                18,
                28,
                32,
                38,
                39,
                41
            ],
            "Recent Neural Graph-Based Models (synthesis)": [
                4,
                7,
                8,
                9,
                12,
                14,
                15,
                16,
                20,
                21,
                22,
                24,
                27,
                29,
                30,
                40
            ],
            "Applications and Effectiveness of Graph-Based Approaches (synthesis)": [
                2,
                8,
                10,
                14,
                15,
                16,
                19,
                21,
                25,
                26,
                29,
                32,
                34,
                38
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background to Graph-Based Summarization",
                "tldr": "Graph-based approaches to multi-document summarization represent documents as graphs where nodes are textual units and edges represent relationships between them. These methods excel at capturing document structure and identifying important information through various centrality measures. (LLM Memory)",
                "text": "\nGraph-based approaches have emerged as a powerful paradigm in multi-document summarization, offering an intuitive and effective way to model the relationships between textual units. In graph-based summarization, documents are represented as graphs where nodes typically correspond to sentences, paragraphs, or other textual units, while edges represent relationships between these units such as semantic similarity, lexical overlap, or discourse connections. This representation allows summarization systems to capture the structure and organization of information across multiple documents. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe fundamental insight behind graph-based summarization is that important information tends to be central within the document graph structure. By analyzing the connectivity patterns, these methods can identify salient content without requiring extensive linguistic or domain knowledge. The core process typically involves three steps: constructing a graph representation of the documents, computing importance scores for nodes using various centrality measures, and selecting the most central nodes to form a summary. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nGraph-based approaches offer several advantages for multi-document summarization. They can effectively handle redundancy across documents by identifying similar content through graph connections. They also provide a natural way to capture both local and global document structure, helping to maintain coherence in the generated summaries. Additionally, graph representations are flexible and can incorporate various types of linguistic information, from simple lexical overlap to complex semantic relationships. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe evolution of graph-based summarization has closely followed broader trends in natural language processing, moving from purely statistical approaches to neural network-based methods that can learn rich representations from data. This progression has maintained the graph structure as a core component while enhancing how nodes and edges are represented and processed. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Classical Graph-Based Approaches",
                "tldr": "Classical graph-based approaches like LexRank and TextRank revolutionized multi-document summarization by using centrality measures to identify important sentences in document graphs. These methods employ various algorithms such as PageRank, HITS, and eigenvector centrality to rank sentences based on their connectivity patterns. (13 sources)",
                "text": "\nGraph-based approaches emerged as a significant paradigm in multi-document summarization in the early 2000s, with LexRank <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> and TextRank being the pioneering algorithms in this domain. These methods represent documents as graphs where sentences serve as nodes and the relationships between sentences, typically based on similarity measures, form the edges <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper>.\n\nLexRank, introduced by Erkan and Radev, applies eigenvector centrality to identify prominent sentences within a document graph. In this approach, a connectivity matrix based on intra-sentence cosine similarity serves as the adjacency matrix for the graph representation <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. The algorithm ranked first in several Document Understanding Conference (DUC) 2004 evaluation tasks, demonstrating its effectiveness in multi-document summarization.\n\nTextRank, another influential graph-based method, operates similarly to LexRank but constructs graphs based on content overlap between sentences <Paper corpusId=\"145993667\" paperTitle=\"(Mohamed et al., 2019)\" isShortName></Paper>. Both LexRank and TextRank derive from Google's PageRank algorithm, adapting its core principles to the task of sentence ranking <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper> <Paper corpusId=\"16689087\" paperTitle=\"(Sipos et al., 2011)\" isShortName></Paper>.\n\nVarious centrality measures have been employed in these graph-based approaches to determine sentence importance. Degree centrality, which counts the number of connections a node has (normalized for graph size), and eigenvector centrality, which considers both direct connections and the connectedness of neighboring nodes, are among the most common metrics <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper>. Several studies have shown that degree centrality often performs best for most summarization tasks <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper> <Paper corpusId=\"13664986\" paperTitle=\"(Ozgur et al., 2008)\" isShortName></Paper>.\n\nIn addition to PageRank-based methods, other graph algorithms like HITS (Hyperlink-Induced Topic Search) have been adapted for summarization. The HITS algorithm determines the authority and hub values of nodes in the graph, with applications in identifying important sentences <Paper corpusId=\"244510269\" paperTitle=\"(Mridha et al., 2021)\" isShortName></Paper> <Paper corpusId=\"2100529\" paperTitle=\"(Mihalcea, 2004)\" isShortName></Paper>.\n\nResearchers have continued to refine these classical graph-based approaches. Wan and Yang developed a method that assigns different weights to intra-document and inter-document links, giving higher priority to sentences with significant inter-document connections <Paper corpusId=\"5457260\" paperTitle=\"(Wan et al., 2006)\" isShortName></Paper> <Paper corpusId=\"33696521\" paperTitle=\"(Kumar et al., 2016)\" isShortName></Paper>. Other improvements include incorporating document-level information into the graph structure <Paper corpusId=\"33696521\" paperTitle=\"(Kumar et al., 2016)\" isShortName></Paper> and applying graph-based ranking with clustering techniques, as demonstrated in CollabSum <Paper corpusId=\"16689087\" paperTitle=\"(Sipos et al., 2011)\" isShortName></Paper>.\n\nTopic-sensitive variations have also been developed, such as topic-sensitive LexRank <Paper corpusId=\"10274362\" paperTitle=\"(Otterbacher et al., 2005)\" isShortName></Paper>, which introduces a query bias in transition probabilities to assign higher scores to sentences similar to a query. This adaptation makes the approach suitable for query-focused summarization tasks <Paper corpusId=\"182712867\" paperTitle=\"(Lierde et al., 2019)\" isShortName></Paper>.\n\nThe success of these classical graph-based methods lies in their ability to capture the structural and semantic relationships between sentences across multiple documents, effectively identifying the most central and important content without requiring extensive linguistic knowledge or domain-specific information <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0.87255859375
                    },
                    {
                        "id": "(Zhang et al., 2013)",
                        "snippets": [
                            "Recently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. (Erkan et al., 2004)(Zhang et al., 2007)(\u00d6zg\u00fcr et al., 2008)) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank (Erkan et al., 2004) and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary."
                        ],
                        "paper": {
                            "corpus_id": 10274390,
                            "title": "Clustering cliques for graph-based summarization of the biomedical research literature",
                            "authors": [
                                {
                                    "authorId": "48213346",
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "1719666",
                                    "name": "M. Fiszman"
                                },
                                {
                                    "authorId": "50877008",
                                    "name": "Dongwook Shin"
                                },
                                {
                                    "authorId": "2982296",
                                    "name": "Bartlomiej Wilkowski"
                                },
                                {
                                    "authorId": "1693891",
                                    "name": "Thomas C. Rindflesch"
                                }
                            ],
                            "year": 2013,
                            "venue": "BMC Bioinformatics",
                            "n_citations": 28
                        },
                        "score": 0.876953125
                    },
                    {
                        "id": "(Mohamed et al., 2019)",
                        "snippets": [
                            "In addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization (Erkan et al., 2004)15,(Canhasi, 2011)(Wei et al., 2010)(Wan, 2010)21,(Azadani et al., 2018)[23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev (Erkan et al., 2004) proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization (Canhasi, 2011)(Wei et al., 2010)(Wan, 2010). In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. (Wei et al., 2010) and Wan (Wan, 2010) considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts (Plaza et al., 2008)25,(Lloret et al., 2011)(Azadani et al., 2018). Of this, Azadani et al. (Azadani et al., 2018) and Plaza et al. (Plaza et al., 2008) have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28]."
                        ],
                        "paper": {
                            "corpus_id": 145993667,
                            "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis",
                            "authors": [
                                {
                                    "authorId": "47302320",
                                    "name": "Muhidin A. Mohamed"
                                },
                                {
                                    "authorId": "1714030",
                                    "name": "M. Oussalah"
                                }
                            ],
                            "year": 2019,
                            "venue": "Information Processing & Management",
                            "n_citations": 101
                        },
                        "score": 0.94140625
                    },
                    {
                        "id": "(Sankar et al., 2022)",
                        "snippets": [
                            "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."
                        ],
                        "paper": {
                            "corpus_id": 248571519,
                            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2064325789",
                                    "name": "Aiswarya Sankar"
                                },
                                {
                                    "authorId": "145934595",
                                    "name": "Ankit R. Chadha"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9169921875
                    },
                    {
                        "id": "(Sipos et al., 2011)",
                        "snippets": [
                            "A popular stohastic graph-based summarization method is LexRank (Erkan et al., 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank [16] is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank [17]. Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. \n\nGraph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities."
                        ],
                        "paper": {
                            "corpus_id": 16689087,
                            "title": "Large-Margin Learning of Submodular Summarization Methods",
                            "authors": [
                                {
                                    "authorId": "2195688",
                                    "name": "Ruben Sipos"
                                },
                                {
                                    "authorId": "2248631",
                                    "name": "Pannagadatta K. Shivaswamy"
                                },
                                {
                                    "authorId": "1680188",
                                    "name": "T. Joachims"
                                }
                            ],
                            "year": 2011,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.91796875
                    },
                    {
                        "id": "(Ozgur et al., 2008)",
                        "snippets": [
                            "Motivation: Understanding the role of genetics in diseases is one of the most important aims of the biological sciences. The completion of the Human Genome Project has led to a rapid increase in the number of publications in this area. However, the coverage of curated databases that provide information manually extracted from the literature is limited. Another challenge is that determining disease-related genes requires laborious experiments. Therefore, predicting good candidate genes before experimental analysis will save time and effort. We introduce an automatic approach based on text mining and network analysis to predict gene-disease associations. We collected an initial set of known disease-related genes and built an interaction network by automatic literature mining based on dependency parsing and support vector machines. Our hypothesis is that the central genes in this disease-specific network are likely to be related to the disease. We used the degree, eigenvector, betweenness and closeness centrality metrics to rank the genes in the network. Results: The proposed approach can be used to extract known and to infer unknown gene-disease associations. We evaluated the approach for prostate cancer. Eigenvector and degree centrality achieved high accuracy. A total of 95% of the top 20 genes ranked by these methods are confirmed to be related to prostate cancer. On the other hand, betweenness and closeness centrality predicted more genes whose relation to the disease is currently unknown and are candidates for experimental study. Availability: A web-based system for browsing the disease-specific gene-interaction networks is available at: http://gin.ncibi.org Contact: radev@umich.edu"
                        ],
                        "paper": {
                            "corpus_id": 13664986,
                            "title": "Identifying gene-disease associations using centrality on a literature mined gene-interaction network",
                            "authors": [
                                {
                                    "authorId": "1702629",
                                    "name": "Arzucan \u00d6zg\u00fcr"
                                },
                                {
                                    "authorId": "2042777",
                                    "name": "T. Vu"
                                },
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2008,
                            "venue": "Intelligent Systems in Molecular Biology",
                            "n_citations": 372
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mridha et al., 2021)",
                        "snippets": [
                            "The graph-based method can be applied to both extractive and abstractive text summarization. This approach is an unsupervised learning method that rates the required sentences or terms using a graph. The purpose of the graphical process is to extract the most relevant sentences from a single text (Moratanch et al., 2017). \n\nGraph-based ranking algorithms determine the relevance of a vertex in a graph based on global information iteratively extracted from the entire chart. When it comes to text summarization, specific graph-based techniques are applied. 1) LexRank: LexRank is a probabilistic graph-based technique for calculating sentence significance based on the notion of eigenvector centrality in a graph representation of phrases for natural language processing. It is a connectivity matrix based on intra-sentence cosine similarity that is utilized as the adjacency matrix in a sentence graph representation (Erkan et al., 2004). (Otterbacher et al., 2009), (Wu et al., 2015), (Verma et al., 2016), (Li et al., 2016), (Samuel et al., 2016) proposed the LexRank algorithm for graph-based text summarization task. 2) Hyperlink-Induced Topic Search (HITS): Hyperlinkinduced topic search is a link analysis algorithm that determines the authority and hub values. The results for the search query are retrieved, and then the computation is performed only on this set of results. A hub value is the total of the scaled authority values of the pages it points to, and an authority value is the sum of the scaled authority values of the pages it points to (Litvak et al., 2008). Some articles focused on the HITS ranking algorithm for graphbased text summarization tasks, as can be seen in (Thakkar et al., 2010), (Mihalcea, 2004), (Bhandari et al., 2008), and (Zhang et al., 2005). 3) PageRank: The PageRank algorithm utilizes the inbound links of specified pages to measure their significance or quality to rank the search results. PageRank links more weight based on the importance of the page from which it originates [267]. Some articles proposed the PageRank algorithm for summarization (Elbarougy et al., 2020), (Mallick et al., 2018), (Wang et al., 2007), (Chongsuntornsri et al., 2006), (Sornil et al., 2006). 4) TextRank: TextRank is an unsupervised method for automatic text summarization to extract the most important keywords from a document."
                        ],
                        "paper": {
                            "corpus_id": 244510269,
                            "title": "A Survey of Automatic Text Summarization: Progress, Process and Challenges",
                            "authors": [
                                {
                                    "authorId": "2348260813",
                                    "name": "M. F. Mridha"
                                },
                                {
                                    "authorId": "2149534558",
                                    "name": "Aklima Akter Lima"
                                },
                                {
                                    "authorId": "2086569",
                                    "name": "Prof. Dr. Kamruddin Nur"
                                },
                                {
                                    "authorId": "2152905283",
                                    "name": "S. Das"
                                },
                                {
                                    "authorId": "2663201",
                                    "name": "M. Hasan"
                                },
                                {
                                    "authorId": "2055992775",
                                    "name": "Muhammad Mohsin Kabir"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Access",
                            "n_citations": 88
                        },
                        "score": 0.9140625
                    },
                    {
                        "id": "(Mihalcea, 2004)",
                        "snippets": [
                            "This paper presents an innovative unsupervised method for automatic sentence extraction using graph-based ranking algorithms. We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 2100529,
                            "title": "Graph-based Ranking Algorithms for Sentence Extraction, Applied to Text Summarization",
                            "authors": [
                                {
                                    "authorId": "145557251",
                                    "name": "Rada Mihalcea"
                                }
                            ],
                            "year": 2004,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 520
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wan et al., 2006)",
                        "snippets": [
                            "Graph-based methods have been proposed to rank sentences or passages. Websumm (Mani et al., 1997) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization."
                        ],
                        "paper": {
                            "corpus_id": 5457260,
                            "title": "Improved Affinity Graph Based Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                },
                                {
                                    "authorId": "1743923",
                                    "name": "Jianwu Yang"
                                }
                            ],
                            "year": 2006,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 109
                        },
                        "score": 0.9384765625
                    },
                    {
                        "id": "(Kumar et al., 2016)",
                        "snippets": [
                            "This approach differs from the cluster based approach where sentences are ranked based on its closeness to cluster centroid. Two well-known graph based ranking algorithms is the HITS algorithm (Kleinberg, 1999) and the Google's PageRank (Brin and Page, 2012). Both methods have been traditionally used in Web-link analysis and social networks. Lexrank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are two successful graph-based ranking systems that implement these algorithms.\n\nFurther studies have been carried to make improvement through modification in the ranking algorithm. Wan and Yang (2006) assigned different weights to intra-document links and inter-document links. \n\nThey gave more priority to sentence with high interdocument links. In later work by Hariharan and Srinivasan (2009), they approached the graph based method differently i.e., by discounting the already selected sentence by removing it from further consideration when they rank the remaining sentences in the document. \n\nApart from sentence level information, Wan (2008) and Wei et al. (2010) devised a document-sensitive graph model to explore document impact on the graphbased summarization, by incorporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process."
                        ],
                        "paper": {
                            "corpus_id": 33696521,
                            "title": "A Review on Automatic Text Summarization Approaches",
                            "authors": [
                                {
                                    "authorId": "1734844",
                                    "name": "Y. J. Kumar"
                                },
                                {
                                    "authorId": "2445567",
                                    "name": "O. Goh"
                                },
                                {
                                    "authorId": "2136088",
                                    "name": "H. Basiron"
                                },
                                {
                                    "authorId": "3388053",
                                    "name": "Ngo Hea Choon"
                                },
                                {
                                    "authorId": "21154901",
                                    "name": "Puspalata C. Suppiah"
                                }
                            ],
                            "year": 2016,
                            "venue": "Journal of Computer Science",
                            "n_citations": 72
                        },
                        "score": 0.90771484375
                    },
                    {
                        "id": "(Otterbacher et al., 2005)",
                        "snippets": [
                            "We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. \"How many victims have been found?\") Judges found sentences providing an answer to each question. To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap. In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline."
                        ],
                        "paper": {
                            "corpus_id": 10274362,
                            "title": "Using Random Walks for Question-focused Sentence Retrieval",
                            "authors": [
                                {
                                    "authorId": "2216583",
                                    "name": "Jahna Otterbacher"
                                },
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2005,
                            "venue": "Human Language Technology - The Baltic Perspectiv",
                            "n_citations": 179
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lierde et al., 2019)",
                        "snippets": [
                            "The earliest graph-based summarizer, called LexRank (Erkan et al., 2004), defines edges as term co-occurrence relationships between sentences. Then, PageRank algorithm is applied to compute relevance scores of sentences. Adapting this idea for the task of query-focused summarization, topic sensitive LexRank (Otterbacher et al., 2005) introduces a query bias in probabilities of transition, which results in higher scores for sentences that are similar to the query. Sim-ilarly, (Wan, 2013) proposes a manifold ranking algorithm in which scores are popagated accross a graph including both sentences and the query as vertices. To remove redundancies in summaries, (Mei et al., 2010) proposes a new node ranking algorithm called DivRank, which tends to select dissimilar sentences. While early graph-based algorithms only involved sentences, a bipartite graph model is proposed in (Wan et al., 2008), involving both sentences and terms as vertices and it applies HITS algorithm to score sentences. (Wei et al., 2010) combines this idea with a PageRank-like method to score sentences, terms and documents simultaneously."
                        ],
                        "paper": {
                            "corpus_id": 182712867,
                            "title": "Learning with fuzzy hypergraphs: A topical approach to query-oriented text summarization",
                            "authors": [
                                {
                                    "authorId": "29364954",
                                    "name": "H. V. Lierde"
                                },
                                {
                                    "authorId": "144134805",
                                    "name": "T. Chow"
                                }
                            ],
                            "year": 2019,
                            "venue": "Information Sciences",
                            "n_citations": 26
                        },
                        "score": 0.90625
                    },
                    {
                        "id": "(Pasunuru et al., 2021)",
                        "snippets": [
                            "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization."
                        ],
                        "paper": {
                            "corpus_id": 235097309,
                            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                            "authors": [
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "2940333",
                                    "name": "Mengwen Liu"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "120209444",
                                    "name": "Sujith Ravi"
                                },
                                {
                                    "authorId": "40262269",
                                    "name": "Markus Dreyer"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 73
                        },
                        "score": 0.9326171875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Graph Construction and Representation Methods",
                "tldr": "Graph construction in multi-document summarization involves creating nodes from textual units and edges from various relationships between them. Different representation approaches range from simple lexical similarity graphs to complex semantic networks incorporating discourse relationships, entity information, and hierarchical structures. (24 sources)",
                "text": "\nGraph-based approaches to multi-document summarization rely on effective graph construction and representation methods to model the relationships between textual units. The way these graphs are constructed significantly impacts the performance of summarization systems. In most implementations, sentences or paragraphs serve as nodes, while edges represent the relationships between these textual units <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper>. However, the specific methods for constructing these graphs vary considerably across different approaches.\n\nThe most common approach is to construct similarity-based graphs, where edges represent lexical or semantic similarity between sentences. LexRank and TextRank, for example, build connectivity matrices based on intra-sentence cosine similarity to form graph representations <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> <Paper corpusId=\"13237013\" paperTitle=\"(Han et al., 2016)\" isShortName></Paper>. These similarity measures are often computed using term frequency-inverse document frequency (TF-IDF) weights to determine the importance of words in establishing connections <Paper corpusId=\"170079112\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>.\n\nBeyond simple similarity graphs, researchers have developed more sophisticated graph representations that incorporate various linguistic features. These include sentence position <Paper corpusId=\"11989149\" paperTitle=\"(Katragadda et al., 2009)\" isShortName></Paper>, term frequency <Paper corpusId=\"5519987\" paperTitle=\"(Lin et al., 1997)\" isShortName></Paper>, topic signatures <Paper corpusId=\"8598694\" paperTitle=\"(Lin et al., 2000)\" isShortName></Paper>, lexical chains, and syntactic patterns <Paper corpusId=\"503868\" paperTitle=\"(Haggag, 2013)\" isShortName></Paper> <Paper corpusId=\"13237013\" paperTitle=\"(Han et al., 2016)\" isShortName></Paper>. Such features enrich the graph representation by capturing more complex relationships between textual units.\n\nDiscourse-based graphs have emerged as another significant approach, where edges represent discourse relations between sentences or documents. These graphs capture the logical and rhetorical structure of the text, helping to generate more coherent summaries <Paper corpusId=\"219036690\" paperTitle=\"(Xu et al., 2019)\" isShortName></Paper> <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>. The G-Flow and DISCOBERT models, for instance, utilize discourse graphs to produce concise and informative summaries <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>.\n\nSeveral researchers have proposed multi-level or heterogeneous graph representations that incorporate different types of nodes and edges. Canhasi and Kononenko developed a three-layer graph representation consisting of terms, sentences, and document vertices, linked via term-sentence and sentence-document connections <Paper corpusId=\"145993667\" paperTitle=\"(Mohamed et al., 2019)\" isShortName></Paper>. This approach enables the model to capture relationships at different granularity levels. Similarly, Wang et al. proposed a heterogeneous graph-based neural network that includes semantic nodes of different granularity levels to enrich cross-sentence relations <Paper corpusId=\"257219819\" paperTitle=\"(Zesheng et al., 2023)\" isShortName></Paper>.\n\nConcept graphs have emerged as an alternative to traditional sentence-based representations. Instead of representing sentences directly, these graphs model the concepts and their relationships within the documents <Paper corpusId=\"145993667\" paperTitle=\"(Mohamed et al., 2019)\" isShortName></Paper>. Plaza et al. adapted this approach for biomedical summarization by modeling concept graphs specific to the domain <Paper corpusId=\"5674504\" paperTitle=\"(Plaza et al., 2008)\" isShortName></Paper>.\n\nMore recent approaches have incorporated entity and event information into graph representations. Zhou et al. presented an entity-aware model that constructs a knowledge graph with text units and entities as nodes, utilizing coreference resolution tools to extract semantic entities <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>. Li et al. proposed using event relation graphs to represent documents for multi-document summarization <Paper corpusId=\"196170254\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper> <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>.\n\nThe evolution of graph representation methods has moved toward incorporating more semantic information and utilizing neural networks to learn graph representations. For instance, Pasunuru et al. developed a graph-enhanced approach that leverages knowledge graphs constructed using natural language processing tools to capture multi-level semantic information within documents <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>. These neural graph-based models can learn rich representations that capture complex relationships between textual units <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"264614849\" paperTitle=\"(Ribeiro, 2022)\" isShortName></Paper>.\n\nAbstract Meaning Representation (AMR) has also been used to construct semantic graphs for summarization. This approach provides a structured representation of sentences that captures their meaning independent of surface form <Paper corpusId=\"5001921\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper> <Paper corpusId=\"49210924\" paperTitle=\"(Liao et al., 2018)\" isShortName></Paper> <Paper corpusId=\"51867906\" paperTitle=\"(Dohare et al., 2018)\" isShortName></Paper> <Paper corpusId=\"52111191\" paperTitle=\"(Hardy et al., 2018)\" isShortName></Paper> <Paper corpusId=\"264614849\" paperTitle=\"(Ribeiro, 2022)\" isShortName></Paper>.\n\nThe diversity of graph construction and representation methods reflects the ongoing exploration of how best to model document structures and relationships for effective summarization. Each approach offers different advantages in capturing various aspects of textual relationships, from simple lexical connections to complex semantic structures <Paper corpusId=\"273784877\" paperTitle=\"(Ragazzi et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2013)",
                        "snippets": [
                            "Recently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. (Erkan et al., 2004)(Zhang et al., 2007)(\u00d6zg\u00fcr et al., 2008)) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank (Erkan et al., 2004) and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary."
                        ],
                        "paper": {
                            "corpus_id": 10274390,
                            "title": "Clustering cliques for graph-based summarization of the biomedical research literature",
                            "authors": [
                                {
                                    "authorId": "48213346",
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "1719666",
                                    "name": "M. Fiszman"
                                },
                                {
                                    "authorId": "50877008",
                                    "name": "Dongwook Shin"
                                },
                                {
                                    "authorId": "2982296",
                                    "name": "Bartlomiej Wilkowski"
                                },
                                {
                                    "authorId": "1693891",
                                    "name": "Thomas C. Rindflesch"
                                }
                            ],
                            "year": 2013,
                            "venue": "BMC Bioinformatics",
                            "n_citations": 28
                        },
                        "score": 0.876953125
                    },
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0.87255859375
                    },
                    {
                        "id": "(Han et al., 2016)",
                        "snippets": [
                            "Another approach of unsupervised extract-based summarization (Wang et al., 2008)(Li, 2005) uses graph-based model. TextRank (Park et al., 2007) and LexRank (Dhillon, 2001) are first two graph-based models applied in text summarization, which use the PageRank-like algorithms to mark sentences. Then, other researchers have integrated the statistical and linguistic features to drive the sentence selection process, for example, the sentence position (Katragadda et al., 2009), term frequency (Lin et al., 1997), topic signature (Lin et al., 2000), lexical chains (Barzilay et al., 1997), and syntactic patterns (Baralis et al., 2013)(Haggag, 2013). Ko and Seo (Ko et al., 2008) composed two sentences nearby into a bigram. Those bigrams were supposed to be context information. First, they extracted the bigrams by using the sentence extraction model. Then they used another extraction module to extract sentences from them. The ClusterCMRW and ClusterHITS models calculated the sentences scores by considering the clusterlevel information in the graph-based ranking algorithm. Canhasi and Kononenko (Canhasi et al., 2013) improve matrix decomposition by employing the archetypal analysis for generic multidocument summarization. While coming to the document set, there must be more consideration about the document-level influence. But it did not consider the relationship between words and sentences. The DsR model (Wei et al., 2010) achieved it by using document-sensitive graph-based ranking model. But this method did not get a satisfied result. Yin et al. improved the summarization quality by adding extra information which came from the query-extraction scenario. Goyal et al. (Goyal et al., 2013) take Bernoulli model of randomness to index weights of sentences taking the context into consideration. The method proposed in [28] decomposed sentences by semantic role analysis, but while building the model, it did not use graphbased algorithms."
                        ],
                        "paper": {
                            "corpus_id": 13237013,
                            "title": "Text Summarization Using FrameNet-Based Semantic Graph Model",
                            "authors": [
                                {
                                    "authorId": "2110981656",
                                    "name": "Xu Han"
                                },
                                {
                                    "authorId": "143750477",
                                    "name": "Tao Lv"
                                },
                                {
                                    "authorId": "48430746",
                                    "name": "Zhirui Hu"
                                },
                                {
                                    "authorId": "2115554077",
                                    "name": "Xinyan Wang"
                                },
                                {
                                    "authorId": "50097174",
                                    "name": "Cong Wang"
                                }
                            ],
                            "year": 2016,
                            "venue": "Scientific Programming",
                            "n_citations": 25
                        },
                        "score": 0.9091796875
                    },
                    {
                        "id": "(Liu et al., 2019)",
                        "snippets": [
                            "The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004;Christensen et al., 2013;Wan, 2008;Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them.\n\nMost previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links."
                        ],
                        "paper": {
                            "corpus_id": 170079112,
                            "title": "Hierarchical Transformers for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "39798499",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 298
                        },
                        "score": 0.9169921875
                    },
                    {
                        "id": "(Katragadda et al., 2009)",
                        "snippets": [
                            "In this paper, we describe a sentence position based summarizer that is built based on a sentence position policy, created from the evaluation testbed of recent summarization tasks at Document Understanding Conferences (DUC). We show that the summarizer thus built is able to outperform most systems participating in task focused summarization evaluations at Text Analysis Conferences (TAC) 2008. Our experiments also show that such a method would perform better at producing short summaries (upto 100 words) than longer summaries. Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task."
                        ],
                        "paper": {
                            "corpus_id": 11989149,
                            "title": "Sentence Position revisited: A robust light-weight Update Summarization \u2018baseline\u2019 Algorithm",
                            "authors": [
                                {
                                    "authorId": "1958707",
                                    "name": "Rahul Katragadda"
                                },
                                {
                                    "authorId": "1941849",
                                    "name": "Prasad Pingali"
                                },
                                {
                                    "authorId": "1704709",
                                    "name": "Vasudeva Varma"
                                }
                            ],
                            "year": 2009,
                            "venue": "",
                            "n_citations": 42
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lin et al., 1997)",
                        "snippets": [
                            "This paper addresses the problem of identifying likely topics of texts by their position in the text. It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure. This method can be used in applications such as information retrieval, routing, and text summarization."
                        ],
                        "paper": {
                            "corpus_id": 5519987,
                            "title": "Identifying Topics by Position",
                            "authors": [
                                {
                                    "authorId": "1781574",
                                    "name": "Chin-Yew Lin"
                                },
                                {
                                    "authorId": "144547315",
                                    "name": "E. Hovy"
                                }
                            ],
                            "year": 1997,
                            "venue": "Applied Natural Language Processing Conference",
                            "n_citations": 316
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lin et al., 2000)",
                        "snippets": [
                            "In order to produce a good summary, one has to identify the most relevant portions of a given text. We describe in this paper a method for automatically training topic signatures-sets of related words, with associated weights, organized around head topics and illustrate with signatures we created with 6,194 TREC collection texts over 4 selected topics. We describe the possible integration of topic signatures with outologies and its evaluaton on an automated text summarization system."
                        ],
                        "paper": {
                            "corpus_id": 8598694,
                            "title": "The Automated Acquisition of Topic Signatures for Text Summarization",
                            "authors": [
                                {
                                    "authorId": "1781574",
                                    "name": "Chin-Yew Lin"
                                },
                                {
                                    "authorId": "144547315",
                                    "name": "E. Hovy"
                                }
                            ],
                            "year": 2000,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 548
                        },
                        "score": 0
                    },
                    {
                        "id": "(Haggag, 2013)",
                        "snippets": [
                            "Text summarization is machine based generation of a shortened version of a text. The summary should be a non-redundant extract from the original text. Most researches of text summarization use sentence extraction instead of abstraction to produce a summary. Extraction is depending mainly on sentences that already contained in the original input, which makes it more accurate and more concise. When all input articles are surrounding a particular event, extracting similar sentences would result in producing a highly repetitive summary. In this paper, a novel model for text summarization is proposed based on removing the non-effective sentences in producing an extract from the text. The model utilizes semantic analysis by evaluating sentences similarity. This similarity is provided by evaluating individual words similarity as well as syntactic relationships between neighboring words. These relationships addressed throughout the model as syntactic patterns. Word senses and the correlating part of speech for the word within context are provided in the semantic processing of matched patterns. The introduction of syntactic patterns knowledge supports text reduction by mapping the matched patterns into summarized ones. In addition, syntactic patterns make use of sentence relatedness evaluation in defining which sentences to keep and which to drop. Experiments proved that the model presented throughout the paper is well performing in results evaluation of compression rate, accuracy, recall and other human criteria like correctness, novelty, fluency and usefulness."
                        ],
                        "paper": {
                            "corpus_id": 503868,
                            "title": "Semantic Text Summarization Based on Syntactic Patterns",
                            "authors": [
                                {
                                    "authorId": "8442172",
                                    "name": "M. Haggag"
                                }
                            ],
                            "year": 2013,
                            "venue": "International Journal of Information Retrieval Research",
                            "n_citations": 6
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2019)",
                        "snippets": [
                            "Graph-based Summarization Graph approach has been explored in text summarization over decades. LexRank introduced a stochastic graphbased method for computing relative importance of textual units (Erkan et al., 2004). (Yasunaga et al., 2017) employed a GCN on the relation graphs with sentence embeddings obtained from RNN. (Tan et al., 2017) also proposed graphbased attention in abstractive summarization model. Fernandes et al. (2018) developed a framework to reason long-distance relationships for text summarization."
                        ],
                        "paper": {
                            "corpus_id": 219036690,
                            "title": "Discourse-Aware Neural Extractive Text Summarization",
                            "authors": [
                                {
                                    "authorId": "34837371",
                                    "name": "Jiacheng Xu"
                                },
                                {
                                    "authorId": "144702900",
                                    "name": "Zhe Gan"
                                },
                                {
                                    "authorId": "145215470",
                                    "name": "Yu Cheng"
                                },
                                {
                                    "authorId": "46700348",
                                    "name": "Jingjing Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 282
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Chen et al., 2021)",
                        "snippets": [
                            "The graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. (Erkan et al., 2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2019) both use discourse graphs to generate concise and informative summaries. (Li et al., 2016) and (Li et al., 2021) propose to utilize event relation graph to represent documents for MDS",
                            ".Most previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. (Erkan et al., 2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Christensen et al., 2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, (Yasunaga et al., 2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. (Li et al., 2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. (Wu et al., 2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases."
                        ],
                        "paper": {
                            "corpus_id": 239768911,
                            "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
                            "authors": [
                                {
                                    "authorId": "2108267116",
                                    "name": "Moye Chen"
                                },
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 22
                        },
                        "score": 0.9423828125
                    },
                    {
                        "id": "(Mohamed et al., 2019)",
                        "snippets": [
                            "In addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization (Erkan et al., 2004)15,(Canhasi, 2011)(Wei et al., 2010)(Wan, 2010)21,(Azadani et al., 2018)[23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev (Erkan et al., 2004) proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization (Canhasi, 2011)(Wei et al., 2010)(Wan, 2010). In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. (Wei et al., 2010) and Wan (Wan, 2010) considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts (Plaza et al., 2008)25,(Lloret et al., 2011)(Azadani et al., 2018). Of this, Azadani et al. (Azadani et al., 2018) and Plaza et al. (Plaza et al., 2008) have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28]."
                        ],
                        "paper": {
                            "corpus_id": 145993667,
                            "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis",
                            "authors": [
                                {
                                    "authorId": "47302320",
                                    "name": "Muhidin A. Mohamed"
                                },
                                {
                                    "authorId": "1714030",
                                    "name": "M. Oussalah"
                                }
                            ],
                            "year": 2019,
                            "venue": "Information Processing & Management",
                            "n_citations": 101
                        },
                        "score": 0.94140625
                    },
                    {
                        "id": "(Zesheng et al., 2023)",
                        "snippets": [
                            "In text summarization, Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled."
                        ],
                        "paper": {
                            "corpus_id": 257219819,
                            "title": "Topic-Selective Graph Network for Topic-Focused Summarization",
                            "authors": [
                                {
                                    "authorId": "1764406",
                                    "name": "Shi Zesheng"
                                },
                                {
                                    "authorId": "2110348767",
                                    "name": "Yucheng Zhou"
                                }
                            ],
                            "year": 2023,
                            "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
                            "n_citations": 4
                        },
                        "score": 0.89453125
                    },
                    {
                        "id": "(Plaza et al., 2008)",
                        "snippets": [
                            "One of the main problems in research on automatic summarization is the inaccurate semantic interpretation of the source. Using specific domain knowledge can considerably alleviate the problem. In this paper, we introduce an ontology-based extractive method for summarization. It is based on mapping the text to concepts and representing the document and its sentences as graphs. We have applied our approach to summarize biomedical literature, taking advantages of free resources as UMLS. Preliminary empirical results are presented and pending problems are identified."
                        ],
                        "paper": {
                            "corpus_id": 5674504,
                            "title": "Concept-Graph Based Biomedical Automatic Summarization Using Ontologies",
                            "authors": [
                                {
                                    "authorId": "145377713",
                                    "name": "Laura Plaza"
                                },
                                {
                                    "authorId": "102639698",
                                    "name": "Alberto D\u00edaz"
                                },
                                {
                                    "authorId": "1798564",
                                    "name": "Pablo Gerv\u00e1s"
                                }
                            ],
                            "year": 2008,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 53
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan et al., 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Wan, 2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, (Tan et al., 2017) propose a graph-based attention mechanism to identify salient sentences. (Yasunaga et al., 2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. (Fan et al., 2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. (huang et al., 2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. (Li et al., 2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, (Wang et al., 2020) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization."
                        ],
                        "paper": {
                            "corpus_id": 236478143,
                            "title": "Entity-Aware Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2053308860",
                                    "name": "Weidong Ren"
                                },
                                {
                                    "authorId": "150112803",
                                    "name": "Gongshen Liu"
                                },
                                {
                                    "authorId": "153253583",
                                    "name": "Bo Su"
                                },
                                {
                                    "authorId": "143844110",
                                    "name": "Wei Lu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 28
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Qu, 2024)",
                        "snippets": [
                            "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP",
                            "Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."
                        ],
                        "paper": {
                            "corpus_id": 269762702,
                            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
                            "authors": [
                                {
                                    "authorId": "2163451228",
                                    "name": "Yutong Qu"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Web Conference",
                            "n_citations": 0
                        },
                        "score": 0.88330078125
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "The key to realize advanced document summarization is semantic representation of documents. This paper investigates the role of Semantic Link Network in representing and understanding documents for multi-document summarization. It proposes a novel abstractive multi-document summarization framework by first transforming documents into a Semantic Link Network of concepts and events and then transforming the Semantic Link Network into the summary of the documents based on the selection of important concepts and events while keeping semantics coherence. Experiments on benchmark datasets show that the proposed summarization approach significantly outperforms relevant state-of-the-art baselines and the Semantic Link Network plays an important role in representing and understanding documents."
                        ],
                        "paper": {
                            "corpus_id": 196170254,
                            "title": "Abstractive Multi-Document Summarization Based on Semantic Link Network",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "143632630",
                                    "name": "H. Zhuge"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Transactions on Knowledge and Data Engineering",
                            "n_citations": 30
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pasunuru et al., 2021)",
                        "snippets": [
                            "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization."
                        ],
                        "paper": {
                            "corpus_id": 235097309,
                            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                            "authors": [
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "2940333",
                                    "name": "Mengwen Liu"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "120209444",
                                    "name": "Sujith Ravi"
                                },
                                {
                                    "authorId": "40262269",
                                    "name": "Markus Dreyer"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 73
                        },
                        "score": 0.9326171875
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models.\n\nMost previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Ribeiro, 2022)",
                        "snippets": [
                            "A growing body of work focuses on using graph-based representations for improving summarization. Whereas different approaches encode graphs into neural models for multi-document summarization (Fan et al., 2019;Li et al., 2020;Pasunuru et al., 2021;Chen et al., 2021), AMR structures have been shown to benefit both document representation and summary generation (Liu et al., 2015;Liao et al., 2018;Hardy and Vlachos, 2018;Dohare et al., 2018) and have the potential of improving controllability in summarization."
                        ],
                        "paper": {
                            "corpus_id": 264614849,
                            "title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations",
                            "authors": [
                                {
                                    "authorId": "10430740",
                                    "name": "Leonardo F. R. Ribeiro"
                                }
                            ],
                            "year": 2022,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 60
                        },
                        "score": 0.91015625
                    },
                    {
                        "id": "(Liu et al., 2018)",
                        "snippets": [
                            "We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-tograph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-totext generator. The framework is data-driven, trainable, and not specifically designed for a particular domain. Experiments on goldstandard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization"
                        ],
                        "paper": {
                            "corpus_id": 5001921,
                            "title": "Toward Abstractive Summarization Using Semantic Representations",
                            "authors": [
                                {
                                    "authorId": "144544919",
                                    "name": "Fei Liu"
                                },
                                {
                                    "authorId": "144683841",
                                    "name": "Jeffrey Flanigan"
                                },
                                {
                                    "authorId": "38094552",
                                    "name": "Sam Thomson"
                                },
                                {
                                    "authorId": "2464164",
                                    "name": "N. Sadeh"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2018,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 302
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liao et al., 2018)",
                        "snippets": [
                            "Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research."
                        ],
                        "paper": {
                            "corpus_id": 49210924,
                            "title": "Abstract Meaning Representation for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "49792730",
                                    "name": "Kexin Liao"
                                },
                                {
                                    "authorId": "50827114",
                                    "name": "Logan Lebanoff"
                                },
                                {
                                    "authorId": "144544919",
                                    "name": "Fei Liu"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 105
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dohare et al., 2018)",
                        "snippets": [
                            "Automatic abstractive summary generation remains a significant open problem for natural language processing. In this work, we develop a novel pipeline for Semantic Abstractive Summarization (SAS). SAS, as introduced by Liu et. al. (2015) first generates an AMR graph of an input story, through which it extracts a summary graph and finally, creates summary sentences from this summary graph. Compared to earlier approaches, we develop a more comprehensive method to generate the story AMR graph using state-of-the-art co-reference resolution and Meta Nodes. Which we then use in a novel unsupervised algorithm based on how humans summarize a piece of text to extract the summary sub-graph. Our algorithm outperforms the state of the art SAS method by 1.7% F1 score in node prediction."
                        ],
                        "paper": {
                            "corpus_id": 51867906,
                            "title": "Unsupervised Semantic Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "13295612",
                                    "name": "Shibhansh Dohare"
                                },
                                {
                                    "authorId": "46346053",
                                    "name": "Vivek Gupta"
                                },
                                {
                                    "authorId": "2376013",
                                    "name": "H. Karnick"
                                }
                            ],
                            "year": 2018,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 19
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hardy et al., 2018)",
                        "snippets": [
                            "Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance on later parses is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset."
                        ],
                        "paper": {
                            "corpus_id": 52111191,
                            "title": "Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation",
                            "authors": [
                                {
                                    "authorId": "46441201",
                                    "name": "Hardy Hardy"
                                },
                                {
                                    "authorId": "2064056928",
                                    "name": "Andreas Vlachos"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 69
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ragazzi et al., 2025)",
                        "snippets": [
                            "Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets."
                        ],
                        "paper": {
                            "corpus_id": 273784877,
                            "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
                            "authors": [
                                {
                                    "authorId": "134327204",
                                    "name": "Luca Ragazzi"
                                },
                                {
                                    "authorId": "2237646991",
                                    "name": "Gianluca Moro"
                                },
                                {
                                    "authorId": "2132084411",
                                    "name": "Lorenzo Valgimigli"
                                },
                                {
                                    "authorId": "2328891652",
                                    "name": "Riccardo Fiorani"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
                            "n_citations": 0
                        },
                        "score": 0.900390625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Centrality and Ranking Algorithms",
                "tldr": "Graph-based summarization systems rely on centrality measures to identify important sentences, with PageRank, HITS, and eigenvector centrality being the most prominent algorithms. Various adaptations of these algorithms have been developed for different summarization tasks, including threshold-based approaches, query-focused variations, and directional edge weighting schemes. (15 sources)",
                "text": "\nAt the core of graph-based summarization lies the concept of centrality - a measure that determines the importance of nodes within a graph structure. Various centrality measures have been employed to identify salient sentences in document graphs, with degree centrality and eigenvector centrality being among the most common <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper>. Degree centrality simply counts the number of connections a node has (normalized for graph size), while eigenvector centrality considers both direct connections and the connectedness of neighboring nodes <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper> <Paper corpusId=\"13664986\" paperTitle=\"(Ozgur et al., 2008)\" isShortName></Paper>. Several studies have shown that degree centrality often performs best for most summarization tasks <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper>.\n\nThe most widely used graph ranking algorithms in summarization are adaptations of web search algorithms. PageRank serves as the foundation for LexRank and TextRank, where the algorithm is modified to work with sentence similarity instead of hyperlinks <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> <Paper corpusId=\"232404911\" paperTitle=\"(Zhang et al., 2021)\" isShortName></Paper>. These algorithms determine the relevance of a vertex based on global information iteratively extracted from the entire graph <Paper corpusId=\"244510269\" paperTitle=\"(Mridha et al., 2021)\" isShortName></Paper>. In LexRank, a connectivity matrix based on intra-sentence cosine similarity serves as the adjacency matrix for the graph representation <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> <Paper corpusId=\"8437725\" paperTitle=\"(Parveen et al., 2014)\" isShortName></Paper>.\n\nLexRank has been implemented in several variations, including LexRank with threshold and continuous LexRank, with studies showing that the threshold method often outperforms other degree-based techniques <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. In addition to PageRank-based methods, other graph algorithms like HITS (Hyperlink-Induced Topic Search) have been adapted for summarization. The HITS algorithm determines the authority and hub values of nodes in the graph to identify important sentences <Paper corpusId=\"244510269\" paperTitle=\"(Mridha et al., 2021)\" isShortName></Paper> <Paper corpusId=\"16689087\" paperTitle=\"(Sipos et al., 2011)\" isShortName></Paper>.\n\nRecent advancements in centrality-based ranking algorithms have led to more sophisticated approaches. PacSum constructs graphs with directed edges, considering that the centrality of two nodes should be influenced by their relative position in the document <Paper corpusId=\"264817506\" paperTitle=\"(Mao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"182952892\" paperTitle=\"(Zheng et al., 2019)\" isShortName></Paper>. DASG selects sentences based on both similarities and relative distances among neighboring sentences, incorporating a graph edge weighting scheme that uses coefficients calculated by relative distance <Paper corpusId=\"264817506\" paperTitle=\"(Mao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"235792514\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. FAR modifies the centrality model by applying a facet-aware ranking approach to filter out insignificant sentences <Paper corpusId=\"264817506\" paperTitle=\"(Mao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"236477774\" paperTitle=\"(Liang et al., 2021)\" isShortName></Paper>.\n\nFor query-focused summarization, adaptations of centrality algorithms have been developed. Topic-sensitive LexRank introduces a query bias in transition probabilities, assigning higher scores to sentences similar to a query <Paper corpusId=\"182712867\" paperTitle=\"(Lierde et al., 2019)\" isShortName></Paper> <Paper corpusId=\"10274362\" paperTitle=\"(Otterbacher et al., 2005)\" isShortName></Paper>. Similarly, manifold ranking approaches have been proposed where scores are propagated across a graph including both sentences and the query as vertices <Paper corpusId=\"182712867\" paperTitle=\"(Lierde et al., 2019)\" isShortName></Paper>.\n\nTo address redundancy in summaries, specialized ranking algorithms have been developed. DivRank, for example, tends to select dissimilar sentences to ensure diversity in the summary <Paper corpusId=\"182712867\" paperTitle=\"(Lierde et al., 2019)\" isShortName></Paper>. Other approaches combine clustering with graph-based ranking, such as CollabSum, which first clusters documents and then applies graph-based algorithms that incorporate both intra-document and inter-document sentence similarities <Paper corpusId=\"16689087\" paperTitle=\"(Sipos et al., 2011)\" isShortName></Paper> <Paper corpusId=\"219690773\" paperTitle=\"(Khan et al., 2020)\" isShortName></Paper>.\n\nSome recent approaches have moved beyond traditional centrality measures to incorporate document-level information. The DsR model uses a document-sensitive graph-based ranking approach that considers the influence of the entire document set at the sentence level <Paper corpusId=\"13237013\" paperTitle=\"(Han et al., 2016)\" isShortName></Paper>. Other improvements include assigning different weights to intra-document and inter-document links, giving higher priority to sentences with significant inter-document connections <Paper corpusId=\"219690773\" paperTitle=\"(Khan et al., 2020)\" isShortName></Paper>.\n\nThe evolution of centrality-based ranking algorithms in graph-based summarization reflects ongoing efforts to better capture the complex relationships between textual units and identify the most salient content for inclusion in summaries. Each approach offers different advantages in modeling document structure and relationships, from simple connectivity measures to sophisticated ranking schemes that account for position, direction, and document-level context.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2013)",
                        "snippets": [
                            "Recently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. (Erkan et al., 2004)(Zhang et al., 2007)(\u00d6zg\u00fcr et al., 2008)) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank (Erkan et al., 2004) and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary."
                        ],
                        "paper": {
                            "corpus_id": 10274390,
                            "title": "Clustering cliques for graph-based summarization of the biomedical research literature",
                            "authors": [
                                {
                                    "authorId": "48213346",
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "1719666",
                                    "name": "M. Fiszman"
                                },
                                {
                                    "authorId": "50877008",
                                    "name": "Dongwook Shin"
                                },
                                {
                                    "authorId": "2982296",
                                    "name": "Bartlomiej Wilkowski"
                                },
                                {
                                    "authorId": "1693891",
                                    "name": "Thomas C. Rindflesch"
                                }
                            ],
                            "year": 2013,
                            "venue": "BMC Bioinformatics",
                            "n_citations": 28
                        },
                        "score": 0.876953125
                    },
                    {
                        "id": "(Ozgur et al., 2008)",
                        "snippets": [
                            "Motivation: Understanding the role of genetics in diseases is one of the most important aims of the biological sciences. The completion of the Human Genome Project has led to a rapid increase in the number of publications in this area. However, the coverage of curated databases that provide information manually extracted from the literature is limited. Another challenge is that determining disease-related genes requires laborious experiments. Therefore, predicting good candidate genes before experimental analysis will save time and effort. We introduce an automatic approach based on text mining and network analysis to predict gene-disease associations. We collected an initial set of known disease-related genes and built an interaction network by automatic literature mining based on dependency parsing and support vector machines. Our hypothesis is that the central genes in this disease-specific network are likely to be related to the disease. We used the degree, eigenvector, betweenness and closeness centrality metrics to rank the genes in the network. Results: The proposed approach can be used to extract known and to infer unknown gene-disease associations. We evaluated the approach for prostate cancer. Eigenvector and degree centrality achieved high accuracy. A total of 95% of the top 20 genes ranked by these methods are confirmed to be related to prostate cancer. On the other hand, betweenness and closeness centrality predicted more genes whose relation to the disease is currently unknown and are candidates for experimental study. Availability: A web-based system for browsing the disease-specific gene-interaction networks is available at: http://gin.ncibi.org Contact: radev@umich.edu"
                        ],
                        "paper": {
                            "corpus_id": 13664986,
                            "title": "Identifying gene-disease associations using centrality on a literature mined gene-interaction network",
                            "authors": [
                                {
                                    "authorId": "1702629",
                                    "name": "Arzucan \u00d6zg\u00fcr"
                                },
                                {
                                    "authorId": "2042777",
                                    "name": "T. Vu"
                                },
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2008,
                            "venue": "Intelligent Systems in Molecular Biology",
                            "n_citations": 372
                        },
                        "score": 0
                    },
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0.87255859375
                    },
                    {
                        "id": "(Zhang et al., 2021)",
                        "snippets": [
                            "Unsupervised graph summarization methods rely on graph connectivity (centrality) or node representations (centroid) (Radev et al., 2004;(Rossiello et al., 2017) to score and rank sentences. Popular centrality-based methods include TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and PACSUM (Zheng and Lapata, 2019). Details of centrality-based and centroid-based summarization will be discussed in Section 3. \n\nResearchers also explore supervised graph neural network (GNN) on the task (Yasunaga et al., 2017;Xu et al., 2019;Wang et al., 2020). Our paper follows this line of works on developing novel unsupervised graph models for single document summarization."
                        ],
                        "paper": {
                            "corpus_id": 232404911,
                            "title": "Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2135688409",
                                    "name": "Haopeng Zhang"
                                },
                                {
                                    "authorId": "1718428",
                                    "name": "Jiawei Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9052734375
                    },
                    {
                        "id": "(Mridha et al., 2021)",
                        "snippets": [
                            "The graph-based method can be applied to both extractive and abstractive text summarization. This approach is an unsupervised learning method that rates the required sentences or terms using a graph. The purpose of the graphical process is to extract the most relevant sentences from a single text (Moratanch et al., 2017). \n\nGraph-based ranking algorithms determine the relevance of a vertex in a graph based on global information iteratively extracted from the entire chart. When it comes to text summarization, specific graph-based techniques are applied. 1) LexRank: LexRank is a probabilistic graph-based technique for calculating sentence significance based on the notion of eigenvector centrality in a graph representation of phrases for natural language processing. It is a connectivity matrix based on intra-sentence cosine similarity that is utilized as the adjacency matrix in a sentence graph representation (Erkan et al., 2004). (Otterbacher et al., 2009), (Wu et al., 2015), (Verma et al., 2016), (Li et al., 2016), (Samuel et al., 2016) proposed the LexRank algorithm for graph-based text summarization task. 2) Hyperlink-Induced Topic Search (HITS): Hyperlinkinduced topic search is a link analysis algorithm that determines the authority and hub values. The results for the search query are retrieved, and then the computation is performed only on this set of results. A hub value is the total of the scaled authority values of the pages it points to, and an authority value is the sum of the scaled authority values of the pages it points to (Litvak et al., 2008). Some articles focused on the HITS ranking algorithm for graphbased text summarization tasks, as can be seen in (Thakkar et al., 2010), (Mihalcea, 2004), (Bhandari et al., 2008), and (Zhang et al., 2005). 3) PageRank: The PageRank algorithm utilizes the inbound links of specified pages to measure their significance or quality to rank the search results. PageRank links more weight based on the importance of the page from which it originates [267]. Some articles proposed the PageRank algorithm for summarization (Elbarougy et al., 2020), (Mallick et al., 2018), (Wang et al., 2007), (Chongsuntornsri et al., 2006), (Sornil et al., 2006). 4) TextRank: TextRank is an unsupervised method for automatic text summarization to extract the most important keywords from a document."
                        ],
                        "paper": {
                            "corpus_id": 244510269,
                            "title": "A Survey of Automatic Text Summarization: Progress, Process and Challenges",
                            "authors": [
                                {
                                    "authorId": "2348260813",
                                    "name": "M. F. Mridha"
                                },
                                {
                                    "authorId": "2149534558",
                                    "name": "Aklima Akter Lima"
                                },
                                {
                                    "authorId": "2086569",
                                    "name": "Prof. Dr. Kamruddin Nur"
                                },
                                {
                                    "authorId": "2152905283",
                                    "name": "S. Das"
                                },
                                {
                                    "authorId": "2663201",
                                    "name": "M. Hasan"
                                },
                                {
                                    "authorId": "2055992775",
                                    "name": "Muhammad Mohsin Kabir"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Access",
                            "n_citations": 88
                        },
                        "score": 0.9140625
                    },
                    {
                        "id": "(Parveen et al., 2014)",
                        "snippets": [
                            "A graph-based representation of documents for summarization is adopted by various approaches. For instance, TextRank by Mihalcea and Tarau (2004) applies the PageRank algorithm (Brin and Page, 1998) to extract important sentences for single document summarization. This ranking algorithm proclaims the importance of a sentence by considering the global information which is computed recursively from the entire graph. Later, the graph is converted into a weighted graph in which the weights are calculated by measuring the similarity of sentences (Mihalcea, 2004). Similarly, in the LexRank approach (Erkan et al., 2004), documents are represented as a similarity graph in which the sentences are nodes and these sentences are then ranked according to centrality measures. The three centrality measures used are degree, LexRank with threshold and continuous LexRank. LexRank is a measure to calculate ranks using the similarity graph of sentences. It is also known as lexical PageRank. The summarization approach developed by (Gong et al., 2001) is also based on ranking sentences where important sentences are selected using a relevance measure and latent semantic analysis."
                        ],
                        "paper": {
                            "corpus_id": 8437725,
                            "title": "Multi-document Summarization Using Bipartite Graphs",
                            "authors": [
                                {
                                    "authorId": "2567321",
                                    "name": "Daraksha Parveen"
                                },
                                {
                                    "authorId": "31380436",
                                    "name": "M. Strube"
                                }
                            ],
                            "year": 2014,
                            "venue": "TextGraphs@EMNLP",
                            "n_citations": 16
                        },
                        "score": 0.90283203125
                    },
                    {
                        "id": "(Sipos et al., 2011)",
                        "snippets": [
                            "A popular stohastic graph-based summarization method is LexRank (Erkan et al., 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank [16] is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank [17]. Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. \n\nGraph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities."
                        ],
                        "paper": {
                            "corpus_id": 16689087,
                            "title": "Large-Margin Learning of Submodular Summarization Methods",
                            "authors": [
                                {
                                    "authorId": "2195688",
                                    "name": "Ruben Sipos"
                                },
                                {
                                    "authorId": "2248631",
                                    "name": "Pannagadatta K. Shivaswamy"
                                },
                                {
                                    "authorId": "1680188",
                                    "name": "T. Joachims"
                                }
                            ],
                            "year": 2011,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.91796875
                    },
                    {
                        "id": "(Mao et al., 2023)",
                        "snippets": [
                            "There are several simple unsupervised summarization extraction frameworks, including TextRank (Mihalcea et al., 2004) and LexRank (Erkan et al., 2004), as well as more robust graph-based ranking methods such as Pac-Sum (Zheng et al., 2019), FAR (Liang et al., 2021), DASG (Liu et al., 2021). Graph-based ranking methods take sentence representations as input, using the algorithm of graph-based sentence centrality ranking for sentence selection. We now introduce extractive summarization backbones. \n\n\u2022 TextRank and LexRank utilize PageRank to calculate node centrality based on a Markov chain model recursively. \n\n\u2022 PacSum (Zheng and Lapata, 2019) constructs graphs with directed edges. The rationale behind this approach is that the centrality of two nodes is influenced by their relative position in the document, as illustrated by Equation 15. \n\n\u2022 DASG (Liu et al., 2021) selects sentences for summarization based on the similarities and relative distances among neighbouring sentences. It incorporates a graph edge weighting scheme to Equation 15, using a coefficient that maps a pair of sentence indices to a value calculated by their relative distance. \n\n\u2022 FAR (Liang et al., 2021) modifies Equation 15by applying a facet-aware centrality-based ranking model to filter out insignificant sentences."
                        ],
                        "paper": {
                            "corpus_id": 264817506,
                            "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders",
                            "authors": [
                                {
                                    "authorId": "67081502",
                                    "name": "Qianren Mao"
                                },
                                {
                                    "authorId": "2263933839",
                                    "name": "Shaobo Zhao"
                                },
                                {
                                    "authorId": "2264274905",
                                    "name": "Jiarui Li"
                                },
                                {
                                    "authorId": "2263823705",
                                    "name": "Xiaolei Gu"
                                },
                                {
                                    "authorId": "2263839211",
                                    "name": "Shizhu He"
                                },
                                {
                                    "authorId": "2264733691",
                                    "name": "Bo Li"
                                },
                                {
                                    "authorId": "2263794896",
                                    "name": "Jianxin Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.916015625
                    },
                    {
                        "id": "(Zheng et al., 2019)",
                        "snippets": [
                            "Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin."
                        ],
                        "paper": {
                            "corpus_id": 182952892,
                            "title": "Sentence Centrality Revisited for Unsupervised Summarization",
                            "authors": [
                                {
                                    "authorId": "2115239329",
                                    "name": "Hao Zheng"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 171
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "Supervised summarization has made significant improvements in recent years by leveraging cutting-edge deep learning technologies. However, the true success of supervised methods relies on the availability of large quantity of human-generated summaries of documents, which is highly costly and difficult to obtain in general. This paper proposes an unsupervised approach to extractive text summarization, which uses an automatically constructed sentence graph from each document to select salient sentences for summarization based on both the similarities and relative distances in the neighborhood of each sentences. We further generalize our approach from single-document summarization to a multi-document setting, by aggregating document-level graphs via proximity-based cross-document edges. In our experiments on benchmark datasets, the proposed approach achieved competitive or better results than previous state-of-the-art unsupervised extractive summarization methods in both single-document and multi-document settings, and the performance is competitive to strong supervised baselines."
                        ],
                        "paper": {
                            "corpus_id": 235792514,
                            "title": "Unsupervised Extractive Text Summarization with Distance-Augmented Sentence Graphs",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Jingzhou Liu"
                                },
                                {
                                    "authorId": "5677927",
                                    "name": "Dominic J. D. Hughes"
                                },
                                {
                                    "authorId": "46286308",
                                    "name": "Yiming Yang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 29
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liang et al., 2021)",
                        "snippets": [
                            "Unsupervised extractive summarization aims to extract salient sentences from documents without labeled corpus. Existing methods are mostly graph-based by computing sentence centrality. These methods usually tend to select sentences within the same facet, however, which often leads to the facet bias problem especially when the document has multiple facets (i.e. long-document and multi-documents). To address this problem, we proposed a novel facet-aware centrality-based ranking model. We let the model pay more attention to different facets by introducing a sentence-document weight. The weight is added to the sentence centrality score. We evaluate our method on a wide range of summarization tasks that include 8 representative benchmark datasets. Experimental re-sults show that our method consistently out-performs strong baselines especially in long-and multi-document scenarios and even performs comparably to some supervised models. Extensive analyses con\ufb01rm that the performance gains come from alleviating the facet bias problem."
                        ],
                        "paper": {
                            "corpus_id": 236477774,
                            "title": "Improving Unsupervised Extractive Summarization with Facet-Aware Modeling",
                            "authors": [
                                {
                                    "authorId": "120436437",
                                    "name": "Xinnian Liang"
                                },
                                {
                                    "authorId": "2362902",
                                    "name": "Shuangzhi Wu"
                                },
                                {
                                    "authorId": "2112143809",
                                    "name": "Mu Li"
                                },
                                {
                                    "authorId": "1707275",
                                    "name": "Zhoujun Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 44
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lierde et al., 2019)",
                        "snippets": [
                            "The earliest graph-based summarizer, called LexRank (Erkan et al., 2004), defines edges as term co-occurrence relationships between sentences. Then, PageRank algorithm is applied to compute relevance scores of sentences. Adapting this idea for the task of query-focused summarization, topic sensitive LexRank (Otterbacher et al., 2005) introduces a query bias in probabilities of transition, which results in higher scores for sentences that are similar to the query. Sim-ilarly, (Wan, 2013) proposes a manifold ranking algorithm in which scores are popagated accross a graph including both sentences and the query as vertices. To remove redundancies in summaries, (Mei et al., 2010) proposes a new node ranking algorithm called DivRank, which tends to select dissimilar sentences. While early graph-based algorithms only involved sentences, a bipartite graph model is proposed in (Wan et al., 2008), involving both sentences and terms as vertices and it applies HITS algorithm to score sentences. (Wei et al., 2010) combines this idea with a PageRank-like method to score sentences, terms and documents simultaneously."
                        ],
                        "paper": {
                            "corpus_id": 182712867,
                            "title": "Learning with fuzzy hypergraphs: A topical approach to query-oriented text summarization",
                            "authors": [
                                {
                                    "authorId": "29364954",
                                    "name": "H. V. Lierde"
                                },
                                {
                                    "authorId": "144134805",
                                    "name": "T. Chow"
                                }
                            ],
                            "year": 2019,
                            "venue": "Information Sciences",
                            "n_citations": 26
                        },
                        "score": 0.90625
                    },
                    {
                        "id": "(Otterbacher et al., 2005)",
                        "snippets": [
                            "We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. \"How many victims have been found?\") Judges found sentences providing an answer to each question. To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap. In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline."
                        ],
                        "paper": {
                            "corpus_id": 10274362,
                            "title": "Using Random Walks for Question-focused Sentence Retrieval",
                            "authors": [
                                {
                                    "authorId": "2216583",
                                    "name": "Jahna Otterbacher"
                                },
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2005,
                            "venue": "Human Language Technology - The Baltic Perspectiv",
                            "n_citations": 179
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khan et al., 2020)",
                        "snippets": [
                            "In recent years, various graph-based methods have attracted more attention and effectively attempted for text summarization. These methods utilize PageRank algorithm [39] and its variants to give rank/score to graph nodes, which represent sentences or passages. The authors in [40] proposed a connectivity graph, which assumes that nodes only carry significant information if they are connected to many other nodes. The authors in [41] introduced a Lex-PageRank approach that is based on eigenvector centrality, which constructs a sentence connectivity matrix and utilizes similar algorithm like PageRank to find the significant sentences for summary. A similar algorithm to PageRank was also proposed in [42] which finds salient sentences for summary generation.\n\nThe authors in [24] presented a graph-based approach, which integrates surface features with text content and investigates subtopic features in multiple documents to incorporate them into the graph-based ranking algorithm. A multidocument summarization approach based on affinity graph [43] exploits similar algorithm to PageRank and computes sentence scores in the affinity graph based on information richness. The authors in [44] demonstrated a document-sensitive graph model for multidocument generic summarization and highlighted the impact of global document set information at sentence level. A weighted graph model for generic multidocument summarization introduced in [45] combines sentence ranking and sentence clustering methods. The authors in [46] presented a graphbased method for multidocument summarization of Vietnamese documents and employed traditional PageRank algorithm to rank the important sentences. The authors in [47] demonstrated an event graph-based approach for multidocument extractive summarization."
                        ],
                        "paper": {
                            "corpus_id": 219690773,
                            "title": "Movie Review Summarization Using Supervised Learning and Graph-Based Ranking Algorithm",
                            "authors": [
                                {
                                    "authorId": "1561421122",
                                    "name": "Atif Khan"
                                },
                                {
                                    "authorId": "1750930091",
                                    "name": "Muhammad Adnan Gul"
                                },
                                {
                                    "authorId": "3060705",
                                    "name": "M. Zareei"
                                },
                                {
                                    "authorId": "9820762",
                                    "name": "R. Rajesh"
                                },
                                {
                                    "authorId": "1411333558",
                                    "name": "Asim Zeb"
                                },
                                {
                                    "authorId": "2057127260",
                                    "name": "Muhammad Naeem"
                                },
                                {
                                    "authorId": "1411821449",
                                    "name": "Yousaf Saeed"
                                },
                                {
                                    "authorId": "1680372",
                                    "name": "N. Salim"
                                }
                            ],
                            "year": 2020,
                            "venue": "Computational Intelligence and Neuroscience",
                            "n_citations": 16
                        },
                        "score": 0.91845703125
                    },
                    {
                        "id": "(Han et al., 2016)",
                        "snippets": [
                            "Another approach of unsupervised extract-based summarization (Wang et al., 2008)(Li, 2005) uses graph-based model. TextRank (Park et al., 2007) and LexRank (Dhillon, 2001) are first two graph-based models applied in text summarization, which use the PageRank-like algorithms to mark sentences. Then, other researchers have integrated the statistical and linguistic features to drive the sentence selection process, for example, the sentence position (Katragadda et al., 2009), term frequency (Lin et al., 1997), topic signature (Lin et al., 2000), lexical chains (Barzilay et al., 1997), and syntactic patterns (Baralis et al., 2013)(Haggag, 2013). Ko and Seo (Ko et al., 2008) composed two sentences nearby into a bigram. Those bigrams were supposed to be context information. First, they extracted the bigrams by using the sentence extraction model. Then they used another extraction module to extract sentences from them. The ClusterCMRW and ClusterHITS models calculated the sentences scores by considering the clusterlevel information in the graph-based ranking algorithm. Canhasi and Kononenko (Canhasi et al., 2013) improve matrix decomposition by employing the archetypal analysis for generic multidocument summarization. While coming to the document set, there must be more consideration about the document-level influence. But it did not consider the relationship between words and sentences. The DsR model (Wei et al., 2010) achieved it by using document-sensitive graph-based ranking model. But this method did not get a satisfied result. Yin et al. improved the summarization quality by adding extra information which came from the query-extraction scenario. Goyal et al. (Goyal et al., 2013) take Bernoulli model of randomness to index weights of sentences taking the context into consideration. The method proposed in [28] decomposed sentences by semantic role analysis, but while building the model, it did not use graphbased algorithms."
                        ],
                        "paper": {
                            "corpus_id": 13237013,
                            "title": "Text Summarization Using FrameNet-Based Semantic Graph Model",
                            "authors": [
                                {
                                    "authorId": "2110981656",
                                    "name": "Xu Han"
                                },
                                {
                                    "authorId": "143750477",
                                    "name": "Tao Lv"
                                },
                                {
                                    "authorId": "48430746",
                                    "name": "Zhirui Hu"
                                },
                                {
                                    "authorId": "2115554077",
                                    "name": "Xinyan Wang"
                                },
                                {
                                    "authorId": "50097174",
                                    "name": "Cong Wang"
                                }
                            ],
                            "year": 2016,
                            "venue": "Scientific Programming",
                            "n_citations": 25
                        },
                        "score": 0.9091796875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Neural Graph-Based Models",
                "tldr": "Neural graph-based models have revolutionized multi-document summarization by integrating graph representations with neural networks. These approaches leverage both traditional graph structures and neural architectures to better capture document relationships, with innovations in graph construction, node representations, and attention mechanisms driving significant performance improvements. (16 sources)",
                "text": "\nThe integration of neural networks with graph-based representations has marked a significant evolution in multi-document summarization approaches. This fusion combines the structural advantages of graphs with the representation power of deep neural networks, enabling more sophisticated modeling of cross-document relationships and content importance.\n\nA pioneering contribution in this domain was made by Yasunaga et al., who proposed a neural multi-document summarization system incorporating sentence relation graphs. Their approach employs Graph Convolutional Networks (GCNs) on relation graphs, with sentence embeddings from Recurrent Neural Networks as node features. Through layer-wise propagation, the GCN generates high-level sentence features for salience estimation, demonstrating advantages over traditional graph-based extractive approaches <Paper corpusId=\"6532096\" paperTitle=\"(Yasunaga et al., 2017)\" isShortName></Paper>.\n\nBuilding on this foundation, subsequent neural graph-based models have explored diverse architectures and graph representations. Liu et al. highlighted the importance of cross-document links in identifying salient information and eliminating redundancy. Their approach utilized graph convolutional networks over relation graphs to recursively compute the importance of passages based on the entire document collection <Paper corpusId=\"170079112\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>.\n\nThe construction of knowledge graphs has emerged as a prominent strategy in neural graph-based summarization. Zhou et al. presented an entity-aware model (EMSum) that constructs a knowledge graph with text units and entities as nodes, using co-reference resolution tools to extract semantic entities <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>. Similarly, Li et al. proposed a graph-informed self-attention mechanism that effectively combines graph structures with abstractive multi-document summarization <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>.\n\nPasunuru et al. developed an efficient graph-enhanced approach (BART-Long-Graph) that leverages knowledge graphs constructed using natural language processing tools to capture multi-level semantic information within documents. Their approach achieved significant improvements on benchmark datasets, demonstrating that graph encodings lead to summaries that are more abstractive, informative, and factually consistent <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>.\n\nHeterogeneous graph structures have been introduced to capture different types of information in documents. Wang et al. proposed a heterogeneous graph-based neural network (HDSG) that includes semantic nodes of different granularity levels to enrich cross-sentence relations <Paper corpusId=\"216552978\" paperTitle=\"(Wang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. This approach addresses the limitations of traditional models that only consider sentence-to-sentence relationships.\n\nRecent advances have also explored the integration of pre-trained language models with graph-based approaches. Wu et al. presented BASS, a framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases across context and conveys rich relations between phrases <Paper corpusId=\"235187330\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper> <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>.\n\nGraph-based neural approaches have demonstrated versatility in handling various summarization scenarios. Chen et al. formulated multi-document summarization as a sub-graph selection problem (SgSum), selecting a salient sub-graph from a relation graph instead of individual sentences <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>. This approach captures both intra and cross-document relations while producing more coherent summaries.\n\nThe incorporation of Abstract Meaning Representation (AMR) into neural graph-based models has further enhanced summarization performance. Ribeiro and others have shown that AMR structures benefit both document representation and summary generation, with the potential to improve controllability in summarization <Paper corpusId=\"264614849\" paperTitle=\"(Ribeiro, 2022)\" isShortName></Paper>.\n\nNeural graph-based models continue to evolve with specialized applications for scientific literature summarization. Chen et al. developed a target-aware related work generator that models relationships between reference papers and target papers with target-centered attention mechanisms <Paper corpusId=\"249097545\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"269157041\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Similarly, Wang et al. presented KGSum, a model centered on knowledge graphs during both encoding and decoding processes for multi-document scientific summarization <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"269157041\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThese neural graph-based models have consistently demonstrated superior performance compared to traditional graph-based methods like LexRank and TextRank, as well as other neural approaches without graph structures <Paper corpusId=\"232404911\" paperTitle=\"(Zhang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"219036690\" paperTitle=\"(Xu et al., 2019)\" isShortName></Paper> <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. The combination of structured graph representations with the learning capabilities of neural networks has proven particularly effective for capturing complex document relationships and generating coherent, informative summaries from multiple sources.",
                "citations": [
                    {
                        "id": "(Yasunaga et al., 2017)",
                        "snippets": [
                            "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems."
                        ],
                        "paper": {
                            "corpus_id": 6532096,
                            "title": "Graph-based Neural Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "19168196",
                                    "name": "Michihiro Yasunaga"
                                },
                                {
                                    "authorId": "144142360",
                                    "name": "Rui Zhang"
                                },
                                {
                                    "authorId": "19266011",
                                    "name": "Kshitijh Meelu"
                                },
                                {
                                    "authorId": "18081101",
                                    "name": "Ayush Pareek"
                                },
                                {
                                    "authorId": "2093939303",
                                    "name": "K. Srinivasan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2017,
                            "venue": "Conference on Computational Natural Language Learning",
                            "n_citations": 258
                        },
                        "score": 0.92041015625
                    },
                    {
                        "id": "(Liu et al., 2019)",
                        "snippets": [
                            "The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004;Christensen et al., 2013;Wan, 2008;Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them.\n\nMost previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links."
                        ],
                        "paper": {
                            "corpus_id": 170079112,
                            "title": "Hierarchical Transformers for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "39798499",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 298
                        },
                        "score": 0.9169921875
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan et al., 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Wan, 2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, (Tan et al., 2017) propose a graph-based attention mechanism to identify salient sentences. (Yasunaga et al., 2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. (Fan et al., 2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. (huang et al., 2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. (Li et al., 2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, (Wang et al., 2020) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization."
                        ],
                        "paper": {
                            "corpus_id": 236478143,
                            "title": "Entity-Aware Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2053308860",
                                    "name": "Weidong Ren"
                                },
                                {
                                    "authorId": "150112803",
                                    "name": "Gongshen Liu"
                                },
                                {
                                    "authorId": "153253583",
                                    "name": "Bo Su"
                                },
                                {
                                    "authorId": "143844110",
                                    "name": "Wei Lu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 28
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Qu, 2024)",
                        "snippets": [
                            "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP",
                            "Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."
                        ],
                        "paper": {
                            "corpus_id": 269762702,
                            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
                            "authors": [
                                {
                                    "authorId": "2163451228",
                                    "name": "Yutong Qu"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Web Conference",
                            "n_citations": 0
                        },
                        "score": 0.88330078125
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models.\n\nMost previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Chen et al., 2021)",
                        "snippets": [
                            "The graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. (Erkan et al., 2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2019) both use discourse graphs to generate concise and informative summaries. (Li et al., 2016) and (Li et al., 2021) propose to utilize event relation graph to represent documents for MDS",
                            ".Most previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. (Erkan et al., 2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Christensen et al., 2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, (Yasunaga et al., 2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. (Li et al., 2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. (Wu et al., 2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases."
                        ],
                        "paper": {
                            "corpus_id": 239768911,
                            "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
                            "authors": [
                                {
                                    "authorId": "2108267116",
                                    "name": "Moye Chen"
                                },
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 22
                        },
                        "score": 0.9423828125
                    },
                    {
                        "id": "(Pasunuru et al., 2021)",
                        "snippets": [
                            "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization."
                        ],
                        "paper": {
                            "corpus_id": 235097309,
                            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                            "authors": [
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "2940333",
                                    "name": "Mengwen Liu"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "120209444",
                                    "name": "Sujith Ravi"
                                },
                                {
                                    "authorId": "40262269",
                                    "name": "Markus Dreyer"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 73
                        },
                        "score": 0.9326171875
                    },
                    {
                        "id": "(Wang et al., 2020)",
                        "snippets": [
                            "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github."
                        ],
                        "paper": {
                            "corpus_id": 216552978,
                            "title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization",
                            "authors": [
                                {
                                    "authorId": "49371126",
                                    "name": "Danqing Wang"
                                },
                                {
                                    "authorId": "144118452",
                                    "name": "Pengfei Liu"
                                },
                                {
                                    "authorId": "3337238",
                                    "name": "Y. Zheng"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                },
                                {
                                    "authorId": "1790227",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 282
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "HDSG (Wang et al., 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."
                        ],
                        "paper": {
                            "corpus_id": 252185277,
                            "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
                            "authors": [
                                {
                                    "authorId": "2073437",
                                    "name": "Pancheng Wang"
                                },
                                {
                                    "authorId": "2145340498",
                                    "name": "Shasha Li"
                                },
                                {
                                    "authorId": "2116489",
                                    "name": "Kunyuan Pang"
                                },
                                {
                                    "authorId": "50670961",
                                    "name": "Liangliang He"
                                },
                                {
                                    "authorId": "2108821455",
                                    "name": "Dong Li"
                                },
                                {
                                    "authorId": "1762106",
                                    "name": "Jintao Tang"
                                },
                                {
                                    "authorId": "38972135",
                                    "name": "Ting Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 15
                        },
                        "score": 0.88427734375
                    },
                    {
                        "id": "(Wu et al., 2021)",
                        "snippets": [
                            "Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks."
                        ],
                        "paper": {
                            "corpus_id": 235187330,
                            "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph",
                            "authors": [
                                {
                                    "authorId": "2139644141",
                                    "name": "Wenhao Wu"
                                },
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "2314396",
                                    "name": "Ziqiang Cao"
                                },
                                {
                                    "authorId": "48831399",
                                    "name": "Sujian Li"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 45
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ribeiro, 2022)",
                        "snippets": [
                            "A growing body of work focuses on using graph-based representations for improving summarization. Whereas different approaches encode graphs into neural models for multi-document summarization (Fan et al., 2019;Li et al., 2020;Pasunuru et al., 2021;Chen et al., 2021), AMR structures have been shown to benefit both document representation and summary generation (Liu et al., 2015;Liao et al., 2018;Hardy and Vlachos, 2018;Dohare et al., 2018) and have the potential of improving controllability in summarization."
                        ],
                        "paper": {
                            "corpus_id": 264614849,
                            "title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations",
                            "authors": [
                                {
                                    "authorId": "10430740",
                                    "name": "Leonardo F. R. Ribeiro"
                                }
                            ],
                            "year": 2022,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 60
                        },
                        "score": 0.91015625
                    },
                    {
                        "id": "(Chen et al., 2022)",
                        "snippets": [
                            "Our extractive baselines include: LexRank (Erkan et al., 2004): a graph-based extractive model that computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences",
                            "EMS (Zhou et al., 2021): an entity-aware model for abstractive multi-document summarization with BERT encoder. It augments the classical Transformer based encoder-decoder framework with a graph consisting of paragraph nodes and entity nodes",
                            "RRG (Chen et al., 2021): an abstractive related work generator based on reference papers. It augments the RNN encoder with a relation-aware graph between multiple references. The relation graph and the document representation are refined iteratively."
                        ],
                        "paper": {
                            "corpus_id": 249097545,
                            "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning",
                            "authors": [
                                {
                                    "authorId": "46772896",
                                    "name": "Xiuying Chen"
                                },
                                {
                                    "authorId": "1753416672",
                                    "name": "Hind Alamro"
                                },
                                {
                                    "authorId": "9255404",
                                    "name": "Li Mingzhe"
                                },
                                {
                                    "authorId": "2112311595",
                                    "name": "Shen Gao"
                                },
                                {
                                    "authorId": "2055864368",
                                    "name": "Rui Yan"
                                },
                                {
                                    "authorId": "2118502950",
                                    "name": "Xin Gao"
                                },
                                {
                                    "authorId": "2928371",
                                    "name": "Xiangliang Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 29
                        },
                        "score": 0.88720703125
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "According to the paradigm adopted, previous studies can be categorized into graph-based (Chen et al., 2022)(Wang et al., 2022), flat-based (Moro et al., 2022)(Shi et al., 2023), and hierarchical-based methods (Chen et al., 2021)[30].Concretely, graph-based methods construct external graphs (e.g., entity-relation graph, AMR, TF-IDF) to assist document representation and cross-document relation modeling.In this regard, Chen et al. (Chen et al., 2022) leverage graphs to encode source articles, constructing keyphrase graphs and using a multi-level contrastive learning strategy to improve the accuracy of generated summaries.\n\nWang et al. (Wang et al., 2022) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template."
                        ],
                        "paper": {
                            "corpus_id": 269157041,
                            "title": "Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization",
                            "authors": [
                                {
                                    "authorId": "2073437",
                                    "name": "Pancheng Wang"
                                },
                                {
                                    "authorId": "2145340498",
                                    "name": "Shasha Li"
                                },
                                {
                                    "authorId": "2214849368",
                                    "name": "Dong Li"
                                },
                                {
                                    "authorId": "2186557380",
                                    "name": "Kehan Long"
                                },
                                {
                                    "authorId": "1762106",
                                    "name": "Jintao Tang"
                                },
                                {
                                    "authorId": "2257363292",
                                    "name": "Ting Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 2
                        },
                        "score": 0.8935546875
                    },
                    {
                        "id": "(Zhang et al., 2021)",
                        "snippets": [
                            "Unsupervised graph summarization methods rely on graph connectivity (centrality) or node representations (centroid) (Radev et al., 2004;(Rossiello et al., 2017) to score and rank sentences. Popular centrality-based methods include TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and PACSUM (Zheng and Lapata, 2019). Details of centrality-based and centroid-based summarization will be discussed in Section 3. \n\nResearchers also explore supervised graph neural network (GNN) on the task (Yasunaga et al., 2017;Xu et al., 2019;Wang et al., 2020). Our paper follows this line of works on developing novel unsupervised graph models for single document summarization."
                        ],
                        "paper": {
                            "corpus_id": 232404911,
                            "title": "Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2135688409",
                                    "name": "Haopeng Zhang"
                                },
                                {
                                    "authorId": "1718428",
                                    "name": "Jiawei Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9052734375
                    },
                    {
                        "id": "(Xu et al., 2019)",
                        "snippets": [
                            "Graph-based Summarization Graph approach has been explored in text summarization over decades. LexRank introduced a stochastic graphbased method for computing relative importance of textual units (Erkan et al., 2004). (Yasunaga et al., 2017) employed a GCN on the relation graphs with sentence embeddings obtained from RNN. (Tan et al., 2017) also proposed graphbased attention in abstractive summarization model. Fernandes et al. (2018) developed a framework to reason long-distance relationships for text summarization."
                        ],
                        "paper": {
                            "corpus_id": 219036690,
                            "title": "Discourse-Aware Neural Extractive Text Summarization",
                            "authors": [
                                {
                                    "authorId": "34837371",
                                    "name": "Jiacheng Xu"
                                },
                                {
                                    "authorId": "144702900",
                                    "name": "Zhe Gan"
                                },
                                {
                                    "authorId": "145215470",
                                    "name": "Yu Cheng"
                                },
                                {
                                    "authorId": "46700348",
                                    "name": "Jingjing Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 282
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0.87255859375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications and Effectiveness of Graph-Based Approaches",
                "tldr": "Graph-based approaches have been successfully applied across various summarization domains including news, scientific literature, and biomedical text, consistently outperforming traditional extractive methods. Their effectiveness stems from the ability to capture complex document relationships, reduce redundancy, and improve summary coherence, with research confirming their superiority in both traditional and neural implementations. (17 sources)",
                "text": "\nGraph-based approaches to multi-document summarization have demonstrated remarkable effectiveness across various domains and applications. Early graph-based methods like LexRank and TextRank revolutionized the field by showing significant improvements over previous approaches, with LexRank ranking first in several Document Understanding Conference (DUC) 2004 evaluation tasks <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. This success established graph-based methods as a standard approach for multi-document summarization, particularly in news domains where capturing cross-document relationships is crucial.\n\nThe effectiveness of graph-based summarization extends to specialized domains as well. In biomedical text summarization, concept graph modeling has proven particularly successful, with approaches like those proposed by Plaza et al. adapting graph-based methods to model biomedical-specific concept graphs <Paper corpusId=\"5674504\" paperTitle=\"(Plaza et al., 2008)\" isShortName></Paper> <Paper corpusId=\"145993667\" paperTitle=\"(Mohamed et al., 2019)\" isShortName></Paper>. These domain-specific adaptations demonstrate the flexibility of graph-based approaches in handling specialized terminology and relationships.\n\nGraph-based approaches have consistently outperformed other extractive methods in comparative evaluations. Multiple studies have confirmed that degree-based methods, including LexRank with threshold, outperform both centroid-based methods and other systems participating in standardized evaluations <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> <Paper corpusId=\"59599960\" paperTitle=\"(Lierde et al._1, 2019)\" isShortName></Paper>. This superiority stems from their ability to capture the structural and semantic relationships between sentences across multiple documents, effectively identifying the most central and important content.\n\nThe effectiveness of graph-based methods has been further enhanced through various improvements to the basic framework. Wan and Yang developed a more sophisticated approach that assigns different weights to intra-document and inter-document links, giving higher priority to sentences with significant inter-document connections <Paper corpusId=\"5457260\" paperTitle=\"(Wan et al., 2006)\" isShortName></Paper> <Paper corpusId=\"33696521\" paperTitle=\"(Kumar et al., 2016)\" isShortName></Paper>. Other researchers have explored incorporating document-level information into the graph structure, considering the influence of global information from document clusters on local sentence evaluation <Paper corpusId=\"33696521\" paperTitle=\"(Kumar et al., 2016)\" isShortName></Paper>.\n\nWith the integration of neural networks, graph-based approaches have reached new levels of effectiveness. Neural graph-based models have consistently demonstrated superior performance compared to traditional graph-based methods like LexRank and TextRank, as well as other neural approaches without graph structures <Paper corpusId=\"235187219\" paperTitle=\"(Hickmann et al., 2021)\" isShortName></Paper>. The GraphSum model, for example, showed substantial improvements compared to strong MDS baselines such as LEAD-3 and LexRank <Paper corpusId=\"235187219\" paperTitle=\"(Hickmann et al., 2021)\" isShortName></Paper>.\n\nRecent studies have confirmed that graphs provide significant benefits to multi-document summarization by helping generate more informative, concise, and coherent summaries <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"259262067\" paperTitle=\"(Nguyen et al., 2023)\" isShortName></Paper>. The ability of graphs to capture relations between textual units has proven especially valuable for detecting salient information from multiple documents and generating overall coherent summaries <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>.\n\nThe evolution of graph-based summarization techniques has followed the broader progression of summarization approaches. Sankar et al. identified four primary approaches in multi-document summarization: graph ranking-based extractive methods (like TextRank and LexRank), syntax and structure-based compression methods, paraphrasing-based approaches, and neural sequence-to-sequence abstractive methods <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>. Graph-based approaches have remained relevant throughout this evolution, with researchers continuing to find ways to integrate graph structures with newer neural architectures.\n\nRecent research has emphasized the complementary strengths of graph-based extractive methods and neural sequence-to-sequence architectures. Graph-based extractive methods excel at identifying relationships between sentences or documents and extracting salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting <Paper corpusId=\"250374734\" paperTitle=\"(Chen et al._1, 2022)\" isShortName></Paper>. This recognition has led to the development of hybrid approaches that leverage both paradigms, with abstractive models based on graphs gaining significant attention <Paper corpusId=\"250374734\" paperTitle=\"(Chen et al._1, 2022)\" isShortName></Paper>.\n\nIn the domain of scientific literature summarization, graph-based approaches have been particularly effective. Wang et al. developed KGSum, a model centered on knowledge graphs during both encoding and decoding processes for multi-document scientific summarization, achieving state-of-the-art performance <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"273963834\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>. Other researchers have explored hierarchical graph-based methods for related work generation in scientific papers <Paper corpusId=\"269157041\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThe widespread adoption of graph-based summarization methods across various applications and domains demonstrates their versatility and effectiveness. From news summarization to scientific literature and biomedical texts, graph-based approaches have consistently proven their ability to capture document relationships, reduce redundancy, and improve summary coherence <Paper corpusId=\"264146402\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>. This versatility, combined with the integration of neural techniques, ensures that graph-based approaches will continue to play a central role in multi-document summarization research and applications.",
                "citations": [
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0.87255859375
                    },
                    {
                        "id": "(Plaza et al., 2008)",
                        "snippets": [
                            "One of the main problems in research on automatic summarization is the inaccurate semantic interpretation of the source. Using specific domain knowledge can considerably alleviate the problem. In this paper, we introduce an ontology-based extractive method for summarization. It is based on mapping the text to concepts and representing the document and its sentences as graphs. We have applied our approach to summarize biomedical literature, taking advantages of free resources as UMLS. Preliminary empirical results are presented and pending problems are identified."
                        ],
                        "paper": {
                            "corpus_id": 5674504,
                            "title": "Concept-Graph Based Biomedical Automatic Summarization Using Ontologies",
                            "authors": [
                                {
                                    "authorId": "145377713",
                                    "name": "Laura Plaza"
                                },
                                {
                                    "authorId": "102639698",
                                    "name": "Alberto D\u00edaz"
                                },
                                {
                                    "authorId": "1798564",
                                    "name": "Pablo Gerv\u00e1s"
                                }
                            ],
                            "year": 2008,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 53
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mohamed et al., 2019)",
                        "snippets": [
                            "In addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization (Erkan et al., 2004)15,(Canhasi, 2011)(Wei et al., 2010)(Wan, 2010)21,(Azadani et al., 2018)[23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev (Erkan et al., 2004) proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization (Canhasi, 2011)(Wei et al., 2010)(Wan, 2010). In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. (Wei et al., 2010) and Wan (Wan, 2010) considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts (Plaza et al., 2008)25,(Lloret et al., 2011)(Azadani et al., 2018). Of this, Azadani et al. (Azadani et al., 2018) and Plaza et al. (Plaza et al., 2008) have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28]."
                        ],
                        "paper": {
                            "corpus_id": 145993667,
                            "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis",
                            "authors": [
                                {
                                    "authorId": "47302320",
                                    "name": "Muhidin A. Mohamed"
                                },
                                {
                                    "authorId": "1714030",
                                    "name": "M. Oussalah"
                                }
                            ],
                            "year": 2019,
                            "venue": "Information Processing & Management",
                            "n_citations": 101
                        },
                        "score": 0.94140625
                    },
                    {
                        "id": "(Lierde et al._1, 2019)",
                        "snippets": [
                            "Extractive summarizers differ in the method used to identify relevant sentences, which leads to a classification of models as either feature-based or graph-based approaches",
                            "Since feature-based approaches generally require datasets with labelled sentences which are hard to produce [12], unsupervised graph-based methods have attracted growing interest in recent years.\n\nGraph-based summarizers represent the sentences of a corpus as the nodes of a graph with the edges modelling relationships of similarity between the sentences [1]. Then, graph-based algorithms are applied to identify relevant sentences. The models generally differ in the type of relationship captured by the graph or in the sentence selection approach. Most graph-based models define the edges connecting sentences based on the co-occurrence of terms in pairs of sentences [1,3,4]. Then, important sentences are identified either based on node ranking algorithms, or using a global optimization approach. Methods based on node ranking compute individual relevance scores for the sentences and build summaries with highly scored sentences. The earliest such summarizer, LexRank [1], applies the PageRank algorithm to compute sentence scores."
                        ],
                        "paper": {
                            "corpus_id": 59599960,
                            "title": "Query-oriented text summarization based on hypergraph transversals",
                            "authors": [
                                {
                                    "authorId": "29364954",
                                    "name": "H. V. Lierde"
                                },
                                {
                                    "authorId": "144134805",
                                    "name": "T. Chow"
                                }
                            ],
                            "year": 2019,
                            "venue": "Information Processing & Management",
                            "n_citations": 55
                        },
                        "score": 0.87890625
                    },
                    {
                        "id": "(Wan et al., 2006)",
                        "snippets": [
                            "Graph-based methods have been proposed to rank sentences or passages. Websumm (Mani et al., 1997) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization."
                        ],
                        "paper": {
                            "corpus_id": 5457260,
                            "title": "Improved Affinity Graph Based Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                },
                                {
                                    "authorId": "1743923",
                                    "name": "Jianwu Yang"
                                }
                            ],
                            "year": 2006,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 109
                        },
                        "score": 0.9384765625
                    },
                    {
                        "id": "(Kumar et al., 2016)",
                        "snippets": [
                            "This approach differs from the cluster based approach where sentences are ranked based on its closeness to cluster centroid. Two well-known graph based ranking algorithms is the HITS algorithm (Kleinberg, 1999) and the Google's PageRank (Brin and Page, 2012). Both methods have been traditionally used in Web-link analysis and social networks. Lexrank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are two successful graph-based ranking systems that implement these algorithms.\n\nFurther studies have been carried to make improvement through modification in the ranking algorithm. Wan and Yang (2006) assigned different weights to intra-document links and inter-document links. \n\nThey gave more priority to sentence with high interdocument links. In later work by Hariharan and Srinivasan (2009), they approached the graph based method differently i.e., by discounting the already selected sentence by removing it from further consideration when they rank the remaining sentences in the document. \n\nApart from sentence level information, Wan (2008) and Wei et al. (2010) devised a document-sensitive graph model to explore document impact on the graphbased summarization, by incorporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process."
                        ],
                        "paper": {
                            "corpus_id": 33696521,
                            "title": "A Review on Automatic Text Summarization Approaches",
                            "authors": [
                                {
                                    "authorId": "1734844",
                                    "name": "Y. J. Kumar"
                                },
                                {
                                    "authorId": "2445567",
                                    "name": "O. Goh"
                                },
                                {
                                    "authorId": "2136088",
                                    "name": "H. Basiron"
                                },
                                {
                                    "authorId": "3388053",
                                    "name": "Ngo Hea Choon"
                                },
                                {
                                    "authorId": "21154901",
                                    "name": "Puspalata C. Suppiah"
                                }
                            ],
                            "year": 2016,
                            "venue": "Journal of Computer Science",
                            "n_citations": 72
                        },
                        "score": 0.90771484375
                    },
                    {
                        "id": "(Hickmann et al., 2021)",
                        "snippets": [
                            "Graph-based approaches on graph neural networks and specifically using knowledge graphs have gained popularity [8,12,20,21] while recent models combine graph neural networks with a transformer architecture [8,12].\n\nFor extractive MDS, a graph-based representation can help to detect salient vertices, so that they are included in the generated summary to cover all important information. In abstractive MDS, salient vertices can guide the summarization process via attention mechanisms of the transformer architecture.\n\nDifferent textual units have been used for graph-based text summarization in the past, as discourse relations [8,20], sentences [3,21], or paragraphs [12].\n\nThe GraphSum model showed substantial improvements compared to strong MDS baselines such as LEAD-3 and LexRank [5]."
                        ],
                        "paper": {
                            "corpus_id": 235187219,
                            "title": "Analysis of GraphSum\u2019s Attention Weights to Improve the Explainability of Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1938939",
                                    "name": "M. Hickmann"
                                },
                                {
                                    "authorId": "2105164214",
                                    "name": "Fabian Wurzberger"
                                },
                                {
                                    "authorId": "2105165105",
                                    "name": "Megi Hoxhalli"
                                },
                                {
                                    "authorId": "2105164598",
                                    "name": "Arne Lochner"
                                },
                                {
                                    "authorId": "2025314313",
                                    "name": "Jessica T\u00f6llich"
                                },
                                {
                                    "authorId": "1753135",
                                    "name": "A. Scherp"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Information Integration and Web-based Applications & Services",
                            "n_citations": 1
                        },
                        "score": 0.96337890625
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models.\n\nMost previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Nguyen et al., 2023)",
                        "snippets": [
                            "Graphs capturing relationships between textual units are of great benefit to MDS, which can help create more concise, informative, and coherent summaries from multiple sources documents. Furthermore, graphs can be easily constructed by representing sentences or paragraph as graph nodes and edges. LexRank (Erkan et al., 2004)) computes sentence importance based on a lexical similarity graph of sentences. Graph representations of documents such as discourse graph based on discourse relations (Christensen et al., 2013)",
                            "In this paper, we approach by switching back to the graph-based extractive problem, specifically, extracting summaries for multi-document by selecting sub-graphs based on the constructed graph. Inspired by study (Chen et al., 2021), we exploit more edge weights representing the relationship of sentences in the same document and train and improve the model for Vietnamese dataset."
                        ],
                        "paper": {
                            "corpus_id": 259262067,
                            "title": "Vietnamese multi-document summary using subgraph selection approach - VLSP 2022 AbMuSu Shared Task",
                            "authors": [
                                {
                                    "authorId": "2220690681",
                                    "name": "Huu-Thin Nguyen"
                                },
                                {
                                    "authorId": "2101316144",
                                    "name": "Tam Doan Thanh"
                                },
                                {
                                    "authorId": "2220630783",
                                    "name": "Cam-Van Thi Nguyen"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90478515625
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan et al., 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Wan, 2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, (Tan et al., 2017) propose a graph-based attention mechanism to identify salient sentences. (Yasunaga et al., 2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. (Fan et al., 2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. (huang et al., 2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. (Li et al., 2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, (Wang et al., 2020) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization."
                        ],
                        "paper": {
                            "corpus_id": 236478143,
                            "title": "Entity-Aware Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2053308860",
                                    "name": "Weidong Ren"
                                },
                                {
                                    "authorId": "150112803",
                                    "name": "Gongshen Liu"
                                },
                                {
                                    "authorId": "153253583",
                                    "name": "Bo Su"
                                },
                                {
                                    "authorId": "143844110",
                                    "name": "Wei Lu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 28
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Sankar et al., 2022)",
                        "snippets": [
                            "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."
                        ],
                        "paper": {
                            "corpus_id": 248571519,
                            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2064325789",
                                    "name": "Aiswarya Sankar"
                                },
                                {
                                    "authorId": "145934595",
                                    "name": "Ankit R. Chadha"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9169921875
                    },
                    {
                        "id": "(Chen et al._1, 2022)",
                        "snippets": [
                            "Existing multi-document summarization methods mainly focus on sentence-level summarization and can be categorized into extractive and abstractive methods. Most extractive methods are operated over graph-based representations of sentences or passages with edge weights computed by tf-idf (Erkan et al., 2004), discourse relations (Christensen et al., 2013) or sentence embeddings (Yasunaga et al., 2017b) and then a specific algorithm is further adopted for ranking text units for inclusion in the final summary. More recently, some extractive summarization works also utilize graph convolutional networks for salient sentences estimation (Kipf et al., 2016) and sentence ordering (Yin et al., 2019)",
                            "Intuitively, graph-based extractive methods are suitable to identify relationship of different sentences or documents and extract salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting. Hence, abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 250374734,
                            "title": "Two-phase Multi-document Event Summarization on Core Event Graphs",
                            "authors": [
                                {
                                    "authorId": "48354529",
                                    "name": "Zengjian Chen"
                                },
                                {
                                    "authorId": "2116315442",
                                    "name": "Jin Xu"
                                },
                                {
                                    "authorId": "145865588",
                                    "name": "M. Liao"
                                },
                                {
                                    "authorId": "2138967543",
                                    "name": "Tong Xue"
                                },
                                {
                                    "authorId": "2190820519",
                                    "name": "Kun He"
                                }
                            ],
                            "year": 2022,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 2
                        },
                        "score": 0.95556640625
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "HDSG (Wang et al., 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."
                        ],
                        "paper": {
                            "corpus_id": 252185277,
                            "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
                            "authors": [
                                {
                                    "authorId": "2073437",
                                    "name": "Pancheng Wang"
                                },
                                {
                                    "authorId": "2145340498",
                                    "name": "Shasha Li"
                                },
                                {
                                    "authorId": "2116489",
                                    "name": "Kunyuan Pang"
                                },
                                {
                                    "authorId": "50670961",
                                    "name": "Liangliang He"
                                },
                                {
                                    "authorId": "2108821455",
                                    "name": "Dong Li"
                                },
                                {
                                    "authorId": "1762106",
                                    "name": "Jintao Tang"
                                },
                                {
                                    "authorId": "38972135",
                                    "name": "Ting Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 15
                        },
                        "score": 0.88427734375
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "In this regard, LexRank (Erkan et al., 2004) and TextRank (Mihalcea and Tarau 2004) first introduced graphs to extractive text summarization in 2004. They compute sentence importance using a graph representation of sentences to extract salient textual units from documents as summarization. (Zou, 2020), Wang et al. (Wang et al. 2020) propose to extract salient textual units from documents as summarization using a heterogeneous graph consisting of semantic nodes at several granularity levels of documents. In 2022, Wang et al. (Wang et al., 2022)) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template to achieve state-of-the-art performance."
                        ],
                        "paper": {
                            "corpus_id": 273963834,
                            "title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation",
                            "authors": [
                                {
                                    "authorId": "2116765029",
                                    "name": "Zhi Zhang"
                                },
                                {
                                    "authorId": "2290114013",
                                    "name": "Yan Liu"
                                },
                                {
                                    "authorId": "2240541436",
                                    "name": "Sheng-hua Zhong"
                                },
                                {
                                    "authorId": "2265358574",
                                    "name": "Gong Chen"
                                },
                                {
                                    "authorId": "2283200843",
                                    "name": "Yu Yang"
                                },
                                {
                                    "authorId": "2312268442",
                                    "name": "Jiannong Cao"
                                }
                            ],
                            "year": 2025,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.89453125
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "According to the paradigm adopted, previous studies can be categorized into graph-based (Chen et al., 2022)(Wang et al., 2022), flat-based (Moro et al., 2022)(Shi et al., 2023), and hierarchical-based methods (Chen et al., 2021)[30].Concretely, graph-based methods construct external graphs (e.g., entity-relation graph, AMR, TF-IDF) to assist document representation and cross-document relation modeling.In this regard, Chen et al. (Chen et al., 2022) leverage graphs to encode source articles, constructing keyphrase graphs and using a multi-level contrastive learning strategy to improve the accuracy of generated summaries.\n\nWang et al. (Wang et al., 2022) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template."
                        ],
                        "paper": {
                            "corpus_id": 269157041,
                            "title": "Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization",
                            "authors": [
                                {
                                    "authorId": "2073437",
                                    "name": "Pancheng Wang"
                                },
                                {
                                    "authorId": "2145340498",
                                    "name": "Shasha Li"
                                },
                                {
                                    "authorId": "2214849368",
                                    "name": "Dong Li"
                                },
                                {
                                    "authorId": "2186557380",
                                    "name": "Kehan Long"
                                },
                                {
                                    "authorId": "1762106",
                                    "name": "Jintao Tang"
                                },
                                {
                                    "authorId": "2257363292",
                                    "name": "Ting Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 2
                        },
                        "score": 0.8935546875
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Traditional approaches usually employ extractive techniques both in the context of single-document and multi-document summarization. Graph-based methods can be applied to both single-document and multi-document summarization by representing the relationships between sentences in one document or several documents as a graph, with sentences as nodes and the edges as the similarity between the sentences. The systems (Erkan et al., 2004)(Lin et al., 2009)(Parveen et al., 2015)(Thakkar et al., 2010) then use algorithms like PageRank, HITS, or LexRank to identify the most important sentences in the graph, which are then extracted and combined to form the summary."
                        ],
                        "paper": {
                            "corpus_id": 264146402,
                            "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
                            "authors": [
                                {
                                    "authorId": "2258949345",
                                    "name": "Guanghua Wang"
                                },
                                {
                                    "authorId": "2258731583",
                                    "name": "Weili Wu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Discret. Math. Algorithms Appl.",
                            "n_citations": 4
                        },
                        "score": 0.88330078125
                    },
                    {
                        "id": "(Chen et al., 2021)",
                        "snippets": [
                            "The graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. (Erkan et al., 2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2019) both use discourse graphs to generate concise and informative summaries. (Li et al., 2016) and (Li et al., 2021) propose to utilize event relation graph to represent documents for MDS",
                            ".Most previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. (Erkan et al., 2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Christensen et al., 2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, (Yasunaga et al., 2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. (Li et al., 2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. (Wu et al., 2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases."
                        ],
                        "paper": {
                            "corpus_id": 239768911,
                            "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
                            "authors": [
                                {
                                    "authorId": "2108267116",
                                    "name": "Moye Chen"
                                },
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 22
                        },
                        "score": 0.9423828125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.34866600000000003
    }
}