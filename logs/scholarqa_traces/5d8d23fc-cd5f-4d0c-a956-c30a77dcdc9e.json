{
    "query": "desirable property LMs they well calibrated LM generate confidence score is to how likely decision is correct recent works seek properly LM confidence judgments",
    "user_id": "lib_user",
    "task_id": "5d8d23fc-cd5f-4d0c-a956-c30a77dcdc9e",
    "timestamp": "2025-06-24T00:42:20.286169",
    "n_retrieval": 256,
    "n_retrieved": 260,
    "n_candidates": 8,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.122241,
    "decomposed_query": {
        "rewritten_query": "Calibration of language models and their confidence scores indicating how likely decisions are correct.",
        "keyword_query": "language models calibration confidence scores decisions",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009681,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 30,
            "citation_count": 355,
            "influential_citation_count": 57,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.14975",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218407338",
                    "name": "Katherine Tian"
                },
                {
                    "authorId": "49688913",
                    "name": "E. Mitchell"
                },
                {
                    "authorId": "2064472884",
                    "name": "Allan Zhou"
                },
                {
                    "authorId": "50465276",
                    "name": "Archit Sharma"
                },
                {
                    "authorId": "102801230",
                    "name": "Rafael Rafailov"
                },
                {
                    "authorId": "18307037",
                    "name": "Huaxiu Yao"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                },
                {
                    "authorId": "144783904",
                    "name": "Christopher D. Manning"
                }
            ],
            "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "corpus_id": 258865733,
            "sentences": [
                {
                    "corpus_id": "258865733",
                    "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
                    "text": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
                    "score": 0.5637147125241657,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74951171875
                }
            ],
            "relevance_judgement": 0.74951171875,
            "relevance_judgment_input_expanded": "# Title: Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Katherine Tian, E. Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D. Manning\n## Abstract\nA trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.\n",
            "reference_string": "[258865733 | Tian et al. | 2023 | Citations: 355]"
        },
        {
            "title": "Few-Shot Recalibration of Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.18286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293910776",
                    "name": "Xiang Lisa Li"
                },
                {
                    "authorId": "3030219",
                    "name": "Urvashi Khandelwal"
                },
                {
                    "authorId": "2091768",
                    "name": "Kelvin Guu"
                }
            ],
            "abstract": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
            "corpus_id": 268723623,
            "sentences": [
                {
                    "corpus_id": "268723623",
                    "title": "Few-Shot Recalibration of Language Models",
                    "text": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
                    "score": 0.5731340342039501,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68994140625
                },
                {
                    "corpus_id": "268723623",
                    "title": "Few-Shot Recalibration of Language Models",
                    "text": "Knowing when to trust a model's predictions is typically mapped to the concept of calibration, where the model's confidence estimate for a prediction reflects how likely it is to be correct.Language models (LMs) have recently been shown to be wellcalibrated in a number of settings (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023;Ope-nAI, 2023).However, while models can be wellcalibrated for aggregate distributions (e.g.mixtures of a number of domains), they can be significantly miscalibrated for narrower domains within that distribution (Yu et al., 2022;Hebert-Johnson et al., 2018).\n\nFor instance, Figure 1 shows an LM that is wellcalibrated on the combined distribution of five domains, achieving near perfect calibration curve with low expected calibration error (ECE).However, curves for the individual domains appear significantly miscalibrated in comparison, with the least calibrated domain virology having a 250% higher calibration error.This miscalibration problem is hidden for the combined distribution because overconfidence in some domains cancels out underconfidence in others.\n\nThis illustrates a key problem: LMs are not well-calibrated for meaningful slices of broader distributions.This is particularly relevant in practice where users querying an LM rarely sample from a broad combination of distributions at any given time, and are more likely to sample from slices like abstract algebra or virology.Our goal is to recalibrate LMs for each of these fine-grained slices of a distribution, thereby allowing users to reliably determine when predictions can be trusted.\n\nTo recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice.",
                    "score": 0.5184721956883453,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 190,
                            "end": 356
                        },
                        {
                            "start": 356,
                            "end": 433
                        },
                        {
                            "start": 433,
                            "end": 599
                        },
                        {
                            "start": 601,
                            "end": 788
                        },
                        {
                            "start": 788,
                            "end": 962
                        },
                        {
                            "start": 962,
                            "end": 1107
                        },
                        {
                            "start": 1109,
                            "end": 1216
                        },
                        {
                            "start": 1216,
                            "end": 1436
                        },
                        {
                            "start": 1436,
                            "end": 1601
                        },
                        {
                            "start": 1603,
                            "end": 1832
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 305,
                            "end": 323,
                            "matchedPaperCorpusId": "247613322"
                        },
                        {
                            "start": 323,
                            "end": 341,
                            "matchedPaperCorpusId": "257039062"
                        },
                        {
                            "start": 570,
                            "end": 598,
                            "matchedPaperCorpusId": "51880858"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.43359375
                }
            ],
            "relevance_judgement": 0.68994140625,
            "relevance_judgment_input_expanded": "# Title: Few-Shot Recalibration of Language Models\n# Venue: arXiv.org\n# Authors: Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu\n## Abstract\nRecent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n## Introduction\nKnowing when to trust a model's predictions is typically mapped to the concept of calibration, where the model's confidence estimate for a prediction reflects how likely it is to be correct.Language models (LMs) have recently been shown to be wellcalibrated in a number of settings (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023;Ope-nAI, 2023).However, while models can be wellcalibrated for aggregate distributions (e.g.mixtures of a number of domains), they can be significantly miscalibrated for narrower domains within that distribution (Yu et al., 2022;Hebert-Johnson et al., 2018).\n\nFor instance, Figure 1 shows an LM that is wellcalibrated on the combined distribution of five domains, achieving near perfect calibration curve with low expected calibration error (ECE).However, curves for the individual domains appear significantly miscalibrated in comparison, with the least calibrated domain virology having a 250% higher calibration error.This miscalibration problem is hidden for the combined distribution because overconfidence in some domains cancels out underconfidence in others.\n\nThis illustrates a key problem: LMs are not well-calibrated for meaningful slices of broader distributions.This is particularly relevant in practice where users querying an LM rarely sample from a broad combination of distributions at any given time, and are more likely to sample from slices like abstract algebra or virology.Our goal is to recalibrate LMs for each of these fine-grained slices of a distribution, thereby allowing users to reliably determine when predictions can be trusted.\n\nTo recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice.",
            "reference_string": "[268723623 | Li et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 17,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.19817, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2285068366",
                    "name": "Johnathan Xie"
                },
                {
                    "authorId": "2111073657",
                    "name": "Annie S. Chen"
                },
                {
                    "authorId": "2260289273",
                    "name": "Yoonho Lee"
                },
                {
                    "authorId": "2260106796",
                    "name": "Eric Mitchell"
                },
                {
                    "authorId": "2279838067",
                    "name": "Chelsea Finn"
                }
            ],
            "abstract": "The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration\u2014how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly. In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.",
            "corpus_id": 272987064,
            "sentences": [
                {
                    "corpus_id": "272987064",
                    "title": "Calibrating Language Models with Adaptive Temperature Scaling",
                    "text": "Large language models (LLMs) have become a cornerstone of modern artificial intelligence, offering impressive capabilities in natural language processing tasks. However, the reliability of LLMs is intertwined with their ability to generate confidence scores that accurately reflect the likelihood of their outputs being correct. This calibration, aligning a model's confidence with its accuracy, is essential, especially when LLMs are deployed in real-world scenarios where decisions based on incorrect outputs can have significant consequences. \n\nWhile unsupervised pre-training methods have shown success in producing well-calibrated LLMs, * Equal contribution. a challenge arises when these models undergo finetuning through reinforcement learning from human feedback (RLHF). While RLHF fine-tuning is effective in enhancing model performance on specific tasks and aligning outputs with human preferences, recent studies indicate a notable degradation in the calibration of LLMs post-RLHF finetuning (Achiam et al., 2023;Tian et al., 2023;Kadavath et al., 2022). This degradation compromises the model's ability to provide reliable confidence scores, an issue that becomes critical when these models are applied to tasks requiring high levels of trust and accuracy. An important question arises: how can we maintain the performance gains achieved through RLHF fine-tuning while ensuring that the model's confidence scores remain reliable? \n\nTo address this challenge, our work introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration technique that predicts a temperature scaling parameter for each token prediction based on a language model's hidden features. Basic temperature scaling is a widely-used calibration method that applies a single temperature parameter across all outputs of a model. This technique, while effective in some contexts, assumes uniform calibration needs across all inputs, which is often not the case for complex models like LLMs. ATS, in contrast, predicts a unique temperature scaling parameter for each set of token predictions. This input-specific approach allows ATS to refine the calibration process, addressing the varying degrees of calibration shift that can occur after RLHF fine-tuning.",
                    "score": 0.5357911581530642,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 545
                        },
                        {
                            "start": 548,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 1065
                        },
                        {
                            "start": 1066,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1441
                        },
                        {
                            "start": 1444,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2075
                        },
                        {
                            "start": 2076,
                            "end": 2241
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1024,
                            "end": 1042,
                            "matchedPaperCorpusId": "258865733"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.685546875
                }
            ],
            "relevance_judgement": 0.685546875,
            "relevance_judgment_input_expanded": "# Title: Calibrating Language Models with Adaptive Temperature Scaling\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Johnathan Xie, Annie S. Chen, Yoonho Lee, Eric Mitchell, Chelsea Finn\n## Abstract\nThe effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration\u2014how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly. In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.\n## Introduction\nLarge language models (LLMs) have become a cornerstone of modern artificial intelligence, offering impressive capabilities in natural language processing tasks. However, the reliability of LLMs is intertwined with their ability to generate confidence scores that accurately reflect the likelihood of their outputs being correct. This calibration, aligning a model's confidence with its accuracy, is essential, especially when LLMs are deployed in real-world scenarios where decisions based on incorrect outputs can have significant consequences. \n\nWhile unsupervised pre-training methods have shown success in producing well-calibrated LLMs, * Equal contribution. a challenge arises when these models undergo finetuning through reinforcement learning from human feedback (RLHF). While RLHF fine-tuning is effective in enhancing model performance on specific tasks and aligning outputs with human preferences, recent studies indicate a notable degradation in the calibration of LLMs post-RLHF finetuning (Achiam et al., 2023;Tian et al., 2023;Kadavath et al., 2022). This degradation compromises the model's ability to provide reliable confidence scores, an issue that becomes critical when these models are applied to tasks requiring high levels of trust and accuracy. An important question arises: how can we maintain the performance gains achieved through RLHF fine-tuning while ensuring that the model's confidence scores remain reliable? \n\nTo address this challenge, our work introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration technique that predicts a temperature scaling parameter for each token prediction based on a language model's hidden features. Basic temperature scaling is a widely-used calibration method that applies a single temperature parameter across all outputs of a model. This technique, while effective in some contexts, assumes uniform calibration needs across all inputs, which is often not the case for complex models like LLMs. ATS, in contrast, predicts a unique temperature scaling parameter for each set of token predictions. This input-specific approach allows ATS to refine the calibration process, addressing the varying degrees of calibration shift that can occur after RLHF fine-tuning.",
            "reference_string": "[272987064 | Xie et al. | 2024 | Citations: 17]"
        },
        {
            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2120099874",
                    "name": "Xin Liu"
                },
                {
                    "authorId": "2131675694",
                    "name": "Farima Fatahi Bayat"
                },
                {
                    "authorId": "2299208178",
                    "name": "Lu Wang"
                }
            ],
            "abstract": "Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs\u2019 reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals. Moreover, simply filtering out low-confidence responses reduces the LM\u2019s helpfulness when the answers are correct. Therefore, effectively using calibration techniques to enhance an LM\u2019s factuality remains an unsolved challenge. In this paper, we first propose an activation-based calibration method, ActCab, which trains a linear layer on top of the LM\u2019s last-layer activations that can better capture the representations of knowledge. Built on top of ActCab, we further propose CoDec, a confidence-guided decoding strategy to elicit truthful answers with high confidence from LMs. By evaluating on five popular QA benchmarks, ActCab achieves superior calibration performance than all competitive baselines, e.g., by reducing the average expected calibration error (ECE) score by up to 39%. Further experiments on CoDec show consistent improvements in several LMs\u2019 factuality on challenging QA datasets, such as TruthfulQA, highlighting the value of confidence signals in enhancing the factuality.",
            "corpus_id": 270620078,
            "sentences": [
                {
                    "corpus_id": "270620078",
                    "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
                    "text": "Despite their impressive language understanding and generation capabilities, language models (LMs) still produce hallucinated content (Zhang et al., 2023), undermining their trustworthiness.One mitigation is to calibrate LMs' confidence in their outputs to align with the actual likelihood of their responses being correct (Jiang et al., 2021;Liu et al., 2024).Well-calibrated confidence scores allow users to discern the reliability of LMs' responses, enabling them not only to determine whether to trust the model's outputs but also to decide when further verification is needed.\n\nPopular LM calibration methods include training-free, inference-only approaches through verbalization (Tian et al., 2023a) or selfconsistency measurements (Wang et al., 2023) and training-based methods, such as tuning temperature parameters (Liang et al., 2018) or learning uncertainty estimations from LMs' logits (Liu et al., 2024).Despite their potential compatibility with many LMs, training-free methods are limited by the models' instruction-following capabilities and can be computationally expensive during inference.\n\nTraining-based methods, on the other hand, directly learn to output model's uncertainty.For instance, Kuhn et al. (2023) leverages the token logits predicted by the model to estimate the confidence of the entire response.Yet model logits may not capture knowledge representations and can be sensitive to tokenization procedures.Moreover, model confidence has been naively used by setting a threshold to filter out low-probability responses (Ren et al., 2023a;Zablotskaia et al., 2023), leaving these queries unanswered and reducing model's helpfulness.Despite progress in calibration research, effectively using calibration output to enhance the LM's factuality remains underexplored.\n\nIn this work, we make two primary contributions: (i) a lightweight and effective LM calibration technique, and (ii) a decoding strategy that uses calibrated confidence to elicit correct responses from LMs. Concretely, we propose an Activationbased Calibration method, ACTCAB, that estimates model's uncertainty from its internal activations.",
                    "score": 0.677124724370582,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 190,
                            "end": 361
                        },
                        {
                            "start": 361,
                            "end": 581
                        },
                        {
                            "start": 583,
                            "end": 917
                        },
                        {
                            "start": 917,
                            "end": 1108
                        },
                        {
                            "start": 1110,
                            "end": 1198
                        },
                        {
                            "start": 1198,
                            "end": 1331
                        },
                        {
                            "start": 1331,
                            "end": 1438
                        },
                        {
                            "start": 1438,
                            "end": 1662
                        },
                        {
                            "start": 1662,
                            "end": 1794
                        },
                        {
                            "start": 1796,
                            "end": 2137
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 323,
                            "end": 343,
                            "matchedPaperCorpusId": "235078802"
                        },
                        {
                            "start": 343,
                            "end": 360,
                            "matchedPaperCorpusId": "264811211"
                        },
                        {
                            "start": 738,
                            "end": 757,
                            "matchedPaperCorpusId": "247595263"
                        },
                        {
                            "start": 824,
                            "end": 844,
                            "matchedPaperCorpusId": "3526391"
                        },
                        {
                            "start": 898,
                            "end": 916,
                            "matchedPaperCorpusId": "264811211"
                        },
                        {
                            "start": 1212,
                            "end": 1230,
                            "matchedPaperCorpusId": "257039062"
                        },
                        {
                            "start": 1550,
                            "end": 1569,
                            "matchedPaperCorpusId": "252668614"
                        },
                        {
                            "start": 1569,
                            "end": 1594,
                            "matchedPaperCorpusId": "258187125"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6728515625
                },
                {
                    "corpus_id": "270620078",
                    "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
                    "text": "Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs\u2019 reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals. Moreover, simply filtering out low-confidence responses reduces the LM\u2019s helpfulness when the answers are correct. Therefore, effectively using calibration techniques to enhance an LM\u2019s factuality remains an unsolved challenge. In this paper, we first propose an activation-based calibration method, ActCab, which trains a linear layer on top of the LM\u2019s last-layer activations that can better capture the representations of knowledge. Built on top of ActCab, we further propose CoDec, a confidence-guided decoding strategy to elicit truthful answers with high confidence from LMs. By evaluating on five popular QA benchmarks, ActCab achieves superior calibration performance than all competitive baselines, e.g., by reducing the average expected calibration error (ECE) score by up to 39%. Further experiments on CoDec show consistent improvements in several LMs\u2019 factuality on challenging QA datasets, such as TruthfulQA, highlighting the value of confidence signals in enhancing the factuality.",
                    "score": 0.5657526691717374,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59521484375
                },
                {
                    "corpus_id": "270620078",
                    "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
                    "text": "Popular language model (LM) calibration methods can be categorized into three main types: verbalization-based, self-consistency-based, and logit-based approaches.Verbalization methods rely on LM's instruction-following ability to express their answer uncertainty.For example, Tian et al. (2023b) prompts the LM to provide a probability between 0.0 and 1.0 to indicate the correctness of its response.Self-consistency-based methods operate on the intuition that a model is likely to generate consistent content when it is confident about a question (Wang et al., 2023).Therefore, multiple responses are sampled and the confidence is estimated after grouping responses according to their semantic similarity (Kuhn et al., 2023;Xiong et al., 2023).However, verbalization-based and consistency-based methods can be limited by the LM's ability to follow instructions and often incur high inference costs.Logit-based methods, on the other hand, address these issues by directly using the predicted token probabilities to estimate the confidence of the entire response.For instance, both Si et al. (2023) and Liu et al. (2024) compute the response uncertainty by using the geometric mean of the token probability sequence.Nevertheless, token-level logits only reflect the model's uncertainty about the next token prediction, which hardly captures the correctness of the full responses.Different from all the prior work, ACTCAB directly estimates the answer uncertainty using LM's internal activations, achieving more accurate confidence scores and greater inference efficiency.",
                    "score": 0.5438314988282411,
                    "section_title": "Language Model Calibration",
                    "char_start_offset": 5434,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 162,
                            "end": 263
                        },
                        {
                            "start": 263,
                            "end": 400
                        },
                        {
                            "start": 400,
                            "end": 568
                        },
                        {
                            "start": 568,
                            "end": 745
                        },
                        {
                            "start": 745,
                            "end": 899
                        },
                        {
                            "start": 899,
                            "end": 1062
                        },
                        {
                            "start": 1062,
                            "end": 1215
                        },
                        {
                            "start": 1215,
                            "end": 1378
                        },
                        {
                            "start": 1378,
                            "end": 1570
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 276,
                            "end": 295,
                            "matchedPaperCorpusId": "258865733"
                        },
                        {
                            "start": 548,
                            "end": 567,
                            "matchedPaperCorpusId": "247595263"
                        },
                        {
                            "start": 706,
                            "end": 725,
                            "matchedPaperCorpusId": "257039062"
                        },
                        {
                            "start": 1081,
                            "end": 1097,
                            "matchedPaperCorpusId": "252917981"
                        },
                        {
                            "start": 1102,
                            "end": 1119,
                            "matchedPaperCorpusId": "264811211"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.384521484375
                },
                {
                    "corpus_id": "270620078",
                    "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
                    "text": "We introduced ACTCAB, a calibration method for language models that leverages internal activations to improve the alignment of model confidence with answer correctness.ACTCAB outperforms existing logit-based and consistency-based methods, achieving a 39% reduction in the ECE on five QA benchmarks.Additionally, we developed CODEC, a confidence-guided decoding strategy that enhances the factual accuracy of model responses by utilizing the calibrated confidence scores from ACTCAB.Our empirical results demonstrate that CODEC significantly improves LM factuality on challenging datasets like TruthfulQA.",
                    "score": 0.5251737753227087,
                    "section_title": "Conclusion",
                    "char_start_offset": 23826,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 168,
                            "end": 298
                        },
                        {
                            "start": 298,
                            "end": 482
                        },
                        {
                            "start": 482,
                            "end": 604
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.338134765625
                }
            ],
            "relevance_judgement": 0.6728515625,
            "relevance_judgment_input_expanded": "# Title: Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xin Liu, Farima Fatahi Bayat, Lu Wang\n## Abstract\nCalibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs\u2019 reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals. Moreover, simply filtering out low-confidence responses reduces the LM\u2019s helpfulness when the answers are correct. Therefore, effectively using calibration techniques to enhance an LM\u2019s factuality remains an unsolved challenge. In this paper, we first propose an activation-based calibration method, ActCab, which trains a linear layer on top of the LM\u2019s last-layer activations that can better capture the representations of knowledge. Built on top of ActCab, we further propose CoDec, a confidence-guided decoding strategy to elicit truthful answers with high confidence from LMs. By evaluating on five popular QA benchmarks, ActCab achieves superior calibration performance than all competitive baselines, e.g., by reducing the average expected calibration error (ECE) score by up to 39%. Further experiments on CoDec show consistent improvements in several LMs\u2019 factuality on challenging QA datasets, such as TruthfulQA, highlighting the value of confidence signals in enhancing the factuality.\n## Introduction\nDespite their impressive language understanding and generation capabilities, language models (LMs) still produce hallucinated content (Zhang et al., 2023), undermining their trustworthiness.One mitigation is to calibrate LMs' confidence in their outputs to align with the actual likelihood of their responses being correct (Jiang et al., 2021;Liu et al., 2024).Well-calibrated confidence scores allow users to discern the reliability of LMs' responses, enabling them not only to determine whether to trust the model's outputs but also to decide when further verification is needed.\n\nPopular LM calibration methods include training-free, inference-only approaches through verbalization (Tian et al., 2023a) or selfconsistency measurements (Wang et al., 2023) and training-based methods, such as tuning temperature parameters (Liang et al., 2018) or learning uncertainty estimations from LMs' logits (Liu et al., 2024).Despite their potential compatibility with many LMs, training-free methods are limited by the models' instruction-following capabilities and can be computationally expensive during inference.\n\nTraining-based methods, on the other hand, directly learn to output model's uncertainty.For instance, Kuhn et al. (2023) leverages the token logits predicted by the model to estimate the confidence of the entire response.Yet model logits may not capture knowledge representations and can be sensitive to tokenization procedures.Moreover, model confidence has been naively used by setting a threshold to filter out low-probability responses (Ren et al., 2023a;Zablotskaia et al., 2023), leaving these queries unanswered and reducing model's helpfulness.Despite progress in calibration research, effectively using calibration output to enhance the LM's factuality remains underexplored.\n\nIn this work, we make two primary contributions: (i) a lightweight and effective LM calibration technique, and (ii) a decoding strategy that uses calibrated confidence to elicit correct responses from LMs. Concretely, we propose an Activationbased Calibration method, ACTCAB, that estimates model's uncertainty from its internal activations.\n\n## Language Model Calibration\nPopular language model (LM) calibration methods can be categorized into three main types: verbalization-based, self-consistency-based, and logit-based approaches.Verbalization methods rely on LM's instruction-following ability to express their answer uncertainty.For example, Tian et al. (2023b) prompts the LM to provide a probability between 0.0 and 1.0 to indicate the correctness of its response.Self-consistency-based methods operate on the intuition that a model is likely to generate consistent content when it is confident about a question (Wang et al., 2023).Therefore, multiple responses are sampled and the confidence is estimated after grouping responses according to their semantic similarity (Kuhn et al., 2023;Xiong et al., 2023).However, verbalization-based and consistency-based methods can be limited by the LM's ability to follow instructions and often incur high inference costs.Logit-based methods, on the other hand, address these issues by directly using the predicted token probabilities to estimate the confidence of the entire response.For instance, both Si et al. (2023) and Liu et al. (2024) compute the response uncertainty by using the geometric mean of the token probability sequence.Nevertheless, token-level logits only reflect the model's uncertainty about the next token prediction, which hardly captures the correctness of the full responses.Different from all the prior work, ACTCAB directly estimates the answer uncertainty using LM's internal activations, achieving more accurate confidence scores and greater inference efficiency.\n\n## Conclusion\nWe introduced ACTCAB, a calibration method for language models that leverages internal activations to improve the alignment of model confidence with answer correctness.ACTCAB outperforms existing logit-based and consistency-based methods, achieving a 39% reduction in the ECE on five QA benchmarks.Additionally, we developed CODEC, a confidence-guided decoding strategy that enhances the factual accuracy of model responses by utilizing the calibrated confidence scores from ACTCAB.Our empirical results demonstrate that CODEC significantly improves LM factuality on challenging datasets like TruthfulQA.",
            "reference_string": "[270620078 | Liu et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.02655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254324716",
                    "name": "Mozhi Zhang"
                },
                {
                    "authorId": "2255220422",
                    "name": "Mianqiu Huang"
                },
                {
                    "authorId": "2294723713",
                    "name": "Rundong Shi"
                },
                {
                    "authorId": "2295100870",
                    "name": "Linsen Guo"
                },
                {
                    "authorId": "2294823462",
                    "name": "Chong Peng"
                },
                {
                    "authorId": "2296600123",
                    "name": "Peng Yan"
                },
                {
                    "authorId": "2278545938",
                    "name": "Yaqian Zhou"
                },
                {
                    "authorId": "2294872225",
                    "name": "Xipeng Qiu"
                }
            ],
            "abstract": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Then, we propose a plug-and-play method, UF Calibration, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on Truly Well-Calibrated Confidence for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.",
            "corpus_id": 268876453,
            "sentences": [
                {
                    "corpus_id": "268876453",
                    "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
                    "text": "In this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Through the decomposition, we propose a plug-and-play method, UF CALIBRA-TION, to calibrate the confidence of language models. Through experiments with 6 RLHF-LMs on 4 multiple-choice question answering benchmarks, our method exhibits good calibration. Besides, we propose two novel metrics, IPR and CE, to evaluate the calibration of language models. Finally, we conduct a detailed discussion on Truly Well-Calibrated Confidence. We believe our method can serve as a strong baseline, and we hope that this work could provide some insights into the language model confidence calibration.",
                    "score": 0.5700261573357426,
                    "section_title": "Conclusion",
                    "char_start_offset": 22728,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 589
                        },
                        {
                            "start": 590,
                            "end": 746
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6376953125
                },
                {
                    "corpus_id": "268876453",
                    "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
                    "text": "As shown in Figure 1, our proposed UF Calibration achieved good calibration across different MCQA datasets. Meanwhile, UF Calibration does not require knowledge of the model's pertoken log-probability, making it broadly applicable to various Black-box RLHF-LMs, which do not provide the per-token log-probability. \n\nTo have a closer look at the calibration of model confidence, we propose two novel metrics for evaluating and observation: 1) Inverse Pair Ratio (IPR), which is the proportion of inverse pairs in the Reliability Diagram. This metric could reflect whether the model is well-calibrated from the perspective of the monotonicity of the Reliability Diagram. If the reliability diagram is monotonic, it indicates that the average accuracy of low-confidence an-swers is always lower than that of high-confidence answers. 2) As shown in Table 7, we find that as the number of model parameters increases, language models still tend to consistently express uncertainty within certain fixed ranges. Thus, we design the Confidence Evenness (CE) to observe to the uniformity of the density of each bar in the reliability diagram. Our experimental results indicate that, after calibration, even within the same dataset, there is a significant difference in the confidence of the answers provided by language models for different questions. We summarize our main contributions as follows: \n\n1) Our proposed method could be viewed as a strong baseline for eliciting model confidence, where answer set is known. And the calibrated confidence could be viewed as a soft label. \n\n2) We propose two new metrics, IPR and CE, to evaluate the calibration of LM's confidence. \n\n3) We conduct a detailed discussion of a research question: \"What kind of Confidence is Truly Well-Calibrated?\", and we hope our discussion can bring some insights to the community.",
                    "score": 0.5700261573357426,
                    "section_title": "Introduction",
                    "char_start_offset": 3582,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 313
                        },
                        {
                            "start": 316,
                            "end": 536
                        },
                        {
                            "start": 537,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 1003
                        },
                        {
                            "start": 1004,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1341
                        },
                        {
                            "start": 1342,
                            "end": 1389
                        },
                        {
                            "start": 1392,
                            "end": 1510
                        },
                        {
                            "start": 1511,
                            "end": 1573
                        },
                        {
                            "start": 1576,
                            "end": 1666
                        },
                        {
                            "start": 1669,
                            "end": 1850
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.482421875
                },
                {
                    "corpus_id": "268876453",
                    "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
                    "text": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Then, we propose a plug-and-play method, UF Calibration, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on Truly Well-Calibrated Confidence for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.",
                    "score": 0.5988920816462342,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.402099609375
                }
            ],
            "relevance_judgement": 0.6376953125,
            "relevance_judgment_input_expanded": "# Title: Calibrating the Confidence of Large Language Models by Eliciting Fidelity\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, Xipeng Qiu\n## Abstract\nLarge language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Then, we propose a plug-and-play method, UF Calibration, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on Truly Well-Calibrated Confidence for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.\n## Introduction\nAs shown in Figure 1, our proposed UF Calibration achieved good calibration across different MCQA datasets. Meanwhile, UF Calibration does not require knowledge of the model's pertoken log-probability, making it broadly applicable to various Black-box RLHF-LMs, which do not provide the per-token log-probability. \n\nTo have a closer look at the calibration of model confidence, we propose two novel metrics for evaluating and observation: 1) Inverse Pair Ratio (IPR), which is the proportion of inverse pairs in the Reliability Diagram. This metric could reflect whether the model is well-calibrated from the perspective of the monotonicity of the Reliability Diagram. If the reliability diagram is monotonic, it indicates that the average accuracy of low-confidence an-swers is always lower than that of high-confidence answers. 2) As shown in Table 7, we find that as the number of model parameters increases, language models still tend to consistently express uncertainty within certain fixed ranges. Thus, we design the Confidence Evenness (CE) to observe to the uniformity of the density of each bar in the reliability diagram. Our experimental results indicate that, after calibration, even within the same dataset, there is a significant difference in the confidence of the answers provided by language models for different questions. We summarize our main contributions as follows: \n\n1) Our proposed method could be viewed as a strong baseline for eliciting model confidence, where answer set is known. And the calibrated confidence could be viewed as a soft label. \n\n2) We propose two new metrics, IPR and CE, to evaluate the calibration of LM's confidence. \n\n3) We conduct a detailed discussion of a research question: \"What kind of Confidence is Truly Well-Calibrated?\", and we hope our discussion can bring some insights to the community.\n\n## Conclusion\nIn this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Through the decomposition, we propose a plug-and-play method, UF CALIBRA-TION, to calibrate the confidence of language models. Through experiments with 6 RLHF-LMs on 4 multiple-choice question answering benchmarks, our method exhibits good calibration. Besides, we propose two novel metrics, IPR and CE, to evaluate the calibration of language models. Finally, we conduct a detailed discussion on Truly Well-Calibrated Confidence. We believe our method can serve as a strong baseline, and we hope that this work could provide some insights into the language model confidence calibration.",
            "reference_string": "[268876453 | Zhang et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Evaluating the Generalisation of an Artificial Learner",
            "venue": "NLP4CALL",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ecp.ep.liu.se/index.php/sltc/article/download/1054/960",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.nlp4call-1.15, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51896370",
                    "name": "Bernardo Stearns"
                },
                {
                    "authorId": "20486050",
                    "name": "Nicolas Ballier"
                },
                {
                    "authorId": "2261396630",
                    "name": "Thomas Gaillat"
                },
                {
                    "authorId": "2261398341",
                    "name": "Andrew Simpkin"
                },
                {
                    "authorId": "1738599770",
                    "name": "John P. Mccrae"
                }
            ],
            "abstract": "This paper focuses on the creation of LLM-based artificial learners. Motivated by the capability of language models to encode language representation, we evaluate such models in predicting masked tokens in learner corpora. We pre-trained two learner models, one in a training set of the EFCAMDAT (natural learner model) and another in the C4200m dataset (syntehtic learner model), evaluating them against a native model using an external corpora of English for Specific purposes corpus of French undergraduates (CELVA) as test set. We measured metrics related to accuracy, consistency and divergence. While the native model performs reasonably well, the natural learner pre-trained model show improvements token in recall at k. We complement the accuracy metric showing that the native language model make \"over-confident\" mistakes where our artificial learners make mistakes where probabilities are uniform. Finally we show that the general tokens choices from the native model diverges from the natural learner model and that this divergence is higher on lower proficiency levels.",
            "corpus_id": 273396026,
            "sentences": [
                {
                    "corpus_id": "273396026",
                    "title": "Evaluating the Generalisation of an Artificial Learner",
                    "text": "To foster trustworthiness in our models, high accuracy is the immediate desired property of our models, assigning high probabilities to correct tokens. \n\nA second desirable property is that our models do not overconfidently make mistakes, assigning high probabilities to incorrect predictions. \n\nOne approach for such analysis is through the \"calibration curve\" method. Initially employed in analysing weather forecasts (Brier, 1950;DeGroot and Fienberg, 1983), this technique has since been applied to neural networks (Guo et al., 2017;Minderer et al., 2021) and recently to evaluate Large Language Models (LLMs) from a semantic perspective Levinstein and Herrmann (2024). For example, (Levinstein and Herrmann, 2024) utilizes calibration curves to assess the veracity of LLM statements on specific datasets and asserts that \"calibration offers another metric for evaluating the quality of probes' forecasts.\" Calibration analyses have been utilized in neural networks and language models (Minderer et al., 2021;Chen et al., 2024), allowing researchers to assess the relationship between a model's prediction confidence and success rate. \n\nCalibration curves help us analyze how well a model performs when it is confident or unconfident about it's prediction. In our experiment, our calibration curves correspond to how many successful predictions (event rate) we observe across different probability scores of the top-1 prediction of each model.",
                    "score": 0.8383359705135784,
                    "section_title": "Calibration Curves",
                    "char_start_offset": 14830,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 154,
                            "end": 293
                        },
                        {
                            "start": 296,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1138
                        },
                        {
                            "start": 1141,
                            "end": 1260
                        },
                        {
                            "start": 1261,
                            "end": 1447
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 420,
                            "end": 433,
                            "matchedPaperCorpusId": "122906757"
                        },
                        {
                            "start": 433,
                            "end": 460,
                            "matchedPaperCorpusId": "109884250"
                        },
                        {
                            "start": 519,
                            "end": 537,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 537,
                            "end": 559,
                            "matchedPaperCorpusId": "235435823"
                        },
                        {
                            "start": 642,
                            "end": 672,
                            "matchedPaperCorpusId": "259316868"
                        },
                        {
                            "start": 990,
                            "end": 1013,
                            "matchedPaperCorpusId": "235435823"
                        },
                        {
                            "start": 1013,
                            "end": 1031,
                            "matchedPaperCorpusId": "79240523"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.546875
                }
            ],
            "relevance_judgement": 0.546875,
            "relevance_judgment_input_expanded": "# Title: Evaluating the Generalisation of an Artificial Learner\n# Venue: NLP4CALL\n# Authors: Bernardo Stearns, Nicolas Ballier, Thomas Gaillat, Andrew Simpkin, John P. Mccrae\n## Abstract\nThis paper focuses on the creation of LLM-based artificial learners. Motivated by the capability of language models to encode language representation, we evaluate such models in predicting masked tokens in learner corpora. We pre-trained two learner models, one in a training set of the EFCAMDAT (natural learner model) and another in the C4200m dataset (syntehtic learner model), evaluating them against a native model using an external corpora of English for Specific purposes corpus of French undergraduates (CELVA) as test set. We measured metrics related to accuracy, consistency and divergence. While the native model performs reasonably well, the natural learner pre-trained model show improvements token in recall at k. We complement the accuracy metric showing that the native language model make \"over-confident\" mistakes where our artificial learners make mistakes where probabilities are uniform. Finally we show that the general tokens choices from the native model diverges from the natural learner model and that this divergence is higher on lower proficiency levels.\n## Calibration Curves\nTo foster trustworthiness in our models, high accuracy is the immediate desired property of our models, assigning high probabilities to correct tokens. \n\nA second desirable property is that our models do not overconfidently make mistakes, assigning high probabilities to incorrect predictions. \n\nOne approach for such analysis is through the \"calibration curve\" method. Initially employed in analysing weather forecasts (Brier, 1950;DeGroot and Fienberg, 1983), this technique has since been applied to neural networks (Guo et al., 2017;Minderer et al., 2021) and recently to evaluate Large Language Models (LLMs) from a semantic perspective Levinstein and Herrmann (2024). For example, (Levinstein and Herrmann, 2024) utilizes calibration curves to assess the veracity of LLM statements on specific datasets and asserts that \"calibration offers another metric for evaluating the quality of probes' forecasts.\" Calibration analyses have been utilized in neural networks and language models (Minderer et al., 2021;Chen et al., 2024), allowing researchers to assess the relationship between a model's prediction confidence and success rate. \n\nCalibration curves help us analyze how well a model performs when it is confident or unconfident about it's prediction. In our experiment, our calibration curves correspond to how many successful predictions (event rate) we observe across different probability scores of the top-1 prediction of each model.",
            "reference_string": "[273396026 | Stearns et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Confidence Estimation For LLM-Based Dialogue State Tracking",
            "venue": "Spoken Language Technology Workshop",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.09629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321493889",
                    "name": "Yi-Jyun Sun"
                },
                {
                    "authorId": "29832722",
                    "name": "Suvodip Dey"
                },
                {
                    "authorId": "2365041900",
                    "name": "Dilek Hakkani-Tur"
                },
                {
                    "authorId": "5108268",
                    "name": "Gokhan Tur"
                }
            ],
            "abstract": "Estimation of a model\u2019s confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance. In this work, we provide an exhaustive exploration of methods, including approaches proposed for open- and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty to improve the reliability of LLM-generated responses, specifically focusing on dialogue state tracking (DST) in task-oriented dialogue systems (TODS). Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models. Furthermore, we assess these methods using an open-weight model fine-tuned for the task of DST, achieving superior joint goal accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can result in enhanced AUC performance, indicating better confidence score calibration.",
            "corpus_id": 272689376,
            "sentences": [
                {
                    "corpus_id": "272689376",
                    "title": "Confidence Estimation For LLM-Based Dialogue State Tracking",
                    "text": "Estimation of a model\u2019s confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance. In this work, we provide an exhaustive exploration of methods, including approaches proposed for open- and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty to improve the reliability of LLM-generated responses, specifically focusing on dialogue state tracking (DST) in task-oriented dialogue systems (TODS). Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models. Furthermore, we assess these methods using an open-weight model fine-tuned for the task of DST, achieving superior joint goal accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can result in enhanced AUC performance, indicating better confidence score calibration.",
                    "score": 0.5160890745514479,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52294921875
                }
            ],
            "relevance_judgement": 0.52294921875,
            "relevance_judgment_input_expanded": "# Title: Confidence Estimation For LLM-Based Dialogue State Tracking\n# Venue: Spoken Language Technology Workshop\n# Authors: Yi-Jyun Sun, Suvodip Dey, Dilek Hakkani-Tur, Gokhan Tur\n## Abstract\nEstimation of a model\u2019s confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance. In this work, we provide an exhaustive exploration of methods, including approaches proposed for open- and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty to improve the reliability of LLM-generated responses, specifically focusing on dialogue state tracking (DST) in task-oriented dialogue systems (TODS). Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models. Furthermore, we assess these methods using an open-weight model fine-tuned for the task of DST, achieving superior joint goal accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can result in enhanced AUC performance, indicating better confidence score calibration.\n",
            "reference_string": "[272689376 | Sun et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Multicalibration for Confidence Scoring in LLMs",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 69,
            "citation_count": 19,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.04689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2295667267",
                    "name": "Gianluca Detommaso"
                },
                {
                    "authorId": "2295665717",
                    "name": "Martin Bertran"
                },
                {
                    "authorId": "2295664744",
                    "name": "Riccardo Fogliato"
                },
                {
                    "authorId": "2295665299",
                    "name": "Aaron Roth"
                }
            ],
            "abstract": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
            "corpus_id": 269004786,
            "sentences": [
                {
                    "corpus_id": "269004786",
                    "title": "Multicalibration for Confidence Scoring in LLMs",
                    "text": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
                    "score": 0.5782437928443434,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51318359375
                }
            ],
            "relevance_judgement": 0.51318359375,
            "relevance_judgment_input_expanded": "# Title: Multicalibration for Confidence Scoring in LLMs\n# Venue: International Conference on Machine Learning\n# Authors: Gianluca Detommaso, Martin Bertran, Riccardo Fogliato, Aaron Roth\n## Abstract\nThis paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.\n",
            "reference_string": "[269004786 | Detommaso et al. | 2024 | Citations: 19]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273396026",
            "title": "Evaluating the Generalisation of an Artificial Learner",
            "text": "To foster trustworthiness in our models, high accuracy is the immediate desired property of our models, assigning high probabilities to correct tokens. \n\nA second desirable property is that our models do not overconfidently make mistakes, assigning high probabilities to incorrect predictions. \n\nOne approach for such analysis is through the \"calibration curve\" method. Initially employed in analysing weather forecasts (Brier, 1950;DeGroot and Fienberg, 1983), this technique has since been applied to neural networks (Guo et al., 2017;Minderer et al., 2021) and recently to evaluate Large Language Models (LLMs) from a semantic perspective Levinstein and Herrmann (2024). For example, (Levinstein and Herrmann, 2024) utilizes calibration curves to assess the veracity of LLM statements on specific datasets and asserts that \"calibration offers another metric for evaluating the quality of probes' forecasts.\" Calibration analyses have been utilized in neural networks and language models (Minderer et al., 2021;Chen et al., 2024), allowing researchers to assess the relationship between a model's prediction confidence and success rate. \n\nCalibration curves help us analyze how well a model performs when it is confident or unconfident about it's prediction. In our experiment, our calibration curves correspond to how many successful predictions (event rate) we observe across different probability scores of the top-1 prediction of each model.",
            "score": 0.8383359705135784,
            "section_title": "Calibration Curves",
            "char_start_offset": 14830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 154,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1447
                }
            ],
            "ref_mentions": [
                {
                    "start": 420,
                    "end": 433,
                    "matchedPaperCorpusId": "122906757"
                },
                {
                    "start": 433,
                    "end": 460,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 519,
                    "end": 537,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 537,
                    "end": 559,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 642,
                    "end": 672,
                    "matchedPaperCorpusId": "259316868"
                },
                {
                    "start": 990,
                    "end": 1013,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 1013,
                    "end": 1031,
                    "matchedPaperCorpusId": "79240523"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.546875
        },
        {
            "corpus_id": "276422013",
            "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
            "text": "The model is instructed to think step-by-step, reasoning why it does or does not need external tools to address the user query, and finally concludes its decision with \"Yes\" or \"No.\" Before delving into the analysis, we provide some background on the concept of calibration in the context of Large Language Models (LLMs). Calibration refers to the alignment between a model's predicted probabilities and the actual likelihood of those predictions being correct. A well-calibrated model generates probability scores that accurately reflect the true probability of its predictions. \n\nIn Figure 6, we present the distribution of P(Yes) scores for both correct and incorrect Yes/No decisions. Our key observations are as follows: \n\n1. When the model is given detailed instructions and few-shot examples, it demonstrates poor calibration. \n\nAs illustrated in Figure 6(a), the distributions of P(Yes) scores for correct and incorrect decisions do not show a clear distinction. \n\n2. Conversely, when the model lacks detailed context and must rely on its internal beliefs to make decisions, it exhibits improved calibration. In Figure 6(b), the peak of the distribution for correct scores clearly deviates from that of incorrect scores. \n\n3. After fine-tuning, the model displays significantly better calibration, as shown in Figures 6(c) and (d). \n\nMost correct decisions have P(Yes) scores of either 1 (indicating \"Yes\") or 0 (indicating \"No\"), while the P(Yes) scores for incorrect decisions vary between 0 and 1.",
            "score": 0.8078100341486113,
            "section_title": "CoT (Chain of Thought):",
            "char_start_offset": 31635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1508
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371826171875
        },
        {
            "corpus_id": "273877483",
            "title": "Measuring short-form factuality in large language models",
            "text": "A factuality benchmark like SimpleQA allows us to measure the scientific phenomenon known as calibration, or whether language models \"know what they know.\" One way to measure calibration is to directly ask the language model to state its confidence in its answer using a prompt like: \"Please give your best guess, along with your confidence as a percentage that that is the correct answer\" (for exact prompt, see Appendix C). Then we can plot the correlation between the stated confidence of the model, and how accurate the model actually was. A perfectly calibrated model would have the same actual accuracy as stated confidence. For instance, on all prompts where the model stated a confidence of 75%, the accuracy would be 75% for a perfectly calibrated model. \n\nThis result is shown in Figure 2 (left). The positive correlation between stated confidence and accuracy is a reassuring sign that models have some notion of confidence. We see that o1-preview is more calibrated than o1-mini, and gpt4o is more calibrated than gpt4o-mini, which is consistent with prior work showing that larger models are more calibrated. However, the fact that performance is well below the line y = x means that models consistently overstate their confidence. Hence, there is a lot of room to improve the calibration of large language models in terms of stated confidence. \n\nAnother way to measure calibration is to ask the language model the same question 100 times (here, we use temperature 1). Since language models may produce different answers upon repeated attempts, we can assess whether frequency of an answer corresponds to its correctness. Higher frequency typically indicates that the model is more confident in its answers, as the model is giving the same answer repeatedly. A calibrated model would have the same accuracy as answer frequency. \n\nIn the Figure 2 (right), we show the calibration of language models as measured by the frequency of their responses. Here we use string match to group together different answers from the language model using the same prompt as the stated confidence figure. For each question, we only consider the most-frequent answer.",
            "score": 0.7233005475264871,
            "section_title": "Measuring calibration",
            "char_start_offset": 13425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1840
                },
                {
                    "start": 1843,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2161
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.183837890625
        },
        {
            "corpus_id": "276647848",
            "title": "ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for Large Language Models",
            "text": "The confidence level reflects the model's self-evaluation of its responses, indicating whether language models truly \"know what they know. \" [23] A perfectly calibrated model should exhibit confidence levels that precisely align with its prediction accuracy [14]. Following SimpleQA [25], we prompt the LLM to generate confidence scores (ranging 0 to 100) alongside its answers to quantify prediction certainty (see prompt details in Appendix C). We subsequently aggregate model accuracy across different confidence intervals. In this section, we evaluate the calibration capabilities of various models on category concept and brand concept, with results visualized in Figure 6. \n\nThe results demonstrate the correlation between the stated confidence of the model, and how accurate the model actually was. Notably, o1-preview exhibits the best alignment performance, followed by o1-mini. Within the Qwen2.5 series, the calibration hierarchy emerges as Qwen2.5-MAX > Qwen2.5-72B > Qwen2.5-14B > Qwen2.5-7B > Qwen2.5-3B, suggesting that larger model scales correlate with improved calibration. However, most models consistently fall below the perfect alignment line, indicating a prevalent tendency towards overconfidence in predictions. This highlights significant room for improving large language model calibration to mitigate overconfident generation of erroneous responses.",
            "score": 0.7231304622825983,
            "section_title": "Analysis of Calibration.",
            "char_start_offset": 14856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1376
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "267211649"
                },
                {
                    "start": 258,
                    "end": 262,
                    "matchedPaperCorpusId": "253384402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1708984375
        },
        {
            "corpus_id": "272703963",
            "title": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty",
            "text": "We compute the accuracy (Acc) of a prediction by language model evaluation (LME) whereby a prompted language model compares the prediction with the ground truth answer (see Prompt 4). To measure calibration, we plot calibration charts between confidence scores and accuracy (Figure 4) where the x-axis is binned according to the probability ranges in the linguistic expressions map (Fagen-Ulmschneider, 2019). Further, to summarize the calibration error into a single scalar, we track,  The lower these scores the better. \n\nOnce the models are finetuned to generate linguistic expressions of uncertainty, we test the calibration of their predictions on held-out test sets for each dataset. For this, we first extract the uncertainty expression from the rest of the prediction using a prompted language model (see Prompt 3). We then convert these uncertainty expressions into probability estimates using the same mapping employed for converting probabilities into linguistic expressions in the previous section. We measure calibration using Expected Calibration Error (ECE) and Brier Score and plot calibration charts based on these probability estimates. The complete evaluation procedure is outlined in Figure 3 and Algorithm 2.  Confidence Bin) and y-axis is probability of that prediction being actually correct (shown here as Accuracy). Expected Calibration Error (ECE) and Brier Score are reported at the top of each plot. \n\nThe error bars show the variance of accuracy in each bin.",
            "score": 0.7077534257648245,
            "section_title": "Evaluation",
            "char_start_offset": 12783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1487
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263671875
        },
        {
            "corpus_id": "276408950",
            "title": "Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models",
            "text": "Large Language Models (LLMs) have transformed natural language understanding, achieving stateof-the-art performance across diverse applications, from conversational AI (Skjuve et al., 2024;Zhang, 2024) to scientific discovery (Kumar, 2024). Their versatility has also facilitated advancements in multimodal learning, where language models integrate with vision-based systems to enhance tasks such as automated content generation and decision support (Zhang et al., 2023;Chhikara et al., 2024). However, as LLMs are increasingly deployed in high-stakes domains such as healthcare, finance, and law, critical concerns arise regarding calibration-the alignment between model confidence and actual correctness (Dhuliawala et al., 2023). When LLMs express unwarranted confidence in incorrect predictions, they risk misleading users, spreading misinformation, and reducing trust and reliability in AI-driven systems (Chen et al., 2023;Zhang et al., 2024). Figure 1 illustrates this issue: when asked \"Who received the IEEE Frank Rosenblatt Award in 2010?\", an LLM incorrectly responds with \"Geoffrey Hinton\", assigning a 93% confidence score, despite correct answer being \"Michio Sugeno\". This overconfidence is especially problematic because users tend to equate high-confidence outputs with reliability (Liu et al., 2024). The right-hand side of Figure 1 further demonstrates the broader miscalibration problem: while a well-calibrated model should align confidence with accuracy, real-world LLMs frequently overestimate their correctness. \n\nAlthough calibration errors in neural networks are well-documented (Guo et al., 2017), the calibration behavior of LLMs remains poorly understood, particularly in response to fine-tuning techniques 1 arXiv:2502.11028v1 [cs.CL] 16 Feb 2025 such as Reinforcement Learning from Human Feedback (RLHF) (Leng et al., 2024;Li et al., 2024).",
            "score": 0.7037169542479839,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 201,
                    "matchedPaperCorpusId": "274023997"
                },
                {
                    "start": 226,
                    "end": 239,
                    "matchedPaperCorpusId": "271961846"
                },
                {
                    "start": 450,
                    "end": 470,
                    "matchedPaperCorpusId": "268066116"
                },
                {
                    "start": 470,
                    "end": 492,
                    "matchedPaperCorpusId": "261243300"
                },
                {
                    "start": 706,
                    "end": 731,
                    "matchedPaperCorpusId": "264405995"
                },
                {
                    "start": 910,
                    "end": 929,
                    "matchedPaperCorpusId": "253244504"
                },
                {
                    "start": 929,
                    "end": 948,
                    "matchedPaperCorpusId": "274023997"
                },
                {
                    "start": 1299,
                    "end": 1317,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 1605,
                    "end": 1623,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08929443359375
        },
        {
            "corpus_id": "271329113",
            "title": "Evaluating language models as risk scores",
            "text": "Fueled by the success of large language models (LLMs), it is increasingly tempting for practitioners to use such models for risk assessment and decision making in consequential domains [1][2][3][4]. Given the CV of a job applicant, for example, some might prompt a model, what are the chances that the employee will perform well on the job? The true answer is likely uncertain. Some applicants of the same features will do well, others won't. A good statistical model should faithfully reflect such outcome uncertainty. \n\nCalibration is perhaps the most basic kind of uncertainty quantification to ask for. A calibrated model, on average, reflects the true frequency of outcomes in a population. Calibrated models must therefore give at least some indication of uncertainty. Fundamental to statistical practice across the board, calibration has also been a central component in the debate around the ethics and fairness of consequential risk scoring in recent years [5][6][7][8]. \n\nThe evaluation of LLMs to date, however, has predominantly focused on accuracy metrics, often in realizable tasks where there is a unique correct label for each data point. Such benchmarks necessarily cannot speak to the use of language models as risk score estimators. A model can have high utility in well-defined question-answering tasks while being wildly miscalibrated. In fact, while accuracy corresponds to knowledge over the expected answer, proper uncertainty quantification corresponds to knowledge over the variance over answers.",
            "score": 0.6975471226927464,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1522
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 191,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 191,
                    "end": 194,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "1443041"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "260897618"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06549072265625
        },
        {
            "corpus_id": "273186420",
            "title": "Calibrating Expressions of Certainty",
            "text": "Appendix B.6 provides more details on prompts used. defining a fixed set of phrases for the model to choose from yields better calibration than generating confidence distributions on-the-fly. Allowing the model to propose its own certainty phrases and rely on its perception of these phrases proves more effective than using those provided by humans. \n\nAppendix B.3 provides additional details. Table 2 provides visualization of the probability density functions of the confidence distributions studied. \n\nMeasuring Language Models' Calibration Figure 4 illustrates the calibration performance of various LMs verbalizing confidence by selecting from a set of certainty phrases generated by prompting gpt-4o. Table 5 (row 5) provides a detailed view on the certainty phrases used. Current state-of-the-art language models exhibit good calibration on the SciQ dataset but perform less well on the TruthfulQA dataset. Larger models, such as gpt-4o, consistently outperform their smaller counterparts, like gpt-4o-mini, in terms of calibration. Models from the same family (e.g., gpt-4o and gpt-4o-mini) exhibit more similar characteristics compared to models from different vendors. \n\nFor instance, gpt-4o tends to be more confident than claude-3.5-sonnet, even though both models achieve similar calibration errors. \n\nAppendix C.3 demonstrates that our method yields a smooth calibration curves that enables clearer differentiation between model calibration profiles and better correspondence to values of ECE. Unlike binned calibration curves, which are sensitive to bin size and harder to interpret, our approach produces stable, consistent curves robust to binning variations. \n\nPost-hoc Calibration of Language Models We compare our optimal transport calibration method with two classic calibration methods: Platt scaling (Platt, 2000) and histogram binning (Zadrozny & Elkan, 2001). These baseline methods are evaluated in two scenarios: (1) verbalized probability and (2) verbalized certainty phrases. For the second scenario, since the baseline methods only work with scalar confidence scores, we mapped the verbalized phrases to the mean of their corresponding confidence distributions.",
            "score": 0.6883968611022787,
            "section_title": "Models",
            "char_start_offset": 22337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 1824,
                    "end": 1837,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1860,
                    "end": 1884,
                    "matchedPaperCorpusId": "9594071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.164306640625
        },
        {
            "corpus_id": "253244504",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "text": "Pre-trained language models (PLMs) are successful in many downstream tasks regarding performance (Wang et al., 2019). But for a reliable deployment in practice, the calibration performance should also be carefully examined (Vaicenavicius et al., 2019). Well-calibrated models assign appropriate confidence scores that truly reflect the outcome probability of their predictions. However, the confidence scores of existing deep neural networks cannot serve as reliable estimates of their uncertainty (Guo et al., 2017), and a deep understanding of PLMs calibration is lacking.\n\nIn this paper, we give a systematical analysis of PLMs calibration. We consider two questions about PLMs calibration: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? We first introduce the metrics we adopt for calibration performance evaluation. The most widely used calibration metric ECE (Naeini et al., 2015) is considered. It measures the difference between confidence and accuracy by portioning samples into various confidence zones. To give a more comprehensive and practical calibration evaluation, we provide an application-driven perspective, describing two undesirable situations in practice: (1) For the first question, we consider the influence of six factors that have influence on PLMs' calibration performance, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. Some of them are overlooked in previous empirical studies (Snoek et al., 2019;Nixon et al., 2019;Minderer et al., 2021). We motivate to conduct fine-grained control experiments to study the dynamic change in PLMs' calibration performance in training through manipulating controlling variables. We empirically observe a consistent change in calibration performance across six factors. All six factors influence PLMs' fitness on the training distribution. This results in two states of PLMs considering calibration performance, namely under-fitted and over-fitted states (see Fig. 1). In the under-fitted state, PLMs' performance and confidence increase at different speeds when more fitted on the training distribution. In the over-fitting state, PLMs' confidence continues to",
            "score": 0.6851183328602748,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 116,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 223,
                    "end": 251,
                    "matchedPaperCorpusId": "67749814"
                },
                {
                    "start": 498,
                    "end": 516,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1566,
                    "end": 1586,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1586,
                    "end": 1605,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1605,
                    "end": 1627,
                    "matchedPaperCorpusId": "235435823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31005859375
        },
        {
            "corpus_id": "270620078",
            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
            "text": "Despite their impressive language understanding and generation capabilities, language models (LMs) still produce hallucinated content (Zhang et al., 2023), undermining their trustworthiness.One mitigation is to calibrate LMs' confidence in their outputs to align with the actual likelihood of their responses being correct (Jiang et al., 2021;Liu et al., 2024).Well-calibrated confidence scores allow users to discern the reliability of LMs' responses, enabling them not only to determine whether to trust the model's outputs but also to decide when further verification is needed.\n\nPopular LM calibration methods include training-free, inference-only approaches through verbalization (Tian et al., 2023a) or selfconsistency measurements (Wang et al., 2023) and training-based methods, such as tuning temperature parameters (Liang et al., 2018) or learning uncertainty estimations from LMs' logits (Liu et al., 2024).Despite their potential compatibility with many LMs, training-free methods are limited by the models' instruction-following capabilities and can be computationally expensive during inference.\n\nTraining-based methods, on the other hand, directly learn to output model's uncertainty.For instance, Kuhn et al. (2023) leverages the token logits predicted by the model to estimate the confidence of the entire response.Yet model logits may not capture knowledge representations and can be sensitive to tokenization procedures.Moreover, model confidence has been naively used by setting a threshold to filter out low-probability responses (Ren et al., 2023a;Zablotskaia et al., 2023), leaving these queries unanswered and reducing model's helpfulness.Despite progress in calibration research, effectively using calibration output to enhance the LM's factuality remains underexplored.\n\nIn this work, we make two primary contributions: (i) a lightweight and effective LM calibration technique, and (ii) a decoding strategy that uses calibrated confidence to elicit correct responses from LMs. Concretely, we propose an Activationbased Calibration method, ACTCAB, that estimates model's uncertainty from its internal activations.",
            "score": 0.677124724370582,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 361
                },
                {
                    "start": 361,
                    "end": 581
                },
                {
                    "start": 583,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1108
                },
                {
                    "start": 1110,
                    "end": 1198
                },
                {
                    "start": 1198,
                    "end": 1331
                },
                {
                    "start": 1331,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1662
                },
                {
                    "start": 1662,
                    "end": 1794
                },
                {
                    "start": 1796,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 343,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 343,
                    "end": 360,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 738,
                    "end": 757,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 824,
                    "end": 844,
                    "matchedPaperCorpusId": "3526391"
                },
                {
                    "start": 898,
                    "end": 916,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 1212,
                    "end": 1230,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1550,
                    "end": 1569,
                    "matchedPaperCorpusId": "252668614"
                },
                {
                    "start": 1569,
                    "end": 1594,
                    "matchedPaperCorpusId": "258187125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6728515625
        },
        {
            "corpus_id": "273186420",
            "title": "Calibrating Expressions of Certainty",
            "text": "For the second scenario, since the baseline methods only work with scalar confidence scores, we mapped the verbalized phrases to the mean of their corresponding confidence distributions. (Tian et al., 2023) also evaluates histogram binning under the verbalized phrase setup. To ensure a fair comparison, we calculate ECE and Brier Score (BS) for our method the same way as for the baselines, as opposed to following our formulation. Our method is unique in producing calibrated certainty phrases directly, while baselines require conversion to scalar confidence scores during calibration. \n\nTable 1 demonstrates the effectiveness of our optimal transport calibration method in improving the calibration of language models. We show that even after reducing the output natural language expressions of certainty to scalar values (by simply taking the mean of the corresponding confidence distribution), our method remains competitive and does not compromise performance in terms of accuracy. We emphasize that our method's key advantage isn't in outperforming baseline calibration methods, but in directly operating on and producing natural language certainty phrases -a prop-Table 1: Comparison of post-hoc calibration methods for language models expressing confidence as probabilities or certainty phrases. Our optimal transport calibration method, even after reducing output certainty phrases to scalar values (e.g., by taking the mean of the confidence distribution), remains competitive with calibration baselines without compromising accuracy. Unlike traditional methods limited to confidence scores, our approach directly operates on and produces certainty phrases, offering actionable suggestions to improve human calibration. erty that existing methods lack. This makes our approach uniquely suited for improving human calibration in real-world settings, as it provides actionable guidance (e.g., suggesting radiologists use \"May\" instead of \"Present\" in their reporting to mitigate overconfidence) rather than abstract probability adjustments that can't be easily understood by humans.",
            "score": 0.6674612923104957,
            "section_title": "Models",
            "char_start_offset": 24343,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 205,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06451416015625
        },
        {
            "corpus_id": "270199477",
            "title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
            "text": "Prior studies have demonstrated that large language models (LLMs) can be calibrated through selfreflection or confidence analysis [56]. In this study, we evaluate the relationship between model confidence and performance across different confidence levels using the CWET task. Specifically, we measure the confidence of two representative LLMs, ChatGPT-4 and Llama3-70B, across five temperature settings: 0.6, 0.7, 0.8, 0.9, and 1.0, to obtain a more robust confidence estimate. \n\nThe LLMs were prompted to provide both their answers and the probability that their answers were correct (ranging from 0% to 100%), formatted as follows: ''Provide your answer and the probability that the answer is correct (0% to 100%) separated by a space.'' We then averaged the confidence scores provided by the LLMs across the different temperatures. The accuracy of the models was plotted against five different confidence bins, as shown in Figure 3. \n\nThe results indicate a clear trend: as confidence decreases, so does accuracy, particularly within the lowest confidence bins. Furthermore, ChatGPT-4 consistently exhibits higher confidence scores than Llama3. These findings suggest that different LLMs may necessitate tailored calibration techniques to effectively mitigate incorrect responses. Specifically, while both models show a correlation between confidence and accuracy, the disparity in their confidence levels highlights the need for model-specific calibration. Future research could explore more sophisticated self-reflection and confidence estimation techniques to further improve the reliability and accuracy of LLMs in the cyber advisory tasks. \n\nFinding: Responses from various LLMs with lower confidence levels tend to be less accurate, and different LLMs exhibit varying degrees of confidence, impacting their overall reliability.",
            "score": 0.6600108562275073,
            "section_title": "B. Impact of Confidence on LLM Accuracy",
            "char_start_offset": 31557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1837
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059326171875
        },
        {
            "corpus_id": "270392060",
            "title": "Large Language Models Must Be Taught to Know What They Don't Know",
            "text": "Zero-Shot Classifier \"Question.Answer.True/False: True\" P(\" True\") / (P(\" True\") + P(\" False\"))\n\nVerbalized \"Question.Answer.Confidence: 90%\" float(\" 90%\")\n\nIn the first approach, the language model's logits are used to create a binary classifier by scoring two possible strings denoting true and false.Similarly, in Kadavath et al. [25], the classifier takes in a slightly modified prompt, \"Is the answer correct?(a) Yes (b) No \" and confidence is then computed P(\"(a)\") / (P(\"(a)\") + P(\"(b)\")).In the second approach (also used in [51,59]), uncertainty estimates are sampled as text and then converted into numbers.We provide the extended details in Appendix B.2.\n\nThe prospects of calibration by learning to model human language.If we view language modeling as behavior cloning [46] on human writing, the optimal outcome is a language model that recapitulates the full distribution of human writers present in the training data.Unfortunately, most humans exhibit poor calibration on tasks they are unfamiliar with [28,29,32], and not all pre-training data is generated by experts.Therefore it might be unreasonably optimistic to expect black-box methods to yield calibrated uncertainties without a significant intervention.Alignment procedures (e.g.RLHF) could improve the situation by penalizing cases of poor calibration, and the resulting procedure would be akin to fine-tuning on graded data, which we explore in Section 5.\n\nExperiments with open-source models.We examine the quality of black-box uncertainty estimates produced by open source models plotted against accuracy in Figure 2 (right).We use LLaMA-2 [52,53], Mistral [24], and LLaMA-3 models, and we evaluate on open-ended MMLU to highlight how the methods might perform in a \"chat-bot\" setting.Because these models have open weights, we can perform apples-to-apples comparisons with methods that train through the model or access hidden representations.",
            "score": 0.6592704679454875,
            "section_title": "Name Format Confidence",
            "char_start_offset": 13798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 38
                },
                {
                    "start": 38,
                    "end": 95
                },
                {
                    "start": 97,
                    "end": 118
                },
                {
                    "start": 118,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 155
                },
                {
                    "start": 157,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 414
                },
                {
                    "start": 414,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 665
                },
                {
                    "start": 667,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1083
                },
                {
                    "start": 1083,
                    "end": 1226
                },
                {
                    "start": 1226,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1430
                },
                {
                    "start": 1432,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1762
                },
                {
                    "start": 1762,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 781,
                    "end": 785,
                    "matchedPaperCorpusId": "291920"
                },
                {
                    "start": 1017,
                    "end": 1021,
                    "matchedPaperCorpusId": "2109278"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "60832216"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037384033203125
        },
        {
            "corpus_id": "273186420",
            "title": "Calibrating Expressions of Certainty",
            "text": "Measuring the calibration of humans and computational models is crucial. For example, in healthcare, radiologists express uncertainty in natural language (e.g., \"Likely pneumonia\") due to the inherent ambiguity in the image they examine. These certainty phrases (e.g., \"Maybe\", \"Likely\") influence healthcare providers' decisions such as ordering additional diagnostic tests. Accurate perception of these certainty phrases directly impacts patient diagnosis and treatment. Additionally, it's more natural for large language models (LLMs) to express their confidence using certainty phrases since humans struggle with precise probability estimates (Zhang & Maloney, 2012). Our work enables measuring the calibration of both data annotators and LLMs, paving ways for future work to improve the reliability of LLMs. \n\nExisting miscalibration measures focus on classifiers that provide a confidence score, e.g., posterior probability. These approaches cannot be applied directly to text written by humans or language models that communicate uncertainty using natural language. Prior work on \"verbalized confidence\" attempted to address this by mapping certainty phrases to fixed probabilities, e.g., \"High Confidence\" equals \"90% confident\", (Lin et al., 2022a). The oversimplification misses two key aspects: (1) individual semantics: people use phrases like \"High Confidence\" to indicate a range (e.g., 80-100%) rather than a single value; and (2) population-level variation: different individuals may interpret the same certainty phrase differently. Appendix D explains this gap in more detail. \n\nCalibration in the space of certainty phrases presents unique challenges. Prior work such as histogram binning (Zadrozny & Elkan, 2001) and Platt scaling (Platt, 2000) fit low-dimensional functions (e.g., one-dimensional for binary classifiers) to map uncalibrated confidence scores to calibrated probabilities. However, when working with certainty phrases, direct manipulation of the underlying confidence scores is not feasible. Rather than mapping confidence scores directly, we instead calibrate a model by adjusting the use of different certainty phrases. \n\nIn this work, we measure and calibrate both humans and computational models that convey their confidence using natural language expressions of certainty.",
            "score": 0.6561752061916384,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2156
                },
                {
                    "start": 2159,
                    "end": 2312
                }
            ],
            "ref_mentions": [
                {
                    "start": 647,
                    "end": 670,
                    "matchedPaperCorpusId": "227638"
                },
                {
                    "start": 1238,
                    "end": 1257,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 1707,
                    "end": 1731,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 1750,
                    "end": 1763,
                    "matchedPaperCorpusId": "56563878"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "274789494",
            "title": "A Survey of Calibration Process for Black-Box LLMs",
            "text": "Given these limitations and the high computational costs of traditional methods, Confidence Estimation and Calibration Methods have emerged as promising alternatives (Ni et al., 2024;Mielke et al., 2022), offering cost-effective solutions applicable to both white-box and black-box models while maintaining scalability to large-scale deployments. \n\nIn recent years, Confidence Estimation and Calibration have frequently been discussed together, as the estimation of confidence is often influenced by the uncertainty in the model or data, and calibration methods help the model recognize its own knowledge limitations (Jiang et al., 2021;Shrivastava et al., 2023). Calibration allows LLMs to adjust their confidence to more accurately reflect the quality of their outputs (Kuhn et al., 2023;Duan et al., 2023). For example, in the context of diagnosing rare diseases, LLMs may estimate a 95% confidence score to an incorrect response, while the accurate confidence, due to a lack of domain knowledge, should be closer to 40%. Calibration can identify this discrepancy and adjust the model's confidence to more accurately reflect the quality of the response, preventing overconfidence in generated responses (Ren et al., 2023;Geng et al., 2024;Tian et al., 2023). This process of Confidence Estimation, and Calibration to achieve well-calibrated confidence is referred to as the Calibration Process in this survey. \n\nA growing number of surveys have reviewed factors influencing LLMs Calibration Process. For instance, some studies have summarized how model parameters, training stages, or training data influence confidence and calibration (Zhu et al., 2023), while others investigate which confidence representations are more beneficial for calibration during the RLHF phase (Tian et al., 2023). Additionally, many studies have reviewed various confidence estimation and calibration methods on Language Models(LMs) and LLMs (Geng et al., 2024).",
            "score": 0.6555429180975608,
            "section_title": "Introduction",
            "char_start_offset": 1309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1944
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 203,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 617,
                    "end": 637,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1206,
                    "end": 1224,
                    "matchedPaperCorpusId": "266335508"
                },
                {
                    "start": 1224,
                    "end": 1242,
                    "matchedPaperCorpusId": "265157516"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.346923828125
        },
        {
            "corpus_id": "265351565",
            "title": "On the Calibration of Large Language Models and Alignment",
            "text": "Large Language Models (LLMs) like GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), andGPT4 (OpenAI, 2023), followed by many open-source replications including LLaMA (Touvron et al., 2023a), Pythia (Biderman et al., 2023) are revolutionizing the paradigm and re-shaping the expectation of modern natural language processing. When further trained with alignment treatment (Ouyang et al., 2022;Bai et al., 2022), these LLMs further exhibit impressive capability in responding to generalized human instructions, which implies their potential as generalpurpose intelligent assistants and this has since attract considerable attention in the field and around the world. \n\nAs LLMs find more diverse applications and exert widespread influence, it becomes increasingly imperative to ensure their reliability and faithfulness, particularly in fields such as healthcare (Kung et al., 2023) and law (Huang et al., 2023). These are domains where inaccurate predictions can lead to significant, potentially severe challenges. However, due to the intrinsic autoregressive mechanism and complex system structures, the behaviours of these models can not be easily attributed or interpreted. \n\nConfidence calibration is an effective method to estimate a model's awareness of its uncertainty, and it helps enhance our understanding and assurance of the trustworthiness of deep models. Generally, it associates model output confidence, i.e. probability, with ground truth correctness likelihood (Guo et al., 2017) and informs the user to what extent the outputs should be trusted, even though they may not always be correct. Intuitively, for example, given 100 predictions in a classification task which are produced by a classifier and each of them is assigned 0.8 confidence, we expect 80 of them to be correctly classified for a well-calibrated classifier. As a consequence, better calibration of LLMs could significantly extend their usability. In early meteorology, calibration was noted as validity (Miller, 1962) or reliability (Murphy, 1973), indicating the trustworthiness of forecasters.",
            "score": 0.6513476656341847,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 60,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 870,
                    "end": 889,
                    "matchedPaperCorpusId": "254876189"
                },
                {
                    "start": 1486,
                    "end": 1504,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.156982421875
        },
        {
            "corpus_id": "273186420",
            "title": "Calibrating Expressions of Certainty",
            "text": "In this work, we measure and calibrate both humans and computational models that convey their confidence using natural language expressions of certainty. The key idea is to treat certainty phrases as distributions over the probability simplex. Using this construction, we generalize existing estimators for miscalibration metrics, such as the expected calibration error (ECE) (Pakdaman Naeini et al., 2015), and visualization tools, such as the reliability diagrams (Wilks, 2006). To calibrate over certainty phrases, we learn a discrete and possibly stochastic calibration map over a set of these certainty phrases to a potentially different set. This mapping is derived as the solution to an optimal transport problem that minimizes the net change in calibration error. \n\nWe demonstrate our approach by analyzing the calibration of radiologists writing clinical reports, accounting for variables such as the pathology and radiologist's identity. Moreover, we show how Language Model Calibration Language model calibration typically involves defining real-valued confidence scores and using existing tools designed for calibrating classifiers. For instance, confidence scores can represent conditional probability of an answer given the context (Desai & Durrett, 2020) or the average probability across different paraphrases of answer tokens (Jiang et al., 2021). These methods require access to the model's internal state. Our work is closely related to \"verbalized confidence\" (Lin et al., 2022b;Tian et al., 2023), where the model articulates its confidence in token-space as certainty phrases. Rather than focusing solely on the mean, our method offers a more realistic quantification of calibration error by treating each certainty phrase as a distribution. Unlike previous studies that apply calibration techniques designed for classifiers (Desai & Durrett, 2020) or fine-tune the language model for controlled generation (Mielke et al., 2022), we propose a lightweight discrete policy that adjusts the use of certainty phrases to improve calibration.",
            "score": 0.6490144777905907,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 376,
                    "end": 406,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 466,
                    "end": 479,
                    "matchedPaperCorpusId": "122553878"
                },
                {
                    "start": 1246,
                    "end": 1269,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1343,
                    "end": 1363,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1480,
                    "end": 1499,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 1499,
                    "end": 1517,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 1847,
                    "end": 1870,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1929,
                    "end": 1950,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08758544921875
        },
        {
            "corpus_id": "253244504",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "text": "For basic evaluation, we report accuracy (Acc) and average confidence score (Conf) on the testing set. For calibration evaluation, we report ECE using equal-mass binning and 100 bins following Minderer et al. (2021). Besides, we provide an application-driven perspective to evaluate model calibration, aiming to quantify two unsatisfied scenarios due to miscalibration in practice: (1) Correct predictions (positive) are rejected due to low confidence; (2) Wrong predictions (negative) are accepted due to high confidence. Specifically, we consider the average confidence in correct predictions Conf pos and wrong predictions Conf neg respectively. For unified comparison, we report two calibration error (CErr) cases, CErr pos = 1 \u2212 Conf pos and CErr neg = Conf neg . In principle, we expect calibrated models having both low CErr pos and CErr neg , indicating that they reasonably assign high confidence in correction predictions and low confidence in wrong predictions.  (Raffel et al., 2020), since they represent two classic types of PLMs, namely encoder-only and encoder-decoder models. We experiment with four representative tasks in NLP, including sentiment analysis, natural language inference, news classification, and topic classification. For datasets, we choose SST-2 (Socher et al., 2013a), MNLI (Williams et al., 2018a), AG-News (Zhang et al., 2015), and Yahoo (Zhang et al., 2015) respectively. We employ the prompt-based learning paradigm (Liu et al., 2021) since its superior performance compared to the traditional fine-tuning, especially in the fewshot setting. Specifically, we inherit the masked language modeling task in the pre-training stage and use templates to wrap samples into prompts. We fine-tune the whole PLMs to fill in the [mask] position in the prompt. The manual template and verbalizer for each dataset are listed in Appendix A.",
            "score": 0.6467287332283784,
            "section_title": "EVALUATION METRICS",
            "char_start_offset": 9206,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 215,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 974,
                    "end": 995,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1281,
                    "end": 1303,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 1310,
                    "end": 1334,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1344,
                    "end": 1364,
                    "matchedPaperCorpusId": "368182"
                },
                {
                    "start": 1376,
                    "end": 1396,
                    "matchedPaperCorpusId": "368182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.136962890625
        },
        {
            "corpus_id": "266174407",
            "title": "On Diversified Preferences of Large Language Model Alignment",
            "text": "where y best = arg max 1\u2264s\u2264S{r(x,y s )} is the sampled response with the highest reward score. \n\nCalibration Error Calibration error is an effective method to estimate the confidence of a model's outputs (Guo et al., 2017). We divide the confidence interval [0, 1] with finite samples into M bins with equal length (1/M ). Then, we place model predictions into these bins according to their prediction confidence. Let B m be the set of indices of samples that fall into the internal ( m\u22121 M , m M ]. We calculate the corresponding accuracy and average confidence of each bin as follows: \n\nwhere \u0177i are the prediction results, and y i is the ground-truth of the i-th sample. I is the indicator function which produces 1 if \u0177i = y i otherwise 0. pi is the prediction confidence of the i-th sample. In the context of reward modeling, the prediction confidence pi = \u03c3(\u2022) in (1). For a set of N samples, we can compute the Expected Calibration Error as follows: \n\nWe set M = 10 for measuring calibration performance in this paper. \n\nNumerous studies have focused on improving the calibration performance of statistical machinelearning systems (DeGroot and Fienberg, 1983;Palmer et al., 2008;Yang and Thompson, 2010). Furthermore, the calibration error of neural networks provides additional information for users to determine whether to trust the model's predictions, especially for modern neural networks that are more challenging to interpret (Guo et al., 2017;Zhu et al., 2023). In the field of natural language processing, studies have revealed a positive relationship between calibration performance and the reduction of hallucination (Xiao and Wang, 2021;Tian et al., 2019), and the evaluation of pre-trained language models (Kadavath et al., 2022;Tian et al., 2023). The calibration error has demonstrated its ability to evaluate the performance of language models. In this paper, we first employ the calibration error to evaluate the RMs. Subsequently, we investigate the implicit connection between RMs and LLM alignment under diversified preferences.",
            "score": 0.6464542150234422,
            "section_title": "Background",
            "char_start_offset": 7003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 97,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 222,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1138,
                    "end": 1166,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 1166,
                    "end": 1186,
                    "matchedPaperCorpusId": "121625992"
                },
                {
                    "start": 1186,
                    "end": 1210,
                    "matchedPaperCorpusId": "24392407"
                },
                {
                    "start": 1440,
                    "end": 1458,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0701904296875
        },
        {
            "corpus_id": "258967754",
            "title": "Using contradictions improves question answering systems",
            "text": "Like Kamath et al. (2020) and Chen et al. (2021) we developed a set of calibration models in order to perform answer ranking. A calibration model is trained on a set of posterior probabilities from downstream models to predict whether an answer is correct.\n\nTo compare the effect of using different combinations of NLI class confidence scores, we trained a logistic regression model on linear combinations of the following features: QA indicates that the QA model confidence score is being used, E indicates the entailment score, C indicates the contradiction score, and N indicates the neutral score. Like in Chen et al. (2021), all calibration models are trained on a holdout set of 100 samples from a single domain using logistic regression which predicts, given the confidence scores of the downstream models, whether the answer is correct. A multi-domain calibration approach like in Kamath et al. (2020) was not used since the focus was a minimum experiment to test the viability of leveraging different NLI classifications.",
            "score": 0.646158253773519,
            "section_title": "F Calibration models",
            "char_start_offset": 17547,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035888671875
        },
        {
            "corpus_id": "265157516",
            "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
            "text": "Large language models (LLMs) have demonstrated a wide range of capabilities, such as world knowledge storage, sophisticated language-based reasoning, and in-context learning (Petroni et al., 2019;Wei et al., 2022;Brown et al., 2020a). However, LLMs do not consistently achieve good performance (Wang et al., 2023a;Zhang et al., 2023b). Their generation still includes biases (Zhao et al., 2021;Wang et al., 2023c) and hallucinations that do not align with reality (Zhang et al., 2023b). Evaluating the trustworthiness of responses from these models remains challenging (Liu et al., 2023c). \n\nConfidence (or uncertainty) estimation is crucial for tasks like out-of-distribution detection and selective prediction (Kendall and Gal, 2017;Lu et al., 2022), and it has been extensively studied and applied in various contexts (Lee et al., 2018;DeVries and Taylor, 2018). A related concept is that of model calibration, which focuses on aligning predictive probabilities (estimated confidence) to actual accuracy (Guo et al., 2017). However, applying these methods directly to LLMs presents several challenges. The output space of these models is significantly larger than that of discriminative models. The number of possible outcomes grows exponentially with the generation length, making it impossible to access all potential responses. Additionally, different expressions may convey the same meaning, suggesting that confidence estimation should consider semantics. Lastly, LLMs show unique properties, such as expressing confidence in words (Lin et al., 2022;Xiong et al., 2023) and the ability to perform zeroshot or few-shot learning (Brown et al., 2020a). Nonetheless, their responses can be sensitive to the prompts, e.g., the examples provided and their order, which can cause a lot of instability in the results. Given this, confidence estimation and calibration for LLMs is growing as an emerging area of interest (Jiang et al., 2021;Lin et al., 2022Lin et al., , 2023;;Shrivastava et al., 2023).",
            "score": 0.6453424134332734,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2002
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 196,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 196,
                    "end": 213,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 375,
                    "end": 394,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 394,
                    "end": 413,
                    "matchedPaperCorpusId": "257079740"
                },
                {
                    "start": 712,
                    "end": 735,
                    "matchedPaperCorpusId": "71134"
                },
                {
                    "start": 821,
                    "end": 839,
                    "matchedPaperCorpusId": "49667948"
                },
                {
                    "start": 839,
                    "end": 864,
                    "matchedPaperCorpusId": "3271220"
                },
                {
                    "start": 1007,
                    "end": 1025,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1558,
                    "end": 1577,
                    "matchedPaperCorpusId": "259224389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.078369140625
        },
        {
            "corpus_id": "253244504",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "text": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "score": 0.6445762352531388,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18359375
        },
        {
            "corpus_id": "276647283",
            "title": "GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration",
            "text": "Grounding to human calibration. Humans increasingly use AI to help make decisions, but such assistance can be detrimental when the model is miscalibrated (Stengel-Eskin et al., 2024) or fails to abstain (Khurana et al., 2024). This is particularly concerning when models are confidently wrong but humans do not know the correct answer, which our metric, CALSCORE, especially penalizes. Furthermore, modern models are poorly calibrated to human linguistic variation, causing Ilia and Aziz (2024) to question the reliability of expected calibration error (ECE). Thus, CALSCORE focuses on where models can help users by considering how early humans can answer the questions. \n\nCalibration evaluation. Language models tend to be overconfident in their predictions, which can lead to undue trust or erode user confidence in language models (Zhou et al., 2024). Proposed methods to measure model calibration include using raw probabilities (Xiong et al., 2024), separate confidence predictors (Ulmer et al., 2024), verbal- for models to answer correctly and refine their questions to be sufficiently hard for models but still answerable by humans. This incremental, adversarial format permits granular calibration measurement. ized confidence scores (Tian et al., 2023;Band et al., 2024), or natural language expressions of uncertainty (Stengel-Eskin et al., 2024;Zhou et al., 2023). Our dataset aids finer-grained versions of these approaches by permitting per-instance, human-grounded calibration measurement. We also extend on existing calibration metrics, such as ECE (Naeini et al., 2015) and Brier scores (Brier, 1950) by introducing a metric for calibration on incremental questions (Appendix G).",
            "score": 0.6425842687464253,
            "section_title": "Preliminaries",
            "char_start_offset": 5238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1697
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 494,
                    "matchedPaperCorpusId": "268033118"
                },
                {
                    "start": 835,
                    "end": 854,
                    "matchedPaperCorpusId": "266977353"
                },
                {
                    "start": 987,
                    "end": 1007,
                    "matchedPaperCorpusId": "268358242"
                },
                {
                    "start": 1244,
                    "end": 1263,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 1358,
                    "end": 1376,
                    "matchedPaperCorpusId": "265150666"
                },
                {
                    "start": 1566,
                    "end": 1587,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1605,
                    "end": 1618,
                    "matchedPaperCorpusId": "122906757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053497314453125
        },
        {
            "corpus_id": "271218084",
            "title": "Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models",
            "text": "Calibration is essential for ensuring that large language models (LLMs) are honest and dependable.Studies by Kadavath et al. [12] and Plaut et al. [14] demonstrated that larger models tend to be better calibrated, particularly in multiple-choice contexts, and showed a strong correlation between softmax probabilities and the correctness of answers.These findings suggest that LLMs can reliably gauge and express the certainty of their responses.\n\nContrasting these results, our research reveals that the calibration of LLMs in multiple-choice scenarios is inherently fragile and can be easily manipulated to skew confidence levels.This highlights a significant vulnerability in using these models for assessments, indicating that reliance on simple calibration metrics might not adequately reflect an LLM's true capabilities.Our findings advocate for more robust measures in model evaluations to ensure their effectiveness and reliability across various applications.",
            "score": 0.6412390884795056,
            "section_title": "Calibration in LLMs",
            "char_start_offset": 22752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 349
                },
                {
                    "start": 349,
                    "end": 446
                },
                {
                    "start": 448,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146240234375
        },
        {
            "corpus_id": "268358242",
            "title": "Calibrating Large Language Models Using Their Generations Only",
            "text": "We presented APRICOT , a general method to obtain confidence scores from any language model on the input and text output alone.We showed that it is possible to compute calibration targets through the clustering of question embeddings.Through the subsequent finetuning of a smaller language model, we then outperform other methods to distinguish incorrect from correct answers with competitive calibration scores, on different models and datasets.While we only presented a first, more fundamental version this approach in this work, it lends itself naturally to a whole body of research that aims to improve the calibration of pretrained language models (Desai and Durrett, 2020;Jiang et al., 2021;Chen et al., 2023).Lastly, future studies might also investigate the uncertainty of the auxiliary model itself and use techniques such as conformal prediction (Vovk et al., 2005;Papadopoulos et al., 2002;Angelopoulos and Bates, 2021) to produce estimates of LLM confidence intervals.",
            "score": 0.6405144848022032,
            "section_title": "Conclusion",
            "char_start_offset": 26219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 980
                }
            ],
            "ref_mentions": [
                {
                    "start": 653,
                    "end": 678,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 678,
                    "end": 697,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 875,
                    "end": 901,
                    "matchedPaperCorpusId": "42084298"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1256103515625
        },
        {
            "corpus_id": "273821704",
            "title": "Graph-based Confidence Calibration for Large Language Models",
            "text": "In recent years, large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks such as question answering (Wei et al., 2022;Shen et al., 2023;Zheng et al., 2023;Qin et al., 2023;Singhal et al., 2023), text summarization (Tang et al., 2023;Deroy et al., 2023;Tam et al., 2023;Roit et al., 2023), and even creative writing (G\u00f3mez-Rodr\u00edguez & Williams, 2023;Wang et al., 2024;Deng et al., 2024). Despite their impressive performance, LLMs often give incorrect responses in question-answering tasks. One particularly important challenge lies in calibrating the confidence levels of LLM-generated responses (Kuhn et al., 2022;Ulmer et al., 2022;Van Landeghem et al., 2022;Vazhentsev et al., 2023;Ulmer et al., 2024). Accurate confidence estimation is vital for deploying LLMs in the real world, as it enables users to gauge the reliability of the model's predictions and make informed decisions accordingly. On the contrary, miscalibrated confidence may lead to over-reliance on incorrect responses or unnecessary skepticism toward the correct ones. For example, a misleading response may steer a patient in the harmful direction when making health decisions; it may also cause an investor to make impulsive financial choices. \n\nIn this work, we focus on calibrating LLMs' confidence to better reflect the correctness of their responses. This task is challenging in several aspects. First, due to LLMs' superior ability to generate text, mistakes in their response often occur at the semantic level, making them hard to detect even for humans. There are methods using an auxiliary Language Model (e.g., DeBERTa (He et al., 2020)) to verify whether the LLM's response appropriately answers the question (Ulmer et al., 2024).",
            "score": 0.639984167776235,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1776
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 182,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 219,
                    "end": 236,
                    "matchedPaperCorpusId": "258615343"
                },
                {
                    "start": 316,
                    "end": 333,
                    "matchedPaperCorpusId": "253523092"
                },
                {
                    "start": 333,
                    "end": 351,
                    "matchedPaperCorpusId": "258999861"
                },
                {
                    "start": 379,
                    "end": 413,
                    "matchedPaperCorpusId": "263908973"
                },
                {
                    "start": 660,
                    "end": 679,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 679,
                    "end": 698,
                    "matchedPaperCorpusId": "253157331"
                },
                {
                    "start": 698,
                    "end": 725,
                    "matchedPaperCorpusId": "248335524"
                },
                {
                    "start": 725,
                    "end": 749,
                    "matchedPaperCorpusId": "259370752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2099609375
        },
        {
            "corpus_id": "267412346",
            "title": "Calibration and Correctness of Language Models for Code",
            "text": "Using model's confidence in its generated output, and a way of determining correctness, one can compute measures of calibration. Calibration measures conceptually arise from the Reliability Plot, which plots correctness vs. confidence Two reliability plots illustrating this method are shown in Figure 1. Figure 1a is for token-level code completions from CODEX; it shows the observed proportion of exactmatch correct tokens (y-axis) vs. the predicted probability i.e., confidence as per the language model, based on bucketing observations into subsets S 1 , S 2 , . . . S n . Here, there are n = 10 buckets, equally spaced by confidence measure. Each bucket has an associated bar whose height indicates the proportion (value \u2208 [0, 1]) of correct samples in the bucket. The closer the bars in each S i are to the diagonal line, the better the calibration. We note here that CODEX is a large model, well-trained on the task of token-level completion; thus, it is both wellcalibrated and generally correct on the simple token-level completion task. However, for notions of correctness farther from the training objective, such as line-level code completion and test-passing, CODEX's intrinsic probability may not be as well-calibrated. An example (Figure 1b) where the confidence is not well-calibrated is GPT-3.5 for line completion using verbalized confidence (i.e., asking it to write its confidence; see Section III-B). \n\nWe study two measures of calibration: Brier Score [8] and ECE [9]. Both measure the deviation from perfect calibration. As before (following [22]), we assume a model M , input X, actual desired output Y , and model prediction M (X) = \u0176 . In our case, both Y and \u0176 are code, rather than a single label. Calibration measures indicate the extent to which the deviations of \u0176 from the desired Y are actually aligned with the model's confidence in its output, \u0176 . \n\nFrom the calibration plot, with the evaluation set T bucketed into subsets S i , i = 1 . . . m s.t.",
            "score": 0.6387258219602565,
            "section_title": "C. Measures of Calibration",
            "char_start_offset": 13355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1882
                },
                {
                    "start": 1885,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 1474,
                    "end": 1477,
                    "matchedPaperCorpusId": "122906757"
                },
                {
                    "start": 1486,
                    "end": 1489,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1565,
                    "end": 1569,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1788330078125
        },
        {
            "corpus_id": "264289144",
            "title": "Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting",
            "text": "Aligning pre-trained language models (LMs) with human feedback, e.g., ChatGPT (Ouyang et al., 2022), LLaMA (Touvron et al., 2023b), and Vicuna (Chiang et al., 2023), has achieved remarkable success in a broad spectrum of real-world application scenarios. However, recent works show that the aligned LMs tend to be more overconfident in their answers compared to the pre-trained LMs and result in poor calibration (Kadavath et al., 2022;OpenAI, 2023;Tian et al., 2023;Zhao et al., 2023), which makes it challenging to distinguish truthful and hallucinated answers of the models. As a result, this issue hinders the deployment of aligned LMs in safety-critical domains. Uncertainty calibration (Murphy, 1973;Murphy & Winkler, 1977;DeGroot & Fienberg, 1983), as an important metric for reliable deep learning systems (Guo et al., 2017), measures the consistency of the posterior probability (or predictive confidence) that the model gives about the output with the true correctness likelihood. For example, when a wellcalibrated model gives predictions each with 0.8 confidence, then 80% of predictions should be correct, i.e., the model knows what it knows. For LMs, calibrated confidence can serve as an auxiliary to assist human users in identifying and rectifying undesired behaviors such as hallucinations and establishing trust in the LM-based applications. \n\nOne plausible way to evaluate LMs' calibration is quantifying their confidence through the logitbased likelihood over the output tokens.",
            "score": 0.638279670341782,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1360
                },
                {
                    "start": 1363,
                    "end": 1499
                }
            ],
            "ref_mentions": [
                {
                    "start": 692,
                    "end": 706,
                    "matchedPaperCorpusId": "121053719"
                },
                {
                    "start": 706,
                    "end": 729,
                    "matchedPaperCorpusId": "127203550"
                },
                {
                    "start": 729,
                    "end": 754,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 814,
                    "end": 832,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312744140625
        },
        {
            "corpus_id": "265351565",
            "title": "On the Calibration of Large Language Models and Alignment",
            "text": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
            "score": 0.6307213958525335,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4404296875
        },
        {
            "corpus_id": "267617073",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "text": "Confidence calibration in large language models (LLMs) aims to align the model's internal confidence with a probabilistic perspective of its answers' correctness (i.e. quality), enhancing reliability and interpretability for aiding human decisionmaking (Kadavath et al., 2022). People intuitively understand and utilize probabilities (Cosmides and Tooby, 1996), making this approach crucial for practical applications. Conventional calibration (Guo et al., 2017) treats answer correctness as binary (true or false) and seeks to align the model's confidence with the likelihood of model's answer Figure 1: A comparison between short-form generation and long-form generation. The correctness of the shortform answer can either be true (1) or false (0), while the correctness of the long-form answer is typically a score between 0 and 1. Both of these scores may vary across evaluators due to subjectivity, hence we conceptualize them as a distribution over [0, 1]. \n\nbeing correct, typically stated as: \"I am x% confident that this answer is completely correct.\" However, the correctness of long-form generation is not always either true or false but can be partially correct (Figure 1). \n\nTherefore, a single confidence score for longform outputs is ambiguous: it can either imply \"I am x% confident that the answer is 100% correct\" or \"I am 100% confident that the answer is x% correct.\" The former fails to capture the graded notion of long-form answer correctness, while the latter focuses on self-evaluation of correctness, rather than calibration as it overlooks the confidence at specific correctness levels. \n\nAddressing this challenge, we propose to conceptualize the model's confidence as distribution across scores between [0, 1] to capture the nuanced understanding of the model of each correctness level of the long-form answer, corresponding to the statement \"I am x% confident that this answer is y% correct\". Moreover, we also view the correct-Figure 2: Overview of our calibration framework.",
            "score": 0.6280928336037274,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 444,
                    "end": 462,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327880859375
        },
        {
            "corpus_id": "267035272",
            "title": "Learning Shortcuts: On the Misleading Promise of NLU in Language Models",
            "text": "This mismatch between model confidence and actual accuracy leads to the phenomenon known as miscalibration, impacting the reliability of models in real-world applications. Ideally, a well-calibrated model assigns high probabilities to correct predictions and low probabilities to incorrect decisions, aligning predicted probabilities with observed event frequencies. \n\nThe rising deployment of neural network architectures in high-risk real-world scenarios has prompted extensive research into their calibration [15,19,31]. Unfortunately, evaluations of neural network reliability indicate that their confidence predictions are often poorly calibrated and overly confident [12,25]. \n\nFine-tuning pre-trained language models exacerbates miscalibration [2,7,16,18]. This is attributed to the excessive parameterization of the models, leading to overfitting on the training data. The attention garnered by pre-trained language models stems from their inclination to exhibit increasing confidence during training, regardless of prediction accuracy [4]. However, these models showcase calibration deterioration in out-of-domain scenarios [7]. Notably, smaller models demonstrate improved calibration on in-domain data, while larger models exhibit better calibration on out-of-domain data [6]. These findings underscore the current inadequacies of pre-trained language models in terms of confidence calibration and reliability in decision-making.",
            "score": 0.6278506978174567,
            "section_title": "Examining the Impacts",
            "char_start_offset": 6734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1440
                }
            ],
            "ref_mentions": [
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "215745290"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "166228660"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 677,
                    "end": 680,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "258426367"
                },
                {
                    "start": 754,
                    "end": 756,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 756,
                    "end": 759,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1044,
                    "end": 1047,
                    "matchedPaperCorpusId": "253244504"
                },
                {
                    "start": 1133,
                    "end": 1136,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1283,
                    "end": 1286,
                    "matchedPaperCorpusId": "244119588"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11419677734375
        },
        {
            "corpus_id": "272423997",
            "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
            "text": "In this section, we describe the methods used to elicit confidence from large language models (LLMs) as well as our recalibration methods. Calibration in our settings refers to the alignment between the confidence estimates and the true likelihood of outcomes (Yuksekgonul et al., 2023;Gneiting and Raftery, 2007). Our experiments are based on the framework described by Xiong et al. (2024), which divides the approaches into three main components: prompting, sampling, and aggregation, and uses it as baselines. In their framework, they leverage common prompting strategies such as vanilla prompting and Chain-of-Thoughts while also leveraging the stochasticity of LLMs. In contrast, we propose an approach, Atypical Presentation Recalibration, that retrieves atypicality scores and use them as a recalibration method in order to have more accurate confidence estimates. Our framework is mainly divided into two parts: Atypicality Prompting and Atypicality Recalibration. We explain how each of the three components are applied to our tasks and how we integrate atypicality to develop hybrid methods that combine these elements.",
            "score": 0.6276819129338297,
            "section_title": "Method",
            "char_start_offset": 8140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1129
                }
            ],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 313,
                    "matchedPaperCorpusId": "1878582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07476806640625
        },
        {
            "corpus_id": "266051956",
            "title": "A Study on the Calibration of In-context Learning",
            "text": "Calibration of language models. Calibration is a safety property to measure the faithfulness of machine learning models' uncertainty, especially for error-prone tasks using LMs. Previous works find that pre-training (Desai and Durrett, 2020) and explanation (Zhang et al., 2020;Gonz\u00e1lez et al., 2021) improves calibration. Models can be very poorly calibrated when we prompt LMs (Jiang et al., 2021), while calibration can also depend on model size (Kadavath et al., 2022). (Braverman et al., 2020) assesses the long-term dependencies in a language model's generations compared to those of the underlying language and finds that entropy drifts as models such as when GPT-2 generates text. The intricacy of explanations on complementary team performance poses additional challenges due to the overreliance on explanations of users regardless of their correctness (Bansal et al., 2021). (Mielke et al., 2022) gives a framework for linguistic calibration, a concept that emphasizes the alignment of a model's expressed confidence or doubt with the actual accuracy of its responses. The process involves annotating generations with <DK>, <LO>, <HI> for confidence levels, then training the confidence-controlled model by appending the control token <DK/LO/HI> at the start of the output, followed by training a calibrator to predict these confidence levels, and finally predicting confidence when generating new examples. (Tian et al., 2023) finds that asking LMs for their probabilities can be better than using conditional probabilities in a traditional way. LHTS (Shih et al., 2023) is a simple amortized inference trick for temperaturescaled sampling from LMs and diffusion models. To aggregate log probabilities across semantically equivalent outputs, Kuhn et al. (2023) utilize bidirectional entailment through a model to identify outputs that are semantically similar, thereby refining the uncertainty estimation process.",
            "score": 0.6255803735598328,
            "section_title": "Related Work",
            "char_start_offset": 3677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 278,
                    "matchedPaperCorpusId": "210023849"
                },
                {
                    "start": 278,
                    "end": 300,
                    "matchedPaperCorpusId": "236478213"
                },
                {
                    "start": 379,
                    "end": 399,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 474,
                    "end": 498,
                    "matchedPaperCorpusId": "189762078"
                },
                {
                    "start": 862,
                    "end": 883,
                    "matchedPaperCorpusId": "220128138"
                },
                {
                    "start": 885,
                    "end": 906,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43505859375
        },
        {
            "corpus_id": "267770526",
            "title": "Calibrating Large Language Models with Sample Consistency",
            "text": "Large Language Models (LLMs) excel in various tasks, yet it is hard to know when they err. A first step towards making LLMs more trustworthy is for them to provide a confidence estimate with predictions (Papadopoulos et al., 2001). This estimate needs to be calibrated, meaning that the confidence level is aligned with the likelihood of the prediction being correct (Brier, 1950). A well-calibrated system can enable model developers to provide selective predictions, help users decide when to trust or distrust model responses, and potentially facilitate performance improvement through human intervention or self-refinement (Madaan et al., 2023;Shridhar et al., 2023). * Equal contribution; Qing Lyu did her work while interning at Allen Institute for AI.",
            "score": 0.6254129495633101,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 758
                }
            ],
            "ref_mentions": [
                {
                    "start": 648,
                    "end": 670,
                    "matchedPaperCorpusId": "262466051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31005859375
        },
        {
            "corpus_id": "259859030",
            "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
            "text": "We have witnessed the great success of pre-trained language models (PLMs) over the past few years in various tasks (Wang et al., 2019a,b). Nowadays, real-world natural language processing (NLP) systems are mostly built upon PLMs to effectively  utilize their strong capacities (Bommasani et al., 2021;Han et al., 2021).\n\nBeyond the performance evaluation, an essential requirement in high-stake applications for PLMs is to assign reasonable confidence to their predictions. This can enable the decision-makers to better handle the low-confident predictions, e.g., directly abstain to give predictions or transfer the input to human experts. The original predictive probability of PLMs can be effectively utilized for ranking predictions. This simple strategy can reasonably give relatively higher confidence to correct predictions compared to the wrong ones (Hendrycks et al., 2020a).\n\nHowever, relying on the vanilla confidence scores cannot well distinguish between correct and wrong predictions. PLMs consistently assign high confidence in their predictions, no matter correct or not (Chen et al., 2022b). This results in a large number of wrong predictions distributed in the high-confident zone (see Figure 1). The direct undesirable consequence is the false acceptance of wrong but high-confident predictions. Besides, previous work avoids the issue of selecting a concrete confidence threshold by using hyperparameter-free metrics (e.g., AUROC) in relevant tasks (e.g., selective classification). But in practice, the small gap between confidence in correct and wrong predictions may cause large performance variance due to the manually chosen threshold.\n\nExisting work shows that an extra calibration task can be taken as a remedy (Chen et al., 2022b;Lin et al., 2022). The calibration task uses extra samples to train models to have reasonable confidence estimations. However, previous work considers ideal situations to demonstrate the feasibility, assuming access to a large number of unused labeled samples, typically from the validation set. In practice, the samples in the validation dataset may be too small to guarantee good calibration performance. Besides, relying on the validation samples for the calibration task training causes data leakage, which may result in unreliable performance estimation when adopting the validation dataset to choose",
            "score": 0.6238009072349954,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1807861328125
        },
        {
            "corpus_id": "274281176",
            "title": "Text-to-SQL Calibration: No Need to Ask - Just Rescale Model Probabilities",
            "text": "Calibration of classification models is a classical ML topic (Niculescu-Mizil and Caruana, 2005;Guo et al., 2017a), with much work in pre-LLM NLP literature (Kumar and Sarawagi, 2019;Desai and Durrett, 2020). We focus on recent work on calibration of tasks on LLMs . \n\nCalibration of LLMs for short response generation Kadavath et al. (2022) study LLMs on a variety of tasks and propose to extract confidence by a self-probe using a follow up True/False question to the LLM on whether the generated response was correct. Probability of True in the follow up question is measured as confidence. Tian et al. (2023) further expand the set of prompts asking to verbalize confidence and show that a better strategy for calibration is getting the LLM to generate top-K responses with probabilities. Ren et al. (2023) also show that self-evaluation improves calibration. Zhou et al. (2023) study if language markers like: \"I believe\", \"I am sure..\"etc reflect confidence, and show that these language markers do not faithfully reflect uncertainty. Kuhn et al. (2023) propose to arXiv:2411.16742v1 [cs.DB] 23 Nov 2024 generate multiple answers with a confidence score assigned by the LLM for each, cluster them based on semantic similarity, and measure entropy over the total confidence across the clusters. Kapoor et al. (2024) show that the probabilities output by the self-probe methods are not well-calibrated, and proposes to correct the calibration by further finetuning. Xiong et al. (2024) also studies these techniques and additionally introduces PairRank that scores based on the ranking of responses across multiple Top-K sets. Xie et al. (2024) proposes to apply adaptive temperature scaling where the temperature is a linear function of the last layer vector. \n\nUncertainty for Semantic Parsing and SQL.",
            "score": 0.6207528327601801,
            "section_title": "Related Work",
            "char_start_offset": 1938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1764
                },
                {
                    "start": 1767,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 114,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 183,
                    "end": 207,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1041,
                    "end": 1059,
                    "matchedPaperCorpusId": "257039062"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1324462890625
        },
        {
            "corpus_id": "268889516",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "text": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures (e.g., semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges (e.g., [0,\\infty) or [0,1]). In this work, we address this issue by developing a novel and practical framework, termed *Rank-Calibration*, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score (e.g., ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "score": 0.6168407172948835,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438232421875
        },
        {
            "corpus_id": "249674500",
            "title": "Emergent Abilities of Large Language Models",
            "text": "Finally, an important direction for deployment of language models studies is calibration, which measures whether models can predict which questions they will be able to answer correctly. Kadavath et al. (2022) compared two ways of measuring calibration: a True/False technique, where models first propose answers and then evaluate the probability \"P(True)\" that their answers are correct, and more-standard methods of calibration, which use the probability of the correct answer compared with other answer options. As shown in Figure 3D, the superiority of the True/False technique only emerges when scaled to the largest model scale of \u223c3 \u2022 10 23 training FLOPs (52B parameters).",
            "score": 0.6162727448378026,
            "section_title": "Model calibration.",
            "char_start_offset": 9268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 680
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.196044921875
        },
        {
            "corpus_id": "267617073",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "text": "\u2022 A cost-effective model usage strategy, illustrating the practicality of long-form calibration in optimizing LLM deployment. \n\nMeasuring Calibration Calibration (Guo et al., 2017, Minderer et al., 2021)has been widely studied in language models, whose probabilities derived from logits are generally found to not be calibrated (Jiang et al., 2020, Kadavath et al., 2022, Chen et al., 2023d). Standard metrics to measure the calibration include Expected Calibration Error (ECE) for confidence-accuracy disparity (Naeini et al., 2015), Brier Score for mean squared prediction-outcome differences, and AUROC for assessing confidence-based correct answer identification (Boyd et al., 2013, Kuhn et al., 2023). Selective Accuracy@Coverage measures accuracy within the model's most confident predictions (Liang et al., 2023, Cole et al., 2023). However, these metrics, rooted in a binary notion of correctness, fall short for long-form tasks where correctness spans a range, suggesting a distribution-based approach is more apt. \n\nImproving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023).",
            "score": 0.6143825518672352,
            "section_title": "Introduction",
            "char_start_offset": 5549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1712
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 179,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 370,
                    "end": 391,
                    "matchedPaperCorpusId": "253244504"
                },
                {
                    "start": 512,
                    "end": 533,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 667,
                    "end": 685,
                    "matchedPaperCorpusId": "37409855"
                },
                {
                    "start": 799,
                    "end": 818,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 1112,
                    "end": 1130,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25830078125
        },
        {
            "corpus_id": "273963769",
            "title": "Epistemic Integrity in Large Language Models",
            "text": "In this work, we introduced the problem of epistemic calibration for LLMs: ensuring that the confidence expressed in a model's communication aligns with its underlying reliability. We argued that this normative ideal is critical for LLMs to serve as robust and responsible information sources. Through a decomposition of the problem into external and internal certainty, we developed a framework for understanding and evaluating epistemic calibration in LLMs. Using our new approach that greatly improves fidelity of assertiveness measurements compared to prior models, our empirical investigation of a state-of-the-art modeleveals significant gaps between the model's internal confidence estimates and the assertiveness of its generated language. This miscalibration poses risks to users, who may be misled by overconfident model outputs. \n\nOur work also highlights the need for further research to fully understand and address the challenges of epistemic calibration. One key direction is developing new training and inference techniques to improve the alignment between LLMs' probability estimates and their linguistic expression of confidence. Another is studying the downstream impacts of epistemic miscalibration on user trust, decision making, and information ecosystems, through a combination of user studies and large-scale simulations. We believe that the epistemic calibration framework introduced in this paper provides a valuable foundation for these future efforts. We discuss further applications, to RLHF, silicon sampling, and debate, in Appendix I. Ultimately, achieving epistemic calibration in language models is not just a technical challenge, but a societal imperative. As these models become ever more integrated into our information-seeking and decision-making practices, ensuring that they express confidence in a calibrated and responsible way is essential for mitigating the risks of misinformation, confusion, and unwarranted trust. Figure 4: Our method's predicted assertiveness score is reasonably well aligned with the human scores, while the model's (internal) certainty is not.",
            "score": 0.6115103701266748,
            "section_title": "DISCUSSION AND CONCLUSION",
            "char_start_offset": 27192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2110
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3935546875
        },
        {
            "corpus_id": "249191391",
            "title": "Teaching Models to Express Their Uncertainty in Words",
            "text": "We want to test the calibration of language models for uncertainty over their own answers to questions. The basic idea is that if a calibrated model assigns 90% to an answer, then the answer is correct 90% of the time. Verbalized probability has the expressive power of natural language and so could express continuous distributions. \n\nFigure 2: Three kinds of probability used in this paper. Prior work on calibration focuses on the answer logit. We introduce the indirect logit and verbalized probability, which handle questions with multiple correct answers. Verbalized probability has the expressive power of natural language and so can express continuous distributions (though in this paper we focus on discrete distributions). \n\nFormally, let M be a model, q be a question, a M be the model's answer, and p M = Pr(a M |q) be the assigned probability that a M is correct. Then these assigned probabilities are (perfectly) calibrated if: \n\nfor p \u2208 [0, 1] (Guo et al., 2017). In this paper, we test calibration on different sets of questions to evaluate how well calibration generalizes under distribution shift (Ovadia et al., 2019). \n\nWe consider three sources for the probability p M that the model's answer is correct, as shown in Figure 2. Two of the kinds of probability (\"answer logit\" and \"indirect logit\") are based on the log-probabilities that a language model assigns to tokens. Thus they cannot be used for models without a tractable likelihood on outputs (e.g. information retrieval models that call out to external resources). By contrast, verbalized probabilities apply to any model that outputs natural language. Moreover, verbalized probabilities mirror human expression of uncertainty. This allows models to respond to prompts from non-technical users (e.g. \"How sure are you about what you just said?\", \"I've told you my confidence on a scale from 1-5. Can you do the same?\"). This also allows models to decide when and how to provide uncertainty information (depending on the human audience).",
            "score": 0.606845921832164,
            "section_title": "Calibration and Three Kinds of Probability",
            "char_start_offset": 5772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 333
                },
                {
                    "start": 336,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2016
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11102294921875
        },
        {
            "corpus_id": "268876453",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "text": "Dataset. We have conducted experiments on four MCQA datasets to verify the effectiveness of our proposed confidence estimation method. ARC (Clark et al., 2018) is a dataset of 7,787 grade-school-level questions. We use the test split of the ARC-Challenge with 1,172 questions for our experiments. MMLU (Hendrycks et al., 2021) is a dataset designed to measure knowledge acquired during pretraining and covers 57 subjects. TruthfulQA (Lin et al., 2022b) is a dataset that contains 817 questions designed to evaluate language models' preference to mimic some human falsehoods. All the experiments are conducted under a 0-shot setting. \n\nMetrics. We utilize multiple metrics to evaluate. We bin the predictions from the model by their confidence and report the ECE (expected calibration error). We also report the Brier Score of different methods in Table 11. In this paper, we also defines two novel metrics to evaluate the calibration. The first one is IPR (Inverse Pair Ratio), which is used to measure the monotonicity of the reliability diagram. If the reliability diagram is monotonic, it indicates that the average accuracy of answers with low confidence is lower than the average accuracy of answers with high confidence. \n\nwhere IP is the inverse pair number in the reliable diagram, and K is the bin number with a density larger than 0. We found that as the number of model parameters increases, the accuracy of the model improves across various datasets. However, language models still tend to consistently express uncertainty within certain fixed ranges, and ECE cannot clearly reflect this phenomenon. Therefore, we suggest using the CE (Confidence Evenness) to evaluate the uniformity of the density of each bar in the reliability diagram. \n\nIn this paper, we adopt 10 equal-size bins to calculate ECE 10 , IPR 10 and CE 10 . We also report the accuracy on these benchmarks to measure whether calibration reduces the accuracy. \n\nBaselines. We compared our approach with different baselines for eliciting the confidence of language model. First, we reproduced the Verb and Ling method proposed by Tian et al. (2023).",
            "score": 0.6062150178079365,
            "section_title": "Experimental Setting",
            "char_start_offset": 12468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1937
                },
                {
                    "start": 1940,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034423828125
        },
        {
            "corpus_id": "273345661",
            "title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF",
            "text": "This paper addresses the issue of overconfidence in RLHF-LLMs by identifying a systematic bias in reward models that favors high-confidence responses, regardless of their actual quality. A RELATED WORKS LLM Calibration. Model Calibration aims to align a model's confidence with its accuracy. It has been observed that modern neural networks, including Large Language Models (LLMs), often exhibit overconfidence, suggesting poor calibration (Tian et al., 2023;Chen et al., 2024a;Xiong et al., 2023;Achiam et al., 2023). Previous studies have explored methods like scaling-based (Deng et al., 2023;Guo et al., 2017;Zhang et al., 2020) approaches and nonparametric methods such as binning (Zadrozny & Elkan, 2001). Among these, temperature scaling (Guo et al., 2017;Zhang et al., 2020) has been proven to be effective when combined with large pre-trained LLMs (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023). However, previous evaluations focus on probabilities derived from model logits (Hendrycks et al., 2020;Mukhoti et al., 2020;Guo et al., 2017;Minderer et al., 2021), which can sometimes be inaccessible in proprietary models and unclear to human users. \n\nRecently, verbalized confidence has been introduced (Lin et al., 2022), prompting models to directly output confidence scores alongside responses. While most studies focus on calibrating pre-trained LLMs through supervised fine-tuning (Lin et al., 2022;Han et al., 2024), which typically involves sampling responses and calculating average accuracy as the estimation for ground truth confidence scores, other studies have examined verbalized confidence in instruction fine-tuned and RLHF-trained LLMs, and propose calibration through prompting strategies (Xiong et al., 2023;Tian et al., 2023).",
            "score": 0.6056690998271924,
            "section_title": "CONCLUSION",
            "char_start_offset": 24478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1765
                }
            ],
            "ref_mentions": [
                {
                    "start": 596,
                    "end": 613,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 613,
                    "end": 632,
                    "matchedPaperCorpusId": "212725167"
                },
                {
                    "start": 686,
                    "end": 710,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 745,
                    "end": 763,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 763,
                    "end": 782,
                    "matchedPaperCorpusId": "212725167"
                },
                {
                    "start": 1021,
                    "end": 1042,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 1042,
                    "end": 1059,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1059,
                    "end": 1081,
                    "matchedPaperCorpusId": "235435823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2064208984375
        },
        {
            "corpus_id": "267412346",
            "title": "Calibration and Correctness of Language Models for Code",
            "text": "Li et al. [49] investigate the calibration of Computer vision (CV) models from an operational perspective i.e., the shift between training input and production inputs, presenting it as a software quality problem that can be addressed using Bayesian approaches. Minderer et al. [45] evaluate the calibration of at the time, state of the art CV models and find improved calibration with more recent models, notably those not using convolutions. Park et al. [46] study the effect of the mixup technique [62] on calibration in a natural language understanding (NLU) setting using older generation models (BERT and RoBERTa). Chen et al. [47] investigate the calibration of pretrained language models on various NLP tasks, also using older generation models (RoBERTa and T5). Bommasani et al. [48] introduce the HELM benchmark, which includes calibration as one of its seven metrics to evaluate language models in a natural language context. Huang et al. [63] explored LM uncertainty with a range of techniques and tasks, including both NLP and function synthesis tasks. They evaluated using correlation measures, rather than focusing on calibration. They explore interesting sample-based and perturbation techniques which could be explored more for calibration on diverse SE tasks. Other work [64] has explored training an ML model that sees code and execution results to estimate correctness probabilities for solution reranking. For natural language question answering tasks, work has explored improving calibration by training a model to adjust token logits [53], and training a model from LLM hidden states specifically around the ECE metric [65]. \n\nWhen suitably prompted, Kadavath et al. [18] found that LLMs can output well-calibrated scores on whether their own answers are correct or not, viz., larger models \"know what they know\". While this work did investigate some function synthesis tasks (HUMANEVAL & an unpublished Python function dataset), they did so using only their private models, and ultimately focused on natural language tasks.",
            "score": 0.6044195052996946,
            "section_title": "VII. RELATED WORK",
            "char_start_offset": 46230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "203838412"
                },
                {
                    "start": 277,
                    "end": 281,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "247450599"
                },
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 1288,
                    "end": 1292,
                    "matchedPaperCorpusId": "256900680"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 1641,
                    "end": 1645,
                    "matchedPaperCorpusId": "270620078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11126708984375
        },
        {
            "corpus_id": "247594202",
            "title": "Calibration of Machine Reading Systems at Scale",
            "text": "With recent advances in machine reading, there has been a surge of interest in practical applications of the technology such as open-domain question answering (Karpukhin et al., 2020; and claim verification (Thorne et al., 2018b). Due to various scale limitations in practical settings, these systems are seldom trained endto-end. Such systems typically make use of a RE-TRIEVER alongside a READER -the evidence is first retrieved from a large corpus and is then used by a machine reading model to provide an answer.\n\nAs these systems are increasingly being deployed in the real world, it is important that they are not only accurate but also trustworthy. A way to make these systems trustworthy is to indicate when they are likely to be incorrect by providing a calibrated confidence measure in addition to the prediction.\n\nA naive solution for this is to use the system's output probability as the confidence. However, this confidence score is often uncalibrated (Kuleshov and Liang, 2015;Guo et al., 2017); i.e. it is not representative of the true correctness likelihood. 1 Previous work (Jiang et al., 2020;Jagannatha and Yu, 2020;Desai and Durrett, 2020) has shown that large language models especially suffer from miscalibration. Thus, several methods have been proposed to calibrate language models based on gradient-based calibration methods such as temperature scaling (Guo et al., 2017) and featurebased forecasters (Kuleshov and Liang, 2015). While gradient-based calibration is intuitive and easy to implement, feature-based forecasters require manual feature engineering.\n\nIn this work, we contribute a simple method to calibrate practical RETRIEVER -READER machine reading pipelines. These systems typically include a hard retrieval step which makes gradientbased calibration infeasible. Thus, we make use of the Gumbel machinery (Jang et al., 2017;Maddison et al., 2017); specifically the Gumbel top-K procedure of Vieira (2014); Xie and Ermon (2019) to obtain a differentiable sampling routine for the retrieval step. This sampler can then be combined with any gradient-based calibration technique such as Platt's scaling.\n\nWe conduct",
            "score": 0.601466890799443,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 965,
                    "end": 991,
                    "matchedPaperCorpusId": "2974522"
                },
                {
                    "start": 1092,
                    "end": 1112,
                    "matchedPaperCorpusId": "208513249"
                },
                {
                    "start": 1427,
                    "end": 1453,
                    "matchedPaperCorpusId": "2974522"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06573486328125
        },
        {
            "corpus_id": "267682759",
            "title": "Addressing the Binning Problem in Calibration Assessment through Scalar Annotations",
            "text": "With recently released large-scale language models (LLMs) demonstrating impressive few-shot, zero-shot, and task-agnostic performance (Brown et al., 2020;Kojima et al., 2022;Ouyang et al., 2022), there is a boom of interest in deploying NLP-based systems to aid various human decision making (Chen et al., 2021;Nori et al., 2023). However, the black-box nature of LLMs gives little insight into how the predictions are made by these models (Zhao et al., 2021), risking user trust in model prediction reliability. \n\nA common proposal to address this concern is to explore model calibration (Guo et al., 2017;Kull et al., 2019), which requires a model to approximately predict the true label distribution. This evaluation has been adopted by many recent language model benchmarking efforts (Desai and Durrett, 2020;Hendrycks et al., 2020;Jiang et al., 2022;OpenAI, 2023); these works often consider confidence calibration for classification and adopt Expected Calibration Error (ECE) (Guo et al., 2017) as the main empirical evaluation metric. ECE, along with variants like Adaptive Calibration Error (ACE) (Nixon et al., 2019), involve binning in their calculation, which groups hard categorical labels into bins to approximate label distributions. This is mainly because many popular NLP tasks are annotated predominantly with categorical labels. However, these empirical evaluations are sensitive to the choice of binning schemes (Nixon et al., 2019), and can severely underestimate calibration error (Ovadia et al., 2019;Kumar et al., 2019;Baan et al., 2022). \n\nInstance-level calibration (Zhao et al., 2020) avoids the binning issue and matches model confidence with human annotations at an individual level, as uncertainty from human annotations is a good surrogate for true label distribution (Nie et al., 2020b;Baan et al., 2022).",
            "score": 0.6013400603078015,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1561
                },
                {
                    "start": 1564,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 194,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 440,
                    "end": 459,
                    "matchedPaperCorpusId": "235828718"
                },
                {
                    "start": 589,
                    "end": 607,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 607,
                    "end": 625,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 788,
                    "end": 813,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 836,
                    "end": 855,
                    "matchedPaperCorpusId": "250551977"
                },
                {
                    "start": 982,
                    "end": 1000,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1105,
                    "end": 1125,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1431,
                    "end": 1451,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1523,
                    "end": 1542,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 1542,
                    "end": 1560,
                    "matchedPaperCorpusId": "253224378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2301025390625
        },
        {
            "corpus_id": "276235552",
            "title": "Confidence Elicitation: A New Attack Vector for Large Language Models",
            "text": "Multiple studies have explored calibration in language models. A common method, which has been thoroughly explored in previous work (Guo et al., 2017a;Jiang et al., 2021) involves using output probabilities as a proxy for confidence. This could be implemented by focusing on the first generated vector for a specific token, by adding a binary classification prediction head that utilizes the last generated token (Kadavath et al., 2022), focusing on the answer specific token or take the average of the probabilities across the whole sequence, these techniques have been classified as white-box confidence estimation. While these approaches could be effective, several challenges arise. Firstly output logits or probabilities may not be accessible, particularly with proprietary models. Secondly the likelihood of the next token primarily signifies lexical confidence and not epistemic uncertainty (Lin et al., 2022), and therefore, struggles to capture the semantic uncertainty in the entire text (Xiong et al., 2024). \n\nAs a result, previous work highlighted the need for models capable of directly expressing uncertainty in natural language in a black-box setting. Some research has explored enhancing calibration by empirically deriving confidence through repetitive model querying (Portillo Wightman et al., 2023). Alternatively, models can be prompted to express their confidence verbally, either through verbalized numerical confidence elicitation (Xiong et al., 2024) or verbal confidence elicitation (Lin et al., 2022). It has been found that some prompts can achieve reasonable uncertainty quantification, especially by querying the model twice, first for the prediction, and the second time for the uncertainty estimates (Tian et al., 2023) (Example of a prompt for confidence elicitation is in Table 6 in the Appendix).",
            "score": 0.6000175989779556,
            "section_title": "CONFIDENCE ELICITATION",
            "char_start_offset": 8691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1831
                }
            ],
            "ref_mentions": [
                {
                    "start": 998,
                    "end": 1018,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 1286,
                    "end": 1318,
                    "matchedPaperCorpusId": "260063200"
                },
                {
                    "start": 1455,
                    "end": 1475,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 1732,
                    "end": 1751,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140869140625
        },
        {
            "corpus_id": "277150701",
            "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey",
            "text": "Uncertainty quantification and confidence estimation are closely related yet distinct concepts in the context of large language models (LLMs). Uncertainty is a property of the model's predictive distribution, capturing the degree of variability or unpredictability given a particular input. In contrast, confidence reflects the model's belief in the correctness of a particular answer or prediction. As a concrete example, in the context of classification, a simple confidence measure is the predicted probability \u02c6 ( = | ) (an uncertainty measure which does not depend on the particular prediction could be entropy, taking the form of \u2212 \u02c6 ( = | ) log \u02c6 ( = | )). The corresponding confidence score in NLG (for an auto-regressive LM) is the joint probability for the generated sequence: \n\nThe log of Eq. ( 1) is sometimes referred to as sequence likelihood [161]. In general, while an uncertainty estimate takes the form of ( ), confidence estimates could be expressed as (x, s). \n\nNote that unlike classification tasks, not all NLG applications have the notion of a \"correct\" answer (e.g. summarization). Thus, while for the ease of writing we use the term correctness throughout this section, it should really be interpreted as the gold-label for the particular application. Note also that in most cases, the correct answer is not unique, and thus such gold-label typically takes the form of a \"correctness function\" that decides whether a particular generation s is good or not. We will denote such a function as (s|x). \n\nThere are usually two dimensions along which researchers improve confidence estimates in NLG, which is unsurprisingly largely influenced by confidence scoring literature from classification [15,54], especially binary classification. We refer to them as ranking performance and calibration: \n\n\u2022 Ranking performance refers to the discriminative power of the confidence measure on the correctness. Like in classification, LLM confidence is often evaluated by its ability to separate correct and incorrect answers, thus typically measured by evaluation metrics like AUROC [59] or AUARC [78] as detailed in Section 4. \n\n\u2022 Calibration refers to closing the gap between the confidence score and the expected correctness conditioned on confidence score.",
            "score": 0.5994785027939503,
            "section_title": "Uncertainty and Confidence in LLMs",
            "char_start_offset": 6631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2137
                },
                {
                    "start": 2140,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 857,
                    "end": 862,
                    "matchedPaperCorpusId": "252683988"
                },
                {
                    "start": 1715,
                    "end": 1719,
                    "matchedPaperCorpusId": "204401816"
                },
                {
                    "start": 1719,
                    "end": 1722,
                    "matchedPaperCorpusId": "44100821"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4013671875
        },
        {
            "corpus_id": "268876453",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "text": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Then, we propose a plug-and-play method, UF Calibration, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on Truly Well-Calibrated Confidence for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.",
            "score": 0.5988920816462342,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402099609375
        },
        {
            "corpus_id": "268889516",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "text": "Language Models (LMs), especially Large Language Models (LLMs), have shown promising performance in Natural Language Generation (NLG). These models, fitted on huge text corpora, can produce responses resembling those of humans (Touvron et al., 2023b;OpenAI, 2023). However, since LMs often generate wrong or hallucinated responses (Weidinger et al., 2021;Xiao and Wang, 2021;Huang et al., 2024), it is crucial to correctly quantify their level of uncertainty in responding to particular inputs. Uncertainty quantification is well-explored in supervised learning, specifically in classification (e.g., Lichtenstein et al., 1977;Gal and Ghahramani, 2016;Lakshminarayanan et al., 2017, etc). In classification, a confidence measure is an estimate of the probability that the predicted class Y matches the true class label Y (Lichtenstein et al., 1977;Lee et al., 2023). A confidence measure C is considered calibrated if it reflects the probability of correct prediction, i.e., P( Y = Y | C) = C, for all values in C's range. The Expected Calibration Error (ECE) measures the miscalibration of a confidence measure (Harrell, 2015;Naeini et al., 2015): \n\nLMs produce potentially long textual responses Y for any given input. Second, for LMs, logits encode the likelihood of selecting the next token and do not necessarily capture linguistic sense (Mielke et al., 2022). Third, even hand-crafted prompts intended to make LMs express confidence explicitly may not lead to reliable confidence values because elicitation is heavily tied to prompt formats (Zhao et al., 2021;Xiong et al., 2024). \n\nRecent works have studied uncertainty measures as an alternative to confidence measures. These capture the \"dispersion\" of an LMs' potential outputs for a fixed input. Kuhn et al. (2023) introduce semantic entropy, which incorporates linguistic invariances arising from the shared meaning of generated responses.",
            "score": 0.598722296379171,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1901
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1810302734375
        },
        {
            "corpus_id": "264405939",
            "title": "Fast and Accurate Factual Inconsistency Detection Over Long Documents",
            "text": "We evaluate Calibration and Relevant Source Text Retrieval for model explanation. \n\nCalibration is the measure of how close the pseudo-probability outputs of a model are to the actual probability of a correct prediction. For example, a well calibrated model that produces a score of 0.2 would have a 20% probability of being classified as 1. In this paper, we use Expected Calibration Error (ECE) (Guo et al., 2017) to compare the calibration of SCALE to other commonly used models. \n\nGiven model outputs spanning from 0 to 1, ECE separates the outputs into K equally sized bins B k between 0 and 1 and takes the difference between accuracy acc and confidence conf in each one. The accuracy of a bin B k is the average amount of predicted labels that match true class labels in a bin, formally defined as \n\nwhere \u0177i and y i are the predicted and true class labels for sample i. Confidence in a bin B k shows the average predicted score in a bin, formally defined as conf \n\nwhere pi is the model output score for sample i. \n\nThen the following equation is used to calculate ECE, \n\nusing equation ( 1) and ( 2). A lower ECE indicates a better calibration. \n\nRelevant Source Text Retrieval tests if each model could return the correct utterance identified as relevant by human labelers. We report the recall of retrieval results.",
            "score": 0.5968623229841072,
            "section_title": "Model Explanation Evaluation",
            "char_start_offset": 17200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 84,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1153
                },
                {
                    "start": 1156,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1326
                }
            ],
            "ref_mentions": [
                {
                    "start": 397,
                    "end": 415,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056976318359375
        },
        {
            "corpus_id": "265351565",
            "title": "On the Calibration of Large Language Models and Alignment",
            "text": "In early meteorology, calibration was noted as validity (Miller, 1962) or reliability (Murphy, 1973), indicating the trustworthiness of forecasters. Well calibrated probabilities can provide extra information for users to decide whether to trust the model's prediction, particularly for modern neural networks whose decisions are harder to interpret (Guo et al., 2017). Studies have also pointed out that calibration is helpful to reduce hallucination in language models (Xiao and Wang, 2021;Tian et al., 2020). Previous works have shown that pre-trained language models can generate well-calibrated predictions (Desai and Durrett, 2020;Kadavath et al., 2022). However, these works mainly concentrate on vanilla language models, while the aligned language models receive less focus. A newly proposed work evaluates calibration of some aligned models by prompting them to verbalize confidence in the token space (Tian et al., 2023), but it mainly studies black-box models, whose training process is not available, and thus can not provide insight into how model calibration is affected by different factors in the alignment training process. To conclude, a systematical study on the calibration of aligned language models is still missing, and our work aims to fill this gap. \n\nIn this work, we study the calibration of aligned language models in the entire building cycle and provide evidence on how to achieve decent model calibration. An overview of the scheme of out study is at Figure 1 Besides the understanding and generating ability, factual faithfulness and reasoning capability are two widely considered issues with large language models (Du et al., 2023). We also follow this path to study models' calibration when applied to different tasks. For this purpose, we design three tasks for each of the stages above. (1) To evaluate model calibration on common text generation, we use Causal Language Modeling (CLM) task, which is also the objective of pre-training stage. \n\n(2) To study model calibration on factuality, we designed a facts generation task where the models are asked to generate fact-related content.",
            "score": 0.5946724807752622,
            "section_title": "Introduction",
            "char_start_offset": 1955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1978
                },
                {
                    "start": 1981,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 100,
                    "matchedPaperCorpusId": "121053719"
                },
                {
                    "start": 350,
                    "end": 368,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1246337890625
        },
        {
            "corpus_id": "247613322",
            "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
            "text": "PLMs (Qiu et al., 2020;Min et al., 2021) have achieved state-of-the-art performance on a broad spectrum of NLP benchmarks (Rajpurkar et al., 2016(Rajpurkar et al., , 2018Wang et al., 2019a,b) and are increasingly popular in various downstream applications such as question answering (Yoon et al., 2019;Garg et al., 2020), text classification (Arslan et al., 2021;Limsopatham, 2021), and relation extraction (Zhou et al., 2021;Xiao et al., 2022). Consequently, it is paramount for PLMs to faithfully communicate when to (or not to) rely on their predictions for decision-making, especially in high-stakes scenarios. In these cases, we need PLMs to quantify their uncertainty accurately and calibrate well (Abdar et al., 2021), meaning that their predictive confidence should be a valid estimate of how likely they are to make a correct prediction. Consider an example of medical question answering (Yoon et al., 2019; where a PLM is asked to assist doctors when diagnosing diseases. If the PLM is 90% sure that a patient is healthy, the predicted outcome should occur 90% of the time in practice. Otherwise, it may adversely affect doctors' judgment and lead to catastrophic consequences. Hence, since PLMs have become the de facto paradigm for many NLP tasks, it is necessary to assess their calibration quality.\n\nWhen constructing a well-calibrated PLM-based prediction pipeline for NLP tasks, various considerations are involved. To name a few:\n\n1. Due to the use of diverse pre-training datasets and strategies, different PLMs may behave differently regarding calibration. 2. The model size of PLMs may also affect their capability in calibration. 3. Leveraging uncertainty quantifiers (e.g., Temp\n\nScaling (Guo et al., 2017) and MC Dropout (Gal and Ghahramani, 2016)) alongside PLMs in the pipeline may reduce calibration error. 4. Some losses (e.g., Focal Loss (Mukhoti et al., 2020) and Label Smoothing (M\u00fcller",
            "score": 0.5945368872947971,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 23,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 122,
                    "end": 145,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 145,
                    "end": 170,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 283,
                    "end": 302,
                    "matchedPaperCorpusId": "202660890"
                },
                {
                    "start": 302,
                    "end": 320,
                    "matchedPaperCorpusId": "207853043"
                },
                {
                    "start": 342,
                    "end": 363,
                    "matchedPaperCorpusId": "235324756"
                },
                {
                    "start": 363,
                    "end": 381,
                    "matchedPaperCorpusId": "241583607"
                },
                {
                    "start": 407,
                    "end": 426,
                    "matchedPaperCorpusId": "225039888"
                },
                {
                    "start": 426,
                    "end": 444,
                    "matchedPaperCorpusId": "237635295"
                },
                {
                    "start": 897,
                    "end": 916,
                    "matchedPaperCorpusId": "202660890"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230712890625
        },
        {
            "corpus_id": "270869459",
            "title": "Calibrated Large Language Models for Binary Question Answering",
            "text": "We will refer to these tokens as answer-tokens.Although these scores are an indication of the token's likelihood to be the true label (or answer), they cannot be directly interpreted as well-calibrated probabilities, since softmax does not guarantee any validity or calibration property.\n\nCalibration We refer to Guo et al. (2017) and define calibration in the following way: given a prediction \u0176 for the label Y , returned with an estimated confidence P , an ML model is perfectly calibrated if\n\nFor instance, let us assume our model made 100 predictions, each with estimated probability P = 0.75.If the model is perfectly calibrated, exactly 75 out of those 100 predictions need to be correct.In our scenario, a well-calibrated model would output probabilities for the \"Yes\"\n\ntoken that reflect the true rate of positive labels in the test set.Simply applying a softmax function to the raw logits is not enough in most cases, and predictions from LLMs are often poorly calibrated.Moreover, using softmax does not provide a measure of confidence in the probability estimates themselves -a property generally enjoyed by imprecise probabilities (Destercke, 2022).\n\nTo overcome these limitations, we employ a recently developed calibration method, which we describe in the following section.",
            "score": 0.5944966658498505,
            "section_title": "Background",
            "char_start_offset": 6586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 47,
                    "end": 287
                },
                {
                    "start": 289,
                    "end": 495
                },
                {
                    "start": 497,
                    "end": 598
                },
                {
                    "start": 598,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 776
                },
                {
                    "start": 778,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1162
                },
                {
                    "start": 1164,
                    "end": 1289
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 330,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1144,
                    "end": 1161,
                    "matchedPaperCorpusId": "253532829"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.290283203125
        },
        {
            "corpus_id": "273821704",
            "title": "Graph-based Confidence Calibration for Large Language Models",
            "text": "For example, p(C|q) is approximately the probability of correctness: \n\nWith this assumption, the more certain the model is about a semantic response, the more likely the response is correct. Conversely, a wide variation in the LLM's responses indicates low confidence in all responses r i and low accuracy. This pattern is also found in previous studies (Kuhn et al., 2022;Lin et al., 2024). \n\nWhile a positive correlation exists between the LLM's confidence and the probability of correctness, the two quantities are unlikely to have the equal relationship shown in (3). Therefore, we need further calibration to estimate the probability of correctness. The overall framework of our confidence calibration model. Given an input question, our approach first generates multiple responses from the LLM and constructs a similarity-weighted graph based on these responses. This graph serves as the input for the GNN model, which calibrates the confidence of the LLM responses. In the weighted graph, the edge weight w ij is defined as sim cos (emb(r i ), emb(r j )), where i, j = 1, . . . , n. A higher weight indicates greater similarity between the responses. We also use cluster memberships as the node features to enhance the performance.",
            "score": 0.5938478097039964,
            "section_title": "Background:",
            "char_start_offset": 11311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 71,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1238
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 373,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 373,
                    "end": 390,
                    "matchedPaperCorpusId": "258967487"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06805419921875
        },
        {
            "corpus_id": "249191391",
            "title": "Teaching Models to Express Their Uncertainty in Words",
            "text": "Current state-of-the-art language models perform well on a wide range of challenging question-answering tasks (Brown et al., 2020;Chowdhery et al., 2022;Hoffmann et al., 2022). They can even outperform the average human on the MMLU benchmark (which consists of exam-like questions across 57 categories) and on BIG-Bench (which consists of 150+ diverse tasks). Yet when models generate long-form text, they often produce false statements or \"hallucinations\" (Lin et al., 2021;Maynez et al., 2020;Shuster et al., 2021). This reduces their value to human users, as users cannot tell when a model is being truthful or not. \n\nThe problem of truthfulness motivates calibration for language models (Nguyen & O'Connor, 2015). If models convey calibrated uncertainty about their statements, then users know how much to trust a given statement. This is important for current models (which often hallucinate falsehoods) but also for any model that makes statements where there is no known ground truth (e.g. economic forecasts, open problems in science or mathematics). \n\nPrevious work on calibration focuses on the model log-probabilities or \"logits\" (Guo et al., 2017;Jiang et al., 2021). Yet the log-probabilities of models like GPT-3 represent uncertainty over tokens (ways of expressing a claim) and not epistemic uncertainty over claims themselves. If a claim can be paraphrased in many different ways, then each paraphrase may have a low log-probability.1 By contrast, when humans express uncertainty, this is epistemic uncertainty about the claim itself. 2 In this paper, we finetune models to express epistemic uncertainty using natural language. We call this \"verbalized probability\". The model is prompted with a question and outputs an answer ('3' in this case) and a level of confidence in its answer ('Medium') using greedy decoding. The model is scored on the MSE of confidence (not on MSE or accuracy of answer).",
            "score": 0.592514320743496,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 691,
                    "end": 716,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 1159,
                    "end": 1178,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058135986328125
        },
        {
            "corpus_id": "270214143",
            "title": "Selectively Answering Visual Questions",
            "text": "A QA system is said to be well-calibrated when each prediction has a confidence score which can help assess how often it is correct.Formally, the paradigm of selective QA uses a scoring function s that attaches to each QA pair (q, a) a numeric score s(q, a).The score can be compared with a threshold \u03c4 so that the system answers-aka triggers-when s(q, a) > \u03c4 or abstains otherwise.As shown in Figure 1, various scoring methods can be used.We present first the methods from Cole et al. (2023a) for text-only QA, and then our proposal.\n\nLikelihood The likelihood-based calibration uses the output language model score p(a|q) computed using the chain rule: p(a|q) = n i=1 p(t i |t 1 , \u2022 \u2022 \u2022 , t i\u22121 , q) where the t i are the tokens that form the answer a.\n\nSampling Repetition Based on Wilcox (1973)'s Variation Ratio we compute the frequency of the most sampled output divided by the total number of samples, which coincides with the probability of the mode of the empirical distribution.When more samples agree with each other, the answer can be considered to be more trustworthy.\n\nSampling Diversity Computed as 1 \u2212 #unique #samples , it is inversely proportional to the number of distinct samples and is zero if all samples are different.",
            "score": 0.5911447821665418,
            "section_title": "Methods",
            "char_start_offset": 4880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 258
                },
                {
                    "start": 258,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 534
                },
                {
                    "start": 536,
                    "end": 754
                },
                {
                    "start": 756,
                    "end": 988
                },
                {
                    "start": 988,
                    "end": 1081
                },
                {
                    "start": 1083,
                    "end": 1241
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 493,
                    "matchedPaperCorpusId": "258866001"
                },
                {
                    "start": 785,
                    "end": 798,
                    "matchedPaperCorpusId": "154845301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0567626953125
        },
        {
            "corpus_id": "273821704",
            "title": "Graph-based Confidence Calibration for Large Language Models",
            "text": "However, manually designed features are limited in their ability to capture the full extent of self-consistency among LLM responses, leading to poor calibration performance. \n\nTo better calibrate the confidence estimation, some methods directly use correctness labels in their calibration procedures. Mielke et al. (2022) train a calibrator to predict the correctness of a response for a given question. With a similar idea, Ulmer et al. (2024) train a language model (e.g., DeBERTa) based on question-response pairs to predict the probability of responses' correctness. Based on SelfCheckGPT (Manakul et al., 2023) and JAFC (Tian et al., 2023), Chen et al. (2024) train supervised models to reduce grouping losses and improve the confidence estimation. The method by Liu et al. (2024) uses an LLM's latent representations to predict the correctness of responses. Detommaso et al. (2024) use the \"multicalibration\" technique to calibrate the probability of correctness. Fadeeva et al. (2023) offer a detailed comparative study of various confidence estimation methods, providing empirical evidence on their effectiveness across different tasks. However, these studies have not sufficiently exploited response consistency to predict the probabilities of the responses being correct.",
            "score": 0.5911144097449227,
            "section_title": "Related Work",
            "char_start_offset": 7867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1281
                }
            ],
            "ref_mentions": [
                {
                    "start": 301,
                    "end": 321,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 593,
                    "end": 615,
                    "matchedPaperCorpusId": "257557820"
                },
                {
                    "start": 625,
                    "end": 644,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1221923828125
        },
        {
            "corpus_id": "272423997",
            "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
            "text": "Black-box large language models (LLMs) are increasingly deployed in various environments, making it essential for these models to effectively convey their confidence and uncertainty, especially in high-stakes settings. However, these models often exhibit overconfidence, leading to potential risks and misjudgments. Existing techniques for eliciting and calibrating LLM confidence have primarily focused on general reasoning datasets, yielding only modest improvements. Accurate calibration is crucial for informed decision-making and preventing adverse outcomes but remains challenging due to the complexity and variability of tasks these models perform. In this work, we investigate the miscalibration behavior of black-box LLMs within the healthcare setting. We propose a novel method, \\textit{Atypical Presentations Recalibration}, which leverages atypical presentations to adjust the model's confidence estimates. Our approach significantly improves calibration, reducing calibration errors by approximately 60\\% on three medical question answering datasets and outperforming existing methods such as vanilla verbalized confidence, CoT verbalized confidence and others. Additionally, we provide an in-depth analysis of the role of atypicality within the recalibration framework.",
            "score": 0.5908674107877752,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11517333984375
        },
        {
            "corpus_id": "274965235",
            "title": "The Reliability Paradox: Exploring How Shortcut Learning Undermines Language Model Calibration",
            "text": "The advent of pre-trained language models (PLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found PLMs to suffer from miscalibration, indicating a lack of accuracy in the confidence estimates provided by these models. Current evaluation methods for PLM calibration often assume that lower calibration error estimates indicate more reliable predictions. However, fine-tuned PLMs often resort to shortcuts, leading to overconfident predictions that create the illusion of enhanced performance but lack generalizability in their decision rules. The relationship between PLM reliability, as measured by calibration error, and shortcut learning, has not been thoroughly explored thus far. This paper aims to investigate this relationship, studying whether lower calibration error implies reliable decision rules for a language model. Our findings reveal that models with seemingly superior calibration portray higher levels of non-generalizable decision rules. This challenges the prevailing notion that well-calibrated models are inherently reliable. Our study highlights the need to bridge the current gap between language model calibration and generalization objectives, urging the development of comprehensive frameworks to achieve truly robust and reliable language models.",
            "score": 0.5879934705971201,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1590576171875
        },
        {
            "corpus_id": "277112828",
            "title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence",
            "text": "Assuming input x and output y, we are looking to find P (y \u2208 Y equivalent | x), the model's likelihood of producing this answer or one that is semantically equivalent; however, this would require marginalization over an infinite set of strings Y . \n\nTo make this a tractable problem, we use a Monte Carlo approximation, where our estimate of the models' predictive distribution improves with N , at the expense of additional computational cost. Note however that we do not assume this quantity is a meaningful probability out-of-the-box due to potential overfitting or underfitting of the base model. To diagnose potential miscalibration of the base model as well as correct for it, we may use held-out calibration data that was not seen during training. 2 n more detail, to fit a post-hoc calibrator, we need a supervised dataset of datapoints not seen at training time {X cal , Y cal }. For each example x \u2208 X cal we sample N candidate answers \n\n) from a model's predictive distribution3 . Before calculating the relative frequency of strings, we apply a normalization function (or set of normalization functions) to consolidate semantically similar outputs. In the short-form QA tasks we consider in \u00a74, we use the simple normalization function of isolating a multiple choice answer using tags, removing punctuation and standardizing capitalization, but for more complex questions other approaches may be required (Huang et al., 2024). For example, Kuhn et al. (2023) use natural language inference to cluster semantically equivalent answers and Tian et al. (2023) use an LLM as a judge of correctness. After consolidating similar events, the relative frequency f of these events is a measure of the LLM's uncertainty in those events, although this may not be a well-calibrated probability.",
            "score": 0.5877960865087538,
            "section_title": "Monte Carlo sampling with semantic normalization",
            "char_start_offset": 10742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1793
                }
            ],
            "ref_mentions": [
                {
                    "start": 1549,
                    "end": 1567,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044097900390625
        },
        {
            "corpus_id": "273228151",
            "title": "QA-Calibration of Language Model Confidence Scores",
            "text": "Calibration for Language Models. Reinforcement learning from human feedback objective may prioritize adherence to user instructions in dialogue over producing well-calibrated predictions. (Kadavath et al., 2022). Lin et al. (2022) introduced the concept of verbalized confidence that prompts LMs to express confidence directly, focusing on fine-tuning, instead of zero-shot verbalized confidence. Mielke et al. (2022) uses an external calibrator for a white-box large language model. Other methods use consistency measures to improve LM calibration (Lyu et al., 2024). Our experimental setup closely relates to recent works in LM confidence elicitation (Tian et al., 2023;Xiong et al., 2024). These methods lack novel posthoc calibrators and do not offer the rigorous calibration guarantees that ours provide. Calibration has been shown to impact selective QA performance Kamath et al. ( 2020), but they focus on uncertainty quantification and assumes that the LM allows access to the model likelihood. \n\nGroup Notions of Calibration. Previous works highlight the limitations of average-case calibration. Group-wise calibration, which uses predefined groupings (Kleinberg et al., 2017;Pleiss et al., 2017), has been adapted for language models (LMs). Li et al. ( 2024) train a model that approximates the precision-threshold curve for a given group by using few-shot samples to predict the LM's empirical precision at various confidence thresholds. Ulmer et al. (2024) train an auxiliary model using accuracy per group as target to predict an LM's confidence based on textual input and output. Detommaso et al. (2024) achieves multicalibration -simultaneous calibration across various intersecting groupings of the data. Our work complements multicalibration, and our methods could extend to this by adapting Algorithm 3 in Detommaso et al. (2024). Luo et al. (2022) measure calibration over a set of similar predictions, quantified by a kernel function on feature space.",
            "score": 0.5877960865087538,
            "section_title": "RELATED WORK",
            "char_start_offset": 23662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 230,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 397,
                    "end": 417,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 653,
                    "end": 672,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 672,
                    "end": 691,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 1161,
                    "end": 1185,
                    "matchedPaperCorpusId": "12845273"
                },
                {
                    "start": 1185,
                    "end": 1205,
                    "matchedPaperCorpusId": "75455"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285888671875
        },
        {
            "corpus_id": "273654253",
            "title": "Rethinking the Uncertainty: A Critical Review and Analysis in the Era of Large Language Models",
            "text": "This section begins by offering a comprehensive definition of uncertainty and its associated concepts-confidence and reliability-within the context of large language models. As illustrated in Figure 1, although these concepts are interrelated, they pertain to distinct aspects of model performance that neces-sitate careful differentiation. We specifically emphasize the differences between uncertainty and confidence, terms that are often used interchangeably in the literature. Uncertainty fundamentally refers to the extent to which a model \"knows\" or \"does not know\" about a given input, based on the training it has received (Malinin & Gales, 2018;Der Kiureghian & Ditlevsen, 2009;H\u00fcllermeier & Waegeman, 2021;Gawlikowski et al., 2022a;Kendall & Gal, 2017). Often, this arises from inadequate or conflicting training data (Guo et al., 2022;H\u00fcllermeier & Waegeman, 2021;Mena et al., 2021), inappropriate model selection (Gawlikowski et al., 2022a;Mena et al., 2021;Battaglia et al., 2018), or factors like noise and inherent data ambiguity (Kendall & Gal, 2017;Mena et al., 2021;Guo et al., 2022). These factors collectively delineate a model's understanding-or misunderstanding-of its operational environment, influencing the reliability of its outputs. \n\nConfidence, often expressed as a predicted probability score, quantifies the likelihood that a model's prediction is correct. Derived from the softmax output applied to logits, this score assigns each class a probability between 0 and 1, with the highest probability indicating the model's chosen prediction (He et al., 2024;Guo et al., 2017;Nandy et al., 2021). However, confidence scores can be misleading; they may exhibit overconfidence, where the score is high despite inaccurate predictions, orunderconfidence, where scores are low even when predictions are correct (Lakshminarayanan et al., 2017;Wang, 2023;Chen et al., 2022;Guo et al., 2017). \n\nReliability. Merely estimating confidence scores is insufficient for safe decision-making.",
            "score": 0.5876585171317952,
            "section_title": "DEFINITION OF UNCERTAINTY AND RELATED CONCEPTS",
            "char_start_offset": 5606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2004
                }
            ],
            "ref_mentions": [
                {
                    "start": 630,
                    "end": 653,
                    "matchedPaperCorpusId": "3580844"
                },
                {
                    "start": 686,
                    "end": 715,
                    "matchedPaperCorpusId": "216465307"
                },
                {
                    "start": 845,
                    "end": 874,
                    "matchedPaperCorpusId": "216465307"
                },
                {
                    "start": 874,
                    "end": 892,
                    "matchedPaperCorpusId": "241258024"
                },
                {
                    "start": 951,
                    "end": 969,
                    "matchedPaperCorpusId": "241258024"
                },
                {
                    "start": 1065,
                    "end": 1083,
                    "matchedPaperCorpusId": "241258024"
                },
                {
                    "start": 1586,
                    "end": 1603,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1893,
                    "end": 1910,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0328369140625
        },
        {
            "corpus_id": "273821704",
            "title": "Graph-based Confidence Calibration for Large Language Models",
            "text": "One important approach to improving the reliability of large language models (LLMs) is to provide accurate confidence estimations regarding the correctness of their answers. However, developing a well-calibrated confidence estimation model is challenging, as mistakes made by LLMs can be difficult to detect. We propose a novel method combining the LLM's self-consistency with labeled data and training an auxiliary model to estimate the correctness of its responses to questions. This auxiliary model predicts the correctness of responses based solely on their consistent information. To set up the learning problem, we use a weighted graph to represent the consistency among the LLM's multiple responses to a question. Correctness labels are assigned to these responses based on their similarity to the correct answer. We then train a graph neural network to estimate the probability of correct responses. Experiments demonstrate that the proposed approach substantially outperforms several of the most recent methods in confidence calibration across multiple widely adopted benchmark datasets. Furthermore, the proposed approach significantly improves the generalization capability of confidence calibration on out-of-domain (OOD) data.",
            "score": 0.585758639368728,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31689453125
        },
        {
            "corpus_id": "273963769",
            "title": "Epistemic Integrity in Large Language Models",
            "text": "As research on estimating the internal confidence of LLMs has increased, scholars have started to focus on model calibration-i.e., the alignment between predicted probabilities and actual correctness. Desai & Durrett (2020) find that pre-trained transformers such as BERT often exhibit poor calibration out-of-the-box, where their confidence estimates fail to correspond to actual correctness. Jiang et al. (2021) also explore methods to improve model calibration, such as temperature scaling, which adjusts predicted probabilities to better match actual outcomes. However, most calibration methods focus on probabilistic outputs (internal confidence) without addressing how certainty is expressed through language generation. The same miscalibration is found by Si et al. (2022), who demonstrate that models like GPT-3 frequently produce overconfident responses even when incorrect. \n\nHowever, we note that despite these known limits, internal confidence scores remain largely inaccessible to users today. Without these scores, it becomes essential for models to effectively communicate uncertainty through external means-such as linguistic cues-to ensure users correctly interpret the model's output.",
            "score": 0.5849857208421204,
            "section_title": "Challenges in Confidence Alignment",
            "char_start_offset": 11412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1202
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 413,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10089111328125
        },
        {
            "corpus_id": "267211649",
            "title": "What large language models know and what people think they know",
            "text": "However, from the perspective of a human interacting with the LLM, this internal model confidence is not usually displayed to human users as part of LLM output. Instead, in current practice, humans rely solely on the language produced by the LLM in order to assess LLM confidence. To contrast with model confidence, in this paper we use the term human confidence to refer to a human's assessment (expressed as a probability) of how likely it is that the LLM's answer is correct based only on the language produced by the LLM without any knowledge of the LLM's internal model confidence. \n\nSurprisingly, studies focused on investigating human confidence in LLM outputs are lacking. In this paper, we take a step in addressing this issue and investigate what we term the calibration gap, namely the difference in the reliability of (i) internal LLM model confidence and (ii) external human confidence. In effect, the calibration gap represents the gap between an LLM's own internal confidence of what it knows and human perception of this confidence. We address two specific research questions in this context: \n\n1. How Large is the Calibration Gap? i.e., is there a significant gap between LLM model confidence and human confidence, in terms of how each assesses the true accuracy of the LLM? \n\n2. Can the Calibration Gap be Reduced? e.g., can the quality of human confidence in an LLM be improved by adapting the textual output of the LLM and by leveraging internal model confidence? \n\nFigure 1: Overview of the evaluation methodology for assessing the calibration gap between model confidence and human confidence in the model. The approach works as follows: \n\n(1) prompt the LLM with a multiple-choice question to obtain the model's internal confidence for each answer choice; \n\n(2) select the most likely answer and prompt the model a second time to generate an explanation for the given answer; \n\n(3) obtain the human confidence by showing users the question and the LLM's explanation and asking users to indicate the probability that the model is correct. In this toy example the model confidence is 0.46 for answer C, whereas the human confidence in 0.95 \n\nThese questions have important implications for the design of reliable LLM assistants.",
            "score": 0.5849314381928591,
            "section_title": "Introduction",
            "char_start_offset": 1894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2160
                },
                {
                    "start": 2163,
                    "end": 2249
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1295166015625
        },
        {
            "corpus_id": "268358242",
            "title": "Calibrating Large Language Models Using Their Generations Only",
            "text": "As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge. We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively in terms of calibration error for white-box and black-box LLMs on closed-book question-answering to detect incorrect LLM answers.",
            "score": 0.5837838240511235,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2578125
        },
        {
            "corpus_id": "271571332",
            "title": "Accelerating Large Language Model Inference with Self-Supervised Early Exits",
            "text": "The calibration starts with the selection of an appropriate confidence metric. Our experimental setup tested several metrics: the maximum probability value, the entropy of the probability distribution, and the difference between the two highest probability scores from the classification vector. The latter, known as the \"breaking ties\" metric, was selected due to its superior empirical performance. Mathematically, the breaking ties metric for a probability vector p is defined as: \n\nwhere p (1) and p (2) are the highest and second-highest entries in p, respectively. \n\nTo calibrate the confidence thresholds, we use a calibration batch provided in Table 1. This batch is composed of diverse starting points designed to cover different linguistic contexts. \n\nFor each starting point, the model generates tokens, and the selected confidence metric is applied to each output from the early exits. We record both the metric value and a Boolean indicating whether the early exit's prediction matches the prediction of the underlying model f \u03b8 . After generating a sufficient number of tokens, we obtain a dataset of metric values paired with correctness indicators. \n\nUpon completing the calibration process, we derive two distinct vectors for each head k in the model. Let \n\nbe the vector containing the metric values obtained at head k during the calibration phase, where x i is an input from the calibration dataset D c and c(h k (x i )) represents the confidence metric applied to the output of the classifier h k for the input x i . Correspondingly, let t k be a binary vector of length N c where each element corresponds to the correctness of the prediction associated with the respective element in c k . Specifically, t k,i is 1 if the i th prediction at head k is the same as the i th prediction associated to of the underlying model f \u03b8 (x i ), otherwise 0. More formally, we can write: \n\n(3) where the maxima are taken over the coordinates of the probability score vectors associated to the token sequence x i . \n\nThe calibration set is subsequently used to establish a confidence threshold for each early exit head. We begin by sorting the metric values obtained during calibration.",
            "score": 0.5835303049552942,
            "section_title": "Calibration of Confidence Thresholds",
            "char_start_offset": 11424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2193
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08331298828125
        },
        {
            "corpus_id": "264811211",
            "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
            "text": "An essential step in evaluating the calibration for LMs involves estimating the confidence from the model. Recent research primarily concentrates on methods classified into three types: logit-based, consistency-based, and verbalization-based. Logit-based estimation (Guo et al., 2017;Cheng et al., 2023) measures the model confidence based on the model predicted logits. Consistency-based estimation (Wang et al., 2023;Kuhn et al., 2023) relies on the intuition that LMs will consistently generate similar outputs when they are confident in responding to a query. A major challenge of consistency-based methods is deciding on the semantic equivalence of multiple long outputs, which is typically accomplished by utilizing a natural language inference model (Kuhn et al., 2023), BERTScore (Zhang et al., 2020b), or QA-based metrics (Fabbri et al., 2022). However, these methods are limited to sentence-length generations, and it is still unclear how to decide on semantic equivalence over long-form outputs. More recent studies (Tian et al., 2023;Xiong et al., 2023) investigate directly prompting instruction-tuned LMs to verbalize their confidence. While consistency-based and verbalization-based methods have demonstrated their effectiveness in recent LMs, their utilization is largely restricted due to their high computational costs during inference and the requirement for LM's instruction-following capability. In contrast, the LITCAB works only adds minimal computational cost and is suitable for use with any LM whose last-layer hidden states are accessible.",
            "score": 0.5809532862444711,
            "section_title": "CONFIDENCE ESTIMATION FOR LANGUAGE MODELS (LMS)",
            "char_start_offset": 8058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1566
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 284,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 284,
                    "end": 303,
                    "matchedPaperCorpusId": "252917981"
                },
                {
                    "start": 400,
                    "end": 419,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 419,
                    "end": 437,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 757,
                    "end": 776,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 831,
                    "end": 852,
                    "matchedPaperCorpusId": "245218667"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35693359375
        },
        {
            "corpus_id": "267938751",
            "title": "C3: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding",
            "text": "Confidence scores are computed sequentially, ranging from small to large size models, to determine the appropriate model to employ. Once a confidence score surpasses a threshold, the corresponding model is selected, and the inference process ends. \n\nCascade-based models, however, exhibit notable limitations in cross-lingual scenarios. The confidence score, which measures the probability of the current prediction being correct in cascade-based models, is determined by the maximum output Premise: One of our number will carry out your instructions minutely. Hypothesis: A member of my team will execute your orders with immense precision. probability, the mean of the output probability, or the entropy of the output probability. Unfortunately, neural networks often generate unreliable confidence scores, particularly in out-of-distribution (OOD) scenarios (Guo et al., 2017;Wei et al., 2022;Han et al., 2024;Choi et al., 2023;Liang et al., 2022). The cross-lingual task represents a typical OOD setting, where the training data exhibits significantly different distributions compared to the testing data. Consequently, the distribution of confidence scores on the training data differs from that on the testing data. Applying a threshold derived from the training set, based on pre-defined inference budgets, may prove either too conservative or radical for the testing data, leading to a performance or efficiency drop. \n\nTo tackle the challenges outlined earlier, we introduce a Confidence Calibration Cascade (C 3 ) framework for efficient cross-lingual inference. The motivation behind our proposed approach is to calibrate the confidence of Multilingual Pre-trained Language Models (mPLMs), allowing the thresh-old determined on English data to be applicable to other languages. Specifically, we introduce a plugin calibration step at the base of mPLMs. Initially, we normalize the logits to alleviate over-confidence during model fine-tuning. Subsequently, we implement a temperature scaling step to adjust the logits with a learnable scalar parameter. The proposed framework calibrates each individual model in the cascade, providing more reliable confidence scores. This, in turn, enhances the model's performance and generalization capabilities, leading to consistent improvements in efficiency and accuracy across different languages.",
            "score": 0.5809359484605856,
            "section_title": "Introduction",
            "char_start_offset": 1968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 861,
                    "end": 879,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 879,
                    "end": 896,
                    "matchedPaperCorpusId": "248887336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040985107421875
        },
        {
            "corpus_id": "273228151",
            "title": "QA-Calibration of Language Model Confidence Scores",
            "text": "Language models (LMs) built on transformer-based architectures are capable of producing texts that are both coherent and contextually relevant for a large range of applications (Brown et al., 2020;Chowdhery et al., 2023;Achiam et al., 2023). In question-and-answering (QA) systems, these models generally perform well, but occasionally produce inaccurate answers -a phenomenon generally referred to as hallucination (Huang et al., 2023). Confidence estimates that are paired with the answers can be used as an interpretable indicator of the LM's accuracy (Steyvers et al., 2024). But for this, these confidence scores have to be well-calibrated, i.e., match the actual accuracy of the model. \n\nTo evaluate whether the obtained confidence scores are actually well-calibrated, a common criterion is the expected (average-case) calibration error (Tian et al., 2023;Xiong et al., 2024). Suppose a model claims that its answer has a confidence of p. Based on only this one answer, it is not possible to know whether this confidence was well-calibrated or not. But when considering multiple question-answer pairs-say N p pairs each with a claimed confidence p, we can verify how many answers are actually correct and measure the error between the claimed confidence and the model's accuracy. Averaging over these errors quantifies of the model's confidence scores. 1 hile this average-case calibration measure makes sense for models trained and evaluated on specific tasks, its applicability to generative QA is questionable because it averages over all QA pairs. This is because generative QA systems can be applied across various domains and topics, such as geography, politics, or medicine. Consider, for instance, the QA pairs shown in Figure 1. On average, this model has a calibration error of 0.5. From User 1's perspective, the calibration is much worse, with an error of 0.8. Meanwhile, User 2 has a completely different experience, finding the model to be Figure 1: Two users interact separately with an LM by inputting questions and obtaining answers and confidence scores from the LM.",
            "score": 0.5804207443432041,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 197,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 197,
                    "end": 220,
                    "matchedPaperCorpusId": "269004786"
                },
                {
                    "start": 843,
                    "end": 862,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 862,
                    "end": 881,
                    "matchedPaperCorpusId": "259224389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.368408203125
        },
        {
            "corpus_id": "273963769",
            "title": "Epistemic Integrity in Large Language Models",
            "text": "Epistemic Calibration is the process of aligning a model's expressed confidence, conveyed through linguistic assertiveness, with its actual reliability or correctness. Achieving this alignment requires that the model's linguistic expressions match the probabilistic confidence it has in its predictions. However, if this balance is disrupted, significant communication issues can arise. For instance, if a model uses assertive language but is internally uncertain, users may place undue trust in potentially incorrect information. Conversely, when a model is internally confident but hedges its language, users might doubt accurate information. \n\nFigure 1 presents two contrasting examples that highlight varying levels of epistemic calibration in LLM outputs. In both cases, the LLM responds to a user query, and we compute two certainty scores: Internal Certainty and External Certainty (through linguistic assertiveness). In the first example, the internal and external certainty scores are closely aligned, demonstrating high epistemic calibration. \n\nHere, the model's linguistic assertiveness accurately reflects its internal confidence, resulting in a trustworthy output. In contrast, the second example reveals low epistemic calibration. Although the internal certainty score is relatively low, the model's linguistic assertiveness remains high, indicating overconfidence in its response. In the following analysis, we compute internal certainty using the method outlined by Rivera et al. (2024) and use linguistic assertiveness, derived from our custom model detailed in Section 3-which is validated against human surveyed human ratings-as a proxy for external certainty quantification.",
            "score": 0.5798909455038843,
            "section_title": "EPISTEMIC CALIBRATION",
            "char_start_offset": 15096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1694
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2127685546875
        },
        {
            "corpus_id": "273228940",
            "title": "Calibrating Verbalized Probabilities for Large Language Models",
            "text": "Large Language models (LLMs), such as GPT-4 (OpenAI, 2023), Claude 1 , Mistral (Jiang et al., 2023) have demonstrated to be versatile tools with strong capabilities across many natural language processing (NLP) tasks. Their performance is grounded on very large numbers of parameters and the refinement to align more closely with human preferences using reinforcement learning from human feedback (RLHF;Stiennon et al., 2020), direct preference optimization (DPO; Rafailov et al., 2024), etc. This provides LLMs with the flexibility to adapt to new tasks through natural language instructions (prompts) and contextual data (in-context learning) without requiring further training or parameter updates. \n\nWhile these LLMs achieved impressive performance on a wide range of tasks, recent studies suggest that the RLHF fine-tuned LLMs are poorly calibrated (Tian et al., 2023;Zheng et al., 2023;Zhao et al., 2021;Xiong et al., 2024). This prevents a reliable assessment of the confidences of LLM outputs, which is a key requirement in risksensitive domains such as medicine (Penso et al., 2024) and finance (Bahnsen et al., 2014), where one wants to prefer high-confidence decisions and avoid low-confidence actions. Unfortunately, calibrating LLMs is challenging, particularly for blackbox LLMs, which only provides closed-source APIs and thus users have no access to model output such as token-level probabilities. \n\nRecently, Tian et al. (2023) proposed prompting large language models (LLMs) to directly generate prediction confidence scores, a concept they refer to as verbalized confidence (Lin et al., 2022). In this approach, the language model is instructed to respond with both the predicted label and a numeric confidence score within the range of [0, 1], or with a natural language expression indicating confidence, such as \"highly likely.\" This procedure is known as confidence elicitation. \n\nTheir empirical results show that these verbalized probabilities offer better calibration than con-arXiv:2410.06707v1",
            "score": 0.5787022941457665,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 425,
                    "matchedPaperCorpusId": "11591887"
                },
                {
                    "start": 464,
                    "end": 486,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 854,
                    "end": 873,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 892,
                    "end": 910,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1071,
                    "end": 1091,
                    "matchedPaperCorpusId": "266998189"
                },
                {
                    "start": 1104,
                    "end": 1126,
                    "matchedPaperCorpusId": "4344478"
                },
                {
                    "start": 1426,
                    "end": 1444,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405029296875
        },
        {
            "corpus_id": "269004786",
            "title": "Multicalibration for Confidence Scoring in LLMs",
            "text": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
            "score": 0.5782437928443434,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51318359375
        },
        {
            "corpus_id": "277621371",
            "title": "An overview of model uncertainty and variability in LLM-based sentiment analysis. Challenges, mitigation strategies and the role of explainability",
            "text": "Furthermore, (H. Yang et al., 2024) reveals alignment issues in multimodal AI, where discrepancies between text, image, and audio input create sentiment misclassification and conflicting interpretations, further exacerbating model inconsistency in sentiment analysis. These findings emphasize the need for bias mitigation strategies, standardized alignment protocols, and multimodal calibration techniques to improve stability in LLM-based sentiment classification. 10. Lack of calibration in confidence scores. \n\nOne of the critical factors contributing to MVP in LLM-based sentiment analysis is the lack of proper calibration of confidence scores. Confidence calibration refers to the alignment between the predicted confidence level of a model and its actual accuracy. LLMs often overestimate their confidence in incorrect predictions while underestimating it in accurate ones, leading to a disconnect between their perceived certainty and realworld performance. This miscalibration is particularly problematic in sentiment classification, where erratic confidence levels may cause inconsistent sentiment assignments, ultimately compromising the reliability of the model in decision-making processes. Without proper calibration, LLMs can misrepresent their predictive confidence, resulting in unstable sentiment scores on inference runs and reducing trust in AI-driven sentiment analysis applications. Empirical evidence is found in (Xie et al., 2025), where the discussion reflects how poorly calibrated models produce highly variable sentiment predictions, as inconsistencies in confidence estimation lead to overconfident but incorrect classifications or fluctuating sentiment scores across inference runs. Similarly, (Beigi et al., 2024) highlights that LLMs often lack uncertainty-aware calibration mechanisms, emphasizing that temperature scaling, Bayesian confidence adjustments, and quantile-based methods can improve model stability in sentiment analysis. These studies reveal that uncalibrated confidence scores introduce variability in model outputs, particularly in subjective sentiment tasks. Calibration errors lead to low trustworthiness in AI-generated sentiment classifications, which requires the integration of uncertainty quantification frameworks to improve LLM reliability in high-stakes applications. \n\n11. Evaluation metrics limitations and sentiment evaluation benchmark. \n\nOne of the fundamental challenges exacerbating MVP in LLM-based sentiment analysis is the limitation of existing evaluation metrics and sentiment benchmarks.",
            "score": 0.5781101387106755,
            "section_title": "Aleatoric and epistemic uncertainty.",
            "char_start_offset": 37619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2326
                },
                {
                    "start": 2329,
                    "end": 2399
                },
                {
                    "start": 2402,
                    "end": 2559
                }
            ],
            "ref_mentions": [
                {
                    "start": 1436,
                    "end": 1454,
                    "matchedPaperCorpusId": "276408437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08831787109375
        },
        {
            "corpus_id": "276408950",
            "title": "Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models",
            "text": "Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration - where models are overconfident or underconfident - poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., GPT-4o) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods.",
            "score": 0.5779540729198787,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22607421875
        },
        {
            "corpus_id": "252917981",
            "title": "Prompting GPT-3 To Be Reliable",
            "text": "In this way, users can put more trust in highly-confidence predictions and discard low-confidence predictions. While various methods have been proposed to obtain confidence scores and perform post-hoc calibration for language models (Jiang et al., 2021;Desai & Durrett, 2020;Ye & Durrett, 2022), they are mostly focused on classification settings rather than free-form generation, which is more common for the use of GPT-3. In this work, we explore two simple (but surprisingly effective) ways of obtaining confidence scores for GPT-3's generated answers and we analyse the impact of scaling as well as prompt design. For studying calibration of GPT-3 style LLMs, Lin et al. (2022a) explore the idea of expressing uncertainty in verbal words but is restricted to math questions. Mielke et al. (2022) study linguistic calibration on conversational models. Kadavath et al. (2022) study adopting a multiple-choice setting in which case obtaining a confidence score is much easier (since the model only needs to predict one token to indicate which option to choose rather than generating the entire answer string). We differ from them in: 1) we focus on obtaining probabilistic confidence scores rather than verbal uncertainty expressions; 2) we study the more general and realistic free-form answer generation setting; and 3) we do not involve finetuning or any additional training of the language model. Table 13: Results on the clean and adversarial test sets of AdvGLUE, we use accuracy as the metric for all datasets. For MNLI, we report the average performance on the matched and mismatched dev sets. The supervised baselines are trained on the clean training data, and GPT-3 uses few-shot prompts sampled from the clean training data. While GPT-3 few-shot prompting lags behind supervised models on clean test sets, it significantly outperforms the supervised models on the adversarial sets. That being said, we still note a performance drop of GPT-3 on the adversarial test sets when using clean demo examples.",
            "score": 0.5773093973363541,
            "section_title": "APPENDIX A MORE RELATED WORK",
            "char_start_offset": 29515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 253,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 253,
                    "end": 275,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 275,
                    "end": 294,
                    "matchedPaperCorpusId": "238856959"
                },
                {
                    "start": 664,
                    "end": 682,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 779,
                    "end": 799,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 855,
                    "end": 877,
                    "matchedPaperCorpusId": "250451161"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.086181640625
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Recently, there has been growing interest in studying calibration of English QA models (Kamath et al., 2020;Zhang et al., 2021;Jiang et al., 2021;Si et al., 2022). Kamath et al. (2020) trains an extra calibrator of confidence scores to improve the calibration performance and examines the calibration performance on an out-of-domain (OOD) setting. They utilize the scores from the calibrator and uses it as a reranker to select the answers. Zhang et al. (2021) extends this work by adding the features of the context and back-translated context. Jiang et al. (2021) analyzes the calibration performance of generative language models, and find that the generative models on QA are not well-calibrated. \n\nOur work in contrast investigates the calibration of pre-trained multilingual LLMs (both extractive, with an encoder-only architecture, and generative, with an encoder-decoder or decoder-only architecture) on QA, and various techniques to improve calibration such as temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016) and cross-lingual data augmentation. \n\n3 Background In this work, we focus on two broad types of models for QA: extractive (or discriminative) and generative (as shown in Figure 2). For models based on encoder-only language models like mBERT and XLM-R (Conneau et al., 2019;Devlin et al., 2019;Liu et al., 2019), the prediction of the answer span within the given context is framed as a classification task and achieved using two linear layers. These linear layers are placed on top of the hidden representations and are responsible for predicting the start and end indices of the answer span Y , respectively. The logit score of the answer z ans is defined as the sum of the logits of the start and end positions, z start and z end (Si et al., 2022).",
            "score": 0.5765947177585429,
            "section_title": "Calibration of models on Question Answering tasks",
            "char_start_offset": 5671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1800
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 108,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 127,
                    "end": 146,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 146,
                    "end": 162,
                    "matchedPaperCorpusId": "253098276"
                },
                {
                    "start": 164,
                    "end": 184,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 546,
                    "end": 565,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 990,
                    "end": 1008,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1026,
                    "end": 1048,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 1323,
                    "end": 1343,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1588134765625
        },
        {
            "corpus_id": "270062423",
            "title": "Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models",
            "text": "As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM's internal confidence, quantified by token probabilities, to the confidence conveyed in the model's response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model's confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed the strongest confidence-probability alignment, with an average Spearman's $\\hat{\\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.",
            "score": 0.5758763397795322,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22509765625
        },
        {
            "corpus_id": "278165590",
            "title": "Bi-directional Model Cascading with Proxy Confidence",
            "text": "A model's calibration is its ability to express some level of confidence associated with an output that accurately reflects the likelihood that it is correct [13]. LLM calibration generally has great value to researchers and industry applications, and is the subject of significant research efforts [12]. The acceptance of LLMs in real-world application depends on their trustworthiness and reliability [4], while in cascading context, calibration has direct and measurable benefits on performance. The success of any model cascade depends on the quality of its deferral process, and therefore the calibration of its component models or accuracy of its confidence metric . \n\nThere are many approaches to approximating LLM confidence. A large subset of these methods involves deriving the confidence from the log-probabilities of the model outputs. In the case of LLMs, these log-probabilities, or \"logits\", reflect a distribution over tokens and the confidence can be estimated using some variation of entropy or maximum probability at the individual token or sequence level. Some techniques that have shown recent promise involve examining consistency across multiple stochastic or permuted generations [18], but the additional computation required makes these approaches poorly suited to efficiency optimizations. Other researchers have asked generative models to explicitly report their confidence in their output with mixed results [22,32], but benchmarks in some domains have found self reported confidence to be very poor, especially among smaller models [29]. \n\nDifferent layers in a language model capture different types of information and fulfill different roles in inference [41]. \n\nOne method of identifying and promoting fact-based and truthful outputs from LLMs is to consider the likelihood of different tokens at different layers within the model, emphasizing the effect of layers most likely to inject knowledge [8]. The internal layers of a pretrained model inform output rather than generate output directly and may hold useful information without the same biases as the output layer [19]. Other, similar techniques applying an output head over internal layers have been used in early-layer exit strategies [40], improve interpretability [52], and to better understand how the internal layers and mechanisms function in LLMs [15,42].",
            "score": 0.5746580722262571,
            "section_title": "LLM Calibration",
            "char_start_offset": 8103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1566
                },
                {
                    "start": 1569,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2352
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 299,
                    "end": 303,
                    "matchedPaperCorpusId": "265157516"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "226965491"
                },
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "276349944"
                },
                {
                    "start": 1686,
                    "end": 1690,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1929,
                    "end": 1932,
                    "matchedPaperCorpusId": "261582463"
                },
                {
                    "start": 2103,
                    "end": 2107,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.347412109375
        },
        {
            "corpus_id": "258297899",
            "title": "Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",
            "text": "In this subsection, we first investigate the level of confidence for both the correct and incorrect samples. Confidence is typically described in terms of a probability value, indicating the likelihood of belonging to a specific category. To obtain prediction probabilities from ChatGPT, we ask it to output the probability (Confidence_Standard and Confidence_Open), as discussed in \u00a7 3.3. Our aim is to investigate whether ChatGPT can provide a reasonable prediction confidence scores for its predictions, thus reducing the risk of misinterpretation. In Table 6, we present the confidence scores of correct and incorrect predictions from different models, referred to correct confidence and incorrect confidence, respectively. Our observations reveal that all the models exhibit high confidence levels in their predictions, this is consistent with previous research on large models (Guo et al., 2017). \n\nAlthough ChatGPT performs worse than its BERT-based counterparts in Standard-IE setting, it displays overconfidence in both correct and incorrect predictions. Consequently, this overconfidence could lead to misguidance of users. Furthermore, we note a significant confidence gap between correct and incorrect predictions, indicating the need for careful evaluation when ChatGPT's prediction has relatively low confidence. \n\nWe then focus on calibration, a critical property of LLMs as it could estimate the predictive uncertainty for the secure application of LLMs. A well-calibrated model not only produces accurate predictions but also provides reliable and informative uncertainty estimates, necessary for sound decision-making. In this research, we evaluate the calibration using the Expected Calibration Error (ECE) metric which measures the deviation be-  tween predicted confidence and accuracy. 8 The results are shown in Table 7, and from that we can observe that ChatGPT shows much poorer calibration compared to BERT-based methods, which indicates that ChatGPT tends to produce confidences that do not represent true probabilities easily. Furthermore, although ChatGPT displays low ECE in tasks such as ET and NER, miscalibration phenomenon dominates most cases. These findings suggest that ChatGPT needs improvement in terms of calibration, especially for IE tasks.",
            "score": 0.5741469937586962,
            "section_title": "Calibration",
            "char_start_offset": 24777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1326
                },
                {
                    "start": 1329,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 883,
                    "end": 901,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1307373046875
        },
        {
            "corpus_id": "271924409",
            "title": "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation",
            "text": "With the rapid development of large language models (LLMs), many powerful models have been deployed into our daily lives for practical usage to help us make decisions (Yao et al., 2023;Sha et al., 2023;Zhao et al., 2024). This makes it urgent for us to know to what extent we can trust the outputs of the models. Calibration is one of the most important indicators beyond accuracy, which provides a confidence measure to the model's predictions (Guo et al., 2017;Hsieh et al., 2023). In LLMs, confidence is exactly the probability for each generated token. Therefore, a well-calibrated model should align its prediction confidence with its ground-truth correctness likelihood as shown in Figure 1. As an example, recent hallucination detection methods rely on model prediction confidence as a significant indicator of potential hallucination (Zhang et al., 2023;Varshney et al., 2023). If the model is incapable of giving accurate confidence levels, people may fail to detect hallucinations due to the model's over-confidence, or people may falsely identify hallucinations due to the model's under-confidence. Mis-calibration brings significant challenges for the deployment of LLMs in real-world applications. \n\nCurrently, there are two methods to obtain a language model for practical usage. First, fine-tuning, which fine-tunes pre-trained LLMs on specific datasets by matching each token entry with a target ground truth token. Although fine-tuning can consistently improve performance on downstream tasks (Dodge et al., 2020;Sun et al., 2020;Ziegler et al., 2020), we identify that the model obtained in this way exhibits a nature of \"tuning-induced miscalibration\". Second, distillation-based methods transfer knowledge (e.g., soft labels) from larger LLMs to smaller models (Gu et al., 2023). Although distillation shows better calibration than fine-tuning as it matches each token entry with a probability distribution instead of a hard label, we find it is still biased because of the mis-calibration nature of teacher models.",
            "score": 0.5737881284754177,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 185,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 185,
                    "end": 202,
                    "matchedPaperCorpusId": "258762841"
                },
                {
                    "start": 202,
                    "end": 220,
                    "matchedPaperCorpusId": "261048772"
                },
                {
                    "start": 445,
                    "end": 463,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 463,
                    "end": 482,
                    "matchedPaperCorpusId": "258461606"
                },
                {
                    "start": 842,
                    "end": 862,
                    "matchedPaperCorpusId": "265351856"
                },
                {
                    "start": 1510,
                    "end": 1530,
                    "matchedPaperCorpusId": "211132951"
                },
                {
                    "start": 1530,
                    "end": 1547,
                    "matchedPaperCorpusId": "53296520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31884765625
        },
        {
            "corpus_id": "256808480",
            "title": "Realistic Conversational Question Answering with Answer Selection based on Calibrated Confidence and Uncertainty Measurement",
            "text": "As it is nearly impossible for models to always make accurate predictions, unreliable predictions become serious issues when deploying machine learning models to realworld settings. Motivated to prevent such a risk, mechanisms of estimating the reliability of model's predictive probabilities based on confidence and uncertainty are recently proposed (Abdar et al., 2021;Houben et al., 2022). We note that confidence is usually measured by the softmax outputs of models (Guo et al., 2017), and that uncertainty can be quantified by Bayesian models, which can be approximated via Monte Carlo (MC) dropout (Gal and Ghahramani, 2016;Kendall and Gal, 2017). With much work on confidence and uncertainty estimations in computer vision tasks (Guillory et al., 2021), related topics have been recently adopted for NLP tasks as well (Shelmanov et al., 2021;Wu et al., 2021;Malinin and Gales, 2021;Vazhentsev et al., 2022). While confidence and uncertainty estimation should also be considered in ConvQA, we believe that this venue is under-explored so far. In particular, since questions are asked sequentially, it is likely that untrustworthy predictions in the conversation history would negatively affect the performance. To tackle this, we propose to exclude low-confident or uncertain predictions when training and evaluating the ConvQA model.\n\nCalibration Confidence and uncertainty help interpret the validity of the model's prediction. However, it is not safe to rely on them when the model is not calibrated, where the correct likelihood does not match the predicted probability (Guo et al., 2017), or the model error does not match the predicted uncertainty (Laves et al., 2019). Since deep neural networks are prone to miscalibration as the number of parameters has much increased, large pre-trained language models are also not free from this problem (Wang et al., 2021;Zhao et al., 2021;Dan and Roth, 2021). One of the most prevalent approaches to calibrating the model is to rescale a logit vector before the softmax function for regularizing the probability, which is known as temperature scaling (Guo et al., 2017). While there exist lots of",
            "score": 0.5735088142512494,
            "section_title": "Confidence and Uncertainty",
            "char_start_offset": 8272,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 371,
                    "matchedPaperCorpusId": "226307260"
                },
                {
                    "start": 604,
                    "end": 630,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 825,
                    "end": 849,
                    "matchedPaperCorpusId": "231648187"
                },
                {
                    "start": 849,
                    "end": 865,
                    "matchedPaperCorpusId": "237421080"
                },
                {
                    "start": 865,
                    "end": 889,
                    "matchedPaperCorpusId": "231895728"
                },
                {
                    "start": 889,
                    "end": 913,
                    "matchedPaperCorpusId": "248780161"
                },
                {
                    "start": 1660,
                    "end": 1680,
                    "matchedPaperCorpusId": "203593350"
                },
                {
                    "start": 1855,
                    "end": 1874,
                    "matchedPaperCorpusId": "238215193"
                },
                {
                    "start": 1874,
                    "end": 1892,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035888671875
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
            "score": 0.5731340342039501,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68994140625
        },
        {
            "corpus_id": "273403321",
            "title": "Atomic Calibration of LLMs in Long-Form Generations",
            "text": "For a language model M, let x \u2208 X represent the response generated by model M for a query q, and let y \u2208 Y t denote the corresponding label, where Y t \u2286 [0, 1] indicates a quality score for a specific task t \u2208 T . Note that unlike multiple-choice and short-form questions, which typically only focus on the correctness of the answer, the tasks in T involve various aspects, such as factuality, coherence, creativity, etc. We define a probability prediction function f : X \u2192 \u2206 |Yt| , where \u2206 |Yt| represents the |Y t |dimensional simplex. In this context, f (x) y denotes the probability assigned to the label y given the generated output x. In the remaining of this work, we focus specifically on calibrating for factuality. In this case, Y represent Y t when t corresponds to factuality. Y takes values in [0, 1], indicating the factuality level of a response. Based on the calibration concepts discussed in (Guo et al., 2017), we define the calibration of each response as follows: \n\nDefinition 1 (Macro Calibration on Factuality) A language model M that produces generations x \u223c M(x | q) is said to be response-level (macro) calibrated if \n\nIn the context of long-form generation, a single response x may encompass multiple atomic claims. Macro calibration at the response level cannot fully present the fine-grained uncertainty at the atomic level. To address this, we decompose the response x into N atomic claims c i , represented as x = N i=1 c i . Each atomic claim c i is assigned a binary label y i \u2208 Y i , where Y i = {0, 1}, indicating its truthfulness. The overall factuality score for the response y is computed as y = 1 N N i=1 y i . Similarly, we define f (c i ) y i as the probability of the label y i given the atomic claim c i . Building on this decomposition, we propose a fine-grained measure of calibration at the atomic level as follows:",
            "score": 0.5729539865350026,
            "section_title": "Atomic Calibration",
            "char_start_offset": 3946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1860
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08135986328125
        },
        {
            "corpus_id": "267617073",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "text": "To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.",
            "score": 0.5725220197621147,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49658203125
        },
        {
            "corpus_id": "264811211",
            "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
            "text": "While modern language models (LMs) exhibit impressive performance across various tasks (Brown et al., 2020;OpenAI, 2023), they suffer from hallucination (Lin et al., 2021;Zhang et al., 2023), where they can provide nonfactual responses with high confidence. The issue of hallucination undermines user trust and significantly limits the applicability of LMs to domains that require a high degree of reliability, such as legal, financial, and educational sectors. While eliminating hallucination in LMs altogether is highly challenging, calibrating LMs (Nguyen & O'Connor, 2015) by aligning their confidence with the actual probability of output correctness can certainly help. Specifically, a well-calibrated model enables users to gauge the model's confidence and make informed decisions about whether to trust its outputs. Moreover, hallucinated facts can be filtered out when the confidence level is below a certain threshold. \n\nPrevious approaches to calibrating neural models mainly fall into two categories: (i) post-processing and (ii) training-based methods, as outlined in Figure 1. Post-processing techniques have the advantage of not changing the model weights by directly manipulating the sequence probabilities. Example techniques from this family are temperature scaling (Liang et al., 2018) and Platt scaling (Niculescu-Mizil & Caruana, 2005). Post-processing calibration directly adjusts the sharpness of the Motivated by reducing the cost of computation, we present LITCAB, a lightweight calibration technique for LLMs. LITCAB takes as input a sequence of hidden states from the LM's final layer and produces a set of logit biases to adjust the generation confidence. Specifically, LITCAB trains a single linear layer using a contrastive max-margin objective, with a goal of maximizing the token probabilities for correct answers and lowering the likelihood for incorrect ones. As LITCAB can adjust the confidence ranking among outputs, a capability that post-processing methods lack, it offers greater flexibility compared to those methods. On the other hand, the trainable parameters count of LITCAB is less than 2% of the original LM parameters, making it significantly more efficient than standard training-based approaches.",
            "score": 0.5704585576687154,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 107,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 551,
                    "end": 576,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 1284,
                    "end": 1304,
                    "matchedPaperCorpusId": "3526391"
                },
                {
                    "start": 1323,
                    "end": 1356,
                    "matchedPaperCorpusId": "207158152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.358642578125
        },
        {
            "corpus_id": "268876453",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "text": "As shown in Figure 1, our proposed UF Calibration achieved good calibration across different MCQA datasets. Meanwhile, UF Calibration does not require knowledge of the model's pertoken log-probability, making it broadly applicable to various Black-box RLHF-LMs, which do not provide the per-token log-probability. \n\nTo have a closer look at the calibration of model confidence, we propose two novel metrics for evaluating and observation: 1) Inverse Pair Ratio (IPR), which is the proportion of inverse pairs in the Reliability Diagram. This metric could reflect whether the model is well-calibrated from the perspective of the monotonicity of the Reliability Diagram. If the reliability diagram is monotonic, it indicates that the average accuracy of low-confidence an-swers is always lower than that of high-confidence answers. 2) As shown in Table 7, we find that as the number of model parameters increases, language models still tend to consistently express uncertainty within certain fixed ranges. Thus, we design the Confidence Evenness (CE) to observe to the uniformity of the density of each bar in the reliability diagram. Our experimental results indicate that, after calibration, even within the same dataset, there is a significant difference in the confidence of the answers provided by language models for different questions. We summarize our main contributions as follows: \n\n1) Our proposed method could be viewed as a strong baseline for eliciting model confidence, where answer set is known. And the calibrated confidence could be viewed as a soft label. \n\n2) We propose two new metrics, IPR and CE, to evaluate the calibration of LM's confidence. \n\n3) We conduct a detailed discussion of a research question: \"What kind of Confidence is Truly Well-Calibrated?\", and we hope our discussion can bring some insights to the community.",
            "score": 0.5700261573357426,
            "section_title": "Introduction",
            "char_start_offset": 3582,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 313
                },
                {
                    "start": 316,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1389
                },
                {
                    "start": 1392,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1850
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482421875
        },
        {
            "corpus_id": "268876453",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "text": "In this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Through the decomposition, we propose a plug-and-play method, UF CALIBRA-TION, to calibrate the confidence of language models. Through experiments with 6 RLHF-LMs on 4 multiple-choice question answering benchmarks, our method exhibits good calibration. Besides, we propose two novel metrics, IPR and CE, to evaluate the calibration of language models. Finally, we conduct a detailed discussion on Truly Well-Calibrated Confidence. We believe our method can serve as a strong baseline, and we hope that this work could provide some insights into the language model confidence calibration.",
            "score": 0.5700261573357426,
            "section_title": "Conclusion",
            "char_start_offset": 22728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 746
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6376953125
        },
        {
            "corpus_id": "271329113",
            "title": "Evaluating language models as risk scores",
            "text": "\u2022 Models' output token probabilities have strong predictive signal2 but are widely miscalibrated. \n\n\u2022 The failure modes of models are different: Multiple-choice answer probabilities generated by base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores. \n\n\u2022 Using a verbalized chat-style prompt results in materially different answer distributions, with significantly improved calibration for instruction-tuned models, accompanied by a small but consistent decrease in predictive power. \n\n\u2022 Instruction-tuning generally worsens calibration of multiple-choice answers, but improves calibration of verbalized numeric answers. \n\nWe hope our package facilitates future investigations into statistical properties of LLMs, not only as a way to faithfully reflect real-world uncertainties, but also as a required stepping stone to trustworthy model responses. \n\nOutline. In Section 2 we provide necessary background on risk scores and calibration in statistical machine learning. In Section 3 we extend this background to the application to language models, providing various design choices around prompting templates and ways to extract risk scores from language models. In Section 4 we evaluate 17 recent LLMs on 5 proposed benchmark tasks and summarize empirical findings.",
            "score": 0.5697952609433237,
            "section_title": "Our contributions",
            "char_start_offset": 3820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1365
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0894775390625
        },
        {
            "corpus_id": "268032879",
            "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
            "text": "Prompting Large Language Models. Recent research (Brown et al., 2020;Kojima et al., 2023) on large language models shows that in-context learning (ICL) achieves great effectiveness in using models as few-shot or zero-shot reasoners. Different styles of prompting such as Knowledge prompting (Liu et al., 2022), Chain of Thought (CoT) prompting (Wei et al., 2022b), Self-Consistency prompting (Wang et al., 2023), Self-ask (Press et al., 2023), Tree-of-Thought prompting (Yao et al., 2023), andSkill-in-Context (Chen et al., 2023) are then proposed to guide the model to elicit its knowledge for reasoning in different ways. \n\nMost previous work mainly focuses on how such a prompting method influences the model performance on various tasks. In this paper, we compare how confidence calibration is influenced by different prompting methods. \n\nConfidence Calibration of LLMs. Extracting honest and constructive confidence scores of large language models is considered an important step towards building faithful and trustworthy AI systems (Desai and Durrett, 2020;Si et al., 2023). Many methods have been proposed recently to get reliable confidence scores with different suffix prompts added after outputting possible answers, such as a follow of True or False multiple choice question (Kadavath et al., 2022), asking models to describe the likelihood of the answer (Lin et al., 2022), and describing the likelihood that the answer is correct with demonstrations (Tian et al., 2023). However, it remains unclear how robust the methods are and how good they are comparatively. Our paper proposes FaR prompting as an orthogonal method to improve calibration and compare different extraction methods with our test bed. \n\nRecently, Yang et al. (2023) discuss the honesty problem of models as part of the alignment. Qian et al. ( 2023) study the confidence change when there is a conflict between in-context and model internal knowledge. Another line of work links model confidence with human confidence (Zhou et al., 2023;Steyvers et al., 2024;Zhou et al., 2024).",
            "score": 0.5687266545294021,
            "section_title": "Related Work",
            "char_start_offset": 23915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 69,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 291,
                    "end": 309,
                    "matchedPaperCorpusId": "239016123"
                },
                {
                    "start": 344,
                    "end": 363,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1038,
                    "end": 1063,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1063,
                    "end": 1079,
                    "matchedPaperCorpusId": "252917981"
                },
                {
                    "start": 1366,
                    "end": 1384,
                    "matchedPaperCorpusId": "249191391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25732421875
        },
        {
            "corpus_id": "263134640",
            "title": "The Confidence-Competence Gap in Large Language Models: A Cognitive Study",
            "text": "To rigorously evaluate the performance of Large Language models across various categories and difficulty levels, we have curated an extensive dataset. This dataset not only records the responses generated by the LLMs but also encompasses their self-assessed confidence levels, both before and after their interactions. This offers a clear understanding of the model's intrinsic capabilities and self-awareness. The evaluation of LLMs is determined upon the examination of their diverse answers or responses to the posed questions. Within our dataset, we have incorporated distinct variables that capture the confidence levels of LLMS not only prior to responding to the questions but also subsequent to providing their responses. This inclusive approach enables us to assess the alterations in their confidence levels before and after generating the response. \n\nTable 3 in Appendix A.2 provides a detailed description of the variables used in this study. The variables represent the problem's category, difficulty level, their confidence before and after answering the questions, and then the correctness of the response of LLMs. In the field of advanced machine learning models, particularly LLMs, evaluating their proficiency goes beyond checking how accurate their output is. It also involves understanding how well these models gauge their abilities, which they express through their confidence levels, and comparing their self-assessment with their actual performance. When we apply these ideas to LLMS, we encounter interesting questions. Do LLMs, despite their computations prowess, exhibit similarities to human cognitive biases like the Dunning-Kruger effect? Can we identify the situations where the model is overly confident or lacks confidence in its abilities based on its confidence scores? Our subsequent analyses explore these questions by examining how well the model's self-assessment aligns with its real-world performance. Calibration of confidence levels and their relationship with the accuracy of LLMs are the two significant aspects of our study. These two metrics are examined in the context of the Dunning-Kruger effect. The section oversees the confidence levels and their relation with the accuracy of the LLMs.",
            "score": 0.5671032965830596,
            "section_title": "Creation of Survey Dataset",
            "char_start_offset": 13318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2239
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03424072265625
        },
        {
            "corpus_id": "258866001",
            "title": "Selectively Answering Ambiguous Questions",
            "text": "A predictor is said to be well-calibrated if its predictions are accompanied by confidence scores and those scores are informative of the likelihood that the prediction is correct. Such confidence scores can be used to support applications such as selective prediction, in which the model can abstain from predicting when its confidence is low (Chow, 1957;El-Yaniv et al., 2010). In selective question answering (e.g., Kamath et al. 2020;Zhang et al. 2021b), we assign each question-answer pair (q, a) a confidence score s(q, a). The system's output is then parameterized by a threshold \u03c4 , \u0177\u03c4 (q) = arg max a s(q, a), max a s(q, a) > \u03c4 \u2205, else, \n\nwith \u2205 representing abstention. Given a probabilistic model P (a | q), a natural choice for the confidence score s(q, a) is the conditional probability of the answer. This and other scoring functions are discussed in Section 3. \n\nFor information-seeking queries such as those found in Natural Questions (Kwiatkowski et al., 2019), the askers have no answer in mind, which makes it more likely that the questions are accidentally ambiguous. The task of answering an ambiguous question requires solving at least two subtasks: determining the meaning of the question (its denotation), and then identifying the answer. This pipeline is shown in Figure 1; in a probabilistic model, this can be represented mathematically as P (a | q) = d P (a | d)P (d | q), with d indicating the denotation. While prior work has investigated the calibration of answer probabilities given by pretrained language models (e.g., Kadavath et al., 2022), it is not clear whether the relatively positive findings of these studies extend to questions with significant denotational ambiguity, as represented by datasets such as AmbigQA (Min et al., 2020) and SituatedQA (Zhang and Choi, 2021). We address this question in Section 6.4.",
            "score": 0.5670635518083106,
            "section_title": "Calibration for Question Answering",
            "char_start_offset": 3711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 344,
                    "end": 356,
                    "matchedPaperCorpusId": "26193635"
                },
                {
                    "start": 419,
                    "end": 438,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 1754,
                    "end": 1772,
                    "matchedPaperCorpusId": "216056269"
                },
                {
                    "start": 1788,
                    "end": 1810,
                    "matchedPaperCorpusId": "237491751"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26513671875
        },
        {
            "corpus_id": "253510101",
            "title": "Calibrated Interpretation: Confidence Estimation in Semantic Parsing",
            "text": "When probabilistic models are used for decisionmaking or interaction, we not only want the model to be accurate but also to be calibrated, meaning that the probability the model assigns to a decision should roughly correspond to its likelihood of being correct. For example, if a model translates a user instruction into a program for a robot, the confidence that the program accurately captures the person's intent may inform whether the robot executes the program. Because of its importance to safety and usability, a large body of work has addressed calibration, with most research considering single-timestep classification models (i.e. one output decision per input). However, probabilistic models are also commonly applied to multitimestep tasks. In these tasks, the model generates a sequence of decisions, with each subsequent 1 Models/Analysis/Data: https://github.com/ esteng/calibration_miso, Metric: https: //github.com/esteng/calibration_metric decision dependent on the input and the previous decisions. For example, text generation models typically predict sequences of words one at a time, with each new word conditioned on the previous outputs. Although calibration has been measured in some text generation tasks like machine translation (Kumar and Sarawagi, 2019;Wang et al., 2020), determining the accuracy of generated text sequences can be challenging, making calibrationthe relationship between accuracy and confidencehard to estimate. As sequence generation models, especially large language models, play a role in a growing variety of tasks, measuring their calibration has become extremely important. \n\nThe task of executable semantic parsing, where a model predicts an executable program from a natural language instruction, is often modeled as a sequence generation task. Such parsing models are used in systems that interact with the real world, such as human-robot interfaces (Tellex et al., 2020) and digital assistants (Gupta et al., 2018;Cheng et al., 2020;Semantic Machines et al., 2020). Since actions in these domains -especially physical domains -can have irreversible effects, the importance of ensuring model safety cannot be understated.",
            "score": 0.5662572922500531,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 1282,
                    "end": 1300,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 1906,
                    "end": 1927,
                    "matchedPaperCorpusId": "213739887"
                },
                {
                    "start": 1951,
                    "end": 1971,
                    "matchedPaperCorpusId": "53017700"
                },
                {
                    "start": 1971,
                    "end": 1990,
                    "matchedPaperCorpusId": "225066960"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1629638671875
        },
        {
            "corpus_id": "270620078",
            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
            "text": "Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs\u2019 reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals. Moreover, simply filtering out low-confidence responses reduces the LM\u2019s helpfulness when the answers are correct. Therefore, effectively using calibration techniques to enhance an LM\u2019s factuality remains an unsolved challenge. In this paper, we first propose an activation-based calibration method, ActCab, which trains a linear layer on top of the LM\u2019s last-layer activations that can better capture the representations of knowledge. Built on top of ActCab, we further propose CoDec, a confidence-guided decoding strategy to elicit truthful answers with high confidence from LMs. By evaluating on five popular QA benchmarks, ActCab achieves superior calibration performance than all competitive baselines, e.g., by reducing the average expected calibration error (ECE) score by up to 39%. Further experiments on CoDec show consistent improvements in several LMs\u2019 factuality on challenging QA datasets, such as TruthfulQA, highlighting the value of confidence signals in enhancing the factuality.",
            "score": 0.5657526691717374,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "268356132",
            "title": "Do Large Language Models have Shared Weaknesses in Medical Question Answering?",
            "text": "The logistic regressions both showed consistent patterns across models, with increasing prompt length corresponding to lower scores, and higher top probabilities corresponding to higher scores (Table 5).Previous work has shown that longer contexts can also lead to poor processing of the input information. 34This effect was more pronounced for the largest models, (GPT-3.5,GPT-4), indicating that the gains in accuracy for these models are more significant for questions with shorter contexts.\n\nTests of whether language models are well-calibrated, predicting higher probabilities for more likely responses, have had mixed results in previous studies. 35,36The results in this study did show a directional relationship, but most models consistently put most of the probability on a single answer, which does not translate directly into a confidence measure or probability of being correct.In the extreme case, Phi 2 did not predict uniformly low probabilities, but gave high probability to seemingly random responses.As such, while we find evidence of some degree of calibration, the particulars are more varied than for other patterns identified.",
            "score": 0.5644415267311393,
            "section_title": "Model accuracy is partially predictable",
            "char_start_offset": 17035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 309
                },
                {
                    "start": 309,
                    "end": 374
                },
                {
                    "start": 374,
                    "end": 494
                },
                {
                    "start": 496,
                    "end": 658
                },
                {
                    "start": 658,
                    "end": 890
                },
                {
                    "start": 890,
                    "end": 1018
                },
                {
                    "start": 1018,
                    "end": 1148
                }
            ],
            "ref_mentions": [
                {
                    "start": 656,
                    "end": 658,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044097900390625
        },
        {
            "corpus_id": "258865733",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "text": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "score": 0.5637147125241657,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "270702325",
            "title": "Confidence Regulation Neurons in Language Models",
            "text": "As large language models (LLMs) increasingly permeate high-stakes applications, the lack of transparency in their decision-making processes poses significant vulnerabilities and risks [5].Understanding the basis of these models' decisions, especially how they regulate confidence in their predictions, is crucial not only for advancing model development but also for ensuring their safe deployment [1,33].LLMs have been empirically shown to be fairly well calibrated: on question-answering tasks, token-level probabilities of a model prediction generally match the probability of the model being correct [39,57].This raises the question of whether LLMs possess mechanisms for general-purpose calibration to mitigate the risks associated with overconfident predictions.\n\nSignificant research has been conducted on estimating model uncertainty in neural networks [19,21] and specifically in LLMs [22].While many studies focus on quantifying and calibrating model confidence [48, 39, 46, 44, 65, inter alia], there is little research into the internal mechanisms LLMs might use to calibrate their predictions.\n\nIn this work, we explore two types of components in Transformer-based language models that we believe serve a calibration function: the recently identified entropy neurons and a new class of components that we term token frequency neurons.Entropy neurons have been brought to light by recent work [41,28] and are characterized by their high weight norm and their low composition with the unembedding matrix despite being in the final layer.The low composition with the unembedding matrix would suggest that they play a minor role in the next-token prediction.).We identify a set of neurons whose effect depends on LayerNorm (red points; metric described in \u00a73.2), and which affect the model's confidence (quantified as entropy of P model ) with minimal impact on the prediction.\n\nHowever, their high weight norm, despite LLMs being trained with weight decay [47], indicates that these neurons must be important for performance.This leads us to infer that these neurons may play a calibration role, as hypothesized by Gurnee et al. [28].",
            "score": 0.5633597405965023,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 188,
                    "end": 405
                },
                {
                    "start": 405,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 768
                },
                {
                    "start": 770,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1106
                },
                {
                    "start": 1108,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1548
                },
                {
                    "start": 1548,
                    "end": 1667
                },
                {
                    "start": 1667,
                    "end": 1669
                },
                {
                    "start": 1669,
                    "end": 1886
                },
                {
                    "start": 1888,
                    "end": 2035
                },
                {
                    "start": 2035,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 865,
                    "end": 868,
                    "matchedPaperCorpusId": "235755082"
                },
                {
                    "start": 1405,
                    "end": 1409,
                    "matchedPaperCorpusId": "258841183"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459228515625
        },
        {
            "corpus_id": "269430565",
            "title": "When to Trust LLMs: Aligning Confidence with Response Quality",
            "text": "Large Language Models (LLMs) have excelled in natural language understanding and generation (Brown et al., 2020;Anil et al., 2023). However, mounting evidence indicates that LLMs generate incorrect or nonsensical text, including fabricated citations or incorrect medical information, risking errors in critical applications (Ji et al., 2023;Zhang et al., 2023b;Agrawal et al., 2023;Du et al., 2023; Cohen et al., 2023). The urgent question is: When can we trust LLMs? Addressing this concern is essential to prevent the uncritical acceptance of misleading information and to guide decisions on when to rely on LLMs versus when to seek external knowledge. \n\nRecently, researchers have explored prompting LLMs to output calibrated confidence alongside text (Tian et al., 2023) for determining LLMs' reliability. The essence of confidence calibration is to ensure that the confidence expressed corresponds with the correctness of the response, which is critical for the model's transparency and trustworthiness. While classification tasks benefit from ground truth labels to calibrate predicted probabilities against actual correctness (Guo et al., 2017), generative tasks confront the challenge of calibration without clear ground truths. Current calibration methods for text generation rely on heuristics that consistency among multiple responses (Xiong et al., 2023) or the top-k responses facilitate calibration (Tian et al., 2023). However, these assumptions often bear little relevance to the intrinsic quality of the responses. Therefore, these strategies frequently result in a misalignment between the expressed confidence and the actual quality of the response, shown in Figure 1. The critical issue is the absence of a gold standard for confidence that directly reflects response quality, leaving models incapable of guiding confidence levels aligned with response quality. \n\nIn this study, we explore a Reinforcement Learning (RL) framework to tackle this challenge, designing reward functions that align confidence levels with response quality. This framework takes advantage of the adaptability afforded by the diverse reward functions in RL to bridge the gap between confidence and response quality (Guo et al., 2021;Ziegler et al., 2019).",
            "score": 0.5620801398389049,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 341,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1133,
                    "end": 1151,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0699462890625
        },
        {
            "corpus_id": "273812005",
            "title": "Shortcut Learning in In-Context Learning: A Survey",
            "text": "If the prediction distribution shifts, then this shift is attributed to the corresponding context. Subsequently, during actual predictions, LLMs' outputs are adjusted based on the biased distribution to mitigate biases in the context. \n\nAs research progresses gradually, Contextual Calibration has spawned various calibration methods tailored for different shortcut mitigation scenarios. Prototypical Calibration employs a Gaussian mixture distribution to estimate prototypical clusters for all categories and then utilizes the likelihood of these prototype clusters to calibrate LLMs' predictions (Han et al., 2022). Domain-context Calibration employs randomly selected in-domain words from the task corpus to estimate the label bias of the language model and utilizes this estimation for bias mitigation (Fei et al., 2023). Similarly, PMI DC re-weighs scores based on the likelihood of hypotheses (answers) given premises (questions) in a specific task domain (Holtzman et al., 2021). \n\nBut these calibration schemes may fail due to their inability to effectively estimate contextual bias with only content-free and in-domain random tokens. To address this, Batch Calibration updates the bias as more batches of input data are processed, allowing the bias to stabilize after multiple mini-batches are considered (Zhou et al., 2024a). Generative Calibration adjusts the margin of the labels estimated via Monte-Carlo sampling over the in-context model to simply calibrate the contextual prediction distribution (Jiang et al., 2023). In-Context Calibration calibrates the test distribution of the demonstration using the prior semantics expected from the samples (Jang et al., 2024). PriDe is applied to the task of multiple choice questions by arranging the option content on a small number of test samples to estimate prior information, and then using the estimated prior information to debias the remaining samples (Zheng et al., 2024). Unlike the aforementioned probability-based calibration methods, NOISYICL calibrates language models by adding Gaussian noise to the parameters of LLMs, thereby reducing the prediction bias and unreliable confidence in ICL (Zhao et al., 2024b).",
            "score": 0.5616347985025608,
            "section_title": "Model-centric Approach",
            "char_start_offset": 26952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 806,
                    "end": 824,
                    "matchedPaperCorpusId": "258865519"
                },
                {
                    "start": 962,
                    "end": 985,
                    "matchedPaperCorpusId": "233296182"
                },
                {
                    "start": 1314,
                    "end": 1334,
                    "matchedPaperCorpusId": "263310485"
                },
                {
                    "start": 1663,
                    "end": 1682,
                    "matchedPaperCorpusId": "268385528"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05975341796875
        },
        {
            "corpus_id": "252780032",
            "title": "Explaining Predictive Uncertainty by Looking Back at Model Explanations",
            "text": "A common way of measuring predictive uncertainty is by calibrating model outputs with the true correctness likelihood, so that the predictive probabilities well represent the confidence of model predictions being correct (Guo et al., 2017;Kong et al., 2020;Desai and Durrett, 2020;Zhao et al., 2021). Lower prediction confidence indicates higher uncertainty (Xu et al., 2020;Jiang et al., 2021). We follow the post-calibration methods and adopt the temperature scaling (Guo et al., 2017;Zhao et al., 2021) to calibrate the pre-trained language models (BERT and RoBERTa) in our experiments. \n\nSpecifically, we use the development set to learn a temperature T which corrects model output probabilities by dividing non-normalized logits before the softmax function. Then the learned T is applied to modify model outputs on the test set. In experiments, we linearly search for an optimal temperature T between [0, 10] with a granularity of 0.01, which empirically performs well. We evaluate model calibration with Expected Calibration Error (ECE) (Guo et al., 2017). The ECE measures the difference between prediction confidence and accuracy, i.e. \n\nwhere the total n predictions are partitioned into K equally-spaced bins, B k represents the predictions fall into the kth bin, acc(\u2022) and conf (\u2022) compute the average accuracy and confidence in each bin respectively. For a perfect calibration, acc(B k ) = conf (B k ), k \u2208 {1, . . . , K}. In this work, we set K = 10. We report the learned temperature scalars and ECEs before and after calibration in Table 6. Temperature scaling performs effectively in decreasing model calibration errors. This enables us to further explain prediction uncertainty based on calibrated confidence. We apply temperature scaling to correct model outputs in experiments.",
            "score": 0.561409894206835,
            "section_title": "A.2 Posterior Calibration",
            "char_start_offset": 15376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 239,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 257,
                    "end": 281,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 358,
                    "end": 375,
                    "matchedPaperCorpusId": "222378161"
                },
                {
                    "start": 375,
                    "end": 394,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 469,
                    "end": 487,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1043,
                    "end": 1061,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05499267578125
        },
        {
            "corpus_id": "273403321",
            "title": "Atomic Calibration of LLMs in Long-Form Generations",
            "text": "Large language models (LLMs) often suffer from hallucinations, posing significant challenges for real-world applications. Confidence calibration, which estimates the underlying uncertainty of model predictions, is essential to enhance the LLMs' trustworthiness. Existing research on LLM calibration has primarily focused on short-form tasks, providing a single confidence score at the response level (macro calibration). However, this approach is insufficient for long-form generations, where responses often contain more complex statements and may include both accurate and inaccurate information. Therefore, we introduce atomic calibration, a novel approach that evaluates factuality calibration at a fine-grained level by breaking down long responses into atomic claims. We classify confidence elicitation methods into discriminative and generative types and demonstrate that their combination can enhance calibration. Our extensive experiments on various LLMs and datasets show that atomic calibration is well-suited for long-form generation and can also improve macro calibration results. Additionally, atomic calibration reveals insightful patterns in LLM confidence throughout the generation process.",
            "score": 0.5596630473679437,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322509765625
        },
        {
            "corpus_id": "259224389",
            "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
            "text": "A key aspect of human intelligence lies in our capability to meaningfully express and communicate our uncertainty in a variety of ways (Cosmides & Tooby, 1996). Reliable uncertainty estimates are crucial for human-machine collaboration, enabling more rational and informed decision-making (Guo et al., 2017;Tomani & Buettner, 2021). Specifically, accurate confidence estimates of a model can provide valuable insights into the reliability of its responses, facilitating risk assessment and error mitigation (Kuleshov et al., 2018;Kuleshov & Deshpande, 2022), selective generation (Ren et al., 2022), and reducing hallucinations in natural language generation tasks (Xiao & Wang, 2021). \n\nIn the existing literature, eliciting confidence from machine learning models has predominantly relied on white-box access to internal model information, such as token-likelihoods (Malinin & Gales, 2020;Kadavath et al., 2022) and associated calibration techniques (Jiang et al., 2021), as well as model fine-tuning (Lin et al., 2022). However, with the prevalence of large language models, these methods are becoming less suitable for several reasons: 1) The rise of closed-source LLMs with commercialized APIs, such as GPT-3.5 (OpenAI, 2021) and GPT-4 (OpenAI, 2023), which only allow textual inputs and outputs, lacking access to token-likelihoods or embeddings; 2) Token-likelihood primarily captures the model's uncertainty about the next token (Kuhn et al., 2023), rather than the semantic probability inherent in textual meanings. For example, in the phrase \"Chocolate milk comes from brown cows\", every word fits naturally based on its surrounding words, but high individual token likelihoods do not capture the falsity of the overall statement, which requires examining the statement semantically, in terms of its claims; 3) Model fine-tuning demands substantial computational resources, which may be prohibitive for researchers with lower computational resources. Given these constraints, there is a growing need to explore black-box approaches for eliciting the confidence of LLMs in their answers, a task we refer to as confidence elicitation.",
            "score": 0.5574347669373613,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 307,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 507,
                    "end": 530,
                    "matchedPaperCorpusId": "49559279"
                },
                {
                    "start": 530,
                    "end": 557,
                    "matchedPaperCorpusId": "250341011"
                },
                {
                    "start": 952,
                    "end": 972,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05865478515625
        },
        {
            "corpus_id": "267412346",
            "title": "Calibration and Correctness of Language Models for Code",
            "text": "LLMs for code are extensively studied [54], [55]. While calibration has a long history in modeling [8], [56], it is not a frequently studied topic in the SE community. Early work moving into modern machine learning studied the calibration of smaller neural models performing classification tasks on text and images; while these early models were poorly calibrated per se, their performance could be improved by simple scaling [22] of their output probabilities. As models became larger, calibration was found to improve [57]. Pre-training was also found to improve calibration [25], [58]; however, these findings have been disputed [47]. \n\nMore recent works evaluated LLM calibration on a wide variety of settings [7], [18], [25], [59]. Desai et al. [25] studied non-code (natural language) tasks such as inference or paraphrasing, with only intrinsic measures using oldergeneration models (BERT and RoBERTA). Jiang et al. [7] studied calibration for natural language question-answering using just intrinsic measures. In contrast, we study calibration for three coding-related tasks, using both artificial and natural code datasets, and both intrinsic and reflective confidence measures, to evaluate calibration in the SE domain. \n\nOther prior work has investigated tokens that might be edited. Vasconcelos et al. [60] discusses code model uncertainty for function-synthesis-style problems, and ran human evaluation of the usefullness of colored highlighting of uncertain tokens. They found highlighting a human-derived groundtruth of which tokens might be edited was helpful, and more useful than raw token probabilities from the model. Johnson et al. [61] developed method of highlighting likely edit tokens via a utility optimization algorithm comparing different file completions. We find exploring more on calibrated uncertainty for local areas be a interesting area for additional work. \n\nLi et al. [49] investigate the calibration of Computer vision (CV) models from an operational perspective i.e., the shift between training input and production inputs, presenting it as a software quality problem that can be addressed using Bayesian approaches.",
            "score": 0.5570499613207507,
            "section_title": "VII. RELATED WORK",
            "char_start_offset": 44335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 42,
                    "matchedPaperCorpusId": "255546225"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "122906757"
                },
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "23794124"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 583,
                    "end": 587,
                    "matchedPaperCorpusId": "59336190"
                },
                {
                    "start": 714,
                    "end": 717,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 923,
                    "end": 926,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1314,
                    "end": 1318,
                    "matchedPaperCorpusId": "253393857"
                },
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "257255061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07708740234375
        },
        {
            "corpus_id": "253098276",
            "title": "Re-Examining Calibration: The Case of Question Answering",
            "text": "While large pretrained language models have conquered many downstream tasks (Devlin et al., 2019;Brown et al., 2020), it is sometimes unclear when we should trust them since they often produce false (Lin et al., 2022) or hallucinated (Maynez et al., 2020) predictions. This is important for both model deployment-where low-confidence outputs can be censored-and for end users who need to know whether to trust a model output. The solution is to make sure that models provide reliable 1 Code available at: https://github.com/ NoviScl/calibrateQA Figure 1: Distribution of predictions on HOTPOTQA in an OOD setting. We put predictions within the same confidence range into the same bucket (10 fixed-range buckets) and compute the average confidence and accuracy within each bucket. The x-axis represents the confidence range of each bucket, the y-axis represents the average answer accuracy for the dashed line plot and represents the relative bucket sizes for the histogram. Before calibration, most predictions have overly high confidence. After temperature scaling, all predictions' confidence values are scaled to become closer to the overall answer accuracy (24.5). Moreover, both correct (green bars) and wrong predictions (blue bars) are mixed in the same buckets, making them hard to distinguish. \n\nconfidence estimates so that we can abstain from wrong predictions and trust the right ones. The prerequisite for such abstention is Model Calibration: making the confidence represent the actual likelihood of being correct (Niculescu-Mizil and Caruana, 2005;Naeini et al., 2015). Past work proposes post-hoc approaches to calibrate model onfidence such as temperature scaling (Guo et al., 2017), and can effectively calibrate multi-class classification, evaluated by the expected calibration error (ECE) metric. \n\nWe re-examine calibration and apply it to a complex task with real-world applications: opendomain question answering (ODQA; Chen et al., 2017). The task takes an input question, retrieves evidence passages from a large corpus such as Wikipedia, and then returns an answer string.",
            "score": 0.556951009807518,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1302
                },
                {
                    "start": 1305,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1816
                },
                {
                    "start": 1819,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 97,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 97,
                    "end": 116,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 199,
                    "end": 217,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 234,
                    "end": 255,
                    "matchedPaperCorpusId": "218487034"
                },
                {
                    "start": 1528,
                    "end": 1563,
                    "matchedPaperCorpusId": "207158152"
                },
                {
                    "start": 1563,
                    "end": 1583,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1681,
                    "end": 1699,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1943,
                    "end": 1961,
                    "matchedPaperCorpusId": "3618568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08758544921875
        },
        {
            "corpus_id": "273233182",
            "title": "Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study of Alignment with Human Responses",
            "text": "Calibration is a measure of the trustworthiness of an LLM, comparing the confidence scores output by the LLM to the ground truth values. It allows users to see whether the LLM has an accurate gauge of its uncertainty [Kassner et al., 2023]. \n\nThis study is similar to the aforementioned studies in that it also evaluates the morality of LLMs. However, our contribution involves evaluating the models' calibration with respect to human annotators' answers by modeling output distributions. This allows for a more comprehensive insight into whether a model is truly aligned with the user population. \n\nExisting research on calibration has been fact-based. One study evaluated the effect of various changes made during the training and construction phases on the calibration of LLMs for causal language modeling, fact generation, and multi-language understanding. It was revealed that larger parameter scales and longer training dynamics during pre-training improve calibration, while instruction tuning and synthetic data deteriorate it. [Kassner et al., 2023].",
            "score": 0.5560092891018036,
            "section_title": "LLM Calibration in Factual Contexts",
            "char_start_offset": 3510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1059
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1796875
        },
        {
            "corpus_id": "253098276",
            "title": "Re-Examining Calibration: The Case of Question Answering",
            "text": "For users to trust model predictions, they need to understand model outputs, particularly their confidence - calibration aims to adjust (calibrate) models' confidence to match expected accuracy. We argue that the traditional calibration evaluation does not promote effective calibrations: for example, it can encourage always assigning a mediocre confidence score to all predictions, which does not help users distinguish correct predictions from wrong ones. Building on those observations, we propose a new calibration metric, MacroCE, that better captures whether the model assigns low confidence to wrong predictions and high confidence to correct predictions. Focusing on the practical application of open-domain question answering, we examine conventional calibration methods applied on the widely-used retriever-reader pipeline, all of which do not bring significant gains under our new MacroCE metric. Toward better calibration, we propose a new calibration method (ConsCal) that uses not just final model predictions but whether multiple model checkpoints make consistent predictions. Altogether, we provide an alternative view of calibration along with a new metric, re-evaluation of existing calibration methods on our metric, and proposal of a more effective calibration method.",
            "score": 0.5558739358382873,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1749267578125
        },
        {
            "corpus_id": "276647283",
            "title": "GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration",
            "text": "Language models are often miscalibrated, leading to confidently incorrect answers. We introduce GRACE, a benchmark for language model calibration that incorporates comparison with human calibration. GRACE consists of question-answer pairs, in which each question contains a series of clues that gradually become easier, all leading to the same answer; models must answer correctly as early as possible as the clues are revealed. This setting permits granular measurement of model calibration based on how early, accurately, and confidently a model answers. After collecting these questions, we host live human vs. model competitions to gather 1,749 data points on human and model teams' timing, accuracy, and confidence. We propose a metric, CalScore, that uses GRACE to analyze model calibration errors and identify types of model miscalibration that differ from human behavior. We find that although humans are less accurate than models, humans are generally better calibrated. Since state-of-the-art models struggle on GRACE, it effectively evaluates progress on improving model calibration.",
            "score": 0.5556412063268429,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.062103271484375
        },
        {
            "corpus_id": "270199379",
            "title": "LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models",
            "text": "This formulation has been applied to generation tasks in a number of ways [Fried et al., 2018a,b, Lazaridou et al., 2020, Vaduguru et al., 2023].In two-player communication games, Wang et al. [2021] introduce methods for improving listener model calibration with the goal of reducing \"semantic drift\" between the meaning of speaker utterances and their original meaning in natural language.In contrast, we directly train speaker models to induce calibrated answers in listener models, and our approach is complementary to the specific choice of listener model.Furthermore, the domains examined by Wang et al. [2021] are limited to a simple communication game, and they do not evaluate with human listeners.While we draw on the RSA formulation for inspiration, we do not directly apply it because listeners choose from a fixed set of interpretations in RSA.Instead, we allow for listener models to interpret implicit and explicit confidence markers in an arbitrary manner, and we train the speaker model to induce calibrated answers in the listener regardless of the listener's manner of interpretation.\n\nCalibration in LLMs.Given that calibration is key to making intelligent decisions on when to trust AI systems, a number of past efforts have documented calibration in neural models [Naeini et al., 2015, Guo et al., 2017, Ovadia et al., 2019, Wang et al., 2020] with recent work focusing on calibration in LLMs [Mielke et al., 2022, Kadavath et al., 2022, Kuhn et al., 2022, Stengel-Eskin and Van Durme, 2023, Tian et al., 2023, Zhang et al., 2023].\n\nWithin this area, several papers have focused on verbalized confidence.Mielke et al. [2022] introduce control codes based on model confidence to get models to better use epistemic markers.",
            "score": 0.5534777322573958,
            "section_title": "Figure 1: (A)",
            "char_start_offset": 10352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 1102
                },
                {
                    "start": 1104,
                    "end": 1124
                },
                {
                    "start": 1124,
                    "end": 1552
                },
                {
                    "start": 1554,
                    "end": 1625
                },
                {
                    "start": 1625,
                    "end": 1742
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 94,
                    "matchedPaperCorpusId": "21015570"
                },
                {
                    "start": 120,
                    "end": 144,
                    "matchedPaperCorpusId": "265128690"
                },
                {
                    "start": 180,
                    "end": 198,
                    "matchedPaperCorpusId": "238583037"
                },
                {
                    "start": 597,
                    "end": 615,
                    "matchedPaperCorpusId": "238583037"
                },
                {
                    "start": 1285,
                    "end": 1305,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1305,
                    "end": 1323,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1323,
                    "end": 1344,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1344,
                    "end": 1364,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 1414,
                    "end": 1434,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1457,
                    "end": 1476,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1476,
                    "end": 1511,
                    "matchedPaperCorpusId": "253510101"
                },
                {
                    "start": 1511,
                    "end": 1530,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1143798828125
        },
        {
            "corpus_id": "269603845",
            "title": "ChatNT: A Multimodal Conversational Agent for DNA, RNA and Protein Tasks",
            "text": "This calibration step is performed for all binary classification tasks. Overall, we achieve the same performance for ChatNT across tasks using these perplexity-based predictions (Fig. 6e) but with improved calibration. As a consequence, our approach can accurately measure the predictive performance of a language model in addition to effectively assessing its uncertainty level. This technique, while being general, should also be beneficial to other language model fields.",
            "score": 0.5528985147593463,
            "section_title": "Assessing the confidence of ChatNT answers",
            "char_start_offset": 29903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 474
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06719970703125
        },
        {
            "corpus_id": "278164768",
            "title": "From Evidence to Belief: A Bayesian Epistemology Approach to Language Models",
            "text": "Calibration of LLMs Calibration of language models is a key metric for ensuring faithful responses, with log probabilities often representing model confidence (Kadavath et al., 2022;Lee et al., 2023;Guo et al., 2017). As models have scaled, research has expanded to verbalized confidence, where models generate their own confidence (Lin et al., 2022;Mielke et al., 2022;Tian et al., 2023b). While confidence can improve model performance (Zhao et al., 2023;Tian et al., 2023a), some studies focus on interpreting this confidence as measuring uncertainty in semantic space (Kuhn et al., 2023) and exploring confidence through prompt and sampling methods (Xiong et al., 2024). Zhou et al. (2023) explored how epistemic markers affect calibration. However, unlike their focus on linguistic markers, our work examines how changes in epistemic evidence, containing information on both content and reliability, influence confidence and calibration. Yu and Ji (2024) report that logical probability and language model probability can differ. This supports our findings that language models may not incorporate evidence into their responses and confidence. \n\nAdversarial Context With in-context learning, studies have examined how few-shot demonstrations and explanations affect responses (Brown et al., 2020;Wei et al., 2022). Wang et al. (2023a) showed even inaccurate demonstrations could be used in Chain-of-Thought (COT) prompting, while Chia et al. ( 2023) improved question accuracy with contrastive demonstrations. Chen et al. ( 2023) studied how the number of demonstrations impacts accuracy. Feng et al. (2023) measures language models' responses and probability shifts in temporal relations based on subtle contextual changes. While these works focus on accuracy, we explore how direct question evidence influences not only accuracy but also confidence and calibration. Turpin et al. (2023); Lanham et al. ( 2023) tested perturbations in COT inputs and their impact on answers, similar to our approach.",
            "score": 0.5528985147593463,
            "section_title": "Related Works",
            "char_start_offset": 4609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 199,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 675,
                    "end": 693,
                    "matchedPaperCorpusId": "265150666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14501953125
        },
        {
            "corpus_id": "253384402",
            "title": "Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates",
            "text": "Accurate estimates of posterior probabilities are crucial for neural networks in various Natural Language Processing (NLP) tasks (Guo et al., 2017;Lakshminarayanan et al., 2017). For example, it would be helpful for humans if the models deployed in practice abstain or interact when they cannot make a decision with high confidence (Jiang et al., 2012). While Pre-trained Language Models (PLMs) have improved the performance of many NLP tasks (Devlin et al., 2019;Liu et al., 2019), how to better avoid miscalibration is still an open research problem (Desai and Durrett, 2020;Dan and Roth, 2021). In this paper, we investigate if * Corresponding authors Positive a fast funny highly enjoyable movie. Negative It's about following your dreams no matter what your parents think.  (Socher et al., 2013). The saturation of the colors signifies the magnitude. The confidence of the model should be easily recognized by looking at token attributions.\n\nand how model explanations can help calibrate the model. Explanation methods have attracted considerable research interest in recent years for revealing the internal reasoning processes behind models (Sundararajan et al., 2017;Heo et al., 2018;Shrikumar et al., 2017). Token attribution scores generated by explanation methods represent the contribution to the prediction (Atanasova et al., 2020). Intuitively, one can draw some insight for analyzing and debugging neural models from these scores if they are correctly attributed, as shown in Table 1. For example, when the model identifies a highly indicative pattern, the tokens involved would have high attribution scores for the predicted label and low attribution scores for other labels. Similarly, if the model has difficulty recognizing the inductive information of any class (i.e., the attribution scores are not high for any label), the model should not be highly confident. As such, the computed explanation of an instance could indicate the confidence of the model in its prediction to some extent.\n\nInspired by this, we propose a simple and effective method named CME that can be applied at training time and improve the performance of the confidence estimates. The estimated confidence measures how confident the model is for a specific example. Ideally",
            "score": 0.5527450930652081,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 147,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 147,
                    "end": 177,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 332,
                    "end": 352,
                    "matchedPaperCorpusId": "7716019"
                },
                {
                    "start": 443,
                    "end": 464,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 552,
                    "end": 577,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 577,
                    "end": 596,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 779,
                    "end": 800,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 1147,
                    "end": 1174,
                    "matchedPaperCorpusId": "16747630"
                },
                {
                    "start": 1174,
                    "end": 1191,
                    "matchedPaperCorpusId": "43946695"
                },
                {
                    "start": 1191,
                    "end": 1214,
                    "matchedPaperCorpusId": "3385018"
                },
                {
                    "start": 1319,
                    "end": 1343,
                    "matchedPaperCorpusId": "221970112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07684326171875
        },
        {
            "corpus_id": "264077507",
            "title": "Explaining pretrained language models' understanding of linguistic structures using construction grammar",
            "text": "Overall, we conclude that without calibration, all models seem to be highly susceptible to different combinations of bias, which completely obfuscate any underlying knowledge of the CC, leading to an accuracy at chance level across the board. \n\nWe therefore turn to our calibration methods, evaluating them first on their influence on the decision flip scores, which directly show if we were able to reduce the impact of the different types of bias. We report these only for order and vocabulary bias as we found name bias to be inconsequential. We report the complete results in Table 7. We see that across all models, while all three calibration methods work to reduce some bias, none does so consistently across all models or types of bias. Even in cases where calibration has clearly reduced the decision flip score, we find that the final calibrated accuracy is still close to 50%. This indicates that despite the effort to retrieve any knowledge that the models have about the CC, they are unable to perform clearly above chance, and we have therefore found no evidence that the investigated models understand and can use the semantics of the CC. \n\nTo investigate if this was result was exclusive to smaller, masked language models, we repeat our experiment and turn to larger, autoregressive models, more specifically, different sizes of OPT (Zhang et al., 2022). \n\n. . Experiments on autoregressive language models . . . Methods",
            "score": 0.5523825177916208,
            "section_title": ". . . Results",
            "char_start_offset": 42927,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1436
                }
            ],
            "ref_mentions": [
                {
                    "start": 1349,
                    "end": 1369,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0265045166015625
        },
        {
            "corpus_id": "271962723",
            "title": "Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from LLMs",
            "text": "Models. To achieve a notion of calibration, Cohen et al. (2021) and Penha and Hauff (2021) suggest leveraging a Bayesian interpretation of neural rankers to capture a distribution of possible scores for each document based on the internal uncertainty of the model. However, these approaches fail to ground the scores on any scale as such uncertainty information only makes sense when comparing documents, and is distinct from the type of calibration discussed in this paper. \n\nNatural Language Explanations. NLEs are an emerging point of research in NLP and IR. These explanations can be presented directly to users to elucidate the rationale behind system decisions, such as in recommendation systems (Li et al., 2020) and search results (Yu et al., 2022). More pertinently to this study, NLEs have been employed as intermediaries to enhance task efficacy (Camburu et al., 2018;Wei et al., 2022b;Ferraretto et al., 2023). Ye and Durrett (2022) use the term overlap ratio between NLEs and their corresponding inputs as a feature to calibrate LLM predictions. (Lin et al., 2022;Kadavath et al., 2022). Other works rely on training external models to associate confidence with LLM responses (Kuhn et al., 2022;Kadavath et al., 2022;Mielke et al., 2022). Tian et al. (2023) conducted systematic evaluations over a wide range of methods to obtain confidence scores from LLMs and discovered that LLM-verbalized probabilities are better calibrated than conditional probabilities across several black-box RLHF-tuned LLMs. This framework was later extended to include hybrid methods (i.e., a mix of self-consistency and verbalized confidence) and evaluated the calibration performance on five different types of datasets (Xiong et al., 2023). In Section 4.3, we introduce two methods that take advantage of the calibration capabilities of LLMs to elicit scale calibrated ranking scores.",
            "score": 0.5515419517367988,
            "section_title": "Uncertainty Calibration of Neural Ranking",
            "char_start_offset": 6424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 63,
                    "matchedPaperCorpusId": "234357857"
                },
                {
                    "start": 68,
                    "end": 90,
                    "matchedPaperCorpusId": "231582969"
                },
                {
                    "start": 702,
                    "end": 719,
                    "matchedPaperCorpusId": "224270828"
                },
                {
                    "start": 739,
                    "end": 756,
                    "matchedPaperCorpusId": "249335548"
                },
                {
                    "start": 857,
                    "end": 879,
                    "matchedPaperCorpusId": "54040953"
                },
                {
                    "start": 879,
                    "end": 897,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1059,
                    "end": 1077,
                    "matchedPaperCorpusId": "249191391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1026611328125
        },
        {
            "corpus_id": "258865733",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "text": "In summary, we study the calibration of widely used RLHF-LMs. We first replicate the finding for GPT-4 (OpenAI, 2023) that RLHF can worsen the calibration of a model's conditional probabilities using the open-source Llama-2-70B base and chat models (Figure 2). To mitigate this regression and ease extraction of calibrated confidence scores for models for which log probabilities are not available, we propose and study new methods that can elicit calibrated confidences from RLHF-LMs by prompting the model to verbalize its confidence in token space. We find verbalized probabilities are better-calibrated than conditional probabilities across several closed models, with mixed results for Llama-2-70B-Chat. \n\nOur results raise several questions for future work. Most notably, the difference between GPT-*, Claude-*, and Llama-2's ability to verbalize confidence is significant. What factors are important for learning this skill? Additionally, the 1-stage and 2-stage verbalized numerical confidence prompts sometimes differ drastically in the calibration of their confidences. How can we reduce sensitivity of a model's calibration to the prompt? Going beyond question-answering, can we leverage good calibration in short-answer settings to improve the reliability of long-form generations, perhaps by breaking down long-form generation into a sequence of short questions? Finally, to what extent does a language model's calibration depend on the domain; do our conclusions in the context of factual recall hold in the context of reasoning or arithmetic? Answering these questions provides one path toward building more trustworthy and useful language systems. Limitations. While our work demonstrates a promising new approach to generating calibrated confidences through verbalization, there are limitations that could be addressed in future work. First, our experiments are focused on factual recalloriented problems, and the extent to which our observations would hold for reasoning-heavy settings is an interesting open question.",
            "score": 0.5511288932805367,
            "section_title": "Discussion",
            "char_start_offset": 13186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2036
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285400390625
        },
        {
            "corpus_id": "263671892",
            "title": "A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores",
            "text": "This paper proposes such a general formalism, including identifying two different risk categories that are relevant to LLMs, and novel metrics for measuring risk. \n\nInitial research on LLMs' 'self-understanding' of their own uncertainty, especially in the deep learning literature, has predominantly relied on interpreting raw softmax probabilities of the final output layer as 'confidence' scores (Vasudevan, Sethy, and Ghias 2019). While studies such as (Guo et al. 2017) have flagged these scores as potentially misleading and not genuinely capturing the model's true uncertainty, (Kadavath et al. 2022) highlighted that genera-tive LLMs exhibit commendable calibration properties facing certain situations. These models can accurately predict which questions they will be able to answer correctly on diverse NLI tasks based on their confidence scores. Nevertheless, a quantitative assessment of risk, even for such models, has been lacking. (Jiang et al. 2021) suggest that, when faced with uncertain situations, LLMs can sometimes be poorly calibrated, with the confidence score estimation barely being correlated with the likelihood of the output being correct. Building on these observations, studies by (Jagannatha and Yu 2020;Jiang et al. 2021;Kuhn, Gal, and Farquhar 2022) have endeavored to re-calibrate confidence, using mechanisms like entropy, or by crafting binary classifiers based on the given confidence scores. More recently, (Zhou, Jurafsky, and Hashimoto 2023) focus on generative models, probing linguistic hedge markers in the models' outputs to evaluate their ability to discern uncertain situations. A further strand of research, such as by (Yin et al. 2023), proposes benchmarks aiming to spotlight areas of knowledge where LLMs grapple with uncertainty. Concurrently, efforts by (Collins et al. 2023) seek to minimize uncertainty in human-AI contexts by addressing risks originating from human errors. \n\nIn our work, we propose a novel re-adjusted confidence calibration for both generative and discriminative LLMs, aimed at discerning uncertain situations. We utilize a binary classifier calibration, previously shown to have exceptional performance (Kamath, Jia, and Liang 2020;Jiang et al. 2021), as an experimental baseline.",
            "score": 0.5507433251161781,
            "section_title": "Related Work",
            "char_start_offset": 5060,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 165,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 432,
                    "matchedPaperCorpusId": "155100063"
                },
                {
                    "start": 456,
                    "end": 473,
                    "matchedPaperCorpusId": "220250257"
                },
                {
                    "start": 945,
                    "end": 964,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1211,
                    "end": 1235,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1253,
                    "end": 1282,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1666,
                    "end": 1683,
                    "matchedPaperCorpusId": "258959258"
                },
                {
                    "start": 1806,
                    "end": 1826,
                    "matchedPaperCorpusId": "257687699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365234375
        },
        {
            "corpus_id": "247083990",
            "title": "Using calibrator to improve robustness in Machine Reading Comprehension",
            "text": "Calibration in NLP The question of whether a model's posterior probabilites provide an accurate empirical measure of how likely the model is to be correct on a given example has been put forward to examine the reliability of the model [Desai and Durrett, 2020]. A well-calibrated model should have output probabilities align with the true probabilities of its predictions, which means the confidences of their predictions should be more aligned with their accuracies. Previous works have found that the model which gives good confidence estimates on in-domain data is overconfident on OOD data [Desai and Durrett, 2020;Kamath et al., 2020]. \n\nIn MRC, models tend to choose results with max softmax probabilites as final outputs. But out-of-domain data leads to the shift of data distribution, so the softmax probabilies are unreliable estimates of confidence [Kamath et al., 2020;Xin et al., 2021]. Previous works used to apply the calibrator as a threshold to decide whether to abstain the prediction and try to avoid making confident yet incorrect predictions on the preserved examples [Kamath et al., 2020;Xin et al., 2021]. Instead of using it as a threshold, we use the calibrator as a reranker based on the inspiration of [Zhang et al., 2021] and our analysis on bad cases in MRC task.",
            "score": 0.5497352464553023,
            "section_title": "Adversarial Examples Generation Methods",
            "char_start_offset": 4800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1291
                }
            ],
            "ref_mentions": [
                {
                    "start": 619,
                    "end": 639,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 859,
                    "end": 880,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 880,
                    "end": 897,
                    "matchedPaperCorpusId": "235719735"
                },
                {
                    "start": 1088,
                    "end": 1109,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1109,
                    "end": 1126,
                    "matchedPaperCorpusId": "235719735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10284423828125
        },
        {
            "corpus_id": "266051956",
            "title": "A Study on the Calibration of In-context Learning",
            "text": "Language models (LMs) that encompass transformer-based architectures (Brown et al., 2020;Chowdhery et al., 2023;OpenAI, 2023) can generate coherent and contextually relevant texts for various use cases. Despite their impressive performance, these models occasionally produce erroneous or overconfident outputs, leading to concerns about their calibration (Dawid, 1982;DeGroot and Fienberg, 1983) which measures how faithful a model's prediction uncertainty is. Such a problem is pressing when users adapt them using a recent paradigm called in-context learning (Brown et al., 2020) to construct performant predictors, especially for applications in safety-critical domains (Bhatt et al., 2021;Pan et al., 2023). \n\nWe provide an in-depth evaluation and analysis of how well these models are calibrated -that is, the alignment between the model's confidence in its predictions and the actual correctness of those predictions. This token-level calibration assessment enables us to measure the discrepancy between the model's perceived and actual performance to assess its accuracy and reliability through a Bayesian uncertainty lens. \n\nWe find that LM such as LLaMA (Touvron et al., 2023a) is poorly calibrated in performant settings and there exists a calibration-accuracy trade-off (Fig. 1) for low-shot settings (k < 4): as we increase the amount of in-context samples, both prediction accuracy and calibration error increase. Such a trade-off can be improved using more ICL examples (k = 8) and larger models. Crucially, this calibration degradation worsens when fine-tuning occurs using specialized data to improve usability, such as curated instructions (Dubois et al., 2023), dialogues (Zheng et al., 2023), or human preference data (Ziegler et al., 2019). Though previous common practice suggests recalibrating models' logits via temperature scaling (Guo et al., 2017), we show that in contrast to classic regimes, the miscalibration issue in ICL can not be easily addressed using such well-established scaling approaches (Platt et al., 1999).",
            "score": 0.5496045905105096,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 89,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 89,
                    "end": 112,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 355,
                    "end": 368,
                    "matchedPaperCorpusId": "121781338"
                },
                {
                    "start": 368,
                    "end": 395,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 561,
                    "end": 581,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 673,
                    "end": 693,
                    "matchedPaperCorpusId": "226965491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406005859375
        },
        {
            "corpus_id": "267627161",
            "title": "Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification",
            "text": "The task of Case Outcome Classification (COC) involves the classification of legal case outcomes based on textual descriptions of their facts. While achieving high performance is desirable, in highstakes domains such as legal and medical decisions, the quantification of a model's predictive confidence, or conversely, its uncertainty, is particularly valuable. It allows experts to make more informed decisions, especially when the model may be uncertain or where the consequences of a misdiagnosis are significant. Evaluating whether models are aware of their limitations is known as assessing their uncertainty, with popular methods such as calibration (Guo et al., 2017;Desai and Durrett, 2020). Calibration assesses the extent to which predictive probabilities accurately reflect the likelihood of a prediction being correct. Models can opt to abstain when the uncertainty exceeds a predefined threshold -a method commonly referred to as selective classification (El-Yaniv et al., 2010;Geifman and El-Yaniv, 2017). \n\nCurrent NLP research focuses on prediction confidence and calibration to assess a classifier's awareness of its predictive performance only. This evaluation is commonly conducted against the human majority class. However, recent developments in NLP research have shed light on the prevalence of inherent human label variation (HLV) (Plank, 2022), observing disagreement * across various tasks (Uma et al., 2021). Scholars in the field argue for the acknowledgment and acceptance of HLV, as it mirrors the diverse and pluralistic nature of human values (Sorensen et al., 2024). Notably, Baan et al. 2022 has demonstrated that widely-used calibration metrics may not be suitable for datasets characterized by inherent human disagreement. \n\nIn light of these findings, we posit that models should not only be calibrated to recognize their own uncertainty (performance calibration) but also be equipped to discern instances where humans exhibit uncertainty (human calibration) (Baan et al., 2022). This dual focus aims to foster trust among end-users and mitigates potential harm caused by models. Consequently, a critical aspect of this trust involves ensuring the alignment of perceived difficulty between human and models.",
            "score": 0.5495608090239527,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 656,
                    "end": 674,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 674,
                    "end": 698,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 991,
                    "end": 1018,
                    "matchedPaperCorpusId": "491127"
                },
                {
                    "start": 1354,
                    "end": 1367,
                    "matchedPaperCorpusId": "253370768"
                },
                {
                    "start": 1415,
                    "end": 1433,
                    "matchedPaperCorpusId": "245589751"
                },
                {
                    "start": 1608,
                    "end": 1624,
                    "matchedPaperCorpusId": "253224378"
                },
                {
                    "start": 1995,
                    "end": 2014,
                    "matchedPaperCorpusId": "253224378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01267242431640625
        },
        {
            "corpus_id": "272703963",
            "title": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty",
            "text": "Large language models (LLMs) are increasingly employed in information-seeking and decision-making tasks. Despite their broad utility, LLMs tend to generate information that conflicts with real-world facts, and their persuasive style can make these inaccuracies appear confident and convincing. As a result, end-users struggle to consistently align the confidence expressed by LLMs with the accuracy of their predictions, often leading to either blind trust in all outputs or a complete disregard for their reliability. In this work, we explore supervised finetuning on uncertainty-augmented predictions as a method to develop models that produce linguistic expressions of uncertainty. Specifically, we measure the calibration of pre-trained models and then fine-tune language models to generate calibrated linguistic expressions of uncertainty. Through experiments on various question-answering datasets, we demonstrate that LLMs are well-calibrated in assessing their predictions, and supervised finetuning based on the model's own confidence leads to well-calibrated expressions of uncertainty, particularly for single-claim answers.",
            "score": 0.549350345802568,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.176513671875
        },
        {
            "corpus_id": "265150666",
            "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
            "text": "\". (Figure 1).Aided by our linguistic typology, we then measure how different epistemic marker types impact model ac-curacy.By doing so, we complement current linguistic calibration work and paint a more complete picture of the role of these epistemic markers.\n\nWe begin with two broad hypotheses on how LMs might respond to expressions of uncertainty.First, we might suppose that models are robust to any added expressions of uncertainty in the prompt.An alternative hypothesis is that, models might respond differently based on the uncertainty cues, and using a marker suggesting certainty or confidence might be more likely to produce the correct response than a prompt with low certainty or confidence.Under the latter hypothesis, we might expect performance for a prompt with no epistemic markers (which we call the standard method) to lie in between these two.This second hypothesis would also be consistent with prior work showing that LMs can generate language in the style of diverse personas (Lee et al., 2022;Park et al., 2022).\n\nSurprisingly, we find that injecting expressions of high certainty like \"I'm certain it's\" or \"I'm 100% sure\" actually leads to lower accuracy.We follow up with qualitative and quantitative analysis of popular training data, suggesting some potential sources of these unexpected behaviors.Our work thus offers three key contributions:\n\n\u2022 We introduce a typology of expressions of uncertainty to evaluate how linguistic features impact LM generation.\n\n\u2022 We demonstrate how model accuracy suffers when expressions of certainty (e.g., \"I'm certain\") are used.\n\n\u2022 We perform qualitative and quantitative analysis to reveal the potential origins of these unexpected behaviors.\n\nTogether, our findings illustrate the shortcomings of models' abilities to interpret expressions of uncertainty and highlight the gaps that still exist between natural and generated language.(Prince et al., 1982).Certainty markers are strengtheners which contain factive verbs.Evidential markers can be both expressions of certainty and uncertainty (strengtheners or weakeners).Personal pronouns and reference to sources are additional dimensions of expressions of (un)certainty not shown in this diagram.group these linguistic devices into weakeners and strengtheners.",
            "score": 0.547264754808038,
            "section_title": "Introduction",
            "char_start_offset": 2172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 14,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 260
                },
                {
                    "start": 262,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 1039
                },
                {
                    "start": 1041,
                    "end": 1184
                },
                {
                    "start": 1184,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1375
                },
                {
                    "start": 1377,
                    "end": 1490
                },
                {
                    "start": 1492,
                    "end": 1597
                },
                {
                    "start": 1599,
                    "end": 1712
                },
                {
                    "start": 1714,
                    "end": 1905
                },
                {
                    "start": 1905,
                    "end": 1927
                },
                {
                    "start": 1927,
                    "end": 1991
                },
                {
                    "start": 1991,
                    "end": 2092
                },
                {
                    "start": 2092,
                    "end": 2219
                },
                {
                    "start": 2219,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 1002,
                    "end": 1020,
                    "matchedPaperCorpusId": "252819096"
                },
                {
                    "start": 1020,
                    "end": 1038,
                    "matchedPaperCorpusId": "251403008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0552978515625
        },
        {
            "corpus_id": "268253470",
            "title": "Large language models surpass human experts in predicting neuroscience results",
            "text": "We tested LLMs by adapting the Eleuther AI Language Model Evaluation Harness framework (25), which evaluates LLMs using a multiple choice setting. We presented LLMs with two versions of the abstracts from each test case separately. We prefixed each abstract with the prompt \"You are a neuroscientist with deep knowledge in neuroscience. Here is an abstract from a neuroscience publication:\" and applied model-specific instruction templates where appropriate. We then measured the perplexity of both passages and used perplexity as the indicator of whether LLMs favor one abstract or the other. \n\nPerplexity (PPL) is one of the most common metrics for evaluating large language models. \n\nPerplexity measures the degree of uncertainty of a model when generating a particular sequence of text. Formally, perplexity is defined as the exponentiated average negative log-likelihood of a tokenized sequence. If we have a tokenized abstract X = (x 0 , x 1 , . . . , x t ), then the perplexity of X, given a LLM parameterized by \u03b8 is, \n\nwhere log p \u03b8 (x i |x <i ) is the log-likelihood of the ith token conditioned on the preceding tokens \n\nx <i according to the LLM. Given both the original abstract X orig and the altered abstract X alt , we followed the decision rule, \n\nand evaluated the overall accuracy over the entire BrainBench accordingly. \n\nAccuracy Accuracy is the primary metric for reporting LLM performance on BrainBench. A correct response was when the model produces a lower perplexity for the original abstract than the altered abstract. \n\nConfidence calibration We used the absolute difference of perplexities of two versions of the abstract as a measure of model confidence. To assess the calibration of LLMs, we compared their accuracies with their confidence levels. First, we ranked and sorted model confidence across all test cases. Subsequently, we created 20 bins based on this sort. Within each bin, we calculated the mean accuracy. A well-calibrated model will exhibit a higher accuracy in bins associated with higher confidence rankings. We fit a linear regression model using the bin number as the independent variable and the mean accuracy of each bin as the dependent variable to evaluate calibration.",
            "score": 0.5471864072526934,
            "section_title": "Model Evaluation",
            "char_start_offset": 23596,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093994140625
        },
        {
            "corpus_id": "248524894",
            "title": "Language Models in the Loop: Incorporating Prompting into Weak Supervision",
            "text": "We find that it is useful to improve the calibration of prompted labeling functions. Calibration is a measurement of how strongly a model's predicted probabilities correlate with observed accuracy, i.e., a predicted probability of p should be correct p \u2022 100% of the time. Current language models are not well-calibrated, with predicted probabilities subject to several forms of biasing, e.g., favoring tokens observed more during pretraining or tokens that appear near the end of a prompt [26; 68]. Miscalibration creates challenges in prompting, which requires choosing the most likely answer from a set of candidate text completions. When using prompts as labelers, we may also want to threshold predictions to select only the most confident answers. Popular recalibration methods such as Platt and vector scaling [38; 25] require labeled data to learn a transformation of the model's predicted probabilities, creating challenges to directly applying these methods in zero-shot settings. Instead, we use contextual calibration [68], where scaling weights are estimated from the predicted token probabilities of a prompt queried using \"content-free\" or null input instances. Contextual calibration has demonstrated empirical performance gains when used in prompt-based, few-shot classification. We use the tokens { N/A, , [MASK], NULL, <|endoftext|> } as our null inputs, using the average predicted probabilities per token to estimate our scaling weights for each prompt. The resulting transformation is then applied to each prompted labeling function's predictions.",
            "score": 0.5468875757104283,
            "section_title": "Calibration",
            "char_start_offset": 25122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1569
                }
            ],
            "ref_mentions": [
                {
                    "start": 1030,
                    "end": 1034,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25927734375
        },
        {
            "corpus_id": "273403321",
            "title": "Atomic Calibration of LLMs in Long-Form Generations",
            "text": "While large language models (LLMs), such as Llama (Touvron et al., 2023), Mistral (Jiang et al., 2023), and GPT models (OpenAI, 2022), excel in various tasks, they still struggle with faithfulness and reliability issues. LLMs often suffer from hallucinations, generating factually inaccurate content and misleading responses (Zhang et al., 2023b;Huang et al., 2023), which limits their application in high-risk real-world scenarios (Hu et al., 2023). To address this, confidence calibration aims to estimate the underlying uncertainty of model predic-Pembroke College is one of the 31 colleges of the \u2026 \u2026 Its members are termed \"Valencians\". tions and reflect the true likelihood of correctness (Guo et al., 2017). A calibrated model is crucial for real-world applications, as it allows us to determine the extent to which we can trust models' predictions (Zhu et al., 2023;Mahaut et al., 2024). \n\nMost existing work on LLM calibration focuses on short-form QA tasks (Jiang et al., 2021;Tian et al., 2023;Zhu et al., 2023;Ulmer et al., 2024). These studies often use datasets like TriviaQA (Joshi et al., 2017) and Natural Questions (Joshi et al., 2017), where answers typically have fewer than 10 words. However, in real-world applications, responses to user queries are often much longer (Zhang et al., 2024), sometimes extending to hundreds or even thousands of words. In such cases, the quality of LLM responses is not simply binary (correct or incorrect), as answers may in-clude both accurate and inaccurate statements. Previous work on long-form calibration has primarily focused on response-level calibration (Zhang et al., 2024;Huang et al., 2024) (which we term macro calibration). This approach provides an overall confidence estimation for the entire response (as shown in the upper part of Figure 1).",
            "score": 0.5463367572117136,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1813
                }
            ],
            "ref_mentions": [
                {
                    "start": 856,
                    "end": 874,
                    "matchedPaperCorpusId": "265351565"
                },
                {
                    "start": 874,
                    "end": 894,
                    "matchedPaperCorpusId": "270619760"
                },
                {
                    "start": 967,
                    "end": 987,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 987,
                    "end": 1005,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 1005,
                    "end": 1022,
                    "matchedPaperCorpusId": "265351565"
                },
                {
                    "start": 1022,
                    "end": 1041,
                    "matchedPaperCorpusId": "268358242"
                },
                {
                    "start": 1090,
                    "end": 1110,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 1133,
                    "end": 1153,
                    "matchedPaperCorpusId": "26501419"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057586669921875
        },
        {
            "corpus_id": "267770526",
            "title": "Calibrating Large Language Models with Sample Consistency",
            "text": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.",
            "score": 0.5458898224358661,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.441650390625
        },
        {
            "corpus_id": "264811211",
            "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
            "text": "Model calibration aims to align the model's confidence with the actual likelihood of output correctness. Therefore, one of the key concerns when assessing calibration is model confidence estimation. \n\nFor classification tasks, this can be achieved by directly using the class probabilities (Guo et al., 2017;Cheng et al., 2023). For generation tasks, however, the output is a sequence of token probabilities, raising the question of how to assess the model's confidence in a generated sequence, e.g., a sentence or a paragraph. Below we describe our methods for eliciting the model confidence for generations of different lengths, as well as evaluating the correctness of model generations.",
            "score": 0.545473186306962,
            "section_title": "EVALUATING CALIBRATION FOR GENERATIONS OF VARYING LENGTHS",
            "char_start_offset": 9686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 690
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 308,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 308,
                    "end": 327,
                    "matchedPaperCorpusId": "252917981"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2274169921875
        },
        {
            "corpus_id": "269149545",
            "title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation",
            "text": "While contemporary large language models (LLMs) have achieved remarkable performance in a variety of tasks ranging from question answering to complex reasoning (Brown et al., 2020;Bubeck et al., 2023), it remains a significant bottleneck for them to produce well-calibrated confidence estimates for their predictions, meaning that their individual confidence is not a reliable indicator of accuracy.Models still often generate hallucinations (Bubeck et al., 2023) or wildly wrong predictions, unknowingly and over-confidently, which is found to be more evident for models fine-tuned with RLHF (Kadavath et al., 2022;Tian et al., 2023).On the other hand, models can exhibit inconsistencies and lack of confidence, by blindly altering decisions and prioritizing incorrect user opinions (Wei et al., 2023).Such miscalibration is claimed to be even more significant for larger and more capable language models (Kong et al., 2020;Xiong et al., 2023), suggesting the ineffectiveness of model scaling (Kaplan et al., 2020) for mitigating this problem, which poses a great challenge in fostering trust in Human-AI collaboration and in developing reliable real-life applications, especially in high-risk domains.\n\nAlthough confidence estimation and calibration have been extensively studied in the broader machine learning literature (Gal & Ghahramani, 2016;Guo et al., 2017), previous work in the context of NLP mostly required extensive fine-tuning (Kong et al., 2020) or temperature-based scaling (Guo et al., 2017;Jiang et al., 2021), which can be expensive for LLMs.Recent studies on confidence estimation for black-box LLMs adopted either consistency-based approaches with repeated sampling (Wang et al., 2023b) or verbalization-based elicitation through direct prompting (Lin et al., 2022;Tian et al., 2023), or combined (Xiong et al., 2023).",
            "score": 0.5448307745234997,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 635
                },
                {
                    "start": 635,
                    "end": 803
                },
                {
                    "start": 803,
                    "end": 1203
                },
                {
                    "start": 1205,
                    "end": 1562
                },
                {
                    "start": 1562,
                    "end": 1840
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 180,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1325,
                    "end": 1349,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 1349,
                    "end": 1366,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1491,
                    "end": 1509,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1509,
                    "end": 1528,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1458740234375
        },
        {
            "corpus_id": "258865733",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "text": "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT2 , GPT-43 , and Claude 24 across three datasets, often reducing expected calibration error (ECE) by over 50%. Related Work. Several studies have examined the calibration of large LMs (Lin et al., 2022a;Park and Caragea, 2022;Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023), finding that combining large pre-trained LMs with temperature scaling (Guo et al., 2017) produces very well-calibrated predictions (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023). Other work focuses on the tendency of language and dialogue models to use linguistic expressions of uncertainty in a well-calibrated manner (Zhou et al., 2023;Mielke et al., 2022). However, existing studies focus on LMs trained purely with unsupervised learning (although Kadavath et al. (2022) briefly examine RLHF-LMs), while widely used models in practice are fine-tuned with instruction-tuning or RLHF (Christiano et al., 2017). RLHF has been shown to effectively leverage annotations of human preferences to control sentiment (Ziegler et al., 2020), improve summarization or instruction-following quality (Stiennon et al., 2022;Ouyang et al., 2022), and inject behavioral priors of harmlessness (Bai et al., 2022b,a). However, recent work has raised the question of whether or not RLHF harms calibration (OpenAI, 2023).",
            "score": 0.5447260277634942,
            "section_title": "Introduction",
            "char_start_offset": 3532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1609
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 217,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 491,
                    "end": 510,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 510,
                    "end": 533,
                    "matchedPaperCorpusId": "247450599"
                },
                {
                    "start": 555,
                    "end": 573,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 573,
                    "end": 591,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 663,
                    "end": 681,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 747,
                    "end": 765,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 765,
                    "end": 783,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 944,
                    "end": 964,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1191,
                    "end": 1216,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 1316,
                    "end": 1338,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1395,
                    "end": 1418,
                    "matchedPaperCorpusId": "232335764"
                },
                {
                    "start": 1418,
                    "end": 1438,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254638671875
        },
        {
            "corpus_id": "270620078",
            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
            "text": "Popular language model (LM) calibration methods can be categorized into three main types: verbalization-based, self-consistency-based, and logit-based approaches.Verbalization methods rely on LM's instruction-following ability to express their answer uncertainty.For example, Tian et al. (2023b) prompts the LM to provide a probability between 0.0 and 1.0 to indicate the correctness of its response.Self-consistency-based methods operate on the intuition that a model is likely to generate consistent content when it is confident about a question (Wang et al., 2023).Therefore, multiple responses are sampled and the confidence is estimated after grouping responses according to their semantic similarity (Kuhn et al., 2023;Xiong et al., 2023).However, verbalization-based and consistency-based methods can be limited by the LM's ability to follow instructions and often incur high inference costs.Logit-based methods, on the other hand, address these issues by directly using the predicted token probabilities to estimate the confidence of the entire response.For instance, both Si et al. (2023) and Liu et al. (2024) compute the response uncertainty by using the geometric mean of the token probability sequence.Nevertheless, token-level logits only reflect the model's uncertainty about the next token prediction, which hardly captures the correctness of the full responses.Different from all the prior work, ACTCAB directly estimates the answer uncertainty using LM's internal activations, achieving more accurate confidence scores and greater inference efficiency.",
            "score": 0.5438314988282411,
            "section_title": "Language Model Calibration",
            "char_start_offset": 5434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 263
                },
                {
                    "start": 263,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 745
                },
                {
                    "start": 745,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1215
                },
                {
                    "start": 1215,
                    "end": 1378
                },
                {
                    "start": 1378,
                    "end": 1570
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 295,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 548,
                    "end": 567,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 706,
                    "end": 725,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1081,
                    "end": 1097,
                    "matchedPaperCorpusId": "252917981"
                },
                {
                    "start": 1102,
                    "end": 1119,
                    "matchedPaperCorpusId": "264811211"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384521484375
        },
        {
            "corpus_id": "272987236",
            "title": "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models",
            "text": "Consistency. Few works have directly addressed the assessment of consistency. Among those that have, many focused on testing whether outputs adhere to predefined constraints. In our work, we focus on the consistency of probabilities rather than outputs. While identical probabilities imply identical outputs, the reverse is not necessarily true in language modeling, making our approach more sensitive to inconsistencies. \n\nCalibration. A common approach to assessing the quality of predicted probabilities is through calibration, which evaluates how well predicted probability scores align with membership probabilities in some reference data. In fully calibrated multiclass classifiers, calibration is considered for every class in every prediction. However, evaluating calibration, even in binned probabilities, becomes challenging with a large number of classes, making meaningful binning for every class with representative data difficult. To address this, many studies opt for top-class calibration (Guo et al., 2017), which focuses solely on calibrating the predicted class. \n\nAlthough top-class calibration is sufficient to assess the confidence of the prediction, and therefore is frequently used (Jiang et al., 2012;Guo et al., 2017), full calibration is an essential requirement for a model to be used as a density estimator in multi-class classification, in structured predictions such as sequence predictions in autoregressive text generation, and complex probabilistic generative models with textual components. \n\nWhile measuring full calibration directly is challenging, our approach, which compares the consistency of assigned probabilities to the same expression using different methods, offers alternative means to identify uncalibrated models -inconsistent estimations across different methods imply that at least one of them is miscalibrated. \n\nMany works measured the calibration of neural models (Guo et al., 2017;Wang et al., 2021), generally finding that neural models are poorly calibrated (Chen et al., 2023). In LMs, most prior work on calibration has focused on downstream tasks, such as classification and question answering (Desai and Durrett, 2020;Dan and Roth, 2021).",
            "score": 0.5438288775081245,
            "section_title": "Previous Work",
            "char_start_offset": 6547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1081
                },
                {
                    "start": 1084,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 1005,
                    "end": 1023,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1206,
                    "end": 1226,
                    "matchedPaperCorpusId": "7716019"
                },
                {
                    "start": 1226,
                    "end": 1243,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1918,
                    "end": 1936,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1936,
                    "end": 1954,
                    "matchedPaperCorpusId": "245011563"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43798828125
        },
        {
            "corpus_id": "272969457",
            "title": "A Survey on the Honesty of Large Language Models",
            "text": "Predictive Probability. A straightforward approach to providing confidence is computing predictive probability, which has been extensively explored in NLP classification tasks with masked language models (Xiao et al., 2022). In the era of LLMs, the predictive probability of an output is formalized as \n\nwhere x and y represent prompt and output respectively. As this measure is biased towards output length T (Wu et al., 2016), the length-normalized version is frequently used by dividing log p(y|x) with T (Adiwardana et al., 2020;Malinin & Gales, 2021;Si et al., 2023a;Kuhn et al., 2023). Kadavath et al. (2022) indicate that the predictive probability of LLM is well-calibrated on multiple-choice tasks (T = 1) and the calibration improves with the capability of LLM. However, empirical experiments show that predictive probability is less suitable for free-form generation tasks (T > 1) (Kuhn et al., 2023;Ren et al., 2023b). Inspired by this observation, Ren et al. (2023b) convert free-form generation into multiple-choice selection by sampling multiple candidate answers and forming them into a multiple-choice format. One potential reason for the weakness of predictive probability in free-form generation is that token probability captures lexical confidence instead of semantic confidence (Kuhn et al., 2023), which is more desired in applications. To better capture semantics, Duan et al. ( 2024) reweight the token probability with a relevance score, which represents the semantic change before and after the token is removed. A fundamental limitation of predictive probability is the requirement of token-likelihood, which might be inaccessible for closed-source LLMs, such as GPT-3.5 and GPT-4 (Achiam et al., 2023). Summary & Discussion. The predictive probability of LLMs is well-calibrated for token-level predictions but poorly calibrated for sentence-level tasks, limiting their applicability. Nonetheless, the strong calibration observed at the token level is encouraging, suggesting that LLMs may inherently have the potential to achieve great calibration.",
            "score": 0.5437968545337396,
            "section_title": "Training-free Approaches",
            "char_start_offset": 28902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 223,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 533,
                    "end": 555,
                    "matchedPaperCorpusId": "231895728"
                },
                {
                    "start": 555,
                    "end": 572,
                    "matchedPaperCorpusId": "252917981"
                },
                {
                    "start": 572,
                    "end": 590,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 892,
                    "end": 911,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 911,
                    "end": 929,
                    "matchedPaperCorpusId": "266335508"
                },
                {
                    "start": 961,
                    "end": 979,
                    "matchedPaperCorpusId": "266335508"
                },
                {
                    "start": 1300,
                    "end": 1319,
                    "matchedPaperCorpusId": "257039062"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0726318359375
        },
        {
            "corpus_id": "278171607",
            "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools",
            "text": "Well-calibrated probabilities can be used to guide downstream decisions, but calibration should never be one's only engineering target, as even a highly unsure classifier may be well-calibrated (see \u00a73.1). \n\nTo that end, we introduce a class of modelinternal confidence estimators (MICE) and an end-to-end metric, expected tool-calling utility (ETCU), to evaluate a tool-calling agent that consults a confidence estimator to decide when to launch the predicted tool call. 1 MICE extracts fea- 1 We train and test confidence estimators specifically on the generation of tool calls-a new setting for confidence estimation. However, MICE could equally well be applied to well-studied confidence estimation settings in NLP, such as machine translation (Blatz et al., 2004;Kumar and Sarawagi, 2019;Wang et al., 2020), long-form generation (Band et al., 2024), and semantic parsing (Stengel-Eskin and Van Durme, 2023a). tures by decoding from the intermediate layers of a transformer-based large language model (LLM) and computes the similarities of those generations to the output of the final layer. Based on these features and the LLM's raw confidence, it learns a model that outputs a confidence score. MICE excels on ETCU, increasingly outperforming two strong baselines as the cost of incorrect tool calls increases, without increasing calibration error. \n\nThis paper makes the following contributions: We propose a class of model-internal confidence estimators (MICE) that are empirically wellcalibrated on the task of assessing generated tool calls ( \u00a72). We introduce a new metric, expected tool-calling utility, that combines accuracy and calibration to better evaluate tool-calling agents ( \u00a73). Finally, we show that MICE is sample-efficient and can generalize to new tools, even in a zero-shot setting ( \u00a75).",
            "score": 0.5419818509851602,
            "section_title": "Introduction",
            "char_start_offset": 1587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 208,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1815
                }
            ],
            "ref_mentions": [
                {
                    "start": 748,
                    "end": 768,
                    "matchedPaperCorpusId": "773282"
                },
                {
                    "start": 793,
                    "end": 811,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 876,
                    "end": 912,
                    "matchedPaperCorpusId": "253510101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1905517578125
        },
        {
            "corpus_id": "270380186",
            "title": "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity",
            "text": "Language models, fundamentally designed to process and generate text, become highly versatile when trained on extensive datasets [27].This versatility underscores the necessity for comprehensive benchmarking to ensure their efficacy and transparency across various applications.Evaluation requires defining specific scenarios, a model, and metrics to assess performance.These evaluations span a broad spectrum of user-centric tasks, including question answering [40]- [42], information retrieval [43], summarizing, sentiment analysis, and toxicity detection.\n\nWhile accuracy is crucial for system reliability [44], it alone does not ensure AI models' utility or desirability.The integration of machine learning models into larger systems demands not only accuracy but also the capability to express uncertainty-vital for error anticipation and management, particularly in high-stakes applications like decision-making processes.Calibration [45], or a model's ability to accurately estimate the probabilities of its predictions, is essential for deploying models in sensitive settings.This is quantitatively assessed through the expected calibration error (ECE), which gauges the alignment between predicted probabilities and actual outcomes [46], [47].",
            "score": 0.5418199205531851,
            "section_title": "D. On Evaluation of Large Language Models",
            "char_start_offset": 13814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 370
                },
                {
                    "start": 370,
                    "end": 558
                },
                {
                    "start": 560,
                    "end": 675
                },
                {
                    "start": 675,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1252
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "52057510"
                },
                {
                    "start": 609,
                    "end": 613,
                    "matchedPaperCorpusId": "249872658"
                },
                {
                    "start": 940,
                    "end": 944,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 1241,
                    "end": 1245,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1247,
                    "end": 1251,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0248870849609375
        },
        {
            "corpus_id": "267751210",
            "title": "Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection",
            "text": "The model's confidence in the answers determines the extent to which we can trust the model's responses. We assess the calibration ability of LLMs using three widely adopted uncertainty estimation techniques. \n\n(1) Verbal-based method: LLMs are induced to generate a direct confidence score ranging from 0% to 100%, coupled with the corresponding answers, as illustrated by Lin et al. (2022); Kadavath et al. (2022). For instance, if an LLM generates the output \"Yes, 80%\", we extract the answer \"Yes\" and its associated confidence \"80%\". \n\n(2) Consistency-based method: For a given statement, we run the model through n rounds of inferences by altering prompt patterns or demonstrations. Candidate answers y i , where i \u2208 (1, ..., n), are voted upon for positive and negative classes Y j . The confidence score, referred to the agreement rate (Wang et al., 2022;Xiong et al., 2023), is calculated by: \n\n(3) Logit-based method (Guo et al., 2017b): In each inference, we obtain the logit p j for both candidate positive and negative tokens in the decoder, with j representing the respective class. In the single inference, the logit directly serves as the confidence score. In multiple inferences, the confidence score for class j is computed by averaging the logits of tokens belonging to class j: \n\nHere, p j i signifies the token logit of class j in the i-th response. \n\nFor both consistency-based and logit-based method, the class with the highest confidence score is deemed the final answer.",
            "score": 0.5418199205531851,
            "section_title": "Assessing Confidence Estimation Task",
            "char_start_offset": 5821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 208
                },
                {
                    "start": 211,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1495
                }
            ],
            "ref_mentions": [
                {
                    "start": 927,
                    "end": 946,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050323486328125
        },
        {
            "corpus_id": "272987064",
            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
            "text": "Recent literature has extensively discussed the challenges of maintaining calibration in LLMs, particularly highlighting the degradation in calibration post-RLHF (Lin et al., 2022;Park and Caragea, 2022;Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023). The concept of verbalized confidence has been explored as a way to counteract this degradation (Xiong et al., 2023;Tian et al., 2023), and dialogue models have been shown to express uncertainty in a well-calibrated manner (Mielke et al., 2022;Zhou et al., 2023). Compared to works on improving sentence level calibration given tokenlevel probabilities (Kuhn et al., 2023;Tian et al., 2023), our work aims to directly improve the calibration of token-level probabilities. \n\nThe calibration of neural networks has been a topic of significant interest, with foundational concepts such as proper scoring rules (Gneiting et al., 2007) laying the groundwork. Model mismatch and distribution shift often degrade calibration, commonly quantified with common metrics including Expected Calibration Error (ECE) (Naeini et al., 2015) and Brier score (Brier, 1950). Modern neural networks have been found to exhibit overconfidence (Guo et al., 2017;Thulasidasan et al., 2019;Wen et al., 2020), especially in the context of image classification (Geirhos et al., 2018;Taori et al., 2020;Wen et al., 2020;Hendrycks et al., 2021).",
            "score": 0.5418199205531851,
            "section_title": "Related Work",
            "char_start_offset": 3041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1377
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 396,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 485,
                    "end": 506,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 634,
                    "end": 652,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 869,
                    "end": 892,
                    "matchedPaperCorpusId": "123181502"
                },
                {
                    "start": 1064,
                    "end": 1085,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1102,
                    "end": 1115,
                    "matchedPaperCorpusId": "122906757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1485595703125
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "Calibration, initially developed for forecasting [92]- [94], is a technique to assess and improve the quality of uncertainty estimates by ensuring that models provide more transparent and reliable predictive probabilities [25]. Since modern deep learning models frequently rely on softmax output probabilities to estimate uncertainty, improper calibration can lead to misleading confidence scores, which is particularly problematic in high-stakes decision-making tasks. To achieve proper calibration, a model's predicted confidence level (e.g., the softmax score \u03b1) should align with its actual accuracy [41]. In other words, among all instances where the model assigns a confidence of \u03b1, the proportion of correct predictions should approximate \u03b1. This alignment can be evaluated through techniques such as reliability diagrams, expected calibration error (ECE), and temperature scaling (TS) [28]. A classification model can be considered calibrated if: \n\nwhere f (X) function that maps the feature space X and label space Y , while pf represents the predicted confidence \u03b1 of f . To assess calibration, we can partition this confidence into intervals and construct reliability diagrams [14], which visualize the relationship between predicted confidence and actual accuracy within each interval. These diagrams help identify whether the model is under-confident, over-confident, or wellcalibrated, providing insights into model reliability based on bin-wise accuracy and confidence. While achieving perfect accuracy in real-world scenarios is often infeasible [29], perfect calibration of uncertainty remains an attainable goal [95], [96]. \n\nPost-hoc or post-training methods, such as TS [28], Platt scaling [97], histogram binning [98], and training with label smoothing [99], are classical methods of calibration. Mixup training with learning the complexity of samples was proposed to calibrate the pre-trained language models [100]. We note that open-box methods of calibration can be complex and difficult to apply to LLMs, but they can be more informative of inner states than post-hoc methods [101].",
            "score": 0.5403180151750457,
            "section_title": "A. Model Calibration",
            "char_start_offset": 18857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "127203550"
                },
                {
                    "start": 55,
                    "end": 59,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1188,
                    "end": 1192,
                    "matchedPaperCorpusId": "207158152"
                },
                {
                    "start": 1690,
                    "end": 1694,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1710,
                    "end": 1714,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 1774,
                    "end": 1778,
                    "matchedPaperCorpusId": "174802983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.467041015625
        },
        {
            "corpus_id": "263134640",
            "title": "The Confidence-Competence Gap in Large Language Models: A Cognitive Study",
            "text": "The dataset table in Appendix B.5 Table 6 illuminates the average confidence scores (both absolute and relative) expressed by different LLMs at different problem levels (ranging from 1 to 5). The visualization for the table is represented in Figure 5. Predominantly, it is conspicuous that as the level of problem increases, the confidence score for LLMs decreases. The pattern is very noticeable with the absolute confidence score. LLMs felt less sure about their answers as the level of the problem increased. This result advocates that LLMs may struggle to sustain high confidence when prompted with convoluted tasks. In contrast, a relative confidence score doesn't follow this trend. Even though there is a slight reduction in relative confidence as the level of problem is increased, it is not as steep as the drop in absolute confidence. This implies that LLMs might understand their performance in comparison to others as relatively stable across different problem levels. \n\nIn addition, it is worth acknowledging that each LLM differs in their response to problem level. To illustrate, GPT-4 maintained a high confidence score across all problem levels, indicating its consistency in self-assessment of its performance. However, models like Claude-2 and Claude-Instant represented higher variability in their confidence scores as the level of the problem changed. This is another indication that some models may adapt differently to task difficulties. For example, Claude-2 shows a notable improvement in confidence levels for certain problems. \n\nIt is imperative to underscore that our analysis provides an overview of the average trends in confidence scores across different problem levels. Individual variations in model behaviors are manifested, and this observation showcases the need for a nuanced understanding of how different language models respond to diverse problem complexities. In addition to this, further investigation with the incorporation of additional factors like problem content and correctness of answer could offer valuable insight into LLMs performance and confidence assessment in particular scenarios.",
            "score": 0.5402699508973103,
            "section_title": "Problem Level vs. Confidence Scores",
            "char_start_offset": 27659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2137
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0360107421875
        },
        {
            "corpus_id": "278171607",
            "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools",
            "text": "Language models are increasingly being used as tool-using agents, where they can generate executable API calls that can change external environments (Schick et al., 2024;Yan et al., 2024;Wang et al., 2024;Roy et al., 2024). Sometimes the generated tool calls are relatively safe, and mistakes will have minimal impact (e.g., if \"how many grand slams has Serena Williams won?\" resulted in the incorrect tool call tennis_reference_count_grand_slams( name=\"venus williams\"), then the user would just be misinformed). But other times, incorrect tool calls can be more harmful (e.g., if \"please remove slash.txt\" resulted in the incorrect tool call cli(args=\"rm -rf /\"), then the user would lose the contents of their filesystem). \n\nA confidence estimator estimates the probability that another model's output is correct. A simple confidence estimator for a language model would be based on the probability that the model itself assigns to its output (i.e., the product of token probabilities) or to its output's semantic equivalence class (Zhong et al., 2023;Farquhar et al., 2024). Yet prior work has shown that this method can be poorly calibrated (Jiang et al., 2021;Mielke et al., 2022;Kadavath et al., 2022;Yin et al., 2023). A probabilistic classifier is well calibrated if on an unseen test distribution, it is correct about as often as it thinks it is (Dawid, 1982;Guo et al., 2017;Desai and Durrett, 2020;Zhao et al., 2021;Hashemi et al., 2024). For example, of those unseen examples that it predicts to be positive with \u2248 25% probability, \u2248 25% really are positive. Well-calibrated probabilities can be used to guide downstream decisions, but calibration should never be one's only engineering target, as even a highly unsure classifier may be well-calibrated (see \u00a73.1).",
            "score": 0.5377428670270953,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 170,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 187,
                    "end": 205,
                    "matchedPaperCorpusId": "268264353"
                },
                {
                    "start": 205,
                    "end": 222,
                    "matchedPaperCorpusId": "266876125"
                },
                {
                    "start": 1035,
                    "end": 1055,
                    "matchedPaperCorpusId": "249062591"
                },
                {
                    "start": 1055,
                    "end": 1076,
                    "matchedPaperCorpusId": "270615909"
                },
                {
                    "start": 1146,
                    "end": 1166,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1166,
                    "end": 1186,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1208,
                    "end": 1225,
                    "matchedPaperCorpusId": "258959258"
                },
                {
                    "start": 1386,
                    "end": 1410,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1428,
                    "end": 1449,
                    "matchedPaperCorpusId": "271923672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08770751953125
        },
        {
            "corpus_id": "276647283",
            "title": "GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration",
            "text": "Because language models are often miscalibrated, they are often confidently wrong (Kaur et al., 2020). This mismatch between accuracy and confidence causes users to trust models more than they should (Caruana, 2019;Deng et al., 2025), even over their own correct judgment (Krause et al., 2023;Stengel-Eskin and Van Durme, 2023;Liu et al., 2024). These issues are particularly severe when models are miscalibrated in ways that humans are not: users expect models to be at least as calibrated as humans, and when models are worse, users are often not prepared to address these errors. \n\nThus, models should be at least as calibrated as humans, making it especially crucial to identify when models commit calibration errors that humans do not. However, existing work on model calibration lacks comparison with human calibration. \n\nWe thus introduce GRACE, a Granular, Humangrounded Benchmark for Model Calibration Evaluation. Each instance allows fine-grained calibration measurement using an incremental question-answering (QA) framework. Expert writers design GRACE questions, each consisting of at least five sentences of clues that gradually become easier. To prevent models from being confused by ambiguities or false presuppositions, we require that clues challenge models but remain clear for humans. This format measures model calibration with human performance as a reference point: models should give correct answers earlier and more confidently than humans, while minimizing confidently incorrect guesses ( \u00a7 3). \n\nGRACE incorporates human responses from live QA competitions we conduct. Unlike prior calibration evaluation methods that only allow modelmodel calibration comparisons, our dataset thus allows direct human-model calibration comparison. GRACE is the first benchmark dataset designed to evaluate model calibration grounded in human needs. \n\nThis unique dataset is the foundation for a new metric (CALSCORE, \u00a7 4). In contrast to other calibration evaluation methods that only calculate aggregate calibration over the entire dataset, GRACE also facilitates per-instance evaluation, which helps in identifying specific contexts where models are much worse than humans at avoiding confidently incorrect answers.",
            "score": 0.5376277031119552,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1859
                },
                {
                    "start": 1862,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 101,
                    "matchedPaperCorpusId": "210154117"
                },
                {
                    "start": 200,
                    "end": 215,
                    "matchedPaperCorpusId": "198952481"
                },
                {
                    "start": 215,
                    "end": 233,
                    "matchedPaperCorpusId": "275819678"
                },
                {
                    "start": 272,
                    "end": 293,
                    "matchedPaperCorpusId": "263609662"
                },
                {
                    "start": 293,
                    "end": 327,
                    "matchedPaperCorpusId": "253510101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046295166015625
        },
        {
            "corpus_id": "273233940",
            "title": "Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models",
            "text": "Following prior work [Mohri and Hashimoto, 2024, Su et al., 2024, Quach et al., 2024], we randomly split our datasets in half by default, calibrating the sampling number and threshold for identifying high-quality responses on the first half, i.e., the calibration set, and measure the EER (i.e., miscoverage rate) at various risk levels (i.e., upper bound) in two steps on the second half, i.e., the test set. We plot the empirical results in the two steps utilizing the GPT-4o mini model on four datasets in Figure 2a and Figure 2b, and defer similar plots employing other MLLMs to Appendix F. \n\nIn the first step, we aim to address the challenge of determining whether acceptable responses are covered by the candidate set in open-ended VideoQA tasks. Figure 2a demonstrates that by deriving the minimum number of responses required to sample for each test data point on the calibration set based on our developed conformal score, we achieve statistically rigorous guarantees on EER. The marginal correctness miscoverage rates by the sampled responses on the test set are consistently bounded by various predetermined risk levels. In this way, we can approximate the fixed options provided in closed-ended tasks through sampling, despite the presence of duplicate options. \n\nIn the second step, we attempt to overcome the limitations of previous approaches, which rely on internal model logits or verbalized confidence scores. We specify the risk level in the first step to be 0.1 (i.e., \u03b1 = 0.1), and then measure the correctness miscoverage rates by the prediction sets, which are calibrated by our devised nonconformity score based on self-consistency theory. Figure 2b illustrates the validity of identifying high-quality responses based on their frequency scores with the candidate sets. The miscoverage rates on the calibrated prediction sets are strictly bounded by the risk level \u03b5 (= \u03b1 + \u03b2 \u2212 \u03b1\u03b2) at various settings of \u03b2. In this way, we obtain an intuitive confidence level by analyzing the frequency of each response in the candidate set, adapting our framework to API-only MLLMs.",
            "score": 0.537349819189929,
            "section_title": "Results for Risk Control",
            "char_start_offset": 13824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 85,
                    "matchedPaperCorpusId": "259203582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04949951171875
        },
        {
            "corpus_id": "269362024",
            "title": "Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach",
            "text": "It helps assess the reliability of responses, enhance the trustworthiness of the model, and guide users on when to trust or question the output. \n\nIt is also worth noting that calibration is closely related and can be viewed as a subclass of uncertainty estimation, where the metric corresponds to the conditional probability in the individual level. Most studies on uncertainty estimation or calibration in language models focus on fixed-dimensional prediction tasks (i.e., the output of the LLM only has one token limited in a finite set), such as sentiment analysis, natural language inference, and commonsense reasoning (Zhou et al., 2023;Si et al., 2022;Xiao et al., 2022;Desai and Durrett, 2020). However, given the structural differences in how modern LLMs are used, alongside their proven capability to handle complex, free-form tasks with variable-length outputs, there is a growing need to address uncertainty estimation and calibration specifically for general language tasks in the domain of LLMs. This work explores a simple supervised method motivated by two ideas in the existing literature on LLMs. First, prior work on uncertainty estimation for LLMs primarily focused on designing uncertainty metrics in an unsupervised way by examining aspects like the generated outputs' consistency, similarity, entropy, and other relevant characteristics (Lin et al., 2023;Manakul et al., 2023;Kuhn et al., 2023;Hou et al., 2023;Lin et al., 2022;Kuhn et al., 2023;Chen et al., 2024). The absence of the need for knowledge of the model's weights enables their application to some black-box or gray-box models. Second, a growing stream of literature argues that hidden layers' activation values within the LLMs offer insights into the LLMs' knowledge and confidence (Slobodkin et al., 2023;Ahdritz et al., 2024;Duan et al., 2024). It has shown success in other fields of LLMs, like hallucination detection (CH-Wang et al., 2023;Azaria and Mitchell, 2023;Ahdritz et al., 2024).",
            "score": 0.5369697354005328,
            "section_title": "Introduction",
            "char_start_offset": 2116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 147,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 1769,
                    "end": 1793,
                    "matchedPaperCorpusId": "264289289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1929931640625
        },
        {
            "corpus_id": "272310114",
            "title": "Assessing Generative Language Models in Classification Tasks: Performance and Self-evaluation Capabilities in the Environmental and Climate Change Domain",
            "text": "For the calibration assessment, we employed the prompt-elicitation method, as detailed in Table 1, to extract verbalized confidence scores from the models, expressed as output tokens. These scores are compared with actual precision measures to assess model calibration effectively [29]. The evaluation process delves into how well the models' stated confidences align with their empirical precision across three tasks. This method provides insights into the consistency of models' self-assessed confidences over 100 iterations, highlighting any mismatches between their provided explanations and the probability scores. We categorized the scores into five probability bins: ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0'] then calibration was evaluated by comparing the mean probabilities with the observed precision for each bin. Below we detail the calibration performance of each model, focusing specifically on the task that demonstrated the most accurate calibration outcomes. \n\nGPT demonstrates well-calibrated outputs across all tasks, with specific results for the stance detection task presented in Table 3. The stance classification exhibits high confidence levels for the supportive and skeptical/opposing labels, albeit encountering some difficulty with the neutral stance. This results indicates that the probabilities are well-calibrated in each bin, indicating that GPT provides confident responses for correct answers, and similarly, manifests considerable uncertainty when the answers are incorrect.",
            "score": 0.5368492497311392,
            "section_title": "Calibration Evaluation",
            "char_start_offset": 17166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0843505859375
        },
        {
            "corpus_id": "258967849",
            "title": "Conformal Prediction with Large Language Models for Multi-Choice Question Answering",
            "text": "Naive Calibration in LLMs. Previous works have studied calibration in context to LLMs. (Si et al., 2022)  Figure 6: Stratified coverage at each size of prediction set. For most subjects, coverage is fairly consistent at all set sizes for prediction sets constructed with the conformal prediction procedure at \u03b1 = 0.1. This means that the true answer is one of the items in the predicted set on average about 90% of the time. (Kadavath et al., 2022) found that large language models are well calibrated for various MCQA tasks. In current work, we examine the calibration error in the softmax probability output for the MCQA task for the LLaMA-13B language model. To this end, we calculate the Expected Calibration Error (ECE) and Maximum Calibration Error (MCE), metrics that measure the average and maximum discrepancy between the confidence of the model's predictions and their accuracy. We find that the naive softmax output of the model is reasonably well calibrated across subjects on average, with ECE varying between a minimum of 1% for high school biology to a maximum of 7% for marketing (refer figure 9 in the appendix.) This aligns with previous findings on calibration error in LLMs (Kadavath et al., 2022). Nonetheless, MCE is significant for most subjects, indicating that the model is under-confident or over-confident at specific confidence levels. Additionally, there are no formal guarantees in terms of calibration errors. \n\nDifference in coverage and set sizes between subjects. \n\nWe next implement the conformal prediction procedure and compare coverage and prediction set size between subjects in Figure 3 and Figure 4 at the error rate \u03b1 = 0.1. The coverage guarantee of conformal prediction holds across all subjects (Figure 3). Comparing Figure 2 and Figure 4, we see that for each of the three categories, uncertaintyas measured by prediction set sizes -is, in general, significant for subjects with low top-1 accuracy and low for subjects with high top-1 accuracy. \n\nFor example, more challenging subjects such as formal logic and college chemistry have the most uncertainty on average, while \"easier\" subjects such as marketing have the lower average uncertainty.",
            "score": 0.5366403786954134,
            "section_title": "Results",
            "char_start_offset": 12960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1990
                },
                {
                    "start": 1993,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 104,
                    "matchedPaperCorpusId": "253098276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09271240234375
        },
        {
            "corpus_id": "272423997",
            "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
            "text": "Confidence and uncertainty quantification is a wellestablished field, but the recent emergence of large language models (LLMs) has introduced new challenges and opportunities. Although studies have shown a distinction between confidence and uncertainty, we will use these terms interchangeably in our work. \n\nResearch on this topic can be broadly categorized into two areas: approaches targeting closedsource models and those focusing on open-source models. The growing applications of commercial LLMs, due to their ease of use, have necessitated particular methods to quantify their confidence. For black-box LLMs, a natural approach is to prompt them to express confidence verbally, a method known as verbalized confidence, first introduced by Lin et al. (2022). Other studies have explored this approach specifically for language models fine-tuned with reinforcement learning from human feedback (RLHF) (Tian et al., 2023;He et al., 2023). Additionally, some research has proposed new metrics to quantify uncertainty (Ye et al., 2024;Tanneru et al., 2023). \n\nOur work aligns most closely with Xiong et al. (2024), who presented a framework that combines prompting strategies, sampling techniques, and aggregation methods to elicit calibrated confidences from LLMs. While previous studies primarily benchmarked their methods on general reasoning tasks, our study focuses on the medical domain, where accurate uncertainty quantification is critical for diagnosis and decision-making. We evaluate LLM calibration using the framework defined by Xiong et al. (2024) and propose a framework consisting of a new prompting strategy and aggregation method, termed Atypicality Presentations Recalibration, which shows significant improvements in calibrating LLM uncertainty in the medical domain.",
            "score": 0.5366025497313913,
            "section_title": "Confidence and Uncertainty quantification in LLMs",
            "char_start_offset": 5058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1789
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.103759765625
        },
        {
            "corpus_id": "263135463",
            "title": "Towards Efficient and Trustworthy AI Through Hardware-Algorithm-Communication Co-Design",
            "text": "No agreement among 5 annotators Fig. 3. When deep learning-based AI models, such as large language models, output wrong decisions, as they are bound to do more or less frequently, they tend to attach a high confidence level to the incorrect output (represented as the length of the corresponding bar). Therefore, deep learning-based AI models are said to be poorly calibrated. \n\nIn response to an input, AI models produce a confidence score for every possible value of their output. For instance, as in Fig. 3, a language model like ChatGPT takes as input a prompt, along with previously generated words, to produce a score for each possible next word. A decision is made by choosing the output value with the largest confidence score. 2, the confidence level is given by the probability assigned by the output layer of the model to a given decision. The true accuracy is estimated by evaluating the average accuracy of all decisions with a given confidence level. It is also useful to plot a histogram of the confidence levels produced by the model (see bottom part), allowing also the evaluation of the average accuracy of a model. \n\nThe AI in Fig. 3 outputs the wrong decision, and it does so quite confidently, assigning a large score to its incorrect output. This is far from being uncommon for current AI models: While they may be accurate for a large fraction of inputs, when they fail, as they are bound to, they tend to do so very confidently. Conventional AI models are hence said to be poorly calibrated. \n\nIn contrast, an AI model is well calibrated if, in a sense, it knows when it knows and it knows when it does not know. That is, the model assigns a high confidence level to outputs that are likely to be correct, and it assigns low confidence levels to outputs that are unlikely to be correct. \n\nReliability diagrams are standard tools to evaluate how trustworthy an AI model is when it comes to the confidence with which it outputs decisions. To create a reliability diagram, as in Fig. 4, the true accuracy of a model's decision is plotted against the confidence level with which that decision is made. If a model provides trustworthy measures of confidence, the accuracy of a decision must always equal the corresponding confidence level.",
            "score": 0.5364818789493888,
            "section_title": "wrong!",
            "char_start_offset": 10729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1810
                },
                {
                    "start": 1813,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2258
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047882080078125
        },
        {
            "corpus_id": "267617073",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "text": "3. Interpretability: It provides a nuanced view of uncertainty, enabling decision-makers to assess confidence across multiple correctness levels, fostering greater transparency and trust in the model's outputs. \n\nWe leverage our framework to measure calibration for several LLMs on multiple datasets across three long-form QA-ASQA (Stelmakh et al., 2022), ELI5 (Fan et al., 2019), QAMPARI (Amouyal et al., 2022)-and one summarization task, CNNDM (Nallapati et al., 2016). Our results show that our methods excel over baselines by leveraging the model's nuanced confidence distribution, stronger LLMs like GPT-3.5 don't necessarily guarantee better calibration, that various calibration metrics complement each other, and LLMs exhibit better calibration on factoid datasets than more open-ended datasets. Furthermore, our analysis highlights that fine-tuning and temperature scaling could enhance calibration. Finally, we illustrate a practical application of long-form calibration: employing a cascading strategy (Chen et al., 2023a) for selective answering to optimize the cost-effectiveness of long-form text generation. In this approach, an open-source model initially handles queries and, based on its confidence levels-assessed using our system-a more advanced API model is engaged as needed. This method ensures cost efficiency while maintaining high-performance levels. \n\nIn summary, our contributions are: \n\n\u2022 A universal calibration framework for text generation tasks, enhancing LLM evaluation for critical applications. \n\n\u2022 Innovative methods for confidence elicitation and calibration measurement, applied to a variety of LLMs. \n\n\u2022 Evidence that calibration can be improved by model fine-tuning and temperature scaling. \n\n\u2022 A cost-effective model usage strategy, illustrating the practicality of long-form calibration in optimizing LLM deployment.",
            "score": 0.5362571016742944,
            "section_title": "Introduction",
            "char_start_offset": 3815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1859
                }
            ],
            "ref_mentions": [
                {
                    "start": 331,
                    "end": 354,
                    "matchedPaperCorpusId": "248157463"
                },
                {
                    "start": 361,
                    "end": 379,
                    "matchedPaperCorpusId": "196170479"
                },
                {
                    "start": 446,
                    "end": 470,
                    "matchedPaperCorpusId": "8928715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3232421875
        },
        {
            "corpus_id": "272987064",
            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
            "text": "Large language models (LLMs) have become a cornerstone of modern artificial intelligence, offering impressive capabilities in natural language processing tasks. However, the reliability of LLMs is intertwined with their ability to generate confidence scores that accurately reflect the likelihood of their outputs being correct. This calibration, aligning a model's confidence with its accuracy, is essential, especially when LLMs are deployed in real-world scenarios where decisions based on incorrect outputs can have significant consequences. \n\nWhile unsupervised pre-training methods have shown success in producing well-calibrated LLMs, * Equal contribution. a challenge arises when these models undergo finetuning through reinforcement learning from human feedback (RLHF). While RLHF fine-tuning is effective in enhancing model performance on specific tasks and aligning outputs with human preferences, recent studies indicate a notable degradation in the calibration of LLMs post-RLHF finetuning (Achiam et al., 2023;Tian et al., 2023;Kadavath et al., 2022). This degradation compromises the model's ability to provide reliable confidence scores, an issue that becomes critical when these models are applied to tasks requiring high levels of trust and accuracy. An important question arises: how can we maintain the performance gains achieved through RLHF fine-tuning while ensuring that the model's confidence scores remain reliable? \n\nTo address this challenge, our work introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration technique that predicts a temperature scaling parameter for each token prediction based on a language model's hidden features. Basic temperature scaling is a widely-used calibration method that applies a single temperature parameter across all outputs of a model. This technique, while effective in some contexts, assumes uniform calibration needs across all inputs, which is often not the case for complex models like LLMs. ATS, in contrast, predicts a unique temperature scaling parameter for each set of token predictions. This input-specific approach allows ATS to refine the calibration process, addressing the varying degrees of calibration shift that can occur after RLHF fine-tuning.",
            "score": 0.5357911581530642,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 1024,
                    "end": 1042,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.685546875
        },
        {
            "corpus_id": "269430565",
            "title": "When to Trust LLMs: Aligning Confidence with Response Quality",
            "text": "Confidence, or uncertainty, refers to the degree of certainty or assurance that accompanies a prediction or decision made by a model (Geng et al., 2023). The calibration of confidence is essential for the reliability of machine learning systems, as it ensures that predicted probabilities match the true likelihood of outcomes as closely as possible (Guo et al., 2017;Minderer et al., 2021). While in traditional classification tasks, this involves aligning predicted probabilities with actual ground truth labels, the task becomes more challenging for generative models due to the inherently ambiguous nature of the ground truth (Gawlikowski et al., 2021;Liu et al., 2023). \n\nConfidence Elicitation in LLMs Confidence elicitation in LLMs aims to gauge the certainty of responses without modifying the model or accessing its internals (Geng et al., 2023). Mielke et al. (Mielke et al., 2022) proposed an external calibrator, which requires access to the model's internals, often impractical. Lin et al. (Lin et al., 2022a) introduced verbalized confidence, prompting models to declare their certainty. Yet, they focused on fine-tuned models and did not explore zero-shot scenarios. Zhou et al. (2023) examined confidence in prompt design but did not provide explicit confidence measures to users. \n\nConfidence Calibration in LLMs Calibration in LLMs is an emerging new research direction. Tian et al. (2023) investigate verbalized approaches to calibrate the confidence of LLMs. They direct LLMs to produce the top k predictions for a query, where each prediction is paired with a distinct probability value that reflects the model's confidence in the accuracy of that prediction. Xiong et al. (Xiong et al., 2023) proposed a hybrid method by combining verbalized numbers and consistency-based scores for benchmarking.",
            "score": 0.5342584939520306,
            "section_title": "Related Works",
            "char_start_offset": 4625,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1818
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 368,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 368,
                    "end": 390,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 856,
                    "end": 891,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1236572265625
        },
        {
            "corpus_id": "264289144",
            "title": "Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting",
            "text": "Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc calibration methods for aligned LMs and propose an easy-to-implement and sample-efficient method to calibrate aligned LMs. We hope our findings could provide insights into the design of more reliable alignment processes for LMs.",
            "score": 0.5339874479327079,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.294677734375
        },
        {
            "corpus_id": "269456957",
            "title": "Calibration of Large Language Models on Code Summarization",
            "text": "We now study how calibration changes with rescaling.Table IV and Figure 5 show Brier scores, ECE, and skill scores at different setups.Following the prior research question,   We also observe that the ECE greatly reduces with rescaling.To summarize, the thresholded raw average token probability is not well-calibrated, but the thresholded rescaled average token probability is well-calibrated.Therefore, the reliability of the model increases with rescaling.\n\nCalibration changes with ASAP.Table V presents the results comparing calibration applying ASAP.We observe that, augmented with static analysis products, more of the model's output exceeds the threshold (higher success rate) in all cases except for Python with the GPT-3 Turbo model.Turning now to the calibration, for Java, the skill score decreases with ASAP, whereas for Python, the skill score increases for the CodeLlama-70B and deepseek-coder-33binstruct models.In these two cases, the skill score increases alongside success rate.It's noteworthy that Java is a strictly typed and verbose language; the additional information provided by ASAP in the prompts may be redundant, and distort the model's confidence.However, for Python, ASAP may verbalized confidence.In the x-axis, we present the verbalized confidence scores, and on the y-axis is accuracy with respect to optimal F1 thresholded Sentence-BERT.For both zero-shot and few-shot settings, we found that the models were similarly not well-calibrated.Note that we tried a higher temperature and generated multiple samples.Majority voting from multiple samples did not change the calibration outcome.\n\nTherefore, we can conclude that the models are not wellcalibrated for any of the self-reflective measures studied.",
            "score": 0.5337010497977193,
            "section_title": "B. RQ2: Platt Rescaled vs. Raw Calibration",
            "char_start_offset": 34784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 52,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 236
                },
                {
                    "start": 236,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 459
                },
                {
                    "start": 461,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 556
                },
                {
                    "start": 556,
                    "end": 743
                },
                {
                    "start": 743,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 997
                },
                {
                    "start": 997,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1474
                },
                {
                    "start": 1474,
                    "end": 1545
                },
                {
                    "start": 1545,
                    "end": 1622
                },
                {
                    "start": 1624,
                    "end": 1738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.016845703125
        },
        {
            "corpus_id": "269362024",
            "title": "Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach",
            "text": "Large language models (LLMs) have marked a significant milestone in the advancement of natural language processing (Radford et al., 2019;Brown et al., 2020;Ouyang et al., 2022;Bubeck et al., 2023), showcasing remarkable capabilities in understanding and generating human-like text. However, their tendency to produce hallucinations-misleading or fabricated information-raises concerns about their reliability and trustworthiness (Rawte et al., 2023). The problem of whether we should trust the response from machine learning models is critical in machine-assisted decision applications, such as self-driving cars (Ramos et al., 2017), medical diagnosis (Esteva et al., 2017), and loan approval processes (Burrell, 2016), where errors can lead to significant loss. This issue becomes even more pressing in the era of generative AI, as the outputs of these models are random variables sampled from a distribution, meaning incorrect responses can still be produced with positive probability. Due to this inherent randomness, the need to address uncertainty estimation in generative AI is even greater than that in other machine learning models (Gal and Ghahramani, 2016;Lakshminarayanan et al., 2017;Guo et al., 2017;Minderer et al., 2021), and yet there has been limited research in this area (Kuhn et al., 2023;Manakul et al., 2023;Tian et al., 2023). \n\nIn this work, we aim to formally define the problem of uncertainty estimation for LLMs and propose methods to address it. As shown in Figure 1, uncertainty estimation for LLMs can be broadly defined as the task of predicting the quality of the generated response based on the input. In this context, \"quality\" typically refers to aspects such as confidence, truthfulness, and uncertainty. Assuming access to a universal metric for evaluating the confidence of the output, the goal of uncertainty estimation is  to produce a confidence score that closely aligns with this metric. Given the inherent randomness in LLMs, where incorrect responses can still be generated with positive probability, uncertainty estimation serves as a crucial safeguard. It helps assess the reliability of responses, enhance the trustworthiness of the model, and guide users on when to trust or question the output.",
            "score": 0.5325524466229509,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 137,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 137,
                    "end": 156,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 156,
                    "end": 176,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 613,
                    "end": 633,
                    "matchedPaperCorpusId": "10729350"
                },
                {
                    "start": 653,
                    "end": 674,
                    "matchedPaperCorpusId": "3767412"
                },
                {
                    "start": 1214,
                    "end": 1236,
                    "matchedPaperCorpusId": "235435823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04559326171875
        },
        {
            "corpus_id": "269157057",
            "title": "Language Model Cascades: Token-level uncertainty and beyond",
            "text": "Another key challenge in measuring the uncertainty for natural language outputs is that there are a lot of semantic equivalent sentences and hence, the probability can be divided among multiple outputs which mean the exact same thing.Kuhn et al. (2023a) proposes to mitigate this problem by sampling multiple outputs and then clustering semantically equivalent outputs together and combining their probability together.It would be interesting to understand how well this method can work for our setting.\n\nGenerative uncertainty.The above has largely focussed on generalizing the standard maximum predictive probability (Equation 2) from classification to the LM setting.While this by itself leads to a rich array of possible confidence measures, LMs intriguingly offer a wholly new possible means of assessing confidence: one may directly probe the model to obtain how confident it is on the proposed answer (Kadavath et al., 2022).Kadavath et al. (2022) discuss various ways of the input prompt format for this confidence probe.They also discuss the training of an additional head of the model to predict the model confidence but again, it is not clear how this compares with the standard probability output by the model without any additional finetuning.However, (Shrivastava et al., 2023) found that the confidence measures generated linguistically give worse estimates of uncertainty compared to the classical softmax-based measures even when these softmax-based probabilities come from a different and weaker model.Moreover, they observed that two sources of uncertainty are complementary and it can be beneficial to combine them.\n\nOther work.Zhao et al. (2023) proposed sequence-level calibration as a means to improve the generative ability of LMs; such calibration could also be useful in improving methods such as Chow-Sum.Kuhn et al. (2023b) proposed to ask the model to detect ambiguous questions which the model is likely to get wrong and answer clarifying questions if the question is indeed ambiguous.Hendy et al. (2023) proposed to use an exogeneous quality estimation model to decide how to route between two models.\u0160akota et al. (2023) similarly proposed to train a meta-model to pick an appropriate model from a family.Fadeeva et al. (2023) did a comprehensive experimental analysis of various uncertainty methods.",
            "score": 0.5322122004735094,
            "section_title": "Uncertainty Quantification for LMs",
            "char_start_offset": 31087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 503
                },
                {
                    "start": 505,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 932
                },
                {
                    "start": 932,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1520
                },
                {
                    "start": 1520,
                    "end": 1635
                },
                {
                    "start": 1637,
                    "end": 1648
                },
                {
                    "start": 1648,
                    "end": 1832
                },
                {
                    "start": 1832,
                    "end": 2015
                },
                {
                    "start": 2015,
                    "end": 2132
                },
                {
                    "start": 2132,
                    "end": 2237
                },
                {
                    "start": 2237,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 253,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1648,
                    "end": 1666,
                    "matchedPaperCorpusId": "252683988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1295166015625
        },
        {
            "corpus_id": "267522811",
            "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
            "text": "Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA. Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \\emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss. The LLMs, after the reconfidencing process, indicate improved confidence alignment with the accuracy of their responses.",
            "score": 0.5321683597955336,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1279296875
        },
        {
            "corpus_id": "263671564",
            "title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
            "text": "In this section, we performed a calibration analysis comparing our approaches with baselines. For our voting-based method, we take n/K as the confidence score. For LLM as an external verifier, we followed the prior works' prompting method (Kadavath et al., 2022) to let the LLM make the decision according to the textual contents. 5 Following their setting, we consider two temperature setups, i.e., T = 1 and T = 2, and donate the variants as LLM-QA T=1 and LLM-QA T=2, respectively. The prompts are shown in Table 31. We also note that, in the original method of Kadavath et al. (2022), the authors used the token probability of an LLM in predicting the answer to be \"True\" as its confidence score. Since we cannot obtain the token probability of GPT-3.5-turbo, the confidence score is obtained by sampling the True/False prediction K times and reporting the frequency of \"True\" (n/K). This will induce significantly more costs than our approach, but we adopted the strategy here for this calibration analysis. We perform the calibration analysis in the GSM8k dataset. \n\nThe calibration results on GSM8k are illustrated in Figure 8(Left). Our major observations are: (1) All decision-making methods yield a monotone calibration curve, implying that when they have higher confidence in a certain answer, the answer is generally more likely to be true. For our three vote-based approaches, this upward trend also supports our initial hypothesis that a question is easy if the weaker LLM exhibits consistency in its responses across multiple sampling paths; (2) LLM-QA T=2 with a larger temperature can lead to the least calibrated decision maker; (3) However, there is no a significant difference among other approaches in terms of their calibration degree (though all our variants showed to be better than LLM-QA). \n\nWe wanted to note that achieving perfect calibration with n/K as the confidence score is not necessary for our task.",
            "score": 0.5318004743484168,
            "section_title": "I CALIBRATION ANALYSIS",
            "char_start_offset": 46936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 1934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045440673828125
        },
        {
            "corpus_id": "263671892",
            "title": "A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores",
            "text": "In the experiments, we employ both discriminative and generative models as inference systems. While both models can generate a confidence distribution for each inference instance, there is an inherent challenge: the absence of explicit decision rules within these models. Discriminative models naturally lack integrated decision rule mechanisms, and Figure 1: The proposed framework for measuring the risk-reward tradeoff of an LLM when it is embedded in a large inference system. The top pane presents the training process of our proposed decision rule, a risk-adjusted calibration named the DwD method. The middle and last pane respectively depict the decision risk and composite risk evaluation of a large language model when influenced by a decision rule. Note that the large language model in the middle and bottom panes can differ from that in the top pane (but not necessarily so). Symbols are further described in the main text. modern generative models, such as ChatGPT, have not disclosed their decision-making protocols, making any decision rules within them unpredictable from a user's standpoint. A decision rule that is compatible with both discriminative and generative LLMs, and that is somewhat independent of the LLM itself (and hence, generalizable), is clearly motivated. An ideal decision rule should use all available information, such as the instance prompts, as well as the outputs from the underlying LLMs, to minimize the risks defined earlier. \n\nAs noted in Related Work, prior research introduced basic re-calibration of confidence scores to better model an LLM's underlying uncertainty. We expand on this idea significantly by proposing a novel risk-adjusted calibration method called 1 DwD that can act as a decision rule to help minimize the decision and composite risks of LLMs, especially in high-risk inference scenarios. DwD addresses the aforementioned challenges and does not have strong dependencies on the underlying LLMs, enabling it to be suitable for both discriminative and generative LLMs. \n\nWhile DwD is modeled as a binary classifier using a conventional machine learning algorithm, it is differentiated in two aspects:",
            "score": 0.5311757136541826,
            "section_title": "Risk-adjusted Calibration Approach",
            "char_start_offset": 15020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2033
                },
                {
                    "start": 2036,
                    "end": 2165
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1265869140625
        },
        {
            "corpus_id": "267682039",
            "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
            "text": "Earlier works (Xiao and Wang, 2019;Desai and Durrett, 2020;Jiang et al., 2021) on uncertainty in language models have focused on the calibration of classifiers (e.g., applying dropout to the model parameters or leveraging ensemble voting) to better assess the confidence of the generated output. When it comes to the era of LLMs, multiple works (Xiao and Wang, 2021;Xiao et al., 2022;Lin et al., 2022;Yu et al., 2022;Lin et al., 2023;Kuhn et al., 2023;Fadeeva et al., 2023) have been proposed to measure the uncertainty of LLM's prediction in multiple aspects (e.g., lexical uncertainty, text uncertainty, and semantic uncertainty) for multiple NLP tasks. Another line of works (Kadavath et al., 2022;Zhou et al., 2023;Amayuelas et al., 2023;Chen et al., 2024) instead tries to analyze how to extract knowledge from a language model correctly and self-evaluate the correctness with a confidence score. However, despite these commendable efforts, existing methods still lack an effective way to directly quantify and decompose the uncertainty inherent in the outputs of LLMs with in-context learning. \n\nWe evaluate the uncertainty decomposition procedure on realistic natural language understanding problems. By comparing state-of-the-art uncertainty quantification methods, we aim to examine what type of uncertainty is the most promising indicator of high confidence for LLMs. In addition, we also provide generalization analysis and two specific out-of-distribution detection applications. Due to the space limit, extra experiments and experiment settings are provided in the Appendix.",
            "score": 0.5309539571769195,
            "section_title": "Related Works",
            "char_start_offset": 14498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1587
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 35,
                    "matchedPaperCorpusId": "53715584"
                },
                {
                    "start": 401,
                    "end": 417,
                    "matchedPaperCorpusId": "252626745"
                },
                {
                    "start": 452,
                    "end": 473,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 742,
                    "end": 760,
                    "matchedPaperCorpusId": "250390527"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08709716796875
        },
        {
            "corpus_id": "258865733",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "text": "Our experiments use three questionanswering datasets assessing factual knowledge. \n\nTriviaQA (Joshi et al., 2017) contains 650k question-answer pairs gathered by trivia enthusiasts; SciQ (Welbl et al., 2017) contains approximately 14k crowdsourced science exam questionanswer pairs; TruthfulQA (Lin et al., 2022b) contains 817 questions designed to test language models' tendency to mimic human falsehoods. We sample 1000 questions from the validation split of TriviaQA (rc.web.nocontext) and SciQ and all 817 questions from the validation split of Truth-fulQA (generation) for our experiments. \n\nEvaluation protocol. For each dataset, we generate a response and corresponding confidence from each method on each of the evaluation questions. \n\nBecause calibration essentially quantifies the relationship between model confidence and correctness, computing correctness is crucial to accurate measurements of calibration. However, we find doing so to be a challenge, especially in datasets where only a single ground-truth answer (but not aliases or semantically equivalent rephrases) is provided. To avoid excessive false negatives in our correctness computation as a result of exact-match evaluation, we use either GPT-4 or GPT-3.5 to evaluate whether a response is essentially equivalent to the ground truth answer; see Appendix C for the complete equivalence-checking procedure. \n\nMethods. We compare a wide variety of methods for extracting confidence estimates from LLMs. For a comprehensive list of the prompts used for each method, see Appendix Table 6. First, we consider two methods that leverage the true conditional distribution of the model to gener- ate confidence scores. The simplest is Label prob., which uses the conditional probability distribution p(y|x) of the model given a question x, which we estimate using n = 10 samples, since many RLHF-LMs are closed-source and do not offer per-token probabilities. 67 We return the most common answer, using the LLM-based equivalence function to determine when two lexically different answers are semantically equivalent.",
            "score": 0.530778354942263,
            "section_title": "Evaluating Calibration in RLHF-LMs",
            "char_start_offset": 6888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 84,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 113,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 294,
                    "end": 313,
                    "matchedPaperCorpusId": "237532606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12127685546875
        },
        {
            "corpus_id": "275570800",
            "title": "Rational Tuning of LLM Cascades via Probabilistic Modeling",
            "text": "Tables 1 and 2 show the overall performance of all the language models across tasks, including the calibration performance. We measure calibration in terms of the expected calibration error (ECE), which we compute adaptively by bucketing confidence scores into 10 bins based on the deciles of their distributions. Tables 1  and 2 yield several interesting findings. First, some of the models often return raw log probabilities indicating certainty (\u2212\u221e or 1.0). This tendency varies strongly by model family. OpenAI's GPT models are especially prone to certainty: on MMLU, for example, GPT-4o Mini returns raw confidence 1.0 on 45.7% of queries, while GPT-4o does so on 22.7% of queries. By contrast, Llama3.1 405B returns perfect confidence only on 0.1% of queries. \n\nSecond, the test ECE for our calibration scheme varies by model and by benchmark. The benchmark yielding the poorest calibration is MedMCQA, giving an average test ECE of 7.4% across models. However, some models give exceptional calibration performance across benchmarks. GPT-4o stands out: its test ECE never exceeds 4.8%, which is its ECE on MMLU. \n\nOverall, we observe that our calibration scheme performs satisfactorily across benchmarks and models, with most benchmarks reporting an average test ECE below 5%. Table 2 ablates the importance of the hyperparameter-free feature transforms ( 16) and (17) for obtaining effective calibration. Applying these transformations results in much lower test ECE scores, reducing them by 28.2% on average. Figure 7 in Appendix E further verifies calibration by showing that, across models and benchmarks, rejecting queries for which the calibrated confidence is < 1 \u2212 q approximately lowers the test error rates to < q.",
            "score": 0.5303942790173835,
            "section_title": "Performance Summary",
            "char_start_offset": 23404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1730
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1480712890625
        },
        {
            "corpus_id": "265456211",
            "title": "Uncertainty-aware Language Modeling for Selective Question Answering",
            "text": "Large-language models (LLMs) have demonstrated great abilities in natural language tasks, including question answering (QA) wherein the model receives a question as input and outputs a response answer. The QA task is a fundamental component in many LLM applications. However, in order to robustly answer questions accurately, the model must understand context and ground its outputs in knowledge obtained from training data, which will typically contain conflicting information. Indeed, it has been shown that LLMs commonly fail in QA tasks (Geiger et al. 2019), and that these failures are associated with a limited understanding of output confidence, out-of-domain data, ambiguity in input prompts, inconsistent training information, and hallucinations, among others. Selective prediction (El-Yaniv et al. 2010;Geifman and El-Yaniv 2017), i.e., calculating confidence estimates along with predictions to forgo outputs likely to be incorrect, can be used to mitigate some of these issues. \n\nSeveral approaches utilize the selective prediction to guide question answering tasks (Pe\u00f1as et al. 2010;Gondek et al. 2012) and generally abstain from answering questions when output confidence is low. The objective is to maintain a given accuracy while answering as many questions as possible, as opposed to the more conventional goal of attempting to answer all questions correctly. One approach used inferred softmax classifier probabilities to calibrate which questions to respond to (Rodriguez et al. 2021). However, Figure 1: Robust, uncertainty-aware language modeling. Our methodology converts large language models (LLMs) -agnostic of architecture -into uncertainty-aware variants and applies to generative (i.e., next-token prediction, left) and extractive (i.e., sub-context answering, right) models. training a calibration model is challenging and softmax classifiers are often unreliable (Guo et al. 2017). Similarly, an out-of-domain (OOD) calibrator can be trained to detect OOD inputs (Kamath, Jia, and Liang 2020) but requires known or synthetic out-of-domain samples and does not consider other sources of inaccuracies like over-represented features or ambiguous labels.",
            "score": 0.5301020044883623,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 791,
                    "end": 813,
                    "matchedPaperCorpusId": "10773394"
                },
                {
                    "start": 813,
                    "end": 839,
                    "matchedPaperCorpusId": "491127"
                },
                {
                    "start": 1078,
                    "end": 1097,
                    "matchedPaperCorpusId": "2437669"
                },
                {
                    "start": 1097,
                    "end": 1115,
                    "matchedPaperCorpusId": "17956612"
                },
                {
                    "start": 1894,
                    "end": 1911,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1300048828125
        },
        {
            "corpus_id": "265213190",
            "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
            "text": "With the widespread application of large language models (LLMs), it is becoming crucial to ensure predictions from LLMs are trustworthy. One critical dimension of trustworthiness is the ability to indicate when generated text is reliable 1 UC Santa Barbara 2 MIT-IBM Watson AI Lab, IBM Research 3 MIT CSAIL. Correspondence to: Bairu Hou <bairu@ucsb.edu>. \n\nProceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). and correct, which may be formalized as the problem of uncertainty quantification (UQ). Uncertainty quantification aims to measure the confidence level of neural networks in their predictions (Gal et al., 2016;Bhatt et al., 2021;H\u00fcllermeier & Waegeman, 2021). A higher uncertainty implies the output of LLMs should be clarified, manually evaluated, or rejected. \n\nQuantifying LLMs' total uncertainty has been the focus of increasing research attention. Existing work observes that LLMs are relatively well-calibrated, especially when predictions are obtained by ensembling multiple reasoning chains (Wang et al., 2022;Huang et al., 2022;Si et al., 2023) or prompts (Jiang et al., 2023), or when LLMs are prompted to directly output their confidence levels (Kadavath et al., 2022;Lin et al., 2022;Tian et al., 2023). Many other methods have been proposed to quantify the uncertainty of LLMs (Lin et al., 2022;Xiao et al., 2022;Kuhn et al., 2022;Lin et al., 2023;Duan et al., 2023;Huang et al., 2023;Park & Kim, 2023;Ren et al., 2023). Accurate quantification of the uncertainty can be used for various applications, such as out-of-distribution detection and misclassified data detection. \n\nHowever, measuring uncertainty is just the first step towards understanding uncertainty in LLM predictions.",
            "score": 0.528539493973394,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1790
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 704,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 704,
                    "end": 723,
                    "matchedPaperCorpusId": "226965491"
                },
                {
                    "start": 723,
                    "end": 752,
                    "matchedPaperCorpusId": "216465307"
                },
                {
                    "start": 1093,
                    "end": 1112,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1131,
                    "end": 1147,
                    "matchedPaperCorpusId": "252917981"
                },
                {
                    "start": 1273,
                    "end": 1290,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 1384,
                    "end": 1402,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 1402,
                    "end": 1420,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 1420,
                    "end": 1438,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1509,
                    "end": 1526,
                    "matchedPaperCorpusId": "259342058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11517333984375
        },
        {
            "corpus_id": "268876453",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "text": "Large language models (LLMs) acquire vast world knowledge and demonstrate powerful capabilities through pre-training (Brown et al., 2020;OpenAI, 2023;Bubeck et al., 2023;Sun et al., 2024). With technologies like RLHF (Ouyang et al., 2022) and RLAIF (Bai et al., 2022;Lee et al., 2023), large language models can become more helpful and harmless to align with human preferences (Askell et al., 2021). However, how to build a more honest system has not yet been fully discussed. An honest model should have a certain understanding of the boundary of its knowledge, that is, knowing what it does not know (Yin et al., 2023;Yang et al., 2023b;Zhou et al., 2024). A plausible method is utilizing the calibrated confidence to estimate the knowledge boundary of language models. For pre-trained language models, the per-token logit can already be considered a well-calibrated confidence score, which implies that pre-trained language models (mostly) know what they know (Kadavath et al., 2022). \n\nHowever, recent studies have indicated that language models optimized with techniques like RLHF will exhibit issues of overconfidence (Lin et al., 2022a;Kadavath et al., 2022;OpenAI, 2023;He et al., 2023;Zhao et al., 2023;Tian et al., 2023;Xiong et al., 2023). This issue could be reflected in Multiple-Choice Question Answering (MCQA) tasks, where the probability of RLHF-LMs generating a token and the likelihood of that token being the correct answer are not well-calibrated. For example, an answer provided by RLHF-LMs with 95% confidence does not mean that there is a 95% probability that the answer is correct. This phenomenon may be due to the optimization objective of RLHF, which is to make the model generate responses aligned with human preferences rather than fitting answers that appear more frequently in the corpus during the pre-training stage.",
            "score": 0.528212169255289,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 602,
                    "end": 620,
                    "matchedPaperCorpusId": "258959258"
                },
                {
                    "start": 639,
                    "end": 657,
                    "matchedPaperCorpusId": "252873167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1259765625
        },
        {
            "corpus_id": "266977353",
            "title": "Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty",
            "text": "Our work focuses on the alignment between LM accuracy and LM-articulated epistemic markers as perceived by users. This is referred to as linguistic calibration (Mielke et al., 2020) which builds off of work in both linguistics and machine learning. \n\nLinguists has extensively studied epistemic markers as ubiquitous linguistic features that signal speaker commitment and stance. These markers broadly fall into two categories: weakenersexpressions of uncertainty, and strengthenersexpressions of certainty (Lakoff, 1975;Hyland, 2005Hyland, , 2014)). \n\nIn machine learning, work has focused on improving model calibration (Jiang et al., 2021;Desai and Durrett, 2020;Jagannatha and Yu, 2020;Kamath et al., 2020;Kong et al., 2020) by calibrating the confidence value assigned by a model and model accuracy through a measure called ECE (Naeini et al., 2015). Recent work has focused explicitly on how pretraining (Hendrycks et al., 2019) and scaling (Srivastava et al., 2022;Chen et al., 2023) impacts the calibration of language models. Most relevant to our work is Dhuliawala et al. (2023)'s studies on how humans interpret numerical confidences in calibrated and miscalibrated settings. A key issue remains, as numeric confidence values are known to be challenging for users to interpret (Miller, 2019). Our work, in contrast, aims for a more comprehensive understanding of how humans interpret LM-generated verbal epistemic markers. \n\nWe begin by eliciting open-ended generations, a departure from prior methods which prompts models to produce a predefined set of confidence expressions either numerically (Kadavath et al., 2022;Tian et al., 2023;Liu et al., 2023;Xiong et al., 2024;Tanneru et al., 2023) or using an ordinal scale (Lin et al., 2022;Mielke et al., 2020).",
            "score": 0.5275685385882871,
            "section_title": "Epistemic Markers in Language Models",
            "char_start_offset": 4463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1771
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 181,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 507,
                    "end": 521,
                    "matchedPaperCorpusId": "29988132"
                },
                {
                    "start": 521,
                    "end": 533,
                    "matchedPaperCorpusId": "144019269"
                },
                {
                    "start": 533,
                    "end": 549,
                    "matchedPaperCorpusId": "220459313"
                },
                {
                    "start": 622,
                    "end": 642,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 666,
                    "end": 690,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 690,
                    "end": 710,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 833,
                    "end": 854,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 910,
                    "end": 934,
                    "matchedPaperCorpusId": "59336190"
                },
                {
                    "start": 972,
                    "end": 990,
                    "matchedPaperCorpusId": "253244504"
                },
                {
                    "start": 1064,
                    "end": 1088,
                    "matchedPaperCorpusId": "264405995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1585693359375
        },
        {
            "corpus_id": "267523099",
            "title": "Open-Vocabulary Calibration for Fine-tuned CLIP",
            "text": ", M }. While the tuning methods achieve promising performance, the reliability of the prediction probability p(c|x) is still a mystery for tuned VLMs. We proceed by introducing the calibration in the prompt tuning setting. \n\nConfidence calibration. For multi-class classification tasks, vision-language models are designed to produce the class probabilities p(c|x) as defined in Equation 2. In addition to high accuracy, it is generally expected for visionlanguage models to be well calibrated, i.e., the predicted class probabilities can faithfully estimate the true probabilities of correctness. Formally, a perfectly calibrated model satisfies P r(c \n\nTo quantify the degree of miscalibration, the Expected Calibration Error (ECE) is defined as the difference between accuracy and confidence. With N samples grouped into K bins {b 1 , b 2 , . . . , b K }, the ECE is calculated as: \n\nwhere acc (\u2022) and conf (\u2022) denotes the average accuracy and confidence in bin b k .",
            "score": 0.5275573449724104,
            "section_title": "Background",
            "char_start_offset": 6472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 970
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474365234375
        },
        {
            "corpus_id": "273963769",
            "title": "Epistemic Integrity in Large Language Models",
            "text": "User input: \"Have scientists discovered a drug that cures Alzheimer's?\" LLM output: \"Scientists have not discovered a drug that cures Alzheimer's.\" \n\nInternal certainty score: 60% \n\nLinguistic assertiveness: 95% \n\nFigure 1: This figure illustrates two examples with varying levels of epistemic calibration in LLM outputs. The one below is poorly calibrated. For each output, we calculate two certainty scores: internal certainty and external certainty (linguistic assertiveness). The internal certainty is computed using the method outlined by Rivera et al. (2024). To assess linguistic assertiveness, we develop a custom model, which we validate using human ratings collected through a survey. \n\nSeveral studies have explored the calibration of internal confidence in LLMs. For instance, Zhang et al. (2024) examine confidence calibration, proposing techniques to reduce hallucinations and enhance the model's ability to answer known questions while avoiding unknown ones. However, they overlook the role of linguistic assertiveness and how external certainty can still lead to epistemic miscalibration even if internal confidence is addressed. Similarly, Ren et al. (2023) focus on factual knowledge and LLM behavior before and after retrieval-augmented generation (RAG). While they investigate internal confidence, they fail to frame miscalibration as an end-to-end issue involving both internal certainty and linguistic assertiveness, therefore ignoring the interplay between model predictions and how confidence is expressed linguistically. \n\nMore recent studies have attempted to bridge the gap between internal confidence and linguistic assertiveness but still face considerable limitations. Although Mielke et al. (2022) explore epistemic calibration, their study uses a limited scoring scale to measure both assertiveness and confidence, which restricts the continuous assessment of LLM output. Their approach also relies on a narrow range of datasets, thereby limiting its applicability across domains. Zhou et al. (2024a) address miscalibration using epistemic markers, but their method lacks real domain grounding and fails to consider the complexity of natural language. \n\nThis review of existing work on LLM calibration and confidence reveals several gaps that our research aims to address:",
            "score": 0.5275303271416338,
            "section_title": "Low Epistemic Calibration",
            "char_start_offset": 2594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 147
                },
                {
                    "start": 150,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2183
                },
                {
                    "start": 2186,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 789,
                    "end": 808,
                    "matchedPaperCorpusId": "265220839"
                },
                {
                    "start": 1708,
                    "end": 1728,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10931396484375
        },
        {
            "corpus_id": "270392060",
            "title": "Large Language Models Must Be Taught to Know What They Don't Know",
            "text": "Error bars show standard deviation over three base models (LLaMA-2 13/7B and Mistral 7B) and their chat variants.\n\nthe rise of restricted models, the increased availability of strong open-source models, such as LLaMA [53] or Mistral [24], has made more effective white-box methods more accessible.\n\nIn this paper, we perform a deep investigation into uncertainty calibration of LLMs, with findings that advance the debate about necessary interventions for good calibration.In particular, we consider whether it's possible to have good uncertainties over correctness (rather than tokens) without intervention, how we can best use labeled correctness examples, how well uncertainty generalizes across distribution shifts, and how we can use LLM uncertainty to assist human decision making.\n\nFirst, we find that fine-tuning for better uncertainties (Figure 1) provides faster and more reliable uncertainty estimates, while using a relatively small number of additional parameters.The resulting uncertainties also generalize to new question types and tasks, beyond what is present in the fine-tuning dataset.We further provide a guide to teaching language models to know what they don't know using a calibration dataset.Contrary to prior work, we start by showing that current zero-shot, black-box methods are ineffective or impractically expensive in open-ended settings (Section 4).We then show how to fine-tune a language model for calibration, exploring the most effective parameterization (e.g.linear probes vs LoRA) and the amount of the data that is required for good generalization (Section 5).\n\nTo test generalization, we evaluate uncertainty estimates on questions with similar formatting to the calibration data as well as questions that test robustness to significant distribution shifts.Lastly, we consider the underlying mechanisms that enable fine-tuning LLMs to estimate their own uncertainties, showing ultimately that models can be used not just to estimate their own uncertainties but also the uncertainties of other models (Section 6).Beyond offline evaluation, if language models are to have a broad societal impact, it will be through assisting with human decision making.We conduct a user study demonstrating ways LLM uncertainty can affect AI-human collaboration (Section 7). 1",
            "score": 0.5274759524308837,
            "section_title": "Introduction",
            "char_start_offset": 2262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 115,
                    "end": 297
                },
                {
                    "start": 299,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 787
                },
                {
                    "start": 789,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1216
                },
                {
                    "start": 1216,
                    "end": 1380
                },
                {
                    "start": 1380,
                    "end": 1495
                },
                {
                    "start": 1495,
                    "end": 1598
                },
                {
                    "start": 1600,
                    "end": 1796
                },
                {
                    "start": 1796,
                    "end": 2051
                },
                {
                    "start": 2051,
                    "end": 2190
                },
                {
                    "start": 2190,
                    "end": 2297
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16162109375
        },
        {
            "corpus_id": "265445194",
            "title": "Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions",
            "text": "Estimating the confidence levels associated with the responses of LLMs is an important research topic. In the existing literature, methods can be grouped into three categories: 1) logits-based methods that focus on the probabilities derived from model logits; 2) verbalized confidence that directly asks a model to output its confidence; 3) consistency-based methods that yield multiple responses and use consistency as a surrogate for confidence. \n\nBefore initiating the reasoning framework, we first conducted a pilot study to investigate whether our explanation logits-based confidence is effective. As shown in Table 5, we compare different confidence calibration methods. For each method, given a question, we query LLMs multiple times (decode one response with greedy decoding, and sample another four with temperature t = 0.7), calculate the confidence score for each response, and take the most confident one as the final answer. Better answer F1 indicates better confidence calibration. We randomly sampled 500 questions from HotpotQA, and 500 from MuSiQue to conduct the pilot study. Answer F1 results show that our explanation logitsbased confidence outperforms others, including the model itself estimating its uncertainty in language, and taking answer tokens as part of scoring. \n\nIn addition, we found that the average confidence score for all the 500 examples in HotpotQA is -0.142, and the average confidence score for examples with answer F1 1.0 is -0.109, and that of examples with answer F1 0.0 is -0.176. The low confidence of incorrect answers and high score of correct ones indicates the effect of our method.",
            "score": 0.526533592985601,
            "section_title": "A Pilot Study for Our Confidence Mechanism",
            "char_start_offset": 29056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 447
                },
                {
                    "start": 450,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1632
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0831298828125
        },
        {
            "corpus_id": "276776192",
            "title": "SteerConf: Steering LLMs for Confidence Elicitation",
            "text": "Large Language Models (LLMs) have revolutionized artificial intelligence by achieving remarkable performance across diverse tasks, from text generation to complex reasoning (Brown et al., 2020;Wei et al., 2022;Petroni et al., 2019). However, their practical deployment faces a critical challenge: misaligned confidence calibration (Jiang et al., 2021;Lin et al., 2022;Shrivastava et al., 2023). LLMs often produce overconfident predictions (Xiong et al., 2023;Tian et al., 2023) that do not reflect their true likelihood of being correct, raising concerns about their reliability in high-stakes applications such as healthcare (Bedi et al., 2024;Savage et al., 2024), legal analysis (Guha et al., 2023), and autonomous systems (Chen et al., 2024;Wang et al., 2024). While prior work has explored verbalized confidence-probing LLMs to self-assess their prediction certainty (Xiong et al., 2023;Tian et al., 2023), the field remains divided on two pivotal questions: Can linguistic interventions, such as prompting, systematically steer an LLM's confidence scores in a controlled manner? And if confidence scores can be steered, can we utilize this steering to get a better calibrated confidence? \n\nRecent studies (Xiong et al., 2023) argue that prompt-induced confidence shifts are negligible, positing that LLMs' calibration is inherently rigid and resistant to linguistic steering. This perspective, however, conflicts with our observations as shown in Figure 1. To resolve this contradiction, we conduct a rigorous empirical investigation on seven benchmarks in Section 6. Our experiments systematically confirm that explicit instructions (e.g., \"Be very cautious\" or \"Be very confident\") induce directional confidence shifts, while mild instructions (e.g., \"Be cautious\" or \"Be confident\") can not induce desired confidence shifts. This finding challenges the prevailing assumption of calibration rigidity and opens new avenues for improving LLM trustworthiness. \n\nBuilding on this insight, we propose Steering-Conf, a novel framework for dynamic confidence calibration.",
            "score": 0.526280588554001,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1965
                },
                {
                    "start": 1968,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 210,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 210,
                    "end": 231,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 440,
                    "end": 460,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 460,
                    "end": 478,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 683,
                    "end": 702,
                    "matchedPaperCorpusId": "261064672"
                },
                {
                    "start": 727,
                    "end": 746,
                    "matchedPaperCorpusId": "263608168"
                },
                {
                    "start": 873,
                    "end": 893,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 893,
                    "end": 911,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 1212,
                    "end": 1232,
                    "matchedPaperCorpusId": "259224389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226318359375
        },
        {
            "corpus_id": "258865733",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "text": "Tables 1-5 show the results of evaluating various methods for extracting confidence from RLHF-LMs on gpt-3.5-turbo, gpt-4, claude-1, claude-2, and Llama-2-70b-chat, respectively. We distill several key conclusions from these experiments. 1. Large RLHF-LMs can often directly verbalize better-calibrated confidences (either a numerical confidence probability or an expression such as 'highly likely') than the models' conditional probabilities. 2. Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1985), and corroborating a similar finding in LMs (Kadavath et al., 2022). \n\n3. Language models can express their uncertainty with numerical probabilities as well or better than with words, which is surprising in light of longstanding difficulties in representing numbers in language models (Thawani et al., 2021). 4. Chainof-thought prompting does not improve verbalized calibration (see Appendix Figure 5 for additional CoT results). 5. The calibration of both Claude models' conditional probabilities roughly falls between gpt-3.5-turbo and gpt-4; however, while Claude 1 is much weaker at verbalizing its confidence, Claude 2 is generally a bit stronger than gpt-3.5-turbo at verbalizing. The verbal calibration of the open source model Llama-2-70b-chat is generally weaker than that of closed source models but still demonstrates improvement over its conditional probabilities by some metrics, and does so most clearly on TruthfulQA.",
            "score": 0.5260945777595287,
            "section_title": "Results",
            "char_start_offset": 11595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1576
                }
            ],
            "ref_mentions": [
                {
                    "start": 624,
                    "end": 643,
                    "matchedPaperCorpusId": "24047702"
                },
                {
                    "start": 929,
                    "end": 951,
                    "matchedPaperCorpusId": "232335764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251953125
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Confidence calibration is one such reliability metric that measures whether the model's prediction probability estimates are aligned with the actual probability of the answer being correct. Confidence calibration has been studied in Computer Vision (Guo et al., 2017;Minderer et al., 2021) and Natural Language Processing (Desai and Durrett, 2020;Dan and Roth, 2021). However, most of the prior works are limited to classification settings, which is inapplicable to the generality of the QA task. Recently, Zhang et al. (2021); Jiang et al. (2021) have shown that state-of-the-art English QA models are surprisingly poorly calibrated. However, there remains a gap in understanding of the calibration properties of multilingual QA models. In this paper, we address this gap by a comprehensive study on the Calibration of Multilingual Question Answering Large Language Models. The main research questions we investigate in this paper are: \n\n1) How well are MLLMs calibrated in the crosslingual transfer scenario? \n\n2) How can we improve MLLMs' confidence calibration on multilingual QA datasets? The contributions of our work are as follows: \n\n\u2022 We provide the first comprehensive benchmarking of confidence calibration of multilingual QA models (architectures including extractive models: mBERT, XLM-R and generative models: mT5, mBART, and LLaMa2) over both low-and high-resource languages, in-distribution and out-ofdistribution settings. \n\n\u2022 We observe that the calibration performance on English is not transferable to other languages, across various datasets and architectures. Distance between the target languages and English, and the distribution of different languages at the pretraining stage, are all highly correlated with calibration performance, across the various model types. \n\n\u2022 An investigation of various calibration strategies including post-hoc methods and regularization methods, aimed at enhancing cross-lingual calibration. Temperature scaling (optimized over a cross-lingual validation dataset) shows the most significant improvement even if the target language is absent in the validation data.",
            "score": 0.5257980699183137,
            "section_title": "Introduction",
            "char_start_offset": 1894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1439
                },
                {
                    "start": 1442,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 267,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 267,
                    "end": 289,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 322,
                    "end": 347,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 347,
                    "end": 366,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 528,
                    "end": 547,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2646484375
        },
        {
            "corpus_id": "259224389",
            "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
            "text": "Modern neural networks are shown to be poorly calibrated, often manifesting overconfidence (Guo et al., 2017;Minderer et al., 2021;Xiong et al., 2023). Calibration seeks to address the issue by aligning the model's confidence with the accuracy of samples within the same confidence level (Guo et al., 2017;Minderer et al., 2021). To achieve this, a variety of methods have been proposed, which can be broadly divided into scaling-based methods (Guo et al., 2017;Deng et al., 2023;Zhang et al., 2020) and binning-based methods (Zadrozny & Elkan, 2001;Zhang et al., 2020). Within the scope of LLMs, Jiang et al. (2021) investigates the calibration of generative language models (T5, BART, and GPT-2) and discovers that these models' probabilities on question-answering tasks are poorly calibrated. Similarly, Chen et al. (2022) finds that PLMs are not well calibrated and pretraining improves model calibration. On the other hand, Kadavath et al. (2022) studies the calibration of LLMs (parameter size ranging 800M to 50B), finding that larger models appear to be well-calibrated on multiple choice and true/false questions when provided in the right format. However, these evaluations mainly focus on the probabilities derived from logits, which are unavailable for closed-source LLMs like GPT-4. This also motivates us to study confidence elicitation methods that do not require model fine-tuning or access to model logits or embeddings.",
            "score": 0.5256879324985996,
            "section_title": "C RELATED WORKS",
            "char_start_offset": 39613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1437
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 109,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 109,
                    "end": 131,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 288,
                    "end": 306,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 306,
                    "end": 328,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 444,
                    "end": 462,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 480,
                    "end": 499,
                    "matchedPaperCorpusId": "212725167"
                },
                {
                    "start": 526,
                    "end": 550,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 550,
                    "end": 569,
                    "matchedPaperCorpusId": "212725167"
                },
                {
                    "start": 597,
                    "end": 616,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.249267578125
        },
        {
            "corpus_id": "267211649",
            "title": "What large language models know and what people think they know",
            "text": "As artificial intelligence systems, particularly large language models (LLMs), become increasingly integrated into decision-making processes, the ability to trust their outputs is crucial. To earn human trust, LLMs must be well calibrated such that they can accurately assess and communicate the likelihood of their predictions being correct. Whereas recent work has focused on LLMs\u2019 internal confidence, less is understood about how effectively they convey uncertainty to users. Here we explore the calibration gap, which refers to the difference between human confidence in LLM-generated answers and the models\u2019 actual confidence, and the discrimination gap, which reflects how well humans and models can distinguish between correct and incorrect answers. Our experiments with multiple-choice and short-answer questions reveal that users tend to overestimate the accuracy of LLM responses when provided with default explanations. Moreover, longer explanations increased user confidence, even when the extra length did not improve answer accuracy. By adjusting LLM explanations to better reflect the models\u2019 internal confidence, both the calibration gap and the discrimination gap narrowed, significantly improving user perception of LLM accuracy. These findings underscore the importance of accurate uncertainty communication and highlight the effect of explanation length in influencing user trust in artificial-intelligence-assisted decision-making environments.",
            "score": 0.5252516481826185,
            "section_title": "abstract",
            "char_start_offset": 2,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445068359375
        },
        {
            "corpus_id": "270620078",
            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
            "text": "We introduced ACTCAB, a calibration method for language models that leverages internal activations to improve the alignment of model confidence with answer correctness.ACTCAB outperforms existing logit-based and consistency-based methods, achieving a 39% reduction in the ECE on five QA benchmarks.Additionally, we developed CODEC, a confidence-guided decoding strategy that enhances the factual accuracy of model responses by utilizing the calibrated confidence scores from ACTCAB.Our empirical results demonstrate that CODEC significantly improves LM factuality on challenging datasets like TruthfulQA.",
            "score": 0.5251737753227087,
            "section_title": "Conclusion",
            "char_start_offset": 23826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 604
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.338134765625
        },
        {
            "corpus_id": "259859030",
            "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
            "text": "In standard classification training, a model F : X \u2192 Y for the main task is trained on a given dataset D = {(x i , y i ) N i=1 } to minimize the predefined classification loss. For the introduced calibration task, a new calibration dataset\n\ni is model's prediction, and c i is the corresponding confidence score. The calibration task aims to predict the models' confidence using the sample and the original prediction. The generation process of D * is one essential part of the calibration task. Lin et al. (2022) propose to deem accuracy on a batch of samples as the confidence c i for samples in this batch. In this work, we simplify this assumption and directly treat c i as a binary value, indicating whether the prediction y * i is correct or not.\n\nOnce D * is generated, one can fit an extra model F * separately or conduct multi-task training using  Table 2. Increasing the dataset size of the calibration task continually brings benefits.  the original model F to learn the calibration task 1 .\n\nIn this work, we adopt the latter paradigm since no obvious performance difference is observed in previous work (Chen et al., 2022b). Specifically, we adopt the unified text-to-text paradigm and use T5 as the backbone model in this paper (Raffel et al., 2020). We use two sets of templates and verbalizers for the main task and the calibration task respectively . See Table 1 for an example used in the sentiment analysis task. Other templates and verbalizers selected are listed in Appendix B. The probability of the \"True\" class in the calibration task is deemed as PLMs' confidence in their predictions. In testing, the original test set is used for evaluating both the original and the calibration tasks.",
            "score": 0.5250594302159357,
            "section_title": "Task Formalization",
            "char_start_offset": 4483,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1242,
                    "end": 1263,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10595703125
        },
        {
            "corpus_id": "273963769",
            "title": "Epistemic Integrity in Large Language Models",
            "text": "Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration $\\unicode{x2013}$ where a model's linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains.",
            "score": 0.5247185231676799,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072265625
        },
        {
            "corpus_id": "270199748",
            "title": "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales",
            "text": "While large language models (LLMs) exhibit remarkable proficiency in reasoning and generating * Equal contribution. \u2020 Corresponding author. \n\neffective responses (OpenAI, 2023;Touvron et al., 2023;Jiang et al., 2023;Wang et al., 2024), they often produce fabricated information (a.k.a, hallucination) and typically hesitate to indicate their uncertainty when faced with unfamiliar questions (Ye et al., 2023;Liu et al., 2023). Determining how to accurately obtain reliable confidence estimates from LLMs is essential (Xiong et al., 2023;Zhou et al., 2023), particularly when the responses are not limited to single tokens1 . \n\nPrevious work on eliciting confidence from LLMs includes prompting-based and trainingbased approaches. Prompting-based methods, such as direct prompting and self-consistency prompting in Figure 1, employ specific prompts to generate confidence scores or use answer consistency as a confidence indicator, even though these can have poor calibration performance or significantly increase inference latency (Tian et al., 2023;Xiong et al., 2023;Diao et al., 2023). Training-based approaches, such as group-based calibration training and R-Tuning in Figure 1, develop specialized datasets for fine-tuning that encourage LLMs to express confidence. However, these methods often provide suboptimal or binary confidence estimates, failing to accurately reflect the models' confidence levels (Lin et al., 2022;Zhang et al., 2023a;Yang et al., 2023). In conclusion, previous work tends to suffer from the following problems: (1) Poor calibration performance; (2) Coarse-grained confidence levels; (3) Long inference latencies. \n\nIn this work, we present SaySelf, a training framework that teaches LLMs to generate more accurate and fine-grained confidence estimates. It successfully tackles the aforementioned problems in previous work. More importantly, SaySelf also goes beyond the confidence elicitation in previous work, and further enables LLMs to Figure 1: The comparison between SaySelf and previous work.",
            "score": 0.5244975079515112,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2030
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1849365234375
        },
        {
            "corpus_id": "265157516",
            "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
            "text": "Mielke et al. ( 2022) proposed a calibratorcontrolled method for chatbots, which involves a trained calibrator to return the model confidence score and fine-tuned generative models to enable control over linguistic confidence. Lin et al. (2022) fine-tuned GPT-3 with the human-labeled dataset containing verbalized words and numbers to express uncertainty naturally. Zhou et al. (2023) empirically found that injecting expressions of uncertainty into prompts significantly increases the accuracy of GPT-3's answers and the calibration scores. \n\nDifferent datasets (Amayuelas et al., 2023;Yin et al., 2023;Wang et al., 2023d;Liu et al., 2023a) have been presented on questions that language models cannot answer or for which there is no clear answer. Amayuelas et al. (2023) analyzed how different language models, including both smaller and open-source models, classify a dataset of various unanswerable questions. They observed that LLMs show varying accuracy levels depending on the question type, while smaller and open-source models tend to perform almost randomly. Liu et al. (2023a) evaluated both open-source models like LLaMA-2 (Touvron et al., 2023b), Vicuna (Chiang et al., 2023), and closed-source models such as GPT-3.5 and GPT-4, focusing on their refusal rate, accuracy, and uncertainty in handling unanswerable questions.",
            "score": 0.5243827294118961,
            "section_title": "Improve the linguistic confidence",
            "char_start_offset": 19714,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1336
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 385,
                    "matchedPaperCorpusId": "257220189"
                },
                {
                    "start": 564,
                    "end": 588,
                    "matchedPaperCorpusId": "258841255"
                },
                {
                    "start": 750,
                    "end": 773,
                    "matchedPaperCorpusId": "258841255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06109619140625
        },
        {
            "corpus_id": "258298551",
            "title": "KInITVeraAI at SemEval-2023 Task 3: Simple yet Powerful Multilingual Fine-Tuning for Persuasion Techniques Detection",
            "text": "In the third step, we perform experiments to determine the best confidence threshold for predicting output classes -the probability threshold after which the specific class is considered to be the output label for the specific sample. For example, if the threshold is set to 0.2, all classes with predicted probability of at least 0.2 are assigned as predicted labels. To determine the optimal threshold, we use the fine-tuned best-performing monolingual, multilingual model, as well as their ensemble and evaluate their performance on different threshold values. In addition, to determine how our solution will perform on the surprise (unseen) languages, we simulate the zero-shot setting. We randomly select two languages from the training ones as surprise, train all three models on the remaining languages only and use the data from the selected languages only for evaluation. In this way, we are able to better estimate the confidence threshold when working in zero-shot setting on the test set.\n\nAnother possibility for the calibration would be to calibrate the confidence threshold for each individual language and class. Although this would improve the performance of the evaluated models on the available data, we believe it would lead to severe overfitting to the distribution of classes on the individual languages and negatively affect the generalizability of our models. Therefore, we opted to pursue the calibration strategy as described in the previous paragraph (single overall calibration for all classes and languages at the same time, with simulated zero-shot setting).\n\nThe comparison of all three models for the analysed spectrum of confidence thresholds, also allow us to select the final best-performing model, which is used in the next experiments.",
            "score": 0.5243822550483284,
            "section_title": "Confidence Threshold Calibration",
            "char_start_offset": 9354,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.008544921875
        },
        {
            "corpus_id": "278164768",
            "title": "From Evidence to Belief: A Bayesian Epistemology Approach to Language Models",
            "text": "We first generate various types of evidence by few-shot prompting large language models (LLMs) with questions and annotated support from SciQ (Welbl et al., 2017), TriviaQA (Joshi et al., 2017), GSM8K (Cobbe et al., 2021). Refer to Appendix I.2 for details. We then used this evidence to evaluate how the models' confidence and responses change based on the type of evidence provided as shown in Figure 1. Influenced by Bayesian epistemology, we defined a confirmation task to measure whether language models can reflect the confirmation, disconfirmation, or irrelevance assumption introduced in Section 1. Also, we created a strength-of-evidence task to assess LLM's ability to represent the various power of evidence. To measure the probability norm for adjusting confidence according to the evidence, we used an average confidence across all samples. In order to measure the response, such as correctness or calibration norm, we used accuracy (ACC) and Expected Calibration Error (ECE). In both the confirmation task and the strength-of-evidence task, we used zero-shot prompting for inference.",
            "score": 0.5240951626637612,
            "section_title": "Methods",
            "char_start_offset": 6942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1097
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 161,
                    "matchedPaperCorpusId": "1553193"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042724609375
        },
        {
            "corpus_id": "259095925",
            "title": "Uncertainty in Natural Language Processing: Sources, Quantification, and Applications",
            "text": "Answer Prefix: 1. I'm 90% sure it's ... 2. I vaguely remember it's ... Answer Suffix: 1. ...But I would need to double-check. 2. ...With 100% confidence. Answer Self-evaluation: 1. ...With what confidence could you answer this question? and output an answer like 0%, 10%, 20%, ..., 100%. For example, Desai and Durrett [114] show that pre-trained Transformers are relatively well calibrated, while Wang et al. [155] find severe miscalibration in MT. Jiang et al. [149] study calibration in generative QA and observe only a weak correlation between the log-likelihood model assigned to their answers and the correctness of the answers. In general, PLMs output a possibility for a given tag sequence, but do not output the entire meaning [4]. \n\nOn the one hand, whether this uncertainty expression is suitable for generative NLP systems remains questionable. We may need stronger correlation indicators to focus on the uncertainty expression of probability. On the other hand, naturalistic representations of uncertainty cover a wide range of discursive behaviors, such as signaling hesitancy, attributing information, or acknowledging limitations, and such representations are intuitive to humans. As the examples displayed in Fig. 9, QA prompts are able to express uncertainty. Through different prefixes, suffixes, or confidence levels of self-assessment outputs, we can obtain numerical uncertainty representations to further analyze the calibration of the model. Kadavath et al. [26] expect to observe large benefits of few-shot evaluation with natural language methods, but instead. Yet no major gains are observed in early QA experiments. Zhou et al. [151] find that non-deterministic expressions can affect language production, and that changes in these expressions can have a substantial impact on overall accuracy, especially when using high-certainty expressions, including in accuracy and calibration. The authors further hypothesize that this may be due to the usage of hyperbolic or exaggerated language in the training set, where numbers are used non-literally.",
            "score": 0.5236083993244964,
            "section_title": "Different Templates for Uncertainty Representation",
            "char_start_offset": 81507,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 324,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 410,
                    "end": 415,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 463,
                    "end": 468,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0083160400390625
        },
        {
            "corpus_id": "258426367",
            "title": "Calibration Error Estimation Using Fuzzy Binning",
            "text": "Neural network-based decision-making systems have evolved rapidly in the recent decade. Within the domain of natural language processing, deep learning has shaped the current evolution in language modeling. These neural network-based language models are trained on large text corpora and can be fine-tuned across a wide range of NLP tasks and further improved using synthetic semantic enhancement schemes [1], yielding state-of-the-art performance [2][3][4][5]. Ideally, a neural model should output reliable and confident prediction probabilities. But recent works have shown that neural networks are unreliable and output highly overconfident predictions, resulting in over-estimation of the model's confidence in decisions [6][7][8]. This leads to model miscalibration, i.e. a lack of alignment between a model's decision probabilities and its actual likelihood of correctness. This lack of calibration can severely impact the trustworthiness of a model's decisions. \n\nA widely adopted measure of the degree of miscalibration is Expected Calibration Error (ECE) [9], used to measure neural network reliability [10][11][12]. The highly overconfident output prediction probabilities of neural networks result in a left-skewed probability distribution [13]. Since ECE utilizes a fixedwidth crisp binning scheme, this skew results in higher probability bins largely contributing to the calibration error estimation, while lower probability bins are ignored [13][14][15]. To overcome these limitations, prior works have proposed alternative binning strategies such as equal-frequency binning [14], adaptive binning [15], replacing binning with smoothed kernel density estimation [16], and more. Most calibration error estimation techniques rely on crisp binning, which discards edge probabilities (probabilities that typically lie on the bin edge) that could have contributed to a more accurate calibration error estimation. Although some works have utilized fuzzification of prediction probabilities for downstream NLP tasks [17], the calibration impacts of such fuzzification are yet to be studied.",
            "score": 0.5236037789676629,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 408,
                    "matchedPaperCorpusId": "233347145"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 726,
                    "end": 729,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1065,
                    "end": 1068,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1117,
                    "end": 1121,
                    "matchedPaperCorpusId": "211555673"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "220546269"
                },
                {
                    "start": 1460,
                    "end": 1464,
                    "matchedPaperCorpusId": "229212600"
                },
                {
                    "start": 1464,
                    "end": 1468,
                    "matchedPaperCorpusId": "262648748"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "229212600"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "262648748"
                },
                {
                    "start": 1677,
                    "end": 1681,
                    "matchedPaperCorpusId": "212725167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.028167724609375
        },
        {
            "corpus_id": "248779946",
            "title": "Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference",
            "text": "The predictive probability of a perfectly-calibrated model can reflect the true correctness likelihood, i.e. empirical accuracy is equal to the prediction confidence. Empirical accuracy is obtained from observations across the human judges. That is, predictive confidence (uncertainty) can represent the human judgment distribution when model is calibrated. To this end, not only label-smoothing, but other recalibration approaches such as temperature scaling can reach the same goal. Our experiments confirmed our hypothesis, when model is calibrated, it can obtain competitive ChaosNLI divergence scores and bring accuracy boost. \n\nOur contributions are two folds: (1) We propose the hypothesis: a well-calibrated network can naturally capture linguistic ambiguity, regardless of using special resource. It reasonably explains the success of training with ambiguity labels, and converts question of \"how to capture human disagreement distribution?\" to a more general one \"how to train a calibrated model?\" Our experiments confirm that commonly-used re-calibration methods are as effective as targeting at ambiguous annotations. (2) Knowledge of linguistic ambiguity learned from the general domain benefits biomedical domain as well, which suggests ambiguity signals can be transferred across domains. But calibration is not an intrinsic property of a model, it's data-dependent.",
            "score": 0.5231471160990435,
            "section_title": "Premise",
            "char_start_offset": 3049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.091552734375
        },
        {
            "corpus_id": "273186420",
            "title": "Calibrating Expressions of Certainty",
            "text": "We present additional results on the application of optimal transport calibration to language models. Figure 13 illustrates the effectiveness of our calibration method in generating an interpretable calibration map. \n\nC.3 RELIABILITY DIAGRAMS: CONFIDENCE SCORES VS. CONFIDENCE DISTRIBUTIONS Figure 14 highlights the effectiveness of our method in producing a smoother, more informative calibration curve. The reliability diagram under our formulation makes it much clearer to differentiate between the calibration profiles of various models. The deviation of the calibration curve (red) from the identity line, weighted by the score density (gray), provides an intuitive sense for the value of ECE. For instance, it's immediately apparent that claude-3.5-sonnet outperforms gemini-1.5-pro based on the calibration curve. In contrast, binned reliability diagrams make it harder to visually assess calibration from calibration curves alone. Specifically, it's challenging to distinguish which model is the strongest by just looking at binned calibration curves. \n\nFigure 15 compares reliability diagrams from previous work (discrete bins) with our approach (still binned, but gets smoother as M increases) with varying the number of bins M . In reliability diagrams previously commonly used, the calibration curves are highly sensitive to the choice of bin size M , implying that the visualization can change dramatically depending on the binning strategy used. This creates an inconsistency when trying to interpret results and comparing different diagrams. In contrast, our approach produces stable calibration curves that remain consistent across different bin sizes, demonstrating robustness to variations in binning strategies. \n\nFigure 14: Comparison of reliability diagrams from prior work (top two rows) and our approach (bottom two rows). Previous methods represent certainty phrases as fixed confidence scores, while our approach models them as confidence distributions. This results in smoother calibration curves, making it easier to discern differences in calibration characteristics between different diagrams. \n\nFigure 15: Comparison of reliability diagrams from prior work (top four rows) and our approach (bottom four rows) across different bin counts M . Calibration curves in previous methods are highly sensitive to the choice of binning, leading to varying visualizations. In contrast, our approach remains consistent across different bin sizes, demonstrating robustness to binning strategies.",
            "score": 0.5228378068713619,
            "section_title": "C.2 CALIBRATING LANGUAGE MODELS",
            "char_start_offset": 42672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2122
                },
                {
                    "start": 2125,
                    "end": 2270
                },
                {
                    "start": 2271,
                    "end": 2391
                },
                {
                    "start": 2392,
                    "end": 2512
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06475830078125
        },
        {
            "corpus_id": "252780032",
            "title": "Explaining Predictive Uncertainty by Looking Back at Model Explanations",
            "text": "Pre-trained language models (e.g., BERT; Devlin et al., 2019) have been indispensable to natural language processing (NLP) due to their remarkable performance (Liu et al., 2019;Yang et al., 2019;Gururangan et al., 2020;Brown et al., 2020). Predictive uncertainty estimation of pre-trained language models is an important measure of how likely people can trust their predictions (Desai and Durrett, 2020;Xu et al., 2020). \n\nA typical way of measuring predictive uncertainty is to calibrate model outputs with the true correctness likelihood (Guo et al., 2017;Kong et al., 2020;Zhao et al., 2021), so that the output probabilities well represent the confidence of model predictions. In this case, higher prediction confidence indicates lower uncertainty (Xu et al., 2020;Jiang et al., 2021). \n\nHowever, little is known about what makes a model prediction uncertain. Explaining predictive uncertainty is important to understanding model prediction behavior and complementary to explaining prediction labels for gaining users' trust, while has been largely ignored (Antor\u00e1n et al., 2020). Most works on model explanations focus on explaining a model from the post-hoc manner by identifying important features in inputs that contribute to model predicted labels (Ribeiro et al., 2016;Lundberg and Lee, 2017;Sundararajan et al., 2017;Chen et al., 2020Chen et al., , 2021)). Figure 1 shows an example of model explanation for sentiment classification, where the model makes the correct prediction (POSITIVE) with a relatively low confidence 69%. The top two salient words highlighted in blue color explain the predicted label. However, users may still wonder what compromises the prediction confidence? \n\nThis work is the first to explain model predictive uncertainty in NLP. Specifically, this work is based on a simple observation that bottom salient words in model explanations (e.g., dreadful and hard in Figure 1) identified as making negative contributions to predicted labels actually explain model predictive uncertainty. The two bottom salient words in Figure 1 indicate the opposite sentiment (NEGATIVE) to the model predicted label. Remov-ing them can improve the model prediction confidence from 69% to 93%.",
            "score": 0.5227050940846525,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 61,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 177,
                    "end": 195,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 195,
                    "end": 219,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 378,
                    "end": 403,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 403,
                    "end": 419,
                    "matchedPaperCorpusId": "222378161"
                },
                {
                    "start": 540,
                    "end": 558,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 752,
                    "end": 769,
                    "matchedPaperCorpusId": "222378161"
                },
                {
                    "start": 769,
                    "end": 788,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1257,
                    "end": 1279,
                    "matchedPaperCorpusId": "13029170"
                },
                {
                    "start": 1279,
                    "end": 1302,
                    "matchedPaperCorpusId": "21889700"
                },
                {
                    "start": 1302,
                    "end": 1328,
                    "matchedPaperCorpusId": "16747630"
                },
                {
                    "start": 1328,
                    "end": 1345,
                    "matchedPaperCorpusId": "214802514"
                },
                {
                    "start": 1345,
                    "end": 1366,
                    "matchedPaperCorpusId": "233204288"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007965087890625
        },
        {
            "corpus_id": "263135463",
            "title": "Towards Efficient and Trustworthy AI Through Hardware-Algorithm-Communication Co-Design",
            "text": "If a model provides trustworthy measures of confidence, the accuracy of a decision must always equal the corresponding confidence level. When this is the case, we say that a model is perfectly calibrated (green line in the figure). Otherwise, calibration is imperfect, and this can happen in two distinct ways: If the accuracy is larger than the confidence, we say that the model is under-confident (blue line); while, if the confidence is larger than the accuracy, we say that the model is over-confident (red line). \n\nTypical AI models tend to be over-confident. When indicating, say, a confidence level of 90%, they may be actually producing decisions that are accurate much less frequently than 90% of the time. This is particularly problematic for language models, as they may confidently provide the questioner with the wrong information [27], [28], [29]. \n\nThe lack of calibration does not impair only reliable decision-making, but also the robustness of the system. For instance, it is known that overconfident models are more prone to membership inference attacks, whereby an attacker aims at inferring whether a certain data point was used in the training of the model [30]. \n\nIn order to improve calibration, one needs to modify the way in which AI models are designed or implemented, moving beyond the conventional focus on accuracy. As pointed out by Alan Turing, \"if a machine is expected to be infallible, it cannot also be intelligent\". Designing a machine to maximise accuracy disregards the fact that errors are inevitable, and an 'intelligent' agent should recognise, or anticipate, them, knowing how to act under uncertainty. \n\nAccuracy and calibration are distinct requirements, and the interplay between the two generally depends on the specific AI model and on the given learning task. It is often the case that there is a trade-off between accuracy and calibration. In fact, intuitively, improving calibration requires models to be more 'cautious' in making a decision, which may decrease the average accuracy. It is also known that larger models tend to have a higher calibration error, even when the accuracy is improved, revealing a connection with the classical problem of overfitting [31].",
            "score": 0.5224602301798368,
            "section_title": "wrong!",
            "char_start_offset": 12851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "189762078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11029052734375
        },
        {
            "corpus_id": "273404268",
            "title": "LLM Confidence Evaluation Measures in Zero-Shot CSS Classification",
            "text": "Large Language Models (LLMs) have transformed the way artificial intelligence is integrated into professional workflows, with applications that span healthcare (Ray, 2024), academia (Meyer et al., 2023), cybersecurity (Zhang et al., 2024), software development (Rasnayaka et al., 2024), and many others. However, research shows that users struggle to identify incorrect LLM responses which poses a problem because LLMs are less likely to refrain from answering questions they do not know as they scale with size and complexity (Zhou et al., 2024). Despite these challenges, LLMs have proven effective in synthesizing vast amounts of data and applying contextual understanding, making them a popular choice for integration into natural language processing tasks, particularly in zero-shot classification settings where prior training data is unavailable (Yang et al., 2024). \n\nWith broad applications in critical industries, LLM-generated responses that are assumed to be correct can lead to drastic second-and third-order consequences when answered incorrectly and integrated into decision-making processes. Although some LLMs incorporate expressions of uncertainty (Tian et al., 2023), developers often restrict the output of the model to a predetermined set of responses to manage nondeterministic behavior or reduce token generation cost (Liu et al., 2024b). However, these constraints can cause LLMs to provide confident answers even when they lack the correct knowledge. While LLMs are useful for large-scale data annotation tasks, there remains uncertainty as to which labels are correct or how to best quantify label confidence in LLM-generated annotations, especially in multi-modal systems. \n\nThis paper evaluates various Uncertainty Quantification (UQ) methods to assess LLM confidence in data annotation tasks applied to Computational Social Science (CSS) problems. Based on these results, we present a simple UQ aggregation strategy to help identify misclassified LLM-labeled data. We constrain our settings to realistic industry scenarios where previously labeled data is unavailable to simulate common, real-world problems.",
            "score": 0.5222619169542408,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 171,
                    "matchedPaperCorpusId": "269647802"
                },
                {
                    "start": 182,
                    "end": 202,
                    "matchedPaperCorpusId": "259834950"
                },
                {
                    "start": 261,
                    "end": 285,
                    "matchedPaperCorpusId": "267312182"
                },
                {
                    "start": 527,
                    "end": 546,
                    "matchedPaperCorpusId": "272882110"
                },
                {
                    "start": 853,
                    "end": 872,
                    "matchedPaperCorpusId": "258331833"
                },
                {
                    "start": 1341,
                    "end": 1360,
                    "matchedPaperCorpusId": "269042931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0325927734375
        },
        {
            "corpus_id": "250451161",
            "title": "Language Models (Mostly) Know What They Know",
            "text": "We saw in section 3.1 that language models seem to be confused by a \"none of the above\" option. Here we take a different approach, and simply ask models if a given answer is true or false. So we use the format: \n\nQuestion: Who was the first president of the United States? Proposed Answer: George Washington Is the proposed answer: \n\nThe proposed answer is: \n\nwhere we expect either ' (A)' or ' (B)' as an answer. If the model responses are correct at more than chance level, and especially if they are calibrated, then the probability P(True) indicates whether the model believes a response is valid. \n\nAs a first test of this approach, we can use answer options from existing multiple choice tasks. For this purpose, we take the correct answer and a randomly chosen incorrect answer, and create a new evaluation set with twice as many problems in the format above, asking models to determine if each answer is correct. In Figure 8 we show the calibration results from this method on BIG Bench. We see that the 52B model is quite well-calibrated in this context. We show similar results for MMLU in Figure 36 in the appendix, and scaling trends comparing a variety of evaluation methods in Figure 7. RLHF @ T=1 RLHF @ T=2.5 \n\nFigure 9 We show calibration curves for RLHF policies finetuned from our language models. Calibration of these models appears to be very poor, but simply adjusting the temperature of their probability distributions to T = 2.5 largely fixes calibration issues for three different evaluations.",
            "score": 0.5221667309345035,
            "section_title": "Models are Well-Calibrated on True/False Tasks",
            "char_start_offset": 20882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 357
                },
                {
                    "start": 360,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1518
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0233306884765625
        },
        {
            "corpus_id": "269157057",
            "title": "Language Model Cascades: Token-level uncertainty and beyond",
            "text": "There has been a large body of work on uncertainty quantification for LMs.We discuss some of the approaches below.They can be broadly divided into the following categories.\n\nConsensus-based.One limitation of Equation 3 is that it considers a single output sequence, e.g., the most likely one.However, as there are many sequences that have similar meaning, it is intuitively more reliable to consider drawing multiple sequences from p(\u2022 | x).One may then assess the consensus in resulting predictions to measure confidence (Wang et al., 2023;Xiao et al., 2021;Chen et al., 2023c); this can help distinguish between models that are locally diffused versus peaked around a candidate sequence.Recently, Yue et al. (2023) explored the use of answer consistency to construct effective cascades.\n\nDeep ensembles and dropout.One approach to measure confidence is to create an ensemble of different models, and suitably aggregate their predictions (e.g., based on disagreement) (Van Landeghem et al., 2022;Wang et al., 2019b;Gleave & Irving, 2022).However, these uncertainty estimation procedures involve additional computation (e.g., multiple inferences with a single model in dropout-based approaches and single inference with multiple models in ensemble-based approaches) compared to simply using softmax probability outputs from a single network.Such approaches are less appealing for use in cascades, where the primary goal is to improve efficiency.\n\nPost-hoc calibration/Answer-and length-bias calibration.For tasks involving question-answering with multiple choices (e.g., (A), (B), (C)), several works have demonstrated that LMs can have prior biases to certain answers, which can be identified and corrected (Zhao et al., 2021;Holtzman et al., 2021;Kumar, 2022;Murray & Chiang, 2018;Mielke et al., 2022;Jiang et al., 2021).\n\nSemantic entropy.Another key challenge in measuring the uncertainty for natural language outputs is that there are a lot of semantic equivalent sentences and hence, the probability can be divided among multiple outputs which mean the exact same thing.",
            "score": 0.5215010324598103,
            "section_title": "Uncertainty Quantification for LMs",
            "char_start_offset": 29245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 74,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 172
                },
                {
                    "start": 174,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 689
                },
                {
                    "start": 689,
                    "end": 788
                },
                {
                    "start": 790,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1341
                },
                {
                    "start": 1341,
                    "end": 1445
                },
                {
                    "start": 1447,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1823
                },
                {
                    "start": 1825,
                    "end": 1842
                },
                {
                    "start": 1842,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 541,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 969,
                    "end": 997,
                    "matchedPaperCorpusId": "248335524"
                },
                {
                    "start": 997,
                    "end": 1016,
                    "matchedPaperCorpusId": "202539965"
                },
                {
                    "start": 1708,
                    "end": 1727,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1727,
                    "end": 1749,
                    "matchedPaperCorpusId": "233296182"
                },
                {
                    "start": 1749,
                    "end": 1761,
                    "matchedPaperCorpusId": "248780520"
                },
                {
                    "start": 1761,
                    "end": 1783,
                    "matchedPaperCorpusId": "52132833"
                },
                {
                    "start": 1783,
                    "end": 1803,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1803,
                    "end": 1822,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.114990234375
        },
        {
            "corpus_id": "276776192",
            "title": "SteerConf: Steering LLMs for Confidence Elicitation",
            "text": "Large Language Models (LLMs) often exhibit misaligned confidence scores, usually overestimating the reliability of their predictions. While verbalized confidence in Large Language Models (LLMs) has gained attention, prior work remains divided on whether confidence scores can be systematically steered through prompting. Recent studies even argue that such prompt-induced confidence shifts are negligible, suggesting LLMs' confidence calibration is rigid to linguistic interventions. Contrary to these claims, we first rigorously confirm the existence of directional confidence shifts by probing three models (including GPT3.5, LLAMA3-70b, GPT4) across 7 benchmarks, demonstrating that explicit instructions can inflate or deflate confidence scores in a regulated manner. Based on this observation, we propose a novel framework containing three components: confidence steering, steered confidence aggregation and steered answers selection, named SteeringConf. Our method, SteeringConf, leverages a confidence manipulation mechanism to steer the confidence scores of LLMs in several desired directions, followed by a summarization module that aggregates the steered confidence scores to produce a final prediction. We evaluate our method on 7 benchmarks and it consistently outperforms the baselines in terms of calibration metrics in task of confidence calibration and failure detection.",
            "score": 0.5213021136600163,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26953125
        },
        {
            "corpus_id": "257219242",
            "title": "A Survey on Uncertainty Quantification Methods for Deep Learning",
            "text": "In recent years, large language models (LLMs) have demonstrated remarkable capabilities in generating coherent and contextually relevant text across various applications, from natural language processing to decision-making tasks. These models, such as OpenAI's GPT-4 [2], have shown proficiency in tasks ranging from text summarization and translation to more complex problemsolving and creative writing. However, despite their impressive performance, a critical challenge remains: how to ensure accurate uncertainty quantification to enhance the trustworthiness of these models? Existing research in LLMs uncertainty quantification mainly falls into two main categories: uncertainty estimation, and confidence calibration [53]. Uncertainty estimation methods for LLMs includes logit-based methods [43], model internal state-based methods [144], and consistency-based estimation [112]. Logit-based methods use the logits in the model output layer to compute the class entropy of output tokens. Internal state-based methods use the variability of sample embedding from model intermediate layers to quantify uncertainty. The embedding that is far from the majority of others indicates high uncertainty. Consistency-based methods measure the consistency between different model outputs from a sampling process and use the consistency as the uncertainty score [86,99,112]. For uncertainty estimation in LLMs, it is important to consider the semantic equivalence of the generated answers as different model outputs may look different in the generated tokens but similar in semantics. Graph-based [99] and semantic methods [89] propose alternative strategies for confidence estimation based on the semantic pattern, using a semantics similarity graph of different model outputs for uncertainty quantification. For confidence calibration, the goal is to adjust the model so that the quantified uncertainty from the above methods is aligned with prediction accuracy [103,116]. However, the field is still in its infancy. In future research, more",
            "score": 0.5204781604399953,
            "section_title": "UQ for Large Language Models",
            "char_start_offset": 104693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 839,
                    "end": 844,
                    "matchedPaperCorpusId": "252668614"
                },
                {
                    "start": 1963,
                    "end": 1967,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.204833984375
        },
        {
            "corpus_id": "258378336",
            "title": "Selective-LAMA: Selective Prediction for Confidence-Aware Evaluation of Language Models",
            "text": "In NLP, the reliability of the model responses has been discussed mainly in the field of question answering. Estimating the confidence of an answer is critical in quiz competitions, such as Jeopardy, since the system has to decide when to answer the questions (Ferrucci et al., 2010). Kamath et al. (2020) recently introduced a selective prediction setting to question answering tasks and then evaluated the performance of the models on out-ofdomain questions. Jiang et al. (2021) addressed a similar problem, but focused on a calibration of the model prediction on QA tasks. While they focused on extractive or multiple-choice QA tasks where a limited number of candidate answers are available, our focus is on the knowledge probing of language models where the candidate answer is the entire vocabulary and, thus, false positives are more frequent. \n\nSeveral studies have addressed the reliability issue of pre-trained language models as a calibration problem; the goal of these studies is to train a wellcalibrated language model that makes accurate confidence estimation. Desai and Durrett (2020) investigate the calibration level of pre-trained language models, focusing on BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). They evaluate the \"out-of-the-box\" performance of these models without post-processing, as well as the performance of post-hoc calibration methods (e.g., temperature scaling and label smoothing). Kong et al. (2020) proposed regularization methods to better calibrate pre-trained language models. Both studies assume access to (at least in-domain) training data of the target tasks on which parameterized calibration models can be trained. In contrast, our study primarily aims to explore better signals in pre-trained language models to estimate the knowledge they store. Thus, we focus on methods that do not require additional training data or an external knowledge source. Although training-based methods (e.g., temperature scaling) have the potential to achieve better performance in terms of calibration, optimal parameters vary depending on models and tasks, especially when evaluated in out-of-domain datasets (Desai and Durrett, 2020). \n\nIn our experiments, all queries have at least one correct answer.",
            "score": 0.5204356573180369,
            "section_title": "Related Work",
            "char_start_offset": 22189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2181
                },
                {
                    "start": 2184,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 283,
                    "matchedPaperCorpusId": "1831060"
                },
                {
                    "start": 285,
                    "end": 305,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 461,
                    "end": 480,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1076,
                    "end": 1100,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1184,
                    "end": 1205,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 2155,
                    "end": 2180,
                    "matchedPaperCorpusId": "212747810"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.114990234375
        },
        {
            "corpus_id": "270392060",
            "title": "Large Language Models Must Be Taught to Know What They Don't Know",
            "text": "The goal of this work is to make LLM outputs have better confidence values associated with them.With successful, calibrated confidence values, the machine systems ultimately become more interpretable and trustworthy by a user [23].When applied correctly, our advancements will help users be able to make decisions based off of LLM outputs in a more informed way.Similar examples in other domains, like AlphaFold [49], have shown how well-calibrated confidence scores can be useful in complex decision-making domains.Our hope is to replicate those broad findings in LLMs.\n\nWe acknowledge the ongoing debate over the appropriateness, limitations, and harms of LLMs.We do highlight that the development of more confident, interpretable, and trustworthy LLMs can lead to continued techno-solutionism in unintended applications.Specifically, we highlight that our work is limited to use-cases with fact-based questions.Many applications of text-based LLMs are generative, meaning that there is no way for our paradigm to be applied appropriately, and the use of a confidences from calibration-tuned models could be misleading or damaging without checks and guardrails.Additionally, even within the fact-based paradigm, what is true can be subjective, with ground truth in machine learning being a contested topic [3,54].\n\nThe philosophical debate on these topics is beyond the expertise of the authors; nonetheless, we believe that the ongoing debate over the appropriateness of LLMs should be considered in context with the benefits of our approach in making LLMs more interpretable and useful.",
            "score": 0.5199082862963833,
            "section_title": "H Broader Impact and Implications",
            "char_start_offset": 50846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 96,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 570
                },
                {
                    "start": 572,
                    "end": 663
                },
                {
                    "start": 663,
                    "end": 823
                },
                {
                    "start": 823,
                    "end": 914
                },
                {
                    "start": 914,
                    "end": 1163
                },
                {
                    "start": 1163,
                    "end": 1315
                },
                {
                    "start": 1317,
                    "end": 1590
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "23281789"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "258834859"
                },
                {
                    "start": 1308,
                    "end": 1311,
                    "matchedPaperCorpusId": "6134326"
                },
                {
                    "start": 1311,
                    "end": 1314,
                    "matchedPaperCorpusId": "245589751"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26025390625
        },
        {
            "corpus_id": "252917981",
            "title": "Prompting GPT-3 To Be Reliable",
            "text": "Language models producing toxic or biased content can cause severe harm especially to the groups being biased against (Bender et al., 2021). A series of benchmarks have been developed to show that LLMs can generate toxic outputs (Gehman et al., 2020), contain gender biases (Rudinger et al., 2018;Zhao et al., 2018) and other categories of social biases (Nangia et al., 2020;Nadeem et al., 2021;Parrish et al., 2022), perform poorly against minority demographic groups ( Koh et al., 2021;Harris et al., 2022) or dialectical variations (Ziems et al., 2022;Tan et al., 2020). Ideally, LLMs should not exhibit biased behaviors and not discriminate against any group. While many of these evaluations focus on evaluating the internal representation of LLMs in a zeroshot setting or evaluating the biases on specific downstream applications in a supervised setting, it remains unclear how these biases change under different prompting schemes in the few-shot setting, which will be the focus of our analysis. A closely related work is Lucy & Bamman (2021) which study representation biases in GPT-3 generated stories. We instead evaluate on the downstream tasks of coreferece resolution and question answering. Apart from few-shot prompting, Solaiman & Dennison (2021) proposed a general method to align language models with human values, but it involves expensive iterative training. \n\nUncertainty Calibration. No model can ever be perfect, and so it is crucial for users to be able to identify model mistakes, especially in high-stage settings where trusting wrong model predictions can cause severe harm. One important way to help identify wrong model predictions is by obtaining well-calibrated confidence scores for model predictions. By definition, a calibrated confidence (probability) score should match the expected accuracy of the prediction (Platt, 1999;Naeini et al., 2015;Guo et al., 2017). In this way, users can put more trust in highly-confidence predictions and discard low-confidence predictions.",
            "score": 0.5193798948939872,
            "section_title": "APPENDIX A MORE RELATED WORK",
            "char_start_offset": 27617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1378
                },
                {
                    "start": 1381,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 139,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 274,
                    "end": 297,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 297,
                    "end": 315,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 354,
                    "end": 375,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 375,
                    "end": 395,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 395,
                    "end": 416,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 471,
                    "end": 488,
                    "matchedPaperCorpusId": "229156320"
                },
                {
                    "start": 488,
                    "end": 508,
                    "matchedPaperCorpusId": "249872614"
                },
                {
                    "start": 535,
                    "end": 555,
                    "matchedPaperCorpusId": "247863188"
                },
                {
                    "start": 555,
                    "end": 572,
                    "matchedPaperCorpusId": "218581359"
                },
                {
                    "start": 1029,
                    "end": 1049,
                    "matchedPaperCorpusId": "235097208"
                },
                {
                    "start": 1846,
                    "end": 1859,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1859,
                    "end": 1879,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1879,
                    "end": 1896,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1143798828125
        },
        {
            "corpus_id": "275570800",
            "title": "Rational Tuning of LLM Cascades via Probabilistic Modeling",
            "text": "We favor this approach over temperature scaling since it yields p values and other statistical metrics that are useful for diagnosing calibration issues, especially in a low-data scenario. \n\nUnfortunately, the overconfidence of the raw token probabilities makes the distribution of raw confidence signals highly peaked. The raw token probabilities accumulate near 1.0, making tiny changes in confidence (for example, p raw = 0.98 vs p raw = 0.99) highly consequential. To enhance the calibration performance of logistic regression, as a pre-processing step we apply hyperparameter-free feature transformations that spread out the overconfident probabilities via asymptotes near p raw = 0.0 and p raw = 1.0. Following Zellinger and Thomson (2024), on multiple-choice tasks we use the transformation \n\nwhereas on natural language generation tasks, we use \n\nImportantly, these feature transformations do not require any hyperparameter tuning. \n\nUnfortunately, models sometimes return perfect certainty p raw = 1.0 or p raw = 0.0, making ( 16) and ( 17) blow up. To address this problem, we reassign all observations with infinite \u03be to the maximum of the finite values of \u03be. In other words, we define \n\nwhere D is the training set consisting of pairs (p raw , y), and y \u2208 {0, 1} indicates correctness of the model's answer. 2 We set all observations where \u03be = \u221e to \u03be max , and treat \u03be min analogously. \n\nBenchmarks: we evaluate our probabilistic model and the cost-error curves of LLM cascades on six language modeling benchmarks including MMLU (Hendrycks et al., 2021); MedMCQA (Pal et al., 2022); TriviaQA (Joshi et al., 2017); XSum (Narayan et al., 2018); GSM8K (Cobbe et al., 2021); and TruthfulQA (Lin et al., 2022b). These tasks include general-purpose knowledge and reasoning, domain-specific QA, open-domain QA, summarization, mathematical reasoning, and truthfulness in the face of adversarially chosen questions.",
            "score": 0.51891889544628,
            "section_title": "Methodology",
            "char_start_offset": 19302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1918
                }
            ],
            "ref_mentions": [
                {
                    "start": 1541,
                    "end": 1565,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1575,
                    "end": 1593,
                    "matchedPaperCorpusId": "247763070"
                },
                {
                    "start": 1604,
                    "end": 1624,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 1631,
                    "end": 1653,
                    "matchedPaperCorpusId": "215768182"
                },
                {
                    "start": 1698,
                    "end": 1717,
                    "matchedPaperCorpusId": "237532606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038970947265625
        },
        {
            "corpus_id": "257805227",
            "title": "Did You Mean...? Confidence-based Trade-offs in Semantic Parsing",
            "text": "Task-oriented dialogue systems (Gupta et al., 2018;Cheng et al., 2020;Semantic Machines et al., 2020) represent one path towards achieving the longstanding goal of using natural language as an API for controlling real-world systems by transforming user requests into executable programs, i.e. translating natural language to code. Central to the systems' success is the ability to take rational actions under uncertainty (Russell and Norvig, 2010). When model confidence is low and the system is unlikely to succeed, we would prefer it defer actions and request clarification, while at high confidence, clarification requests may annoy a user. Relying on model confidence requires it to be well-correlated with accuracy, i.e. it requires a calibrated model. \n\nRecent work has focused on the calibration of semantic parsing models. Specifically, Stengel-Eskin and Van Durme (2022) benchmarked the calibration characteristics of a variety of semantic parsing models, finding some of them to be well-calibrated, especially on parsing for task-oriented dialogue. Given the relatively well-calibrated nature of these * Work done while at Johns Hopkins University. models, we first examine how they could be used in an annotation interface, with a view to balancing the trade-off between annotation cost and correctness. We simulate a human-in-the-loop (HITL) experiment where high-confidence tokens are automatically annotated and low-confidence tokens trigger a dialogue with an oracle annotator who either picks the correct token from a top-K list or manually inserts it. With a small number of interactions we substantially boost annotator accuracy. \n\nA similar trade-off exists between usability and safety in task-oriented user interfaces. We examine how sequence-level model confidence scores can be used to balance this trade-off by reducing the number of incorrect programs executed while also minimizing the number of follow-up user interactions and their cognitive burden. We find that thresholding outputs based on model confidence (i.e. rejecting outputs falling below a tuned threshold) reduces the number of incorrect programs executed by 76% compared to the baseline.",
            "score": 0.5186597668949835,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 51,
                    "matchedPaperCorpusId": "53017700"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.118408203125
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "Knowing when to trust a model's predictions is typically mapped to the concept of calibration, where the model's confidence estimate for a prediction reflects how likely it is to be correct.Language models (LMs) have recently been shown to be wellcalibrated in a number of settings (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023;Ope-nAI, 2023).However, while models can be wellcalibrated for aggregate distributions (e.g.mixtures of a number of domains), they can be significantly miscalibrated for narrower domains within that distribution (Yu et al., 2022;Hebert-Johnson et al., 2018).\n\nFor instance, Figure 1 shows an LM that is wellcalibrated on the combined distribution of five domains, achieving near perfect calibration curve with low expected calibration error (ECE).However, curves for the individual domains appear significantly miscalibrated in comparison, with the least calibrated domain virology having a 250% higher calibration error.This miscalibration problem is hidden for the combined distribution because overconfidence in some domains cancels out underconfidence in others.\n\nThis illustrates a key problem: LMs are not well-calibrated for meaningful slices of broader distributions.This is particularly relevant in practice where users querying an LM rarely sample from a broad combination of distributions at any given time, and are more likely to sample from slices like abstract algebra or virology.Our goal is to recalibrate LMs for each of these fine-grained slices of a distribution, thereby allowing users to reliably determine when predictions can be trusted.\n\nTo recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice.",
            "score": 0.5184721956883453,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 599
                },
                {
                    "start": 601,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1107
                },
                {
                    "start": 1109,
                    "end": 1216
                },
                {
                    "start": 1216,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1601
                },
                {
                    "start": 1603,
                    "end": 1832
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 323,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 323,
                    "end": 341,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 570,
                    "end": 598,
                    "matchedPaperCorpusId": "51880858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43359375
        },
        {
            "corpus_id": "268033118",
            "title": "Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>",
            "text": "Language models (LMs) are trained to assign probability to human-generated text. The typical LM treats a piece of text as a sequence of tokens whose joint probability it factorises autoregressively, with conditional token probabilities predicted from the available context by a neural network (Mikolov et al., 2010;Radford et al., 2019;Scao et al., 2022). An LM can be viewed as a representation of uncertainty about human linguistic production (Serrano et al., 2009;Takahashi and Tanaka-Ishii, 2019;Meister and Cotterell, 2021;Giulianelli et al., 2023), specifically, one that reflects the production variability exhibited by the population(s) who generated the training data. Despite how plausible this variability is, LMs are not consistently exposed to it at the level of individual contexts (i.e., due to data sparsity, most contexts are unique) leading us to investigate their ability to predict it well. One way to appreciate plausible variability is to ask humans to perform next word prediction: show multiple participants the same prefix of a passage and ask each of them to contribute a word that plausibly extends it. An LM that assigns probability to any next-word candidate similar to the proportion of the human population contributing it as the next word serves as a good proxy to the production variability of that human population-a desideratum Baan et al. (2022) termed calibration to human uncertainty. 1 Studying different notions of cal-ibration of text classifiers, Baan et al. (2022) show that the very popular expected calibration error (ECE;Guo et al., 2017) is flawed in the presence of data uncertainty (e.g., due to the task's inherent ambiguity (Plank, 2022)). As data uncertainty is hardly avoidable in language modelling, we must entertain the possibility that ECE is not a reliable tool to assess the predictive distributions of an LM, despite its widespread use (Kumar and Sarawagi, 2019;Wang et al., 2020;Tian et al., 2023).",
            "score": 0.516646267906927,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1959
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08270263671875
        },
        {
            "corpus_id": "272689376",
            "title": "Confidence Estimation For LLM-Based Dialogue State Tracking",
            "text": "Estimation of a model\u2019s confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance. In this work, we provide an exhaustive exploration of methods, including approaches proposed for open- and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty to improve the reliability of LLM-generated responses, specifically focusing on dialogue state tracking (DST) in task-oriented dialogue systems (TODS). Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models. Furthermore, we assess these methods using an open-weight model fine-tuned for the task of DST, achieving superior joint goal accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can result in enhanced AUC performance, indicating better confidence score calibration.",
            "score": 0.5160890745514479,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "277150701",
            "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey",
            "text": "\u2022 Calibration refers to closing the gap between the confidence score and the expected correctness conditioned on confidence score. It has a long history preceding LLM or even modern machine learning [25,97,98], but bears slightly different meanings in NLP. In general, we could define a perfectly calibrated confidence measure to achieve the following: \n\nwhere the expectation is taken over the joint distribution of x and generation s. A lot of papers focus on evaluating the calibration quality of specific LMs and tasks [61,63,134]. Evaluation typically relies on variants of Expected Calibration Error (ECE) [63,71,124]. Oftentimes confidence scores from classification could be directly applied [55,120,162] in order to evaluate whether an LM is overor under-confident, especially for de facto classification tasks like sentiment analysis or multiple-choice QA. \n\nAs uncertainty and confidence are often intertwined, many approaches used in uncertainty quantification have their counterpart in confidence estimation. For example, for black-box methods, Lin et al. [77] computes a similarity matrix of sampled responses and derives confidence estimates for each generation via its degree or distance derived from the graph Laplacian, before using these scores to compute uncertainty. Zhang et al. [153] extends such black-box methods to longer generations. For logit-based methods, Malinin and Gales [86] normalize Eq. ( 1) with the length of s. Further improvements include replacing the logit-sum or mean with weighted sum, by attention values on the important tokens [78] or by importance inferred from natural language inference (NLI) models [29]. Such variants of sequence likelihood could then be fed for (entropy-style) uncertainty computation [62,78]. \n\nAnother popular approach is asking the LM itself whether a particular free-form generation is correct [59]. However, this formulation also poses a restriction on the confidence estimation method, as it is essentially a scalar logit. Thus, many extensions focus on applying calibration methods from classification to calibrate such self-evaluation.",
            "score": 0.5157562937692939,
            "section_title": "Uncertainty and Confidence in LLMs",
            "char_start_offset": 8771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 203,
                    "end": 206,
                    "matchedPaperCorpusId": "127203550"
                },
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "122150550"
                },
                {
                    "start": 523,
                    "end": 527,
                    "matchedPaperCorpusId": "252847587"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 704,
                    "end": 708,
                    "matchedPaperCorpusId": "253510101"
                },
                {
                    "start": 708,
                    "end": 712,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "258967487"
                },
                {
                    "start": 1301,
                    "end": 1306,
                    "matchedPaperCorpusId": "268793903"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "231895728"
                },
                {
                    "start": 1650,
                    "end": 1654,
                    "matchedPaperCorpusId": "270095084"
                },
                {
                    "start": 1755,
                    "end": 1759,
                    "matchedPaperCorpusId": "257039062"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.378173828125
        },
        {
            "corpus_id": "270392060",
            "title": "Large Language Models Must Be Taught to Know What They Don't Know",
            "text": "\"I have high cortisol but low ACTH on a dexamethasone suppression test.What should I do?\"If the answer to such a question is given without associated confidence, it is not actionable, and if the answer is presented with erroneously high confidence, then acting on the answer is dangerous.One of the biggest open questions about whether large language models (LLMs) can benefit society and reliably be used for decision making hinges on whether or not they can accurately represent uncertainty over the correctness of their output.\n\nThere is anything but consensus on whether LLMs accurately represent uncertainty, or even how we should approach uncertainty representation with language models.Claims regarding language models' ability to estimate uncertainty vary widely, with some works suggesting that language models are increasingly capable of estimating their uncertainty directly through prompting, without any fine-tuning or changes to the training data [25,51], and others suggesting that LLMs remain far too overconfident in their predictions [59,60].The task of uncertainty estimation in LLMs is further exacerbated by linguistic variances in freeform generation, all of which cannot be exhaustively accounted for during training.LLM practitioners are therefore faced with the challenge of deciding which estimation method to use.\n\nOne particular dichotomy in uncertainty estimation methods for language models centers around whether the estimates are black-or white-box.Black-box estimates do not require training and can be used with closed-source models like GPT-4 [1] or Gemini [48], while white-box methods require training parameters on a calibration dataset.Although black-box estimates have become popular with Figure 1: Large language models struggle to assign reliable confidence estimates to their generations.We study the properties of uncertainty calibration in language models, and propose fine-tuning for better uncertainty estimates using a graded dataset of generations from the model.We evaluate our methods on a new open-ended variant of MMLU [18].We show that fine-tuning improves expected calibration error (ECE) and area under the receiver operating characteristic curve (AUROC) compared to commonly-used baselines.Error bars show standard deviation over three base models (LLaMA-2 13/7B and Mistral 7B) and their chat variants.",
            "score": 0.5157047050494824,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 71,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 530
                },
                {
                    "start": 532,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1240
                },
                {
                    "start": 1240,
                    "end": 1340
                },
                {
                    "start": 1342,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1675
                },
                {
                    "start": 1675,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2012
                },
                {
                    "start": 2012,
                    "end": 2077
                },
                {
                    "start": 2077,
                    "end": 2247
                },
                {
                    "start": 2247,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 1056,
                    "end": 1059,
                    "matchedPaperCorpusId": "258959258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18115234375
        },
        {
            "corpus_id": "276482534",
            "title": "MCQA-Eval: Efficient Confidence Evaluation in NLG with Gold-Standard Correctness Labels",
            "text": "Confidence Estimation Confidence estimation is fundamental to machine learning, providing mechanisms to assess model reliability and guide decision-making across tasks. Early confidence estimation research concentrated on classification settings, where confidence scores enabled Selective Classification (Geifman and El-Yaniv, 2017;El-Yaniv et al., 2010;Feng et al., 2022)-allowing models to abstain from low-quality predictions. \n\nThe rapid advancement of NLG and LLMs has brought renewed attention to confidence estimation. While NLG poses unique challenges due to semantic invariance and vast output spaces (Kuhn et al., 2023), recent works have advanced the field by measuring similarities among sampled responses (Lin et al., 2024b) and deriving measures from LMs' internal states (Malinin and Gales; Lin et al., 2024a;Azaria and Mitchell, 2023). A related aspect is calibration. While extensively considered in classification (Zhang et al., 2020;Kull et al., 2019;Ma and Blaschko, 2021), it has received lass attention in NLG. Since the distribution of confidence scores could vary significantly across different methods due to their underlying principles (Geng et al., 2023;Da et al., 2024), calibrated confidence measures align better with human intuition for probabilities and are more interpretable (Guo et al., 2017;Cosmides and Tooby, 1996). While this paper focuses on evaluating confidence estimation methods, the same framework could be applied to evaluate future NLG calibration methods. We demonstrate this by including results using common calibration metrics like Expected Calibration Error (ECE). \n\nEvaluation of Confidence Measures While confidence estimation has received considerable attention, the evaluation of confidence measures remains under-explored. Many evaluation methods have been adapted from the classification literature, including Expected Calibration Error (ECE) (Guo et al., 2017;Xiong et al., 2024) and Area Under the Receiver Operating Characteristic Curve (AU-ROC) (Kuhn et al., 2023). These metrics assess the relationship between confidence scores and prediction accuracy, typically requiring high-quality correctness labels for the evaluated responses.",
            "score": 0.5154935573444477,
            "section_title": "Related Work",
            "char_start_offset": 3600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1616
                },
                {
                    "start": 1619,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 332,
                    "matchedPaperCorpusId": "491127"
                },
                {
                    "start": 610,
                    "end": 629,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 718,
                    "end": 737,
                    "matchedPaperCorpusId": "258967487"
                },
                {
                    "start": 806,
                    "end": 824,
                    "matchedPaperCorpusId": "270226302"
                },
                {
                    "start": 824,
                    "end": 850,
                    "matchedPaperCorpusId": "258352729"
                },
                {
                    "start": 932,
                    "end": 952,
                    "matchedPaperCorpusId": "212725167"
                },
                {
                    "start": 952,
                    "end": 970,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 970,
                    "end": 992,
                    "matchedPaperCorpusId": "231895728"
                },
                {
                    "start": 1309,
                    "end": 1327,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1901,
                    "end": 1919,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1919,
                    "end": 1938,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 2007,
                    "end": 2026,
                    "matchedPaperCorpusId": "257039062"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307373046875
        },
        {
            "corpus_id": "259370520",
            "title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
            "text": "(1) With prompt tuning and extra training data, we can better induce large-scale language models to generate their memorized training data and successfully achieves a 9.5% performance improvement on Recall and a 12.4% performance improvement on Recall Early stop .\n\n(2) The proposed smoothing loss can further enhance the ability to extract training data, boosting the Recall score from 60.8% to 62.3%. (3) The calibrated confidence provides a 6.3% improvement on Recall Early stop as expected, demonstrating the importance of calibrating confidence estimation for this task. (4) The smoothing loss is more effective in predicting exact suffixes while the calibrated confidence is more beneficial for identifying highly confident predictions, according to the significant drop in Recall without smoothing and the substantial decrease in Recall Early stop without calibration. (5) The calibrated confidence estimation is effective regardless of whether using prompt tuning. And it demonstrates greater advantages compared to the comparing (LM) baseline in recognizing predictions with higher confidence when using prompt tuning, indicated by increasing Recall Early stop (from 48.7 to 52.4).",
            "score": 0.5152830931254525,
            "section_title": "Main Results",
            "char_start_offset": 19939,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10284423828125
        },
        {
            "corpus_id": "268033118",
            "title": "Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>",
            "text": "There has been work that exploits predictive distributions of LMs in various ways. LeBrun et al. (2022) analyses such distributions and finds that they overestimate the probability of ill-formed sequences. Others investigate alternative training signals that minimise the distance between the data and model distributions (Ji et al., 2023;Labeau and Cohen, 2019;Zhang et al., 2023). Our work exploits predictive distributions as an uncertainty representation of human linguistic production and study their calibration. Several works study how well-calibrated LMs are and how to alleviate miscalibration (He et al., 2023;Lee et al., 2022;Xiao et al., 2022;Ahuja et al., 2022;Chen et al., 2022;Kumar and Sarawagi, 2019;Li et al., 2022;Xiao and Wang, 2021) -the majority using ECE to substantiate their findings, whose inadequacy makes us believe that a new round of studies is needed to assess this matter; our work being an example. \n\nThere is a line of work that stresses the value of obtaining multiple human labels per input (Plank, 2022;Basile et al., 2020;Grossmann et al., 2022;Prabhakaran et al., 2021), embracing data uncertainty in classification; Baan et al. (2022) propose calibration metrics that accommodate label variability in natural language inference (NLI; Bowman et al., 2015). In concurrent work, Lee et al. ( 2023",
            "score": 0.5152441020817581,
            "section_title": "Related Work",
            "char_start_offset": 14330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1333
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06396484375
        },
        {
            "corpus_id": "273654253",
            "title": "Rethinking the Uncertainty: A Critical Review and Analysis in the Era of Large Language Models",
            "text": "Despite their potential, these methods are constrained by the model's limited self-awareness, which can lead to circular reasoning and overconfident inaccuracies (Ji et al., 2023b;Chen et al., 2023). Another challenge is the interpretability and validity of obtained probabilities that align with specific linguistic and psychological interpretations, including expressions like 'I think,' 'undoubtedly,' or 'high confidence.' \n\nInternal-Based Approach Recently, Beigi et al. (2024) used a mutual information framework to theoretically demonstrate that the internal states of large language models provide additional insights into the correctness of their answers. Burns et al. (2023) introduced an innovative unsupervised method that maps hidden states to probabilities. This approach involves responding to \"Yes\" or \"No\" questions, extracting model activations, converting these activations into probabilities. Furthering this research, studies have employed linear probes (Li et al., 2023b;Azaria & Mitchell, 2023b) and contrastive learning (Beigi et al., 2024) to assess whether the internal states across various layers can distinguish between correct and incorrect answers. Empirical results suggest that certain middle layers and specific attention heads show strong discriminative abilities. Beigi et al. (2024) expanded these findings by illustrating that for tasks requiring contextual processing, such as reading comprehension, the outputs of multi-head self-attention (MHSA) components are crucial for assessing response correctness. However, current methodologies exhibit limitations. Each task and dataset requires training a specific \"confidence estimator\" model, which restricts their generalizability. This limitation is evident as the performance of these methods often declines when trained on one task and dataset and tested on another, highlighting their limited transferability across different applications (Bashkansky et al., 2023). Additionally, the computational resources required to train these confidence estimators pose challenges for their deployment in real-time applications, further complicating their practical utility. \n\nWhat are the main properties of these methods in estimating uncertainty in LLMs?",
            "score": 0.5152441020817581,
            "section_title": "APPROACHES FOR ESTIMATING AND EVALUATING UNCERTAINTY IN LLMS",
            "char_start_offset": 30964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2154
                },
                {
                    "start": 2157,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 180,
                    "matchedPaperCorpusId": "266176951"
                },
                {
                    "start": 180,
                    "end": 198,
                    "matchedPaperCorpusId": "264289232"
                },
                {
                    "start": 665,
                    "end": 684,
                    "matchedPaperCorpusId": "254366253"
                },
                {
                    "start": 975,
                    "end": 993,
                    "matchedPaperCorpusId": "259088877"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043365478515625
        },
        {
            "corpus_id": "273186420",
            "title": "Calibrating Expressions of Certainty",
            "text": "Figure 13 provides visualizations of the calibration process for different language models. While the calibration maps follow a similar overall trend, they reveal distinct differences based on each model's distinct preference for certainty phrases.",
            "score": 0.5152441020817581,
            "section_title": "Model",
            "char_start_offset": 26445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 248
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0284423828125
        },
        {
            "corpus_id": "260063200",
            "title": "Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement",
            "text": "The modern framing of language modeling problems now includes the ability to perform numerous tasks previously handled by specialized supervised discriminative systems. For example, binary and multi-class classification tasks can be framed as text generation, where a large language model (LLM) is given the input and the possible labels, and it generates the best label. More broadly, many reading comprehension, reasoning, and question-answering (QA) tasks can be framed in this multiple-choice style. An advantage to framing tasks in this manner is the ability to perform Figure 1: A comparison of our proposed prompt agreement confidence scores (2) and the commonly used log probability (1). Log probability is based on a single prompt, while the prompt agreement confidence estimate uses multiple prompts to determine a confidence estimate using (2a) the log probabilities from majority label or (2b) the Rand index of label frequencies. \n\nfew-shot learning via in-context learning, in which a task can be performed based on only a handful of examples (Arora et al., 2022;Brown et al., 2020;Kojima et al., 2022;Sanh et al., 2021;Zhou et al., 2022a). Instead of collecting a large dataset and investing time in training a new model, a user could utilize an existing LLM for a new task by labeling a few examples and crafting a prompt: the input template which instructs the model to perform the given task. \n\nOne challenge to utilizing LLMs in this manner is producing well-calibrated confidence scores for model predictions. A calibrated confidence score aids in the interpretation of model predictions (Guo et al., 2017) and may be crucial if models become integrated into high-risk domains like healthcare and finance (Jiang et al., 2021). A model is considered well calibrated if its prediction probabilities are aligned with the actual probability of its predictions being correct (Jiang et al., 2021). If a model says an answer has 90% confidence, then we should expect it to be correct 90% of the time. \n\nFormally, the probability that the predicted label \u0176 is equal to the correct label Y for input X should be equal to the model's predicted confidence (Nguyen and O'Connor, 2015).",
            "score": 0.5152441020817581,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1410
                },
                {
                    "start": 1413,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2013
                },
                {
                    "start": 2016,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 1608,
                    "end": 1626,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1725,
                    "end": 1745,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1890,
                    "end": 1910,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.351318359375
        },
        {
            "corpus_id": "267539494",
            "title": "Do Large Language Models Show Human-like Biases? Exploring Confidence - Competence Gap in AI",
            "text": "Despite their computational power, do LLMs display human cognitive biases like the Dunning-Kruger effect? Based on its confidence scores, can we identify situations where a model is overly confident or lacks confidence in its abilities? Our subsequent analyses explore these questions, examining how well the models' self-assessment aligned with their real-world performance. After collecting the confidence scores, we analyzed them to study the calibration of LLMs based on their reported confidence levels. The calibration of confidence levels and their relationship with the accuracy of LLMs are two significant aspects of our study. To evaluate these, we employed the following two metrics. \n\nFor the first metric, we focused on the instances when the models were highly confident and their responses were correct, and vice versa. The four scenarios considered were as follows (for A1): \n\n1. \n\nHigh Confidence, Correct Answers: LLMs with a high A1 score (e.g., A1 > 7) and correct answers.",
            "score": 0.5148762680038131,
            "section_title": "Confidence Calibration Metrics",
            "char_start_offset": 16932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 993
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0889892578125
        },
        {
            "corpus_id": "258865733",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "text": "We return the most common answer, using the LLM-based equivalence function to determine when two lexically different answers are semantically equivalent. In a variation of the method described by Kadavath et al. (2022) (again, we use samples since model probabilities are not available), 'Is True' prob. samples a single answer \u0177 from the model given a question x, and the probability it is true is estimated by the probability the model assigns to 'True' when asked if the given answer is true (where once again the probabilities are estimated via samples), i.e., p(True|x, \u0177). \n\nNext, we consider methods that extract confidence scores through verbalization (Lin et al., 2022a), i.e., where the model expresses its confidence in token space, either with numerical probabilities or linguistic expressions of likelihood. 8 First, Verb. 1S top-k prompts the model to produce k guesses and a probability that each is correct all in a single response (i.e., '1 stage'). We take the highest-probability prediction and its as- 6 We evaluated gpt-3.5-turbo on all three datasets using n = 20 samples, but the calibration did not meaningfully improve, so we always use n = 10 to reduce API costs. 7 For each closed LM, we use its default sampling parameters (top-p 1.0 for GPT-* and top-p 0.7 for Claude). For Llama-2, we use temperature 1.0 and top-p 1.0. \n\n8 However, note that none of the methods described finetune the model to perform better on verbalization. sociated probability as the model's output and confidence. Verb. 2S top-k similarly uses numerical probabilities, except the model is first asked to provide only its answers, and afterwards, in a second round of dialogue, asked to assign probabilities of correctness to each answer (i.e., '2 stages'). Verb.",
            "score": 0.5146999622852134,
            "section_title": "Evaluating Calibration in RLHF-LMs",
            "char_start_offset": 8817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1349
                },
                {
                    "start": 1352,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1765
                }
            ],
            "ref_mentions": [
                {
                    "start": 660,
                    "end": 679,
                    "matchedPaperCorpusId": "249191391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0966796875
        },
        {
            "corpus_id": "267627416",
            "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
            "text": "Large language models (LLMs), when prompted appropriately, can articulate their confidence using words or sentences [22]. Unlike accuracy, which evaluates AI's overall performance and reliability, confidence levels quantify the uncertainty of each individual task [23,24]. Thus, AI's confidence may serve as a more powerful indicator of task-level uncertainty that influence human trust and use of AI. Higher AI confidence facilitates trust in AI and its usage, whereas lower confidence may lead to preference for human judgment [17,25]. Accurately calibrated AI confidence can enhance not only trust calibration but also the efficiency and effectiveness of human-AI collaboration [23,19,26]. \n\nWhile existing research often regards well-calibrated AI confidence as a foundational premise [25,18,19,27], it overlooks a critical reality: achieving well-calibrated AI confidence is more challenging and less common than assumed. Despite numerous efforts to calibrate AI confidence [28,23], many AI systems still exhibit misaligned confidence levels, failing to accurately reflect their actual CL [23,29,30,31]. This discrepancy is evident in AI systems that exhibit overconfidence (i.e., their expressed confidence exceeds their actual CL), notably in some LLMs providing highly confident yet incorrect answers [21,22]. Conversely, there are instances of unconfidence, where AI's confidence falls short of their actual capability [31]. This misalignment highlights the complexity and necessity of accurate confidence calibration in AI systems. Furthermore, the influences and risks of uncalibrated AI confidence are not sufficiently Fig. 1 (a) A flowchart illustrating the study procedure. Participants were required to sequentially complete these four phases. (b) Conceptual map of variables: attitude is informed by perception and influences behavior. Human-AI collaboration outcome is the result of behavior. (c) City photo recognition task interface. Participants were tasked with identifying the origins of city photos from three choices. In each task, participants made an initial decision, followed by the AI providing a suggestion along with a percentage confidence score, after which participants made their final decision. \n\nunderstood.",
            "score": 0.5143354830641278,
            "section_title": "Introduction",
            "char_start_offset": 2081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2230
                },
                {
                    "start": 2233,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 533,
                    "matchedPaperCorpusId": "34838594"
                },
                {
                    "start": 1104,
                    "end": 1107,
                    "matchedPaperCorpusId": "238215193"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "238215193"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07080078125
        },
        {
            "corpus_id": "273345872",
            "title": "LLM$\\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models",
            "text": "When long sequences are split into multiple chunks, conflicts may arise due to incomplete information. The aforementioned information structure provides a confidence score for the answer from each chunk. During the collapse and reduce stages, confidence scores are crucial for guiding the merging of information and generating both the rationale and the final answer. \n\nTo make the confidence scores across different chunks comparable, we propose to calibrate them through in-context learning, without adjusting model parameters. Specifically, we provide confidence estimation principles alongside a typical example for different levels of confidence score. By referring to the principles and the examples, the model is expected to apply a consistent criterion when processing different chunks. Figure 2 provides an example of the calibration prompt. We can customize different calibration prompts for various tasks. Claims fully supported by the provided text are assigned high confidence, while those inferred by the model receive medium confidence. Claims not related to the provided text are assigned low confidence. Experiments in Section 3.3 demonstrate the necessity of the proposed in-context confidence calibration mechanism. \n\nAssign a confidence score (out of 5) to your answer based on the completeness and reliability of the extracted information and your rationale. The following is some assigning scoring cases: <Text: [ Jerry is 18 years old this year. He can swim and wants to be an athlete. ]. Examples of confidence estimation: [ Jerry can swim, 5 points; Jerry will become an athlete in the future, 3.5 points; Jerry will become a swimming athlete in the future, 3 points; Jerry is strong, 3 points; Jerry can play chess, 0 points; Jerry likes talking, 0 points ] >. We employ vLLM4 for model inference, and the decoding temperature is set to 0.7. \n\nEvaluation We evaluate the performance of the involved models and methods on In-finiteBench (Zhang et al., 2024a), where the average input length exceeds 100K tokens. This benchmark assesses the long-text capabilities of LLMs across several dimensions, including longrange retrieval, language comprehension, code understanding, and mathematical problem-solving. We exclude the subsets Code.Run and Math.Calc, as nearly all models achieve less than 5% accuracy on these tasks, making it difficult to differentiate performance among the models.",
            "score": 0.5141455349014259,
            "section_title": "In-Context Confidence Calibration",
            "char_start_offset": 13184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1234
                },
                {
                    "start": 1237,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2231
                },
                {
                    "start": 2232,
                    "end": 2412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15625
        },
        {
            "corpus_id": "273821704",
            "title": "Graph-based Confidence Calibration for Large Language Models",
            "text": "There are methods using an auxiliary Language Model (e.g., DeBERTa (He et al., 2020)) to verify whether the LLM's response appropriately answers the question (Ulmer et al., 2024). Since the LLM is supposed to be much stronger than the LM, the LLM should be able to avoid most mistakes that can be detected by an LM; so this type of method may omit a significant fraction of wrong answers. Second, it is hard to detect mistakes from the LLM's internal working mechanism. Because the LLM uses many hidden layers to process the information, it is hard to discern the signal from a small number of hidden units. Even if this is possible, it is not easy to apply this type of method to black-box LLMs. \n\nRecently, there has been some progress in quantifying the model's confidence in its own responses through consistency among the outputs generated by the model itself (Chen & Mueller, 2023;Lin et al., 2024). These approaches demonstrate a strong correlation between an LLM's self-consistency and the actual correctness of its responses. However, because these methods depend on hand-crafted features, they often fail to accurately calibrate confidence, resulting in a mismatch between predicted confidence levels and the true accuracy of the answers. This raises an important research question: can we improve confidence calibration by learning from patterns of consistency across the LLM's own responses? \n\nIn this work, we propose an auxiliary learning model to improve confidence calibration. We begin by constructing similarity graphs from the LLM's multiple responses to the same questions, where graph edges represent the degree of agreement between responses. We then train a separate calibration model using graph neural networks (GNNs) to predict the correctness of each response. The key insight is that consistency among responses carries strong signals of correctness -for instance, a response that aligns well with many others is more likely to be accurate. Importantly, our model operates solely on response consistency and does not analyze the actual language content. This work focuses on practical scenarios where correctness reflects alignment with the training data, and does not address the case where training data itself contains consistent but incorrect information.",
            "score": 0.5136745994607912,
            "section_title": "Introduction",
            "char_start_offset": 1612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2287
                }
            ],
            "ref_mentions": [
                {
                    "start": 887,
                    "end": 904,
                    "matchedPaperCorpusId": "258967487"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.137451171875
        },
        {
            "corpus_id": "260438666",
            "title": "LOUC: Leave-One-Out-Calibration Measure for Analyzing Human Matcher Performance",
            "text": "In our settings (Section 2.4) human matchers provide confidence scores for matching pairs. We view the confidence score as a measure of match decision correctness. However, in real-life scenarios, a reference match does not always exist, and we must identify matchers for whom we can rely on their reported confidence. In this context, trustworthy matchers are matchers that report confidence levels that closely align with their actual performance. Such matchers are termed calibrated [31] and they are valuable in the absence of ground truth, allowing us to correctly assess their performance by reported confidence. \n\nUncalibrated matchers can be categorized into two groups, underconfident and over-confident. The former includes matchers that tend to report confidence levels that are lower than their actual performance, underestimating their performance. The latter includes matchers that provide confidence levels that are higher than the actual performance, overestimating their performance. Identifying the group of a matcher can help improve labeling quality by adjusting the reported confidence levels. \n\nThe calibration measureis a well known metric in the field of meta-cognition [3,16]. The measure quantifies the success of a matcher in self-assessing performance, enabling us to classify matchers as under-confident, calibrated, or over-confident. Calibration is an aggregative measure, and it is essentially the difference between the mean of the reported normalized confidence and the actual performance, by some traditional measure (see Section 2.3). For example, an accuracy-based calibration for a matcher \u210e, calculated over a subset of questions  \u2032 \u2286  is computed as follows. \n\nThe mean reported normalized confidence should be calculated only over decisions that are relevant to the performance measure. For example, the precision ignores   and   . We use  ( \u2032 ) to denote the mean normalized confidence over a subset of question  \u2032 \u2286 , and indicate the group of decisions for which we compute the mean using a subscript. For example,   ,  ( \u2032 ) denotes the mean normalized confidence over   or   decisions in  \u2032 . \n\nEq. 5 presents calibration over precision. The simplicity and informativeness of the measure make it an effective tool for evaluating human matchers performance.",
            "score": 0.5131238081473956,
            "section_title": "The Calibration Measure",
            "char_start_offset": 12832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2138
                },
                {
                    "start": 2141,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 486,
                    "end": 490,
                    "matchedPaperCorpusId": "227248132"
                },
                {
                    "start": 1194,
                    "end": 1197,
                    "matchedPaperCorpusId": "2282422"
                },
                {
                    "start": 1197,
                    "end": 1200,
                    "matchedPaperCorpusId": "144034126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1826171875
        },
        {
            "corpus_id": "249679786",
            "title": "Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression",
            "text": "Abstract State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making. While recent work has focused on calibration of classifiers, there is almost no work in NLP on calibration in a regression setting. In this paper, we quantify the calibration of pre- trained language models for text regression, both intrinsically and extrinsically. We further apply uncertainty estimates to augment training data in low-resource domains. Our experiments on three regression tasks in both self-training and active-learning settings show that uncertainty estimation can be used to increase overall performance and enhance model generalization.",
            "score": 0.5119139561132567,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2293701171875
        },
        {
            "corpus_id": "273345661",
            "title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF",
            "text": "As Large Language Models (LLMs) significantly expand their functionality across a wide range of applications from complex problem solving (Wei et al., 2022;Song et al., 2023a) to science discovery (Imani et al., 2023;OpenAI, 2023), the importance of their reliability becomes increasingly critical. A key aspect of this reliability is language model calibration -the alignment between model confidence and its actual performance. LLM confidence can be assessed using two primary methods: logit-based approaches, derived from output token probability distributions, and verbalized expressions, where the model explicitly states its confidence level. In this paper, we focus on verbalized confidence, where we prompt LLMs to express a confidence score for their responses (Figure 1, Top). \n\nReinforcement Learning from Human Feedback (RLHF) has become a widely adopted technique to improve the performance and alignment of LLMs. The improvement is achieved through two primary components: reward modeling, which learns to predict human preferences from ranking datasets, and policy optimization, guided by reward models and typically implemented with Proximal Policy Optimization (PPO) (Schulman et al., 2017). However, recent studies (Kadavath et al., 2022;OpenAI, 2023) show that RLHF-trained LLMs tend to exhibit overconfidence, potentially due to sharpened output distributions. Previous research has explored various approaches to addressing LLM overconfidence. Scaling-based approaches (Guo et al., 2017;Zhang et al., 2020) adjust model logits using decoding temperature, while verbalized confidence is enhanced through prompting The vanilla model shows bias towards high confidence though the answer is incorrect. Our calibrated reward model can correctly assign a higher reward to low-confidence one for the incorrect answer. \n\nstrategies (Tian et al., 2023) and supervised fine-tuning (Lin et al., 2022) with ground truth accuracy. Recently, RLHF-based calibration methods (Xu et al., 2024;Tao et al., 2024) have been proposed. \n\nOur study investigates the underlying causes of overconfidence introduced by RLHF.",
            "score": 0.5110318388009212,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2034
                },
                {
                    "start": 2037,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 156,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 156,
                    "end": 175,
                    "matchedPaperCorpusId": "254408960"
                },
                {
                    "start": 1490,
                    "end": 1508,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1508,
                    "end": 1527,
                    "matchedPaperCorpusId": "212725167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313232421875
        },
        {
            "corpus_id": "265213190",
            "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
            "text": "LLMs With the wide application of LLMs, how to accurately quantify the predictive uncertainty has also drawn attention (Xiao et al., 2022;Lin et al., 2022;Mielke et al., 2022;Zhou et al., 2023;Huang et al., 2023;Duan et al., 2023;Chen & Mueller, 2023;Ott et al., 2018;Malinin & Gales, 2020). Semantic Uncertainty (Kuhn et al., 2022) clusters string-valued LLM outputs by synonymy for better uncertainty quantification. Lin et al. (2023) explores uncertainty quantification within the challenging black-box context, where the token generation probability is inaccessible. In this pursuit, BSDETECTOR (Chen & Mueller, 2023) combines two strategies to estimate the model's predictive uncertainty. The first approach involves sampling multiple answers from the LLM and assessing their consistency, while the second directly queries the LLM for its confidence in the generated answer. Although there have been some explorations in this direction, existing methods can only estimate the total uncertainty. In comparison, we propose a more principled framework that can both quantify the total uncertainty and decompose it into aleatoric uncertainty and epistemic uncertainty, leading to a more fine-grained understanding of LLMs. \n\nAnother line of research is model calibration for LLMs. Model calibration is the process of ensuring that the predicted probabilities or confidence scores from a machine learning model align with the true probabilities or likelihoods of events occurring (i.e., the prediction is correct). Well-calibrated model predictions help improve the reliability of uncertainty quantification. Using existing model calibration methods (Hendrycks & Gimpel, 2016;Guo et al., 2017;Ovadia et al., 2019;Riquelme et al., 2018;Desai & Durrett, 2020), prior work (Huang et al., 2022;Jiang et al., 2023;2021;Ye & Durrett, 2022) has shown that LLMs are relatively well-calibrated on factual QA and complex reasoning tasks when properly prompted.",
            "score": 0.5107505711393933,
            "section_title": "Uncertainty Quantification and Model Calibration for",
            "char_start_offset": 8092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 138,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 138,
                    "end": 155,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 251,
                    "end": 268,
                    "matchedPaperCorpusId": "4375156"
                },
                {
                    "start": 268,
                    "end": 290,
                    "matchedPaperCorpusId": "231895728"
                },
                {
                    "start": 313,
                    "end": 332,
                    "matchedPaperCorpusId": "257039062"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.150634765625
        },
        {
            "corpus_id": "266335508",
            "title": "Self-Evaluation Improves Selective Generation in Large Language Models",
            "text": "Large language models (LLMs) are often pre-trained on a vast corpus of text and then fine-tuned on supervised data to follow instructions [Devlin et al., 2018, Radford et al., 2018, Raffel et al., 2020, Adiwardana et al., 2020, Wei et al., 2021, Ouyang et al., 2022, Chung et al., 2022]. Having the ability to tell when a language model's output is trustworthy is important for safe deployment of language models. For example, the model's trustworthiness can be used as signal to selectively generate answers based on how confident the LLM is in the quality of its output. \n\nPrior research has demonstrated that the distance to the training distribution in the embedding space predicts output quality for conditional generative models [Ren et al., 2023b]. Extending this work to large language models is challenging because their training distribution is too large to estimate and extracting embeddings from well-integrated LLM systems requires significant engineering effort. \n\nAlternatively, a straightforward approach to estimating a language model's confidence in its output is to calculate the sequence probability or the length-normalized sequence probabilities [Adiwardana et al., 2020]. However, studies have shown that language models' sequence probabilities on openended generations do not reliably rank-order their outputs by quality [Liu et al., 2022, Ren et al., 2023b]. Human feedback can be used to fine-tune language models to better align with human-judged quality, such as with Reinforcement Learning from Human Feedback (RLHF) [Stiennon et al., 2020], SLiC-HF [Zhao et al., 2023] and DPO [Rafailov et al., 2023], resulting in better quality-calibrated models. \n\nSince human feedback data is expensive to obtain, we explore leveraging the self-evaluation ability of LLMs to improve quality-calibration. Despite the poor calibration on sequence-level likelihood, recent work has shown that LLM token-level probability can be quite well-calibrated on choosing the arXiv:2312.09300v1",
            "score": 0.5102152737140622,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 201,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1546,
                    "end": 1569,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1993408203125
        },
        {
            "corpus_id": "253510101",
            "title": "Calibrated Interpretation: Confidence Estimation in Semantic Parsing",
            "text": "Given the utility of calibrated models in decisionmaking, a large body of research has focused on describing the calibration characteristics of different architectures and models, with some work finding neural networks to be relatively well-calibrated (Niculescu-Mizil and Caruana, 2005;Minderer et al., 2021;Carrell et al., 2022) -including neural encoders pre-trained on text data (Desai and Durrett, 2020) -and other research indicating they are not (Guo et al., 2017;Wang et al., 2020;Si et al., 2022). These mixed results preclude drawing general conclusions about neural models' calibration, and motivate studies like ours documenting the calibration characteristics of standard models. \n\nPast work has examined a variety of classification problems, often focusing on binary or multiclass classification (Naeini et al., 2015;Guo et al., 2017;Minderer et al., 2021;Khojah et al., 2022). Some papers have addressed sequential NLP tasks: e.g. Jagannatha and Yu (2020) address calibration in structured prediction tasks. More related to our sequence generation setting, Kumar and Sarawagi (2019) and Wang et al. (2020) examine calibration in machine translation, both finding models to be over-confident. Measuring calibration in translation tasks is limited by the metrics used, which are noisy proxies for accuracy and have well-documented limitations (Callison-Burch et al., 2006;Mathur et al., 2020). In semantic parsing specifically, past work has focused on improving confidence estimation for certain parsers. Dong et al. (2018) propose a confidence estimation method based on model and input features. Similarly, Chen et al. (2022) introduce an additional confidence-estimation model for semantic parses. In our work, we extract confidence scores from the same model used for parsing, and focus on analyzing calibration rather than improving it.",
            "score": 0.5101122198941723,
            "section_title": "Calibration in NLP",
            "char_start_offset": 5469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 287,
                    "matchedPaperCorpusId": "207158152"
                },
                {
                    "start": 287,
                    "end": 309,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 383,
                    "end": 408,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 471,
                    "end": 489,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 489,
                    "end": 505,
                    "matchedPaperCorpusId": "253098276"
                },
                {
                    "start": 810,
                    "end": 831,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 831,
                    "end": 848,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 848,
                    "end": 870,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 870,
                    "end": 890,
                    "matchedPaperCorpusId": "252847587"
                },
                {
                    "start": 946,
                    "end": 970,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 1102,
                    "end": 1120,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 1519,
                    "end": 1537,
                    "matchedPaperCorpusId": "13686145"
                },
                {
                    "start": 1623,
                    "end": 1641,
                    "matchedPaperCorpusId": "253098276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09515380859375
        },
        {
            "corpus_id": "266051956",
            "title": "A Study on the Calibration of In-context Learning",
            "text": "Setting. Given a pre-trained language model P \u03b8 (w t |w <t ), we seek to adapt it using the prompt w 0 = [x 1 , y 1 , x 2 , y 2 , . . . , x n\u22121 , y n\u22121 , x n ] to generate a predicted answer y n = P \u03b8 (w 0 ). In the context of reasoning, a popular approach is to hand-craft some explanations/rationales/chainof-thoughts e in the prompt w 0 = [x 1 , e 1 , y 1 , x 2 , e 2 , y 2 , . . . , x n\u22121 , e n\u22121 , y n\u22121 , x n ] to generate explanation e n and answer y n , for the test sample: en w 1 , w 2 , . . . , w k , y n = P \u03b8 (w 0 ). We extract answer token probabilities of LMs, e.g. for binary classification tasks, we filter and extract probabilities P (\"Yes\") and P (\"No\"), based on which we calculate the following statistics for studying the confidence and calibration of LMs: Confidence and feature norm. We record the maximum probability of the answer token as its confidence Conf = P \u03b8 (y n |w <n ) and the feature norm z n as the intermediate hidden state before the linear prediction layer. Entropy rate. \n\nWe denote the entropy of a token w t at position t as H(w t |w <t ) = \u2212E wt\u223cP \u03b8 (\u2022|w<t) [log P \u03b8 (w t |w <t )]. We typically measure it based on the answer token via setting w t = y n . Note that auto-regressive LMs are trained via maximizing the negative log-likelihood objective L = \u2212E t [log P \u03b8 (w t |w <t )] on massive corpora. Empirical estimate of the expected calibration error (ECE) In the realm of probabilistic classifiers, calibration is a crucial concept.",
            "score": 0.5101122198941723,
            "section_title": "Background",
            "char_start_offset": 7257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1482
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1622314453125
        },
        {
            "corpus_id": "268041551",
            "title": "Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension",
            "text": "We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness ohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs.",
            "score": 0.5101122198941723,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045013427734375
        },
        {
            "corpus_id": "273345593",
            "title": "On Calibration of LLM-based Guard Models for Reliable Content Moderation",
            "text": "Confidence calibration is a critical aspect in developing reliable and trustworthy language models (Nguyen & O'Connor, 2015;Guo et al., 2017;Minderer et al., 2021). In the context of LLMs, prior research has explored calibration in question-answering tasks (Jiang et al., 2021) and has empirically examined calibration during the pre-training and alignment stages (Chen et al., 2022;Zhu et al., 2023). Studies such as Lin et al. (2022); Mielke et al. (2022); Xiong et al. (2023) have investigated uncertainty estimation through verbalized confidence, and Kadavath et al. (2022) demonstrated improved calibration of larger models when handling multiple choice and true/false questions given appropriate formats. Another line of research addresses the calibration of biases stemming from in-context samples, instruction templates, sample ordering, and label distribution (Zhao et al., 2021;Zhou et al., 2023b;Liu & Wang, 2023;Fei et al., 2023;Abbas et al., 2024). These bias calibration techniques indirectly influence the prediction confidence by altering the linear decision boundary (Zhou et al., 2023a), yet they are not designed for explicit confidence calibration. In contrast, our work specifically addresses the challenge of confidence calibration in instruction-tuned guard models for content moderation tasks.",
            "score": 0.5101122198941723,
            "section_title": "Calibration of LLMs.",
            "char_start_offset": 7004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1317
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 141,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 141,
                    "end": 163,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 257,
                    "end": 277,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 437,
                    "end": 457,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 869,
                    "end": 888,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 941,
                    "end": 960,
                    "matchedPaperCorpusId": "267094982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1104736328125
        },
        {
            "corpus_id": "260063200",
            "title": "Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement",
            "text": "Confidence estimation is the counterpart to uncertainty estimation, which quantifies a model's lack of confidence in its predictions. Previous work has shown that modeling uncertainty improves task performance on neural machine translation (Wang et al., 2019), document quality prediction (Shen et al., 2019), sentiment analysis, named entity recognition, and language modeling using convolutional and recurrent neural network models (?). \n\nWork on model confidence estimation for NLP has included a range of models-Naive Bayes and logistic regression (Nguyen and O'Connor, 2015), neural networks (Jagannatha and yu, 2020)-and tasks-structured prediction (Jagannatha and yu, 2020), natural language understanding (Desai and Durrett, 2020;Kamath et al., 2020;Kong et al., 2020), and neural machine translation systems (Kumar and Sarawagi, 2019). Kamath et al. (2020) found that QA models are overconfident in out-ofdomain tasks when asked to answer as many questions as possible while maintaining high accuracy. More recently, this work has turned to language models, and researchers have struggled to obtain sensible confidence measures. Jiang et al. (2021) found that language models such as T5, BART, and GPT-2 did not produce well-calibrated scores based on generation probabilities for QA tasks. \n\nA variety of methods have been proposed to obtain calibrated confidence measures from LLMs. Jiang et al. (2021) experiment with several calibration methods, including fine-tuning, post hoc probability modification, or adjustment of the predicted outputs or inputs. Kong et al. (2020) use a regularized fine-tuning method to obtain better calibration for both in-distribution and out-of-distribution data. Xiao et al. (2022) focus on the design choices for pre-trained language model-based prediction pipelines, suggesting that the calibration of the model depends on the choice of the fine-tuning loss function. Desai and Durrett (2020) demonstrated a more calibrated model trained with label smoothing.",
            "score": 0.5101122198941723,
            "section_title": "Confidence Estimation",
            "char_start_offset": 7202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "202539965"
                },
                {
                    "start": 289,
                    "end": 308,
                    "matchedPaperCorpusId": "208331285"
                },
                {
                    "start": 552,
                    "end": 579,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 655,
                    "end": 680,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 738,
                    "end": 758,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 845,
                    "end": 865,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 1138,
                    "end": 1157,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1394,
                    "end": 1413,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.094482421875
        },
        {
            "corpus_id": "256846523",
            "title": "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers",
            "text": "While pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL\u2019s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs.",
            "score": 0.5099913362655635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1942138671875
        },
        {
            "corpus_id": "268253470",
            "title": "Large language models surpass human experts in predicting neuroscience results",
            "text": "We considered whether large language models (LLMs) can forecast the outcome of neuro- Mistral-7B (instruct) \n\nFigure 4: Accuracy and confidence are calibrated for human experts and large language models (LLMs). When human experts and LLMs are confident in their BrainBench judgments, they are more likely to be correct. Confidence ratings were sorted and placed in equally-sized bins with the mean accuracy for items in that bin plotted. The positive slope of the black regression lines for human experts and all LLMs indicates that confidence is well calibrated (i.e., higher confidence corresponds to higher accuracy). Calibration is beneficial for humanmachine teams. generative model that captured the patterns underlying neuroscience. To evaluate this possibility, we constructed a new forward-looking (Fig. 2) benchmark, BrainBench. \n\nBrainBench assesses a test taker's ability to select which of two versions of a neuroscience abstract contains the actual results of the study (see Fig. 2). We found that LLMs outperform human experts on BrainBench by a considerable margin (see Fig. 3A) across all neuroscience subfields we consider (Fig. 3B). Moreover, the LLMs knew when their predictions were likely to be right or wrong (Fig. 4). LLMs' superior performance arose from their ability to integrate information throughout the abstract, such as text pertaining to the method and study design. \n\nWhen access to such information was removed, LLM performance drastically declined (Fig.",
            "score": 0.5098651220940889,
            "section_title": "Discussion",
            "char_start_offset": 12988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 110,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1489
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0567626953125
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Pre-trained Large Language Models (LLMs) like BERT, RoBERTA, T5, BART (Devlin et al., 2019;Liu et al., 2019;Wolf et al., 2020;Raffel et al., 2020;Lewis et al., 2019a), and their multilingual counterparts like mT5, mBART, XLM (Xue et al., 2021;Liu et al., 2020;Conneau et al., 2019), have * The first two authors contributed equally to this paper. This indicates that despite high zero-shot cross-lingual accuracy, zero-shot cross-lingual calibration is not good for LLMs, unless dedicated calibration strategies, like TS, are used to improve them. \n\ngreatly advanced Natural Language Understanding. The multilingual LLMs (MLLMs) are able to transfer knowledge acquired from English to other languages, leading to impressive cross-lingual performances on various downstream tasks in the zeroshot and few-shot settings, i.e., for languages not seen during fine-tuning (Wu and Dredze, 2020;Xue et al., 2021). Question Answering (QA) is a common task for understanding how well machines understand human language, and more importantly, it comes in a variety of formats such as multiple choice, span selection, or free-form answer generation. Several NLP/multimodal tasks can be cast as QA, highlighting the universality of the QA format. \n\nMultilingual QA as a task is becoming increasingly important with the globally widespread deployment and democratization of AI systems (Loginova et al., 2021). Despite the amazing progress of LLMs across several benchmarks, they unfortunately suffer from sometimes generating incorrect answers, with very high confidence. This can have severe consequences for safety-critical applications such as healthcare, autonomous driving, or finances where mistakes can be very costly. With the increasing application of LLMs to such tasks, it is crucial to understand whether the predictions are reliable, and when the models are unsure of the answer. Confidence calibration is one such reliability metric that measures whether the model's prediction probability estimates are aligned with the actual probability of the answer being correct.",
            "score": 0.5097259127624267,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 91,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 108,
                    "end": 126,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 126,
                    "end": 146,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 866,
                    "end": 887,
                    "matchedPaperCorpusId": "218684346"
                },
                {
                    "start": 1371,
                    "end": 1394,
                    "matchedPaperCorpusId": "212848801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09271240234375
        },
        {
            "corpus_id": "265150666",
            "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
            "text": "Here, we discuss a number of recommendations and future work for the greater NLP community.\n\nCan Models Generate Expressions of Uncertainty?Having studied the impact of uncertainty as part of the prompt, a natural follow-up question is: how does model performance change when models learn to emit their own expressions of uncertainty?In preliminary experiments, we find it is challenging to calibrate models to generate epistemic markers.The same methods which work for numerical calibration do not transfer well to verbal calibration (Lin et al., 2022).However, we find some evidence that expressions of uncertainty are slightly better calibrated when compared with expressions of certainty.See A.1.\n\nNavigating Idiomatic Language Humans use language that contains expressions of certainty when they are, in fact, not certain, and models appear to be mimicking this behavior.However, our QA models are still unable to recognize the meaning of those markers in our QA setup, raising concerns for how they would respond in downstream applications.Future work must seek to enable LMs to accurately interpret when expressions are meant idiomatically versus literally.",
            "score": 0.5096705757023654,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 28245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 93,
                    "end": 140
                },
                {
                    "start": 140,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 700
                },
                {
                    "start": 702,
                    "end": 876
                },
                {
                    "start": 876,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1164
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 553,
                    "matchedPaperCorpusId": "249191391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09124755859375
        },
        {
            "corpus_id": "259095925",
            "title": "Uncertainty in Natural Language Processing: Sources, Quantification, and Applications",
            "text": "There are various techniques for evaluating the performance and credibility of PLMs, including self-evaluation techniques, calibration scores, and other methods [26,149,150,151]. These techniques can help to ensure that NLP systems are trustworthy and reliable, and can be used with confidence by users in a range of different applications. \n\nLast but not least, an intuitive question is how to express uncertainty to improve model evaluation and obtain valuable information? This is a natural challenge because language expression itself is inherently uncertain. Quantifying the uncertainty expressed by the model, making effective evaluations, and reducing the interaction bias caused by expression are all important considerations. We provide a more detailed discussion of this challenge in Sec 5.3.",
            "score": 0.509149896807465,
            "section_title": "Reliability and Trustworthy Assessment",
            "char_start_offset": 76002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 802
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05108642578125
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "Our work deals with calibration of confidence predictions of classifiers trained on top of pre-trained multilingual models. Having well calibrated predictions is imperative for building robust NLP systems especially when using them for security sensitive applications like Hate Speech Detection to flag social media accounts (Cuthbertson, 2021), decision making in law enforcement and fraud detection (Metz and Satariano, 2020), where extreme actions should only be taken when we are confident about the system's prediction. However, the predicted confidences mean essentially nothing if the model is miscalibrated, posing major risks in using such models. Through our work we highlight that the commonly used multilingual models are highly miscalibrated when used in a zero-shot setting for low resource and typologically diverse languages. Additionally, we manage to substantially improve the calibration of these models across languages, addressing this linguistic disparity and boosting the reliability of such models.",
            "score": 0.5086452720087022,
            "section_title": "Ethics Statement",
            "char_start_offset": 14747,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1022
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056854248046875
        },
        {
            "corpus_id": "267617073",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "text": "Moreover, we also view the correct-Figure 2: Overview of our calibration framework. We prompt an LLM to produce an answer to a specific question, assess the answer's correctness distribution using an evaluator (task-specific metric/GPT-4 metric/human metric), and determine the model's confidence distribution through self-evaluation or self-consistency approaches. Finally, we calculate the calibration score by comparing the correctness and confidence distributions against our predefined metrics. \n\nness of an LLM's response as a distribution across scores between [0, 1] to capture graded and subjective assessments of long-form generations quality. This subjectivity arises from the multifaceted nature of evaluating long-form outputs, where factors like factuality, coherence, clarity, and comprehensiveness each play a role, potentially introducing variability in judgment (Bakker et al., 2022). We can then measure both the classical notion of calibration error, averaged across different correctness levels, as well as new notions of alignment between the correctness and confidence distributions and their utility in selective prediction ( \u00a7 3.4). Figure 2 shows an overview of our framework, which consists of three modular components: estimating the target correctness distributions, eliciting confidence distributions from LLMs, and measuring calibration between these distributions. \n\nOur unified framework offers three key advantages. 1. Generalizability: Our framework applies to both long-form and short-form generation tasks by representing correctness and confidence as distributions, regardless of whether the correctness of task is binary, continuous, subjective, or objective. 2. Flexibility: The framework is evaluation-metric agnostic, allowing the integration of any metric or confidence elicitation method, and can adapt as evaluation methods evolve. 3. Interpretability: It provides a nuanced view of uncertainty, enabling decision-makers to assess confidence across multiple correctness levels, fostering greater transparency and trust in the model's outputs.",
            "score": 0.5085058057376933,
            "section_title": "Introduction",
            "char_start_offset": 1938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18994140625
        },
        {
            "corpus_id": "265213392",
            "title": "Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation",
            "text": "Confidence Estimation for LLMs. Confidence estimation for LLMs has been studied in several related works. Kadavath et al. (2022) show that Claude's model probabilities are well-calibrated on multiple/choice and True/False questions. Zhou et al. (2023) study the effect of introducing expressions of uncertainty into prompts, on model accuracy. Our work differs from these since we focus on confidence elicitation for models which don't provide log probabilities. Concurrent work (Xiong et al., 2023) studies calibration and selective classification of linguistic confidence scores generated by LLMs. While this work also elicits prompted confidences, they focus on self-consistency (SC) based methods which are expensive because they require prompting GPT-4 several times. Our proposed Surrogate and Mixture of models methods are less expensive, since model probabilities from smaller models (Llama 2) are used to improve the confidence estimates of larger models (GPT-4). We also show performance improvements over their best method. (Lin et al., 2022) examine fine-tuning language models to improve confidence estimation, which we do not have access to. Figure 5: Embeddings of Incorrect Questions for GPT-4 and Surrogate Models Plots of the embeddings of questions GPT-4 and two surrogate models (Llama 2 70B and Llama 2 13B) answer incorrectly on two representative datasets -TruthfulQA and College Chemistry. Questions only GPT-4 answers incorrectly are in blue, questions GPT-4 and the surrogate answer incorrectly are in black, and questions only the surrogate answers incorrectly are in green. There are more questions that both GPT-4 and Llama 2 70B answer incorrectly and more semantic similarity in their incorrect questions. This indicates that Llama 2 70B and GPT-4 struggle with semantically related concepts and that the 70B model may more closely estimate GPT-4's uncertainty than the 13B model. \n\nSelective Classification and OOD Detection.",
            "score": 0.5083801639230352,
            "section_title": "RELATED WORK",
            "char_start_offset": 23814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 1957
                }
            ],
            "ref_mentions": [
                {
                    "start": 1035,
                    "end": 1053,
                    "matchedPaperCorpusId": "249191391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055511474609375
        },
        {
            "corpus_id": "277112828",
            "title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence",
            "text": "We compute the calibrated probability p = c(f ) associated with each prediction in the held-out calibration data, and choose a mapping into discrete confidence bins. Several options are possible for this binning function b, including adaptive schemes as well as uniform schemes, the number of bins B, and so on. In this paper, we focus on a simple fixed-width scheme with 5 bins. Let \u0176 denote the set of all predictions on X cal , and, if the model was previously fine-tuned on a supervised training set X train , we include predictions on X train . We deterministically transform each prediction and calibrated confidence into a training example for a round of supervised fine-tuning by verbalizing the corresponding bin in the answer. For example, the fifth of five bins may correspond to \"very high confidence.\" The token sequences chosen to encode each bin are arbitrary; for easy interpretability, we use short descriptors in this paper, namely \"very low confidence,\" \"low confidence,\" \"medium confidence,\" \"high confidence,\" and \"very high confidence.\" \n\nIn our scheme, we simply append the verbalized confidence to all answers. For instance, if the model generates 900 correct answers and 100 in-correct answers, there are two available data points that could potentially be added to the dataset: <correct answer> (with very high confidence) \n\n<incorrect answer> (with very low confidence) \n\nWhile correct answers should always be added as training data, appending the confidence scores to incorrect answers may improve the model's ability to correctly verbalize its own confidence. However, it may also decrease the accuracy of the QA model. We introduce a hyperparameter to control the number of incorrect answers added to the training data. In \u00a76.2, we further investigate the impact of this hyperparameter. \n\nStarting from the sampled model, we perform supervised fine-tuning on these self-annotated targets with verbalized confidences to estimate a second model capable of verbalizing its confidence. If training an instruction-tuned model, we append an additional instruction such as \"Additionally state how confident you are in your answer.\" to the preexisting instruction6 .",
            "score": 0.5082621889213232,
            "section_title": "Self-annotation and fine-tuning",
            "char_start_offset": 13610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1817
                },
                {
                    "start": 1820,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2189
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.079345703125
        },
        {
            "corpus_id": "250451161",
            "title": "Language Models (Mostly) Know What They Know",
            "text": "We would eventually like to train AI systems that are honest, which requires that these systems accurately and faithfully evaluate their level of confidence in their own knowledge and reasoning. So AI systems must be able to recognize what they do and do not know, as a prerequisite. In this work, we study the extent to which Language Models (LMs) possess this ability and how it can be elicited and imparted. \n\nAs a starting point, we examine calibration: do the probabilistic predictions from language models match up with frequencies of occurrence? Language models can produce well-calibrated predictions for token probabilities on-distribution [Guo et al., 2017]. We show that large language models are also well-calibrated on a diverse array of multiple choice questions, as long as the questions are formatted appropriately. In particular, calibration improves with model size and few-shot prompting. \n\nGood calibration opens up the possibility for using models to evaluate the accuracy of their own outputs (\"self-evaluation\"). For example, given any open-ended query, we can sample an answer from the model and then have the model evaluate P(True), the probability that its answer is correct. We may expect selfevaluation to be challenging, because the model may be overconfident that its own samples1 are correct. Our self-evaluation procedure nevertheless distinguishes correct and incorrect samples, as summarized in Figure 1. Furthermore, as model size and capabilities increase, models improve at self-evaluation, which suggests that verification improves faster than generation quality in this context. \n\nWe also show that self-evaluation can be improved if we provide a model with many of its own samples, before asking it to evaluate any single sample. That is, 'brainstorming other possibilities' helps large models to evaluate the validity of a given answer option. \n\nThese techniques address a question about the world, as they ask models to evaluate \"according to accepted truth in the wider world (i.e. according to humans), is a particular answer to a question correct?\" In the case of self-evaluation, the proposed answer was provided by the model, but its validity is nevertheless an external fact. \n\nBut we are also interested in having language models attempt to directly evaluate their own state of knowledge.",
            "score": 0.5080588183908076,
            "section_title": "Introduction",
            "char_start_offset": 653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2223
                },
                {
                    "start": 2226,
                    "end": 2337
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371826171875
        },
        {
            "corpus_id": "277112828",
            "title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence",
            "text": "Figure 1: An overview of how our method might be applied to a simple QA example. In Monte Carlo sampling with semantic normalization, we sample repeatedly from our language model, and use a normalization function to consolidate answers with the same semantic meaning. By consolidating the counts, we obtain a Monte Carlo estimate of each answer's probability. In post-hoc calibration, we pass this estimate through a learned post-hoc calibration function to better align it with its likelihood of correctness. Finally, in self-annotation and fine-tuning, we translate these probabilities to verbalized signifiers and fine-tune a model to output verbalized confidences in addition to the answer. \n\ntically equivalent but lexically different predictions to be assigned the same confidence, and a single generation with multiple claims can each be assigned different confidences. Additionally, uncertainty distillation is computationally inexpensive at inference time, generating only a handful of additional tokens. And compared to methods such as P(IK) (Farquhar et al., 2024), we do not require a separate uncertainty network; our approach uses standard supervised fine-tuning recipes for LLMs. \n\nRather than specializing to a specific model or task, uncertainty distillation involves selfannotation of any desired QA dataset with the base model's calibrated uncertainties, which are then used to fine-tune that model to produce verbalized confidences. At a high level (Figure 1), our approach consists of three steps: (1) obtaining semantic uncertainty estimates from the model on held-out calibration data; (2) post-hoc calibrating these into meaningful probabilities; and (3) teaching the model via supervised fine-tuning to output verbalized confidences along with its predictions.",
            "score": 0.5075239785011578,
            "section_title": "\ud83e\udd16",
            "char_start_offset": 4064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1785
                }
            ],
            "ref_mentions": [
                {
                    "start": 1052,
                    "end": 1075,
                    "matchedPaperCorpusId": "270615909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07879638671875
        },
        {
            "corpus_id": "273403990",
            "title": "Learning to Route LLMs with Confidence Tokens",
            "text": "Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-REF, a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.",
            "score": 0.5073732779235416,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2454833984375
        },
        {
            "corpus_id": "252847587",
            "title": "Evaluating N-best Calibration of Natural Language Understanding for Dialogue Systems",
            "text": "We took established calibration measurement approaches and applied them to intent classification of publicly available NLUs. We also extended the chosen measurements with a rank-level analysis. Our findings show that the best calibrated NLU is Rasa-Sklearn and the least calibrated NLU is Snips, while Watson takes the lead as the best performing NLU and Rasa-Sklearn as the worst performing NLU. The results indicate a trade-off between confidence calibration and performance. We also showed differences in degree of calibration across ranks and discussed their implication  In future work, it would be interesting to extend the investigation with qualitative analyses of how differences in confidence estimation play out in concrete examples. It could also be valuable to find a better way of assessing how well the NLUs capture genuine ambiguity -something which is difficult with a dataset that assumes a single correct intent for a given utterance.",
            "score": 0.5071046262984146,
            "section_title": "Conclusions and future work",
            "char_start_offset": 21660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 953
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15087890625
        },
        {
            "corpus_id": "267522811",
            "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
            "text": "In this section, we present a simple yet effective solution to reconfidence LLMs. The core idea is to calibrate each sub-group separately. \n\nStandard Calibration Following standard calibration procedures, we train a regressor, commonly known as a calibrator, to conduct the calibration of a model (Niculescu-Mizil and Caruana, 2005) This calibrator works by mapping the model's output to a refined probability within the interval [0, 1], with the aim of aligning closely with the true probability. Concretely, we train an isotonic regressor using our constructed training and validation sets for calibration purposes (Zadrozny and Elkan, 2002). Subsequently, we apply this trained regressor to calibrate the confidence scores on the test set. \n\nReconfidencing The standard calibration approaches are marginal: they control average error on confidence and overlook the nuances of subgroups, where confidence errors can be especially marked. Inspired by this, we propose a more refined method to calibrate LLMs from the sub-group perspective. Adapting Perez-Lebel et al. ( 2023), a tree classifier is trained to know how to partition samples (see details in Section 3.4). We employ a loss function derived from the squared loss for the Brier score on labels (Y ) to optimize the predicted confidence scores. This decision tree algorithm partitions the data into sub-groups that minimize the specified loss. The tree's input consists of embeddings from the top layer of a LLM for a given query, which can effectively cluster samples exhibiting similar levels of over-confidence or under-confidence. This, in contrast to user-defined sub-groups, does not need background knowledge and thus applies to queries that are not matched to the knowledge base. Following this step, a distinct isotonic regressor is trained for each identified subgroup. The final step is to apply this refined method to reconfidence the test set The reconfidencing can effectively reduce the grouping loss thus yielding improved calibration results. \n\nTo validate our proposed solution, we conduct a comparative analysis of calibration performance between the standard calibration and our reconfidencing approach.",
            "score": 0.506928294037745,
            "section_title": "Reconfidencing LLMs",
            "char_start_offset": 22516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2020
                },
                {
                    "start": 2023,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 617,
                    "end": 643,
                    "matchedPaperCorpusId": "3349576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146484375
        },
        {
            "corpus_id": "253224378",
            "title": "Stop Measuring Calibration When Humans Disagree",
            "text": "Neural text classifiers are becoming more powerful but increasingly difficult to interpret (Rogers et al., 2020). In response, the demand for transparency and trust in their predictions is growing (Yin et al., 2019;Bansal et al., 2019;Bianchi and Hovy, 2021). One step towards understanding when to trust predictions is to evaluate whether models know when they do not know-i.e., whether predictive probabilities are a good indication of how likely a prediction is to be correct-known as calibration. This is crucial in user-facing and high-stake applications. \n\nCalibration, and particularly the Expected Calibration Error (ECE;Naeini et al., 2015;Guo et al., 2017), is widely studied in Machine Learning and Computer Vision (Mena et al., 2021), and is gaining increased attention in Natural Language Processing (NLP; Desai and Durrett, 2020;Kong et al., 2020;Jiang et al., 2021;Dan and Roth, 2021). \n\nAn important implicit assumption in the widely used definition of perfect calibration proposed by Guo et al. (2017) is that predictions are either right or wrong-in other words, that the true class distribution, i.e., human judgement distribution, is deterministic (one-hot). However, for many problems, while categories exist, their boundaries are fluid: there exists inherent disagreement about labels. This means that gold labels are at best an idealization-as irreconcilable disagreement is abundant (Plank et al., 2014;Aroyo and Welty, 2015;Jamison and Gurevych, 2015;Palomaki et al., 2018;Pavlick and Kwiatkowski, 2019). Evidence for this can be found in various tasks, including those which involve linguistic and subjective judgements (Akhtar et al., 2020;Basile et al., 2021). Surprisingly, however, while limitations of calibration are studied ( \u00a72), this fundamental assumption is ignored. \n\nIn this work, we show that popular calibration metrics-such as ECE-are not applicable to data with inherent human disagreement ( \u00a73).",
            "score": 0.5065663686847076,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 560
                },
                {
                    "start": 563,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1803
                },
                {
                    "start": 1806,
                    "end": 1939
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 215,
                    "matchedPaperCorpusId": "109927933"
                },
                {
                    "start": 215,
                    "end": 235,
                    "matchedPaperCorpusId": "201685074"
                },
                {
                    "start": 235,
                    "end": 258,
                    "matchedPaperCorpusId": "236477333"
                },
                {
                    "start": 624,
                    "end": 629,
                    "matchedPaperCorpusId": "201685074"
                },
                {
                    "start": 629,
                    "end": 649,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 649,
                    "end": 666,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 726,
                    "end": 745,
                    "matchedPaperCorpusId": "241258024"
                },
                {
                    "start": 819,
                    "end": 843,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 861,
                    "end": 880,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 880,
                    "end": 899,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 1001,
                    "end": 1018,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1407,
                    "end": 1427,
                    "matchedPaperCorpusId": "12227512"
                },
                {
                    "start": 1427,
                    "end": 1449,
                    "matchedPaperCorpusId": "6134326"
                },
                {
                    "start": 1449,
                    "end": 1476,
                    "matchedPaperCorpusId": "9314689"
                },
                {
                    "start": 1476,
                    "end": 1498,
                    "matchedPaperCorpusId": "56487787"
                },
                {
                    "start": 1498,
                    "end": 1528,
                    "matchedPaperCorpusId": "207933446"
                },
                {
                    "start": 1646,
                    "end": 1667,
                    "matchedPaperCorpusId": "225111848"
                },
                {
                    "start": 1667,
                    "end": 1687,
                    "matchedPaperCorpusId": "236486317"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0129241943359375
        },
        {
            "corpus_id": "265213392",
            "title": "Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation",
            "text": "Since fake-fewshot examples do including answer choices (D, A in the example) and specific confidence scores (0.4, 0.7), we experimented with variations of the prompt modifying the exact answers and confidences included in the example and found the change in generated confidences to be minimal -indicating that models were able to use these examples to learn the format of the task without over-conditioning on the specific answers and confidences provided.: We compute correlations and covariances between the answer correctness (set of binary scores indicating if a model answered a question correctly or not) for every pair of main model and potential surrogate model. We find that in general if a surrogate model S has a high degree of correlation in answer correctness with a main model M , then S is likely to be a good surrogate for M . For example, GPT-4 has a higher correlation with Llama 2 Base, Llama 2 Chat, and text-davinci-003 than it does with Llama 2 13B indicating that those models can be better surrogates for GPT-4 than Llama 2 13B. Similar trends also hold for covariances. \n\nA.9 CALIBRATION OF MIXTURE OF MODELS Figure 14: Calibration of GPT-4 with Linguistic Confidence and Mixture of Models In the first row we see the calibration of GPT-4 on MedQA, Professional Law, and US Foreign Policy when using linguistic confidences. In the second row, we see GPT-4's calibration using our Mixture of Models confidence method. A perfectly calibrated model would have all bars aligned with the red line (average confidence in each bucket is exactly equal to the average accuracy). We can see that calibration improves demonstrably, when using Mixture of Models.",
            "score": 0.5063984240954741,
            "section_title": "Preprint",
            "char_start_offset": 31755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1677
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0631103515625
        },
        {
            "corpus_id": "273163200",
            "title": "On Uncertainty In Natural Language Processing",
            "text": "The last decade in deep learning has brought on increasingly capable systems that are deployed on a wide variety of applications. In natural language processing, the field has been transformed by a number of breakthroughs including large language models, which are used in increasingly many user-facing applications. In order to reap the benefits of this technology and reduce potential harms, it is important to quantify the reliability of model predictions and the uncertainties that shroud their development. This thesis studies how uncertainty in natural language processing can be characterized from a linguistic, statistical and neural perspective, and how it can be reduced and quantified through the design of the experimental pipeline. We further explore uncertainty quantification in modeling by theoretically and empirically investigating the effect of inductive model biases in text classification tasks. The corresponding experiments include data for three different languages (Danish, English and Finnish) and tasks as well as a large set of different uncertainty quantification approaches. Additionally, we propose a method for calibrated sampling in natural language generation based on non-exchangeable conformal prediction, which provides tighter token sets with better coverage of the actual continuation. Lastly, we develop an approach to quantify confidence in large black-box language models using auxiliary predictors, where the confidence is predicted from the input to and generated output text of the target model alone.",
            "score": 0.5057072760402386,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0104522705078125
        },
        {
            "corpus_id": "265213190",
            "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
            "text": "Using existing model calibration methods (Hendrycks & Gimpel, 2016;Guo et al., 2017;Ovadia et al., 2019;Riquelme et al., 2018;Desai & Durrett, 2020), prior work (Huang et al., 2022;Jiang et al., 2023;2021;Ye & Durrett, 2022) has shown that LLMs are relatively well-calibrated on factual QA and complex reasoning tasks when properly prompted. Specifically, Kadavath et al. (2022); Tian et al. (2023) estimate the prediction confidence of LLMs by prompting LLMs to output their confidence of their answers. For complex reasoning tasks, LLMs may output both the reasoning chains and the final answer. To estimate the confidence score, previous approaches (Huang et al., 2022) sample multiple outputs for the input question and use the answer frequency to indicate the confidence. Researchers further ensemble multiple prompts for better calibration performance (Jiang et al., 2023). Our uncertainty quantification is based on the well-calibrated predictions of LLMs, which lead to a more precise and accurate quantification result. \n\nModeling Ambiguity with language models Ambiguity is a longstanding issue in the NLP domain, extensively explored in tasks such as syntactic and semantic parsing (Koller et al., 2008), open-domain questionanswering (Min et al., 2020;Cole et al., 2023), conversational question-answering (Guo et al., 2021) and natural language inference (Liu et al., 2023). Prior work, such as AmbigQA (Min et al., 2020) and AmbigEnt (Liu et al., 2023), have identified the widespread ambiguities and established benchmarks with ambiguous inputs. These studies have demonstrated that existing language models lack the capability to effectively recognize and manage ambiguities. Our work models the ambiguity from the perspective of uncertainty quantification, where a high aleatoric uncertainty can indicate the potential existence of input ambiguity. By decomposing the aleatoric uncertainty, we show that it is possible to enhance the ambiguity detection performance of existing LLMs.",
            "score": 0.5054282304016351,
            "section_title": "Uncertainty Quantification and Model Calibration for",
            "char_start_offset": 9701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 84,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 84,
                    "end": 104,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 104,
                    "end": 126,
                    "matchedPaperCorpusId": "3502463"
                },
                {
                    "start": 126,
                    "end": 148,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1193,
                    "end": 1214,
                    "matchedPaperCorpusId": "2629900"
                },
                {
                    "start": 1264,
                    "end": 1282,
                    "matchedPaperCorpusId": "258866001"
                },
                {
                    "start": 1318,
                    "end": 1336,
                    "matchedPaperCorpusId": "247296568"
                },
                {
                    "start": 1368,
                    "end": 1386,
                    "matchedPaperCorpusId": "258352700"
                },
                {
                    "start": 1448,
                    "end": 1466,
                    "matchedPaperCorpusId": "258352700"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1390380859375
        },
        {
            "corpus_id": "265351565",
            "title": "On the Calibration of Large Language Models and Alignment",
            "text": "Aligned Large Language Models are large language models that are specially trained to follow human's intents or instructions. Large language models are proved to have the ability of completing some downstream tasks without any gradient updating (Brown et al., 2020). To better make use of such ability, many researches have found that instruction following models can be constructed by fine-tuning models with instruction-response pairs, which is called instruction tuning (Weller et al., 2020;Mishra et al., 2022;Sanh et al., 2022;Wei et al., 2022a;Xu et al., 2023b). While these models can understand human instructions and make reasonable responses, they often produce unexpected results like lies, made-up facts, biased or toxic texts and so on. To better align models with human intents, reinforcement learning with human feedback is introduced to the training of large language models (Ouyang et al., 2022). Though instruction tuning and RLHF can significantly improve the models' ability of interacting with humans, how they influence the calibration of large language models have not been researched on. \n\nConfidence calibration is a concerned problem for classification models. A large amount of works have studied the calibration of statistical machine learning systems and the methods to improve their calibration (DeGroot and Fienberg, 1983;Palmer et al., 2008;Yang and Thompson, 2010). Later, calibration of neural networks have also been researched on (Hendrycks and Gimpel, 2016;Nguyen and O'Connor, 2015;Nixon et al., 2019;Minderer et al., 2021). Guo et al. (2017) points out that modern neural networks are not as calibrated as their ancestors and proposes a temperature scaling methods to calibrate neural networks. In natural language processing field, calibration of transformer-based language models are evaluated among different tasks, including machine translation (Kumar and Sarawagi, 2019), QA (Jiang et al., 2021) and selective prediction (Varshney et al., 2022).",
            "score": 0.5054251652562662,
            "section_title": "Related Work",
            "char_start_offset": 5774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1111
                },
                {
                    "start": 1114,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1989
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 265,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 473,
                    "end": 494,
                    "matchedPaperCorpusId": "226262281"
                },
                {
                    "start": 1325,
                    "end": 1353,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 1353,
                    "end": 1373,
                    "matchedPaperCorpusId": "121625992"
                },
                {
                    "start": 1373,
                    "end": 1397,
                    "matchedPaperCorpusId": "24392407"
                },
                {
                    "start": 1520,
                    "end": 1539,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1539,
                    "end": 1561,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 1563,
                    "end": 1580,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029205322265625
        },
        {
            "corpus_id": "272987236",
            "title": "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models",
            "text": "Pretrained Large Language Models (LLMs) emerged as high-performance predictors for diverse tasks (Brown et al., 2020). Tuning based on instructions and human feedback has further advanced their performance (Wei et al., 2022;Ouyang et al., 2022). In various applications, the model's success often depends solely on assigning high scores to the correct options. Yet, their interpretation as probabilities is beneficial in several applications, such as Figure 1: Experimental Design -Joint Prediction Estimation with Masked Language Modeling. The middle white row displays the original unmasked tokens. Below, in blue, the joint probability is calculated by first estimating the probability of the correct token in MASK 1 and then of MASK 2 (after revealing the correct token in MASK 1 ). In the top rows, in green, the calculation is in the reversed order -first estimating the probability of the correct token in MASK 2 and then in MASK 1 (after revealing MASK 2 ). \n\ntheir treatment as confidence indicators for detecting hallucinations (Manakul et al., 2023;Ren et al., 2023) and robust ranking in sequence generation tasks (Zhao et al., 2022). Indeed, LLMs are commonly trained using the cross-entropy objective, which due to Gibbs' inequality is minimized by the true distribution. However, this global minima is hard to achieve (Chang and McCallum, 2022;Zhu et al., 2023) , raising the question of whether the model's outputs can still be interpreted as estimated probabilities. \n\nWhile calibration of produced scores has been extensively studied (Zhao et al., 2022;Shen et al., 2024), multiple other aspects remain unexplored. Measuring calibration for string density estimation requires a ground-truth measure for the real distribution, which is challenging to obtain (see \u00a73). Therefore, alternative methods to validate the assumption that produced scores correspond to es-timated probabilities, are needed. \n\nFor LLM scores to be interpreted as probabilities, consistency across estimation methods is essential. However, regardless of probabilistic interpretation, detecting and understanding inconsistencies among estimation methods is crucial, especially in the identification of preferable estimation methods.",
            "score": 0.5050808841994938,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 1038,
                    "end": 1060,
                    "matchedPaperCorpusId": "257557820"
                },
                {
                    "start": 1333,
                    "end": 1359,
                    "matchedPaperCorpusId": "248780434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1954345703125
        },
        {
            "corpus_id": "247793145",
            "title": "WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models",
            "text": "During the evaluation, we find that the performances of our models are not stable. We therefore implement the calibration technique reported by [6] to reduce the bias introduced by the language models. We empirically find the calibration brings improvement to the classification accuracy in most cases.",
            "score": 0.5050389877887302,
            "section_title": "Calibration",
            "char_start_offset": 6798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 302
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.137939453125
        },
        {
            "corpus_id": "253384402",
            "title": "Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates",
            "text": "As accurate estimates are required for many difficult or sensitive prediction tasks (Platt, 1999), probability calibration is an important uncertainty estimation task for NLP. Unlike other uncertainty estimation task (e.g., out-of-domain detection, selective inference), calibration focuses on aleatoric uncertainty measured by the probability of the prediction and adjusts the overall model confidence level (Hendrycks and Gimpel, 2017;Pereyra et al., 2017;Guo et al., 2017;Qin et al., 2021). For example, Gal and Ghahramani (2016) propose to adopt multiple predictions with different dropout masks and then combine them to get the confidence estimate. Recently, several works focus on the calibration of PLMs models for NLP tasks (Hendrycks et al., 2019;Desai and Durrett, 2020;Jung et al., 2020;He et al., 2021;Park and Caragea, 2022;Bose et al., 2022). Dan and Roth (2021) investigate the calibration properties of different transformer architectures and sizes of BERT. In line with recent work (Ye and Durrett, 2022), our work focuses on how explanations can help calibration in three NLP tasks. However, we do not need to learn a calibrator by using model interpretations with heuristics, and also do not compare due to its intensive computation cost when generating attributions. In contrast, we explore whether model explanations are useful for calibrating black-box models during training.",
            "score": 0.5050389877887302,
            "section_title": "Related Work",
            "char_start_offset": 12379,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 97,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 409,
                    "end": 437,
                    "matchedPaperCorpusId": "13046179"
                },
                {
                    "start": 437,
                    "end": 458,
                    "matchedPaperCorpusId": "9545399"
                },
                {
                    "start": 458,
                    "end": 475,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 475,
                    "end": 492,
                    "matchedPaperCorpusId": "245010935"
                },
                {
                    "start": 507,
                    "end": 532,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 732,
                    "end": 756,
                    "matchedPaperCorpusId": "59336190"
                },
                {
                    "start": 756,
                    "end": 780,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "216867328"
                },
                {
                    "start": 798,
                    "end": 814,
                    "matchedPaperCorpusId": "231632895"
                },
                {
                    "start": 814,
                    "end": 837,
                    "matchedPaperCorpusId": "247450599"
                },
                {
                    "start": 857,
                    "end": 876,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 999,
                    "end": 1021,
                    "matchedPaperCorpusId": "238856959"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.145751953125
        },
        {
            "corpus_id": "273654857",
            "title": "EfficientEQA: An Efficient Approach for Open Vocabulary Embodied Question Answering",
            "text": "Measuring the confidence of language models in an answer is a fundamental task. Early methods depend on extra data and finetuning techniques to calibrate the language models. [23] trains an additional model to analyze the certainty of uncalibrated language models. [18] finetunes GPT-3 [2] to generate verbalized calibration explicitly. With the rise of LLMs, research focuses on utilizing only the model output in a black-box setting. [12] converts the output to the semantic space and calculates the entropy as an estimation for uncertainty. [32] proposes to elicit verbal confidence instead of estimating condition probability via sampling. [34] defines a framework to prompt LLMs to elicit confidence and analyzes numerous possible strategies. In this work, we find that the verbalized confidence of vision language models helps determine the direction to explore.",
            "score": 0.5050389877887302,
            "section_title": "C. Confidence Estimation of Language Models",
            "char_start_offset": 6277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 868
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "259224389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04217529296875
        },
        {
            "corpus_id": "258866001",
            "title": "Selectively Answering Ambiguous Questions",
            "text": "We investigate the calibration of large language models, extending prior work by distinguishing uncertainty about the answer (epistemic uncertainty) from uncertainty about the meaning of the question (denotational uncertainty). We propose a disambiguate-and-answer paradigm, where the model first attempts to rephrase the question before providing its answer. This paradigm enables straightforward techniques for quantifying model confidence by counting the frequency of answers within repeated samples from the language model. These sample-based confidence metrics are particularly effective for ambiguous questions, and are significantly better calibrated than traditional likelihood-based measures and self-verification. For ambiguous questions, the proposed method is similar to calibration through self-consistency.",
            "score": 0.5050389877887302,
            "section_title": "Conclusion",
            "char_start_offset": 28316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 820
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266357421875
        },
        {
            "corpus_id": "277633781",
            "title": "Thinking Out Loud: Do Reasoning Models Know When They're Right?",
            "text": "Leveraging the confidence scores elicited from LLMs, we investigate two complementary tasks: calibration and failure prediction (Yuan et al., 2021;Xiong et al., 2022). Calibration assesses how well a model's predicted confidence matches its actual accuracy. For example, a well-calibrated model should be correct 70% of the time when it assigns 70% confidence to its predictions. In contrast, failure prediction evaluates a model's ability to distinguish between correct and incorrect predictions based on its confidence scores. Ideally, a model should assign higher confidence to correct answers and lower confidence to incorrect ones. \n\nTo quantify calibration performance, we use the Expected Calibration Error (ECE) (Guo et al., 2017), which measures the average discrepancy between predicted confidence and empirical accuracy across bins. Specifically, ECE involves dividing samples into M equal bins by confidence scores, then computing the mean absolute difference be-tween each bin's accuracy and average confidence: \n\nwith n as the total number of samples and B m is the set of samples in the m-th bin. Additionally, to address ECE's sensitivity to binning strategies and its potential high variance, we also employ the Adaptive Calibration Error (ACE) (Nixon et al., 2019) \n\nwhich dynamically adjusts bin boundaries to ensure each bin contains an equal number of samples based on the data distribution. In all experiments, we use M = 10 bins for both ECE and ACE. Specifically, when we are evaluating calibration on factuality benchmarks, we only take attempted questions into calculation. \n\nTo assess how well confidence scores distinguish correct from incorrect predictions, we report the AUROC. We also include AUPRC for both positive and negative instances, as it offers complementary insight in imbalanced settings or when model accuracy varies. Finally, we report accuracy as a baseline measure of overall performance.",
            "score": 0.5048440227728211,
            "section_title": "Tasks and Metrics",
            "char_start_offset": 9144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 147,
                    "matchedPaperCorpusId": "237438763"
                },
                {
                    "start": 720,
                    "end": 738,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1262,
                    "end": 1282,
                    "matchedPaperCorpusId": "102486060"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259521484375
        },
        {
            "corpus_id": "252847587",
            "title": "Evaluating N-best Calibration of Natural Language Understanding for Dialogue Systems",
            "text": "In order to numerically measure the degree of calibration, we assess the correlation between confidence estimates (scores in the range 0-1) and instance-level accuracies (1 for correct classifications, 0 for incorrect classifications). More specifically, we measure the extent to which an increase in confidence estimate is associated with an increase in instancelevel accuracy -in other words, the monotonicity of the relationship between confidence estimate and accuracy. The degree of monotonicity is measured using Spearman's correlation coefficient (Xiao et al., 2016). 5 iven two variables (X and Y ) of size N (x 1 , x 2 , ..x n and y 1 , y 2 ...y n respectively), Spearman's correlation coefficient (\u03c1) is calculated through the formula: \n\nwhere n is the number of samples, and d is the pairwise differences of the elements of the variables x i and y i . \n\nA perfectly-calibrated model has a Spearman's correlation coefficient of 1, while a correlation coefficient of 0 conveys a lack of correlation between confidence and accuracy. \n\nNote that other approaches to numerically estimating calibration have been discussed in the literature, e.g. negative log-likelihood (Quinonero-Candela et al., 2005), Brier score (Brier et al., 1950) and expected calibration error (Nixon et al., 2019). Different measurement approaches have different advantages and weaknesses (Ashukha et al., 2020), and no gold standard seems to exist. In this study, we have opted for Spearman's correlation due to the fact that monotonicity in the relation between confidence estimate and accuracy is an important characteristic of good calibration. Spearman's correlation has been previously used to evaluate confidence scores for neural semantic parsers (Dong et al., 2018).",
            "score": 0.5043990661567784,
            "section_title": "Spearman's correlation coefficient",
            "char_start_offset": 12517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 554,
                    "end": 573,
                    "matchedPaperCorpusId": "42155264"
                },
                {
                    "start": 1176,
                    "end": 1208,
                    "matchedPaperCorpusId": "7259908"
                },
                {
                    "start": 1222,
                    "end": 1242,
                    "matchedPaperCorpusId": "122906757"
                },
                {
                    "start": 1274,
                    "end": 1294,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1736,
                    "end": 1755,
                    "matchedPaperCorpusId": "13686145"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1007080078125
        },
        {
            "corpus_id": "274859541",
            "title": "On Verbalized Confidence Scores for LLMs",
            "text": "The rise of large language models (LLMs) and their tight integration into our daily life make it essential to dedicate efforts towards their trustworthiness. Uncertainty quantification for LLMs can establish more human trust into their responses, but also allows LLM agents to make more informed decisions based on each other's uncertainty. To estimate the uncertainty in a response, internal token logits, task-specific proxy models, or sampling of multiple responses are commonly used. This work focuses on asking the LLM itself to verbalize its uncertainty with a confidence score as part of its output tokens, which is a promising way for prompt- and model-agnostic uncertainty quantification with low overhead. Using an extensive benchmark, we assess the reliability of verbalized confidence scores with respect to different datasets, models, and prompt methods. Our results reveal that the reliability of these scores strongly depends on how the model is asked, but also that it is possible to extract well-calibrated confidence scores with certain prompt methods. We argue that verbalized confidence scores can become a simple but effective and versatile uncertainty quantification method in the future. Our code is available at https://github.com/danielyxyang/llm-verbalized-uq .",
            "score": 0.5042784039986561,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.220703125
        },
        {
            "corpus_id": "268876453",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "text": "This phenomenon may be due to the optimization objective of RLHF, which is to make the model generate responses aligned with human preferences rather than fitting answers that appear more frequently in the corpus during the pre-training stage. \n\nTo alleviate the issue of miscalibration, previous work focuses on two perspectives: the logit-based method and the verbalization-based method. Logitbased methods are usually post-hoc. We need to find a higher temperature (usually above 2.0), known as Temperature-Tuning (Guo et al., 2017), to make the distribution of the model's token logit smoother for mitigating overconfidence (Kadavath et al., 2022;He et al., 2023). The verbalizationbased method usually requires prompt engineering to elicit the model's confidence, and it also necessitates the model to have strong Self-Awareness (Lin et al., 2022a;Tian et al., 2023;Yin et al., 2023). Aggregating the model's logit-based and verbalizationbased confidence can also calibrate the model confidence to some extent (Xiong et al., 2023). As shown in Figure 2 and Appendix Tabel 6, by replacing the model's answer with \"All other options are wrong.\", we can assess whether the model had high fidelity to its previously given answer. Inspired by this phenomenon, we decompose the language model confidence into two dimensions: the Uncertainty about the question and the Fidelity to the answer generated by language models. First, if the answers provided by language model are consistent under multiple samplings, it indicates that language model has lower uncertainty regarding that question. Thus, we could utilize the information entropy of the frequency distribution of sampled answers to calculate the model's uncertainty about a question. Second, we design a novel method to estimate the model's fidelity to each of its sampled answers. Last, the uncertainty regarding question Q and the fidelity to the answer a i together determine the model's confidence. As shown in Figure 1, our proposed UF Calibration achieved good calibration across different MCQA datasets.",
            "score": 0.5034704021992529,
            "section_title": "Introduction",
            "char_start_offset": 1622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 871,
                    "end": 888,
                    "matchedPaperCorpusId": "258959258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1175537109375
        },
        {
            "corpus_id": "273963769",
            "title": "Epistemic Integrity in Large Language Models",
            "text": "Large Language Models (LLMs) have markedly transformed how humans seek and consume information, becoming integral across diverse fields such as public health (Ali et al., 2023), coding (Zambrano et al., 2023), and education (Whalen & et al., 2023). Despite their growing influence, LLMs are not without shortcomings. One notable issue is the potential for generating responses that, while convincing, may be inaccurate or nonsensical-a long-standing phenomenon often referred to as \"hallucinations\" (Jo, 2023;Huang et al., 2023;Zhou et al., 2024b). This raises concerns about the reliability and trustworthiness of these models. \n\nA critical aspect of trustworthiness in LLMs is epistemic calibration, which represents the alignment between a model's internal confidence in its outputs and the way it expresses that confidence through natural language. Misalignment between internal certainty and external expression can lead to users being misled by overconfident or underconfident statements, posing significant risks in high-stakes domains such as legal advice, medical diagnosis, and misinformation detection. While of great normative concern, how LLMs express linguistic uncertainty has received relatively little attention to date (Sileo & Moens, 2023;Belem et al., 2024). \n\nFigures 1 and 5 illustrate the issue of epistemic calibration providing insights into the operation of certainty in the context of human interactions with LLMs. We highlight the following key points in these figures: \n\n\u2022 Distinct Roles of Certainty: Internal certainty and linguistic assertiveness have distinct functions within LLM interactions that shape individual beliefs. \u2022 Human access to LLM certainty: Linguistic assertiveness holds a critical role as the primary form of certainty available to users. Unlike internal certainty, which remains hidden within the model's computational processes, linguistic assertiveness is directly perceivable and influences how users interpret the model's outputs. \u2022 Beyond Content: Users retrieve more than just the content from an LLM's output. The style and assertiveness of the language used also play a significant role by shaping perceptions through the communication of certainty.",
            "score": 0.5033014351552596,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2210
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 208,
                    "matchedPaperCorpusId": "266048321"
                },
                {
                    "start": 528,
                    "end": 547,
                    "matchedPaperCorpusId": "272882110"
                },
                {
                    "start": 1237,
                    "end": 1258,
                    "matchedPaperCorpusId": "253383825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01262664794921875
        },
        {
            "corpus_id": "271709438",
            "title": "Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference",
            "text": "As previously noted, the discriminative models' decision rule was inherently designed to respond to every query, defaulting to dr(q,Y ) = 1. Modern generative models, such as ChatGPT, have not disclosed their decision-making protocols, making any decision rules within them unpredictable from a user's standpoint. To evaluate and mitigate the decision and composite risk across both discriminative and generative models, an external decision rule method that is compatible with both types of models, and that is somewhat independent of the LM itself (and hence, generalizable), is clearly motivated. An ideal decision rule should use all available information, such as the instance prompts, as well as the confidence outputs from the underlying LLM, to minimize the risks defined earlier. \n\nPrevious studies 22,29 have developed fundamental techniques for the re-calibration of confidence scores, aimed at more precisely capturing a model's intrinsic uncertainty. We expand on this idea significantly by proposing a novel risk-adjusted calibration method called 'Deciding when to decide' or DwD. An external decision rule method in the architecture helps minimize the decision and composite risks of LMs, especially in high-risk inference scenarios. Notably, DwD does not have strong dependencies on the underlying LMs, enabling it to be suitable for a variety of models without knowing their internal workings. As we subsequently demonstrate in experiments, this feature makes it well-suited for risk-adjusted inference even for blaxk-box commercial models like OpenAI's GPT-4. \n\nSimilar to its predecessors, the decision rule in DwD operates as a binary classifier, utilizing traditional machine learning techniques. However, it distinguishes itself from previous work by tackling two principal challenges: the injection of risk into the training process, and calibration refinement with risk adjustment. Concerning the former, DwD injects risk into training set construction, a step not explicitly considered by earlier calibrators. Existing NLI benchmarks usually provide a definitive answer for all instances. Theoretically, an effective decision rule could use this knowledge and just answer all instances to minimize the decision risk.",
            "score": 0.5023138087303376,
            "section_title": "Risk-adjusted Calibration Approach",
            "char_start_offset": 14388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "248780161"
                },
                {
                    "start": 811,
                    "end": 813,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12841796875
        },
        {
            "corpus_id": "267412346",
            "title": "Calibration and Correctness of Language Models for Code",
            "text": "Generative large language models (LLMs) are now widelyused for code completion in IDEs. However, LLMs make mistakes: they can generate known buggy code [1], or code with risky vulnerabilities [2], [3]. Despite these risks, LLM \"copilots\" [4] are growing in popularity-thus there is a growing concern that bad LLM-generated code could be integrated into widely-used software. Given that LLMs might generate * Equal contribution. Order determined by random coin flip. buggy code, how should a developer decide whether generated code is correct or not? \n\nOne possibility is to use the confidence, or probability assigned to the generated code by the LLM itself. Consider a developer who asks GPT-3.5 to complete some unfinished code. For example, given the prefix with an average per-token confidence of 91%, suggesting high confidence (based on its training) that the code is a likely completion for the given prefix. However, this code is known to be buggy! In fact, when we test thousands of line completions, in cases where the average probability was greater than 90%, only 52% actually passed test cases. One can also find reverse examples, where the LLM has very little confidence, but the generated code is actually correct. \n\nWe make two observations. First, since LLMs can make mistakes, users would benefit from a reliable indication of confidence that the generated code is actually correct. Second, such indications of confidence, when well-aligned with actual correctness, can support well-justified software quality control measures, and thus improve the reliability of the AI4SE ecosystem. When an LLM-offered code suggestion is accompanied by a numerical \"confidence\" signal e.g., a probability measure, then this signal should be well-aligned with the likelihood that the code is actually correct. Such a measure is said to be wellcalibrated. \n\nCalibration has been studied in other settings e.g., classically in weather prediction and recently for software-related classification tasks [5]. In this paper, we study the calibration of generative1 large language models, when used in practical software engineering settings, such as line-level code completion, function synthesis, and code-repair.",
            "score": 0.5017236438917674,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "257636802"
                },
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "248085518"
                },
                {
                    "start": 197,
                    "end": 200,
                    "matchedPaperCorpusId": "220363858"
                },
                {
                    "start": 2002,
                    "end": 2005,
                    "matchedPaperCorpusId": "269129802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22216796875
        },
        {
            "corpus_id": "267365203",
            "title": "Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration",
            "text": "Previous works on knowing what language models know have focused on calibration, prompting, or training. Calibration-based approaches attempt to extract confidence scores from models to gauge their uncertainty (Sun et al., 2022;Kuhn et al., 2022;Zhou et al., 2023a;Liu et al., 2023a). Previous works have evaluated (Radford et al., 2019;Liang et al., 2023;Tao et al., 2023;He et al., 2023) and improved the calibration of language models (Desai and Durrett, 2020;Kong et al., 2020;Jagannatha and Yu, 2020;Kamath et al., 2020;Jiang et al., 2021;Mielke et al., 2022;Lin et al., 2022;Tian et al., 2023;Zhou et al., 2023b), while the calibration quality could vary for different downstream tasks (Desai and Durrett, 2020;Wang et al., 2020;Stengel-Eskin and Van Durme, 2023;Kalai and Vempala, 2023). In the setting of AbstainQA where LLMs decide whether to abstain or not given a question, calibration-based approaches would rely on a held-out set to set a threshold over calibrated confidence scores: such a threshold and reliance on a held-out set could jeopardize the generalization of calibration-based approaches across knowledge domains and reasoning contexts. \n\nPrompting-based approaches leverage the instruction-following abilities of LLMs and employ instructions to induce self-reflection and gauge whether the generated answer should be trusted. Kadavath et al. (2022) investigates whether a \"noneof-the-above\" option or self-evaluation prompting would induce good estimations of LLMs' internal factuality. Huang et al. (2023a) follows existing works (Kim et al., 2024;Shinn et al., 2023) to evaluate whether LLMs could self-correct their own reasoning with a three-step prompting strategy.",
            "score": 0.5007900046279343,
            "section_title": "Related Work",
            "char_start_offset": 22908,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 228,
                    "matchedPaperCorpusId": "254183614"
                },
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 315,
                    "end": 337,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 337,
                    "end": 356,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 356,
                    "end": 373,
                    "matchedPaperCorpusId": "261076328"
                },
                {
                    "start": 438,
                    "end": 463,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 481,
                    "end": 505,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 505,
                    "end": 525,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 525,
                    "end": 544,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 544,
                    "end": 564,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 581,
                    "end": 599,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 599,
                    "end": 618,
                    "matchedPaperCorpusId": "265150666"
                },
                {
                    "start": 692,
                    "end": 717,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 717,
                    "end": 735,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 735,
                    "end": 769,
                    "matchedPaperCorpusId": "253510101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.064208984375
        },
        {
            "corpus_id": "273549835",
            "title": "Task Calibration: Calibrating Large Language Models on Inference Tasks",
            "text": "Our work is inspired by mutual information (Tishby et al., 1999;Peng et al., 2005), which measures how much one random variable tells us about another. Intuitively, for a specific task, proper use of mutual information can reveal how much more informative the combined presence of premise and hypothesis is concerning the label, compared to their individual presences. Based on this insight, we reformulate LLM inference by factoring out the probabilities of premise-only and hypothesis-only inputs. TC requires no annotated data and is easy to imple-Figure 1: An example from QNLI dataset (Rajpurkar et al., 2016). Sentence-Only, Question-Only and Both indicate the inputs with only the sentence, question and using both components, respectively. While the initial model prediction is incorrect, potentially due to the influence of the hypothesis, we observe that task calibration finally leads to a correct prediction. \n\nment, involving only two extra inference stages using premise-only and hypothesis-only inputs for each sample. As shown in Figure 1, although the model's initial answer is incorrect, it finally makes the correct prediction after task calibration, by using output probabilities derived from premise-only, hypothesis-only, and combined inputs. \n\nExperimental results demonstrate superior performance of TC over other calibration methods in the zero-shot setup, showcasing a noteworthy boost of three different LLMs on 13 inference datasets. Specifically, TC outperforms the best-performing baseline in 12, 9 and 10 out of 13 datasets on the Mistral-7B-Instruct-v0.3, Llama-2-7B-chat and Phi-3-mini-4k-instruct models, respectively. In addition, TC is robust to various prompt templates, demonstrating its effectiveness in few-shot setups and 4 different natural language understanding (NLU) tasks such as sentiment analysis and hate speech detection. Finally, we find that the combination of TC and other calibration methods can yield better performance, which indicates their complementary strengths in fixing spurious correlations. \n\nTo summarize, our key contributions are as follows:",
            "score": 0.5001124448880367,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 64,
                    "matchedPaperCorpusId": "8936496"
                },
                {
                    "start": 64,
                    "end": 82,
                    "matchedPaperCorpusId": "206764015"
                },
                {
                    "start": 590,
                    "end": 614,
                    "matchedPaperCorpusId": "11816014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09552001953125
        },
        {
            "corpus_id": "267412346",
            "title": "Calibration and Correctness of Language Models for Code",
            "text": "Language models are now widely-integrated into Software Engineering practice, via tools like Copilot [32] and Didact 11 . We raise here the importance of calibration when integrating generative LLMs in coding practice. We evaluate the calibration of generative LLM use (especially code completion) with large samples of realistic data (DYPYBENCH, SSTUBS), using widely adopted models, as well as some more academic datasets (HUMANEVAL, MBPP). \n\na) Using a well-calibrated model-beyond simple defect prediction: To clarify how a well-calibrated model enables more well-grounded decision-making concerning generated outputs, as compared to as compared to a traditional process choosing a binary decision point-we consider GPT-3.5 working on code completion, where correctness is determined by test-passing, and confidence is assigned by few-shotting, average token probability. The base correctness (test-passing) rate of completions is about 33%. With few-shotting, we get a very high skill score of 0.15 (Section IV-D, Table V). \n\nIf we didn't have a well-calibrated model, we might very cautiously accept only those completions that are generated at a very high-confidence threshold; here, the FP rate could be low (of course TP rate would be low as well). While this may lower the risk of bad code, it also regrettably reduces the available help from the LLM. However, a well-calibrated confidence measure allows a more rational, graduated set of decisions. Such a well-calibrated measure is visualized in Figure 3. In this setting, for much of the confidence scale, a user could look at the confidence level, and get a very good idea of how likely the code is to be correct, and make a well-reasoned, situation-specific set of decisions to manage risk, and allocate reviewing resources, based on the model's confidence.",
            "score": 0.5000241930276248,
            "section_title": "V. DISCUSSION",
            "char_start_offset": 37763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1822
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 105,
                    "matchedPaperCorpusId": "248798468"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.085693359375
        },
        {
            "paperId": "d738ea69de602f30856d10876e6f354e909146b0",
            "corpusId": 271754816,
            "title": "Detecting correct answers to open questions and its impact on language models' confidence scores",
            "venue": "LatinX in AI at North American Chapter of the Association for Computational Linguistics Conference 2024",
            "year": 2024,
            "referenceCount": 16,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52591/lxai202406219?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52591/lxai202406219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2213060824",
                    "name": "Guido Ivetta"
                },
                {
                    "authorId": "2139773809",
                    "name": "Hern\u00e1n Maina"
                },
                {
                    "authorId": "2066254822",
                    "name": "Luciana Benotti"
                }
            ],
            "abstract": "In this paper, we study how different methods for classifying correct answers to open questions impact calibration scores in language models. We compare seven different techniques to perform this task. In this setting, we find that even though answer classification techniques have up to 21% differences between each other, calibration scores are not affected significantly. We find these results show evidence of unreliability on commonly used metrics in this field.",
            "corpus_id": "271754816",
            "text": "In this paper, we study how different methods for classifying correct answers to open questions impact calibration scores in language models. We compare seven different techniques to perform this task. In this setting, we find that even though answer classification techniques have up to 21% differences between each other, calibration scores are not affected significantly. We find these results show evidence of unreliability on commonly used metrics in this field.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.11920166015625
        },
        {
            "paperId": "df03d8e8851fbdc20911bfad09a642cccea32a51",
            "corpusId": 266087110,
            "title": "Being confident in confidence scores: calibration in deep learning models for camera trap image sequences",
            "venue": "bioRxiv",
            "year": 2024,
            "referenceCount": 52,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rse2.412",
                "status": "GOLD",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/rse2.412?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/rse2.412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1606149516",
                    "name": "Gaspard Dussert"
                },
                {
                    "authorId": "1397357310",
                    "name": "S. Chamaill\u00e9\u2010Jammes"
                },
                {
                    "authorId": "2266758320",
                    "name": "St\u00e9phane Dray"
                },
                {
                    "authorId": "2190441630",
                    "name": "Vincent Miele"
                }
            ],
            "abstract": "In ecological studies, machine learning models are increasingly being used for the automatic processing of camera trap images. Although this automation facilitates and accelerates the identification step, the results of these models may lack interpretability and their immediate applicability to ecological downstream tasks (e.g occupancy estimation) remain questionable. In particular, little is known about their calibration, a property that guarantees that confidence scores can be reliably interpreted as probabilities that a model\u2019s predictions are true. Using a large and diverse European camera trap dataset, we investigate whether deep learning models for species classification in camera trap images are well calibrated, or in contrast over/under-confident. Additionally, as camera traps are often configured to take multiple photos of the same event, we also explore the calibration of predictions at the sequence level. Finally, we study the effect and the practicality of a post-hoc calibration method, i.e. temperature scaling, for predictions made at image and sequence levels. Based on five established models and three independent test sets, our findings show that, using the right methodology, it is possible to enhance the interpretability of the confidence scores, with clear implication for, for instance, the calculation of error rates or the selection of confidence score thresholds in ecological studies making use of artificial intelligence models.",
            "corpus_id": "266087110",
            "text": "In ecological studies, machine learning models are increasingly being used for the automatic processing of camera trap images. Although this automation facilitates and accelerates the identification step, the results of these models may lack interpretability and their immediate applicability to ecological downstream tasks (e.g occupancy estimation) remain questionable. In particular, little is known about their calibration, a property that guarantees that confidence scores can be reliably interpreted as probabilities that a model\u2019s predictions are true. Using a large and diverse European camera trap dataset, we investigate whether deep learning models for species classification in camera trap images are well calibrated, or in contrast over/under-confident. Additionally, as camera traps are often configured to take multiple photos of the same event, we also explore the calibration of predictions at the sequence level. Finally, we study the effect and the practicality of a post-hoc calibration method, i.e. temperature scaling, for predictions made at image and sequence levels. Based on five established models and three independent test sets, our findings show that, using the right methodology, it is possible to enhance the interpretability of the confidence scores, with clear implication for, for instance, the calculation of error rates or the selection of confidence score thresholds in ecological studies making use of artificial intelligence models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.061767578125
        },
        {
            "paperId": "b58b319d2b3f933ae201f747dabb4b9ea070e50e",
            "corpusId": 278637152,
            "title": "Linguistic Calibration of Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 58,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2404.00474?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2404.00474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294359410",
                    "name": "Neil Band"
                },
                {
                    "authorId": "2294511010",
                    "name": "Xuechen Li"
                },
                {
                    "authorId": "2286610204",
                    "name": "Tengyu Ma"
                },
                {
                    "authorId": "2294362683",
                    "name": "Tatsunori Hashimoto"
                }
            ],
            "abstract": "Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as \u201cI estimate a 30% chance of...\u201d or \u201cI am certain that...\u201d, followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under distribution shift on question-answering and under a significant task shift to person biography generation. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.",
            "corpus_id": "278637152",
            "text": "Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as \u201cI estimate a 30% chance of...\u201d or \u201cI am certain that...\u201d, followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under distribution shift on question-answering and under a significant task shift to person biography generation. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.232177734375
        },
        {
            "paperId": "939cbdc260d6c2b02e72fd871ebb0f26d643ce7d",
            "corpusId": 270702756,
            "title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "referenceCount": 59,
            "citationCount": 25,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15765, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "10385489",
                    "name": "Zhongzhi Yu"
                },
                {
                    "authorId": "2308276343",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "2116099023",
                    "name": "Yonggan Fu"
                },
                {
                    "authorId": "2310569409",
                    "name": "Huihong Shi"
                },
                {
                    "authorId": "2308034263",
                    "name": "Khalid Shaikh"
                },
                {
                    "authorId": "2305737055",
                    "name": "Y. Lin"
                }
            ],
            "abstract": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established, remains limited. Inspired by recent studies that explore the presence of attention sink in the initial token, which receives disproportionately large attention scores despite their lack of semantic importance, this work delves deeper into this phenomenon. We aim to provide a more profound understanding of the existence of attention sinks within LLMs and to uncover ways to enhance the achievable accuracy of LLMs by directly optimizing the attention distributions, without the need for weight finetuning. Specifically, this work begins with comprehensive visualizations of the attention distributions in LLMs during inference across various inputs and tasks. Based on these visualizations, to the best of our knowledge, we are the first to discover that (1) attention sinks occur not only at the start of sequences but also within later tokens of the input, and (2) not all attention sinks have a positive impact on the achievable accuracy of LLMs. Building upon our findings, we propose a training-free Attention Calibration Technique (ACT) that automatically optimizes the attention distributions on the fly during inference in an input-adaptive manner. Extensive experiments validate that ACT consistently enhances the accuracy of various LLMs across different applications. Specifically, ACT achieves an average improvement of up to 7.30% in accuracy across different datasets when applied to Llama-30B. Our code is available at https://github.com/GATECH-EIC/ACT.",
            "corpus_id": "270702756",
            "text": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established, remains limited. Inspired by recent studies that explore the presence of attention sink in the initial token, which receives disproportionately large attention scores despite their lack of semantic importance, this work delves deeper into this phenomenon. We aim to provide a more profound understanding of the existence of attention sinks within LLMs and to uncover ways to enhance the achievable accuracy of LLMs by directly optimizing the attention distributions, without the need for weight finetuning. Specifically, this work begins with comprehensive visualizations of the attention distributions in LLMs during inference across various inputs and tasks. Based on these visualizations, to the best of our knowledge, we are the first to discover that (1) attention sinks occur not only at the start of sequences but also within later tokens of the input, and (2) not all attention sinks have a positive impact on the achievable accuracy of LLMs. Building upon our findings, we propose a training-free Attention Calibration Technique (ACT) that automatically optimizes the attention distributions on the fly during inference in an input-adaptive manner. Extensive experiments validate that ACT consistently enhances the accuracy of various LLMs across different applications. Specifically, ACT achieves an average improvement of up to 7.30% in accuracy across different datasets when applied to Llama-30B. Our code is available at https://github.com/GATECH-EIC/ACT.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.10687255859375
        },
        {
            "paperId": "5d3105a5ffa133b873537bda8ff1ec6244c2b841",
            "corpusId": 271325803,
            "title": "Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 62,
            "citationCount": 19,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2403.09972?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2403.09972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "31734386",
                    "name": "Fengbin Zhu"
                },
                {
                    "authorId": "2260433198",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ],
            "abstract": "Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trusta-bility of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for superior calibration. Experimental results on six datasets of three tasks demonstrate the rationality and effectiveness of the proposed framework.",
            "corpus_id": "271325803",
            "text": "Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trusta-bility of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for superior calibration. Experimental results on six datasets of three tasks demonstrate the rationality and effectiveness of the proposed framework.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1971435546875
        },
        {
            "paperId": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "corpusId": 259287216,
            "title": "Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 53,
            "citationCount": 14,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2306.16564",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2306.16564?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2306.16564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2220964895",
                    "name": "Theodore Zhao"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "145675605",
                    "name": "J. S. Preston"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the con\ufb01dence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop veri\ufb01cation. An important source of calibration signals stems from expert-stipulated programmatic super-vision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in signi\ufb01cant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "corpus_id": "259287216",
            "text": "Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the con\ufb01dence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop veri\ufb01cation. An important source of calibration signals stems from expert-stipulated programmatic super-vision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in signi\ufb01cant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.33349609375
        }
    ],
    "quotes": {
        "cost": 0.038229,
        "quotes": [
            {
                "idx": 0,
                "key": "[258865733 | Tian et al. | 2023 | Citations: 355]",
                "snippets": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 425,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[268723623 | Li et al. | 2024 | Citations: 5]",
                "snippets": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 469,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[268876453 | Zhang et al. | 2024 | Citations: 13]",
                "snippets": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 284,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[269004786 | Detommaso et al. | 2024 | Citations: 19]",
                "snippets": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 285,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[270620078 | Liu et al. | 2024 | Citations: 6]",
                "snippets": "Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs' reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 390,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs' reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[272689376 | Sun et al. | 2024 | Citations: 1]",
                "snippets": "Estimation of a model's confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance...Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 197,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Estimation of a model's confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance"
                    },
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 532,
                        "end": 940,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[272987064 | Xie et al. | 2024 | Citations: 17]",
                "snippets": "The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration\u2014how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration\u2014how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[273396026 | Stearns et al. | 2024 | Citations: 0]",
                "snippets": "A second desirable property is that our models do not overconfidently make mistakes, assigning high probabilities to incorrect predictions. \n\nOne approach for such analysis is through the \"calibration curve\" method. Initially employed in analysing weather forecasts (Brier, 1950)(Degroot et al., 1983), this technique has since been applied to neural networks (Guo et al., 2017)(Minderer et al., 2021) and recently to evaluate Large Language Models (LLMs) from a semantic perspective (Levinstein et al., 2023). For example, (Levinstein and Herrmann, 2024) utilizes calibration curves to assess the veracity of LLM statements on specific datasets and asserts that \"calibration offers another metric for evaluating the quality of probes' forecasts.\" Calibration analyses have been utilized in neural networks and language models (Minderer et al., 2021)(Lewis, 1949), allowing researchers to assess the relationship between a model's prediction confidence and success rate.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[109884250 | Degroot et al. | 1983 | Citations: 745]": "In this paper we present methods for comparing and evaluating forecasters whose predictions are presented as their subjective probability distributions of various random variables that will be observed in the future, e.g. weather forecasters who each day must specify their own probabilities that it will rain in a particular location. We begin by reviewing the concepts of calibration and refinement, and describiing the relationship between this notion of refinement and the notion of sufficiency in the comparison of statistical experiments. We also consider the question of interrelationships among forecasters and discuss methods by which an observer should combine the predictions from two or more different forecasters. Then we turn our attention to the concept of a proper scoring rule for evaluating forecasters, relating it to the concepts of calibration and refinement. Finally, we discuss conditions under which one fore- caster can exploit the predictions of another forecaster to obtain a better score. In this paper we describe some concepts and methods appropriate for evaluating and com- paring forecasters who repeatedly present their predictions of whether or not various events will occur in terms of their subjective probabilities of those events. The ideas we describe here are relevant in almost any situation in which forecasters must repeatedly make such probabilistic predictions, regardless of the particular subject matter or substantive area of the events being forecast. The forecaster might be an economist who at the beginning of each quarterly period makes predictions about unemployment, the rate of inflation, or Gross National Product in that quarter based on the values of various economic indicators; the forecaster might even make predictions using a large-scale econometric model of the United States economy based on hundreds of variables and econometric relations. In a different field, the forecaster might be the weatherman for a television station who at the beginning of each day must announce his probability that it will rain during the day. For ease of exposition, we present our discussions here in the context of such a weather forecaster who day after day must specify his subjective probability x that there will be at least a certain amount of rain at some given location during a specified time interval of the day. We refer to the occurrence of this well-specified event simply as \"rain\". Thus, at the begin- ning of each day the forecaster must specify his probability of rain and at the end of each day he observes whether or not rain actually occurred. The probability x specified by the forecaster on any particular day is called his prediction for that day. We shall make the realistic, and simultaneously simplifying, assumption that the",
                    "[235435823 | Minderer et al. | 2021 | Citations: 367]": "Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.",
                    "[259316868 | Levinstein et al. | 2023 | Citations: 61]": "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.",
                    "[28671436 | Guo et al. | 2017 | Citations: 5869]": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                },
                "metadata": [
                    {
                        "section_title": "Calibration Curves",
                        "pdf_hash": "",
                        "start": 154,
                        "end": 1138,
                        "sentence_offsets": [
                            {
                                "start": 154,
                                "end": 293
                            },
                            {
                                "start": 296,
                                "end": 369
                            },
                            {
                                "start": 370,
                                "end": 673
                            },
                            {
                                "start": 674,
                                "end": 910
                            },
                            {
                                "start": 911,
                                "end": 1138
                            }
                        ],
                        "ref_mentions": [
                            "122906757",
                            "109884250",
                            "28671436",
                            "235435823",
                            "259316868",
                            "235435823",
                            "79240523"
                        ],
                        "quote": "A second desirable property is that our models do not overconfidently make mistakes, assigning high probabilities to incorrect predictions. \n\nOne approach for such analysis is through the \"calibration curve\" method. Initially employed in analysing weather forecasts (Brier, 1950)(Degroot et al., 1983), this technique has since been applied to neural networks (Guo et al., 2017)(Minderer et al., 2021) and recently to evaluate Large Language Models (LLMs) from a semantic perspective (Levinstein et al., 2023). For example, (Levinstein and Herrmann, 2024) utilizes calibration curves to assess the veracity of LLM statements on specific datasets and asserts that \"calibration offers another metric for evaluating the quality of probes' forecasts.\" Calibration analyses have been utilized in neural networks and language models (Minderer et al., 2021)(Lewis, 1949), allowing researchers to assess the relationship between a model's prediction confidence and success rate."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.015261,
        "cot": "To effectively answer the user's query about well-calibrated language models and their confidence judgments, I'll organize the information into meaningful dimensions that help structure a comprehensive response.\n\n1. \"Introduction to Calibration in Language Models\" - This dimension will provide background information on what calibration means in the context of language models and why it's important. This should be a synthesis format to provide a coherent explanation of the concept. Even though the query doesn't explicitly ask for an introduction, establishing what calibration is will help frame the rest of the answer.\n\n2. \"Properties of Well-Calibrated LMs\" - This dimension will focus on the desirable properties of well-calibrated language models, addressing the first part of the query about \"desirable property LMs they well calibrated.\" This should be a synthesis format to coherently explain these properties. Quotes [0], [1], [5], and [7] discuss these properties.\n\n3. \"Methods for Achieving and Measuring Calibration\" - This dimension will cover the approaches and techniques for achieving proper confidence judgments, addressing the \"recent works seek properly LM confidence judgments\" part of the query. This should be a list format to clearly enumerate the different methods mentioned in the quotes. Quotes [3], [4], and [5] discuss different methods.\n\n4. \"Challenges in LM Calibration\" - This dimension will address issues and limitations in current calibration approaches, which is important for a comprehensive understanding of the topic. This should be a synthesis format to provide a coherent explanation of these challenges. Quotes [1], [2], and [6] discuss challenges with calibration.\n\nThe introduction section doesn't have specific quotes assigned to it as it will synthesize general concepts from across the quotes to provide necessary background information.",
        "plan": {
            "Introduction to Calibration in Language Models (synthesis)": [],
            "Properties of Well-Calibrated LMs (synthesis)": [
                0,
                1,
                5,
                7
            ],
            "Methods for Achieving and Measuring Calibration (list)": [
                3,
                4,
                5
            ],
            "Challenges in LM Calibration (synthesis)": [
                1,
                2,
                6
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Calibration in Language Models",
                "tldr": "Calibration in language models refers to how well the model's confidence in its outputs matches the actual correctness of those outputs. Well-calibrated LMs provide confidence scores that reliably indicate how likely their decisions or predictions are to be correct. (LLM Memory)",
                "text": "\nCalibration in language models (LMs) refers to the alignment between a model's confidence in its predictions and the actual likelihood that those predictions are correct. A perfectly calibrated language model would assign confidence scores that precisely match the probability of the answer being correct - for example, answers given with 80% confidence would be correct 80% of the time. This property is increasingly important as language models are deployed in critical applications where users and systems need to know when to trust or question model outputs. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent research has focused on evaluating and improving calibration in LMs, as these models often exhibit overconfidence or underconfidence in their predictions. Calibration is particularly important in scenarios where LMs are used for decision support, content generation, or knowledge-intensive tasks. Well-calibrated models allow users to set appropriate thresholds for automated decision-making and help identify when human intervention might be necessary. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe query specifically mentions \"confidence judgments\" from language models - these are explicit or implicit signals from the model about its certainty in the generated content or decisions. These confidence scores can be derived from various sources, including the model's output probabilities, specially designed prompting techniques, or through additional calibration layers added to the model architecture. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Properties of Well-Calibrated LMs",
                "tldr": "Well-calibrated language models provide confidence scores that accurately reflect their likelihood of correctness, enabling better decision-making in applications. These models should maintain calibration across different domains and avoid both overconfidence and underconfidence. (7 sources)",
                "text": "\nWell-calibrated language models possess several key properties that make them reliable tools for decision-making systems. First and foremost, their confidence scores should accurately indicate the likelihood that their answers are correct <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This alignment between expressed confidence and actual correctness enables practical applications such as deferring to human experts in low-confidence scenarios, thus creating more trustworthy AI systems.\n\nA critical characteristic of truly well-calibrated models is maintaining calibration across diverse domains and tasks. Recent research has revealed that apparent overall calibration can mask significant domain-specific miscalibration - for example, a model might appear well-calibrated in aggregate while being systematically overconfident in mathematics and underconfident in history <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This finding highlights the importance of evaluating calibration across different \"slices\" of knowledge rather than just in aggregate.\n\nWell-calibrated models also avoid overconfidently making mistakes. The calibration curve approach, originally used in weather forecasting <Paper corpusId=\"109884250\" paperTitle=\"(Degroot et al., 1983)\" isShortName></Paper> and later applied to neural networks <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper> <Paper corpusId=\"235435823\" paperTitle=\"(Minderer et al., 2021)\" isShortName></Paper>, offers a method to evaluate whether models assign appropriate confidence levels to their predictions <Paper corpusId=\"273396026\" paperTitle=\"(Stearns et al., 2024)\" isShortName></Paper>. A properly calibrated model shows a strong relationship between prediction confidence and actual success rate.\n\nFor conversational AI systems based on large language models, well-calibrated confidence scores are particularly important for reducing hallucination and preventing over-reliance on model outputs. Effective confidence estimation methods might include approaches based on softmax probabilities, raw token scores, verbalized confidences, or combinations of these techniques <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. The quality of calibration can be assessed using metrics like the area under the curve (AUC), with higher values indicating better calibration.",
                "citations": [
                    {
                        "id": "(Tian et al., 2023)",
                        "snippets": [
                            "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated."
                        ],
                        "paper": {
                            "corpus_id": 258865733,
                            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
                            "authors": [
                                {
                                    "authorId": "2218407338",
                                    "name": "Katherine Tian"
                                },
                                {
                                    "authorId": "49688913",
                                    "name": "E. Mitchell"
                                },
                                {
                                    "authorId": "2064472884",
                                    "name": "Allan Zhou"
                                },
                                {
                                    "authorId": "50465276",
                                    "name": "Archit Sharma"
                                },
                                {
                                    "authorId": "102801230",
                                    "name": "Rafael Rafailov"
                                },
                                {
                                    "authorId": "18307037",
                                    "name": "Huaxiu Yao"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 355
                        },
                        "score": 0.74951171875
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)."
                        ],
                        "paper": {
                            "corpus_id": 268723623,
                            "title": "Few-Shot Recalibration of Language Models",
                            "authors": [
                                {
                                    "authorId": "2293910776",
                                    "name": "Xiang Lisa Li"
                                },
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.68994140625
                    },
                    {
                        "id": "(Degroot et al., 1983)",
                        "snippets": [
                            "In this paper we present methods for comparing and evaluating forecasters whose predictions are presented as their subjective probability distributions of various random variables that will be observed in the future, e.g. weather forecasters who each day must specify their own probabilities that it will rain in a particular location. We begin by reviewing the concepts of calibration and refinement, and describiing the relationship between this notion of refinement and the notion of sufficiency in the comparison of statistical experiments. We also consider the question of interrelationships among forecasters and discuss methods by which an observer should combine the predictions from two or more different forecasters. Then we turn our attention to the concept of a proper scoring rule for evaluating forecasters, relating it to the concepts of calibration and refinement. Finally, we discuss conditions under which one fore- caster can exploit the predictions of another forecaster to obtain a better score. In this paper we describe some concepts and methods appropriate for evaluating and com- paring forecasters who repeatedly present their predictions of whether or not various events will occur in terms of their subjective probabilities of those events. The ideas we describe here are relevant in almost any situation in which forecasters must repeatedly make such probabilistic predictions, regardless of the particular subject matter or substantive area of the events being forecast. The forecaster might be an economist who at the beginning of each quarterly period makes predictions about unemployment, the rate of inflation, or Gross National Product in that quarter based on the values of various economic indicators; the forecaster might even make predictions using a large-scale econometric model of the United States economy based on hundreds of variables and econometric relations. In a different field, the forecaster might be the weatherman for a television station who at the beginning of each day must announce his probability that it will rain during the day. For ease of exposition, we present our discussions here in the context of such a weather forecaster who day after day must specify his subjective probability x that there will be at least a certain amount of rain at some given location during a specified time interval of the day. We refer to the occurrence of this well-specified event simply as \"rain\". Thus, at the begin- ning of each day the forecaster must specify his probability of rain and at the end of each day he observes whether or not rain actually occurred. The probability x specified by the forecaster on any particular day is called his prediction for that day. We shall make the realistic, and simultaneously simplifying, assumption that the"
                        ],
                        "paper": {
                            "corpus_id": 109884250,
                            "title": "The Comparison and Evaluation of Forecasters.",
                            "authors": [
                                {
                                    "authorId": "39416713",
                                    "name": "M. Degroot"
                                },
                                {
                                    "authorId": "1684961",
                                    "name": "S. Fienberg"
                                }
                            ],
                            "year": 1983,
                            "venue": "",
                            "n_citations": 745
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2017)",
                        "snippets": [
                            "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                        ],
                        "paper": {
                            "corpus_id": 28671436,
                            "title": "On Calibration of Modern Neural Networks",
                            "authors": [
                                {
                                    "authorId": "144993411",
                                    "name": "Chuan Guo"
                                },
                                {
                                    "authorId": "10804137",
                                    "name": "Geoff Pleiss"
                                },
                                {
                                    "authorId": "2117103358",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "7446832",
                                    "name": "Kilian Q. Weinberger"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 5869
                        },
                        "score": 0
                    },
                    {
                        "id": "(Minderer et al., 2021)",
                        "snippets": [
                            "Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties."
                        ],
                        "paper": {
                            "corpus_id": 235435823,
                            "title": "Revisiting the Calibration of Modern Neural Networks",
                            "authors": [
                                {
                                    "authorId": "46352821",
                                    "name": "Matthias Minderer"
                                },
                                {
                                    "authorId": "2941141",
                                    "name": "J. Djolonga"
                                },
                                {
                                    "authorId": "3451951",
                                    "name": "Rob Romijnders"
                                },
                                {
                                    "authorId": "73774594",
                                    "name": "F. Hubis"
                                },
                                {
                                    "authorId": "2743563",
                                    "name": "Xiaohua Zhai"
                                },
                                {
                                    "authorId": "2815290",
                                    "name": "N. Houlsby"
                                },
                                {
                                    "authorId": "47497262",
                                    "name": "Dustin Tran"
                                },
                                {
                                    "authorId": "34302129",
                                    "name": "Mario Lucic"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 367
                        },
                        "score": 0
                    },
                    {
                        "id": "(Stearns et al., 2024)",
                        "snippets": [
                            "A second desirable property is that our models do not overconfidently make mistakes, assigning high probabilities to incorrect predictions. \n\nOne approach for such analysis is through the \"calibration curve\" method. Initially employed in analysing weather forecasts (Brier, 1950)(Degroot et al., 1983), this technique has since been applied to neural networks (Guo et al., 2017)(Minderer et al., 2021) and recently to evaluate Large Language Models (LLMs) from a semantic perspective (Levinstein et al., 2023). For example, (Levinstein and Herrmann, 2024) utilizes calibration curves to assess the veracity of LLM statements on specific datasets and asserts that \"calibration offers another metric for evaluating the quality of probes' forecasts.\" Calibration analyses have been utilized in neural networks and language models (Minderer et al., 2021)(Lewis, 1949), allowing researchers to assess the relationship between a model's prediction confidence and success rate."
                        ],
                        "paper": {
                            "corpus_id": 273396026,
                            "title": "Evaluating the Generalisation of an Artificial Learner",
                            "authors": [
                                {
                                    "authorId": "51896370",
                                    "name": "Bernardo Stearns"
                                },
                                {
                                    "authorId": "20486050",
                                    "name": "Nicolas Ballier"
                                },
                                {
                                    "authorId": "2261396630",
                                    "name": "Thomas Gaillat"
                                },
                                {
                                    "authorId": "2261398341",
                                    "name": "Andrew Simpkin"
                                },
                                {
                                    "authorId": "1738599770",
                                    "name": "John P. Mccrae"
                                }
                            ],
                            "year": 2024,
                            "venue": "NLP4CALL",
                            "n_citations": 0
                        },
                        "score": 0.546875
                    },
                    {
                        "id": "(Sun et al., 2024)",
                        "snippets": [
                            "Estimation of a model's confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance",
                            "Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration."
                        ],
                        "paper": {
                            "corpus_id": 272689376,
                            "title": "Confidence Estimation For LLM-Based Dialogue State Tracking",
                            "authors": [
                                {
                                    "authorId": "2321493889",
                                    "name": "Yi-Jyun Sun"
                                },
                                {
                                    "authorId": "29832722",
                                    "name": "Suvodip Dey"
                                },
                                {
                                    "authorId": "2365041900",
                                    "name": "Dilek Hakkani-Tur"
                                },
                                {
                                    "authorId": "5108268",
                                    "name": "Gokhan Tur"
                                }
                            ],
                            "year": 2024,
                            "venue": "Spoken Language Technology Workshop",
                            "n_citations": 1
                        },
                        "score": 0.52294921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Methods for Achieving and Measuring Calibration",
                "tldr": "Various methods have been developed to calibrate language models, including multicalibration approaches, confidence estimation techniques, and specialized calibration metrics. These methods aim to align model confidence with actual performance while maintaining efficiency and interpretability. (3 sources)",
                "text": "\nSeveral approaches have been developed to achieve and measure calibration in language models:\n\n1. **Multicalibration**: This technique aims to produce reliable confidence scores by ensuring calibration across various intersecting groups of data, not just marginally. Multicalibration provides more interpretable and reliable confidence scores for LLM outputs by addressing calibration simultaneously across different data groupings. <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>\n\n2. **Softmax-based confidence estimation**: This approach uses the probability distribution from the model's softmax layer to derive confidence scores. While computationally efficient, these methods may not always provide the most informative calibration signals. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>\n\n3. **Raw token score methods**: These techniques examine the raw scores associated with generated tokens to estimate confidence. They can provide more granular information about the model's certainty at different points in generation. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>\n\n4. **Verbalized confidence**: This method involves prompting the model to explicitly state its confidence level in natural language. This approach leverages the model's ability to reflect on its own outputs. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>\n\n5. **Combination methods**: Hybrid approaches that combine multiple confidence estimation techniques often achieve better calibration results than single methods alone. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>\n\n6. **Self-consistency-based calibration**: These methods evaluate consistency across multiple generations to estimate confidence, though they may be limited in inference-time efficiency. <Paper corpusId=\"270620078\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>\n\n7. **Logit-based calibration**: Approaches that use model logits to derive confidence scores, though these sometimes fall short of providing sufficiently informative signals. <Paper corpusId=\"270620078\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>\n\n8. **Area Under the Curve (AUC) metric**: This evaluation method assesses the quality of confidence estimation, with higher AUC values indicating better calibration. It provides a quantitative measure of how well confidence scores align with actual performance. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>\n\nResearchers continue to develop new calibration methods that balance the trade-offs between computational efficiency, interpretability, and accuracy of confidence estimation. Effective calibration techniques are particularly important for reducing hallucination and preventing over-reliance on model outputs in conversational AI systems. <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Detommaso et al., 2024)",
                        "snippets": [
                            "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data."
                        ],
                        "paper": {
                            "corpus_id": 269004786,
                            "title": "Multicalibration for Confidence Scoring in LLMs",
                            "authors": [
                                {
                                    "authorId": "2295667267",
                                    "name": "Gianluca Detommaso"
                                },
                                {
                                    "authorId": "2295665717",
                                    "name": "Martin Bertran"
                                },
                                {
                                    "authorId": "2295664744",
                                    "name": "Riccardo Fogliato"
                                },
                                {
                                    "authorId": "2295665299",
                                    "name": "Aaron Roth"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 19
                        },
                        "score": 0.51318359375
                    },
                    {
                        "id": "(Sun et al., 2024)",
                        "snippets": [
                            "Estimation of a model's confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance",
                            "Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration."
                        ],
                        "paper": {
                            "corpus_id": 272689376,
                            "title": "Confidence Estimation For LLM-Based Dialogue State Tracking",
                            "authors": [
                                {
                                    "authorId": "2321493889",
                                    "name": "Yi-Jyun Sun"
                                },
                                {
                                    "authorId": "29832722",
                                    "name": "Suvodip Dey"
                                },
                                {
                                    "authorId": "2365041900",
                                    "name": "Dilek Hakkani-Tur"
                                },
                                {
                                    "authorId": "5108268",
                                    "name": "Gokhan Tur"
                                }
                            ],
                            "year": 2024,
                            "venue": "Spoken Language Technology Workshop",
                            "n_citations": 1
                        },
                        "score": 0.52294921875
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs' reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals."
                        ],
                        "paper": {
                            "corpus_id": 270620078,
                            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
                            "authors": [
                                {
                                    "authorId": "2120099874",
                                    "name": "Xin Liu"
                                },
                                {
                                    "authorId": "2131675694",
                                    "name": "Farima Fatahi Bayat"
                                },
                                {
                                    "authorId": "2299208178",
                                    "name": "Lu Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 6
                        },
                        "score": 0.6728515625
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Challenges in LM Calibration",
                "tldr": "Language models face significant calibration challenges including hidden miscalibration across knowledge domains and calibration degradation after alignment training. These issues compromise reliability and make it difficult to trust confidence scores in practical applications. (3 sources)",
                "text": "\nDespite recent progress in language model calibration, several fundamental challenges remain that complicate efforts to create truly well-calibrated systems:\n\nFirst, apparent calibration can mask significant domain-specific miscalibration. Language models may appear well-calibrated when evaluated on broad distributions while hiding substantial miscalibration within specific knowledge domains - for example, systematic overconfidence in mathematics might balance out systematic underconfidence in history, creating an illusion of good overall calibration <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This phenomenon makes it difficult to rely on aggregate calibration metrics and requires more granular evaluation across different knowledge \"slices.\"\n\nSecond, the alignment process itself can undermine calibration. Models fine-tuned with reinforcement learning from human feedback (RLHF) and other alignment techniques often show degraded calibration compared to their pre-trained versions <Paper corpusId=\"272987064\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. While these alignment processes successfully make models more helpful and harmless, they frequently introduce overconfidence problems where the model's expressed confidence fails to match its actual correctness rate <Paper corpusId=\"268876453\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This creates a challenging tension between making models more aligned with human values and maintaining their calibration properties.\n\nThird, calibration methods must contend with the inherent complexity of language as an evaluation domain. Unlike classification tasks with clear right/wrong answers, language generation involves nuanced evaluations of correctness that can be subjective or context-dependent <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This ambiguity makes it challenging to establish ground truth for calibration assessment and complicates the development of reliable confidence estimation methods.\n\nFinally, there are practical implementation challenges in deploying well-calibrated models. Calibration approaches often require additional computation or post-processing steps that may not be feasible in real-time applications. Finding methods that balance computational efficiency with calibration quality remains an open research challenge <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)."
                        ],
                        "paper": {
                            "corpus_id": 268723623,
                            "title": "Few-Shot Recalibration of Language Models",
                            "authors": [
                                {
                                    "authorId": "2293910776",
                                    "name": "Xiang Lisa Li"
                                },
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.68994140625
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration\u2014how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly."
                        ],
                        "paper": {
                            "corpus_id": 272987064,
                            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
                            "authors": [
                                {
                                    "authorId": "2285068366",
                                    "name": "Johnathan Xie"
                                },
                                {
                                    "authorId": "2111073657",
                                    "name": "Annie S. Chen"
                                },
                                {
                                    "authorId": "2260289273",
                                    "name": "Yoonho Lee"
                                },
                                {
                                    "authorId": "2260106796",
                                    "name": "Eric Mitchell"
                                },
                                {
                                    "authorId": "2279838067",
                                    "name": "Chelsea Finn"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 17
                        },
                        "score": 0.685546875
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate."
                        ],
                        "paper": {
                            "corpus_id": 268876453,
                            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
                            "authors": [
                                {
                                    "authorId": "2254324716",
                                    "name": "Mozhi Zhang"
                                },
                                {
                                    "authorId": "2255220422",
                                    "name": "Mianqiu Huang"
                                },
                                {
                                    "authorId": "2294723713",
                                    "name": "Rundong Shi"
                                },
                                {
                                    "authorId": "2295100870",
                                    "name": "Linsen Guo"
                                },
                                {
                                    "authorId": "2294823462",
                                    "name": "Chong Peng"
                                },
                                {
                                    "authorId": "2296600123",
                                    "name": "Peng Yan"
                                },
                                {
                                    "authorId": "2278545938",
                                    "name": "Yaqian Zhou"
                                },
                                {
                                    "authorId": "2294872225",
                                    "name": "Xipeng Qiu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0.6376953125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.05907
    }
}