{
    "query": "Could you references work on multi document summarization",
    "user_id": "lib_user",
    "task_id": "bd5a90e8-2e29-49f8-82a2-6a69312597b1",
    "timestamp": "2025-06-23T23:56:42.061665",
    "n_retrieval": 256,
    "n_retrieved": 229,
    "n_candidates": 38,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.64575,
    "decomposed_query": {
        "rewritten_query": "Multi-document summarization.",
        "keyword_query": "multi document summarization",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009318,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Summarization of Investment Reports Using Pre-trained Model",
            "venue": "IIAI International Conference on Advanced Applied Informatics",
            "year": 2023,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.01744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2879326",
                    "name": "Hiroki Sakaji"
                },
                {
                    "authorId": "2276796103",
                    "name": "Ryotaro Kobayashi"
                },
                {
                    "authorId": "2276798525",
                    "name": "Kiyoshi Izumi"
                },
                {
                    "authorId": "2276797477",
                    "name": "Hiroyuki Mitsugi"
                },
                {
                    "authorId": "2276798124",
                    "name": "Wataru Kuramoto"
                }
            ],
            "abstract": "In this paper, we attempt to summarize monthly reports as investment reports. Fund managers have a wide range of tasks, one of which is the preparation of investment reports. In addition to preparing monthly reports on fund management, fund managers prepare management reports that summarize these monthly reports every six months or once a year. The preparation of fund reports is a labor-intensive and time-consuming task. Therefore, in this paper, we tackle investment summarization from monthly reports using transformer-based models. There are two main types of summarization methods: extractive summarization and abstractive summarization, and this study constructs both methods and examines which is more useful in summarizing investment reports.",
            "corpus_id": 266599825,
            "sentences": [
                {
                    "corpus_id": "266599825",
                    "title": "Summarization of Investment Reports Using Pre-trained Model",
                    "text": "Related research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain [6]. Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization [7]. Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset [8]. Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing [10]. They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries [11]. Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries [12]. Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words [13]. Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature [14]. \n\nAs related work of extractive summarization, there is research by Cui et al. [15]. They proposed extractive summarization that can summarize long-form documents without content loss. Xu et al. proposed the neural network framework for extractive and compressive summarization [16]. \n\nAs related work of abstractive summarization, there is research by Nallapati et al. [17].",
                    "score": 0.6543664239361845,
                    "section_title": "VII. RELATED WORKS",
                    "char_start_offset": 9491,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 79
                        },
                        {
                            "start": 80,
                            "end": 241
                        },
                        {
                            "start": 242,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 846
                        },
                        {
                            "start": 847,
                            "end": 1010
                        },
                        {
                            "start": 1011,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1388
                        },
                        {
                            "start": 1389,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1702
                        },
                        {
                            "start": 1705,
                            "end": 1787
                        },
                        {
                            "start": 1788,
                            "end": 1887
                        },
                        {
                            "start": 1888,
                            "end": 1986
                        },
                        {
                            "start": 1989,
                            "end": 2078
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 237,
                            "end": 240,
                            "matchedPaperCorpusId": "248780330"
                        },
                        {
                            "start": 370,
                            "end": 373,
                            "matchedPaperCorpusId": "49210924"
                        },
                        {
                            "start": 471,
                            "end": 474,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 841,
                            "end": 845,
                            "matchedPaperCorpusId": "52011473"
                        },
                        {
                            "start": 1153,
                            "end": 1157,
                            "matchedPaperCorpusId": "170079112"
                        },
                        {
                            "start": 1383,
                            "end": 1387,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1521,
                            "end": 1525,
                            "matchedPaperCorpusId": "220045815"
                        },
                        {
                            "start": 1697,
                            "end": 1701,
                            "matchedPaperCorpusId": "233231380"
                        },
                        {
                            "start": 1782,
                            "end": 1786,
                            "matchedPaperCorpusId": "235097475"
                        },
                        {
                            "start": 1981,
                            "end": 1985,
                            "matchedPaperCorpusId": "59599804"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: Summarization of Investment Reports Using Pre-trained Model\n# Venue: IIAI International Conference on Advanced Applied Informatics\n# Authors: Hiroki Sakaji, Ryotaro Kobayashi, Kiyoshi Izumi, Hiroyuki Mitsugi, Wataru Kuramoto\n## Abstract\nIn this paper, we attempt to summarize monthly reports as investment reports. Fund managers have a wide range of tasks, one of which is the preparation of investment reports. In addition to preparing monthly reports on fund management, fund managers prepare management reports that summarize these monthly reports every six months or once a year. The preparation of fund reports is a labor-intensive and time-consuming task. Therefore, in this paper, we tackle investment summarization from monthly reports using transformer-based models. There are two main types of summarization methods: extractive summarization and abstractive summarization, and this study constructs both methods and examines which is more useful in summarizing investment reports.\n## VII. RELATED WORKS\nRelated research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain [6]. Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization [7]. Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset [8]. Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing [10]. They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries [11]. Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries [12]. Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words [13]. Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature [14]. \n\nAs related work of extractive summarization, there is research by Cui et al. [15]. They proposed extractive summarization that can summarize long-form documents without content loss. Xu et al. proposed the neural network framework for extractive and compressive summarization [16]. \n\nAs related work of abstractive summarization, there is research by Nallapati et al. [17].",
            "reference_string": "[266599825 | Sakaji et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
            "venue": "Findings",
            "year": 2020,
            "reference_count": 27,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.231.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.findings-emnlp.231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491212422",
                    "name": "Hanqi Jin"
                },
                {
                    "authorId": "145078589",
                    "name": "Xiaojun Wan"
                }
            ],
            "abstract": "Single-document and multi-document summarizations are very closely related in both task definition and solution method. In this work, we propose to improve neural abstractive multi-document summarization by jointly learning an abstractive single-document summarizer. We build a unified model for single-document and multi-document summarizations by fully sharing the encoder and decoder and utilizing a decoding controller to aggregate the decoder\u2019s outputs for multiple input documents. We evaluate our model on two multi-document summarization datasets: Multi-News and DUC-04. Experimental results show the efficacy of our approach, and it can substantially outperform several strong baselines. We also verify the helpfulness of single-document summarization to abstractive multi-document summarization task.",
            "corpus_id": 226283949,
            "sentences": [
                {
                    "corpus_id": "226283949",
                    "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
                    "text": "The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Recently, two multi-document summarization datasets have been proposed, one for very long input, aimed at generating Wikipedia  and another dedicated to generating a comprehensive summary of multiple real-time news (Fabbri et al., 2019). Several works have begun to explore abstractive multi-document summarization.  concatenated multiple source documents into a long flat text and modeled multidocument summarization as a long sequence-tosequence task. Liu and Lapata (2019) represented cross-document relationships via an attention mechanism that allows sharing information as opposed to simply concatenating text spans and processing them as a flat sequence. Fabbri et al. (2019) incorporated MMR into a hierarchical pointer-generator network to address the information redundancy in multi-document summarization. The above works were all trained and tested on multi-document summarization corpus.",
                    "score": 0.7051649907790778,
                    "section_title": "Multi-Document Summarization",
                    "char_start_offset": 5305,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 451,
                            "end": 474,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 499,
                            "end": 521,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 738,
                            "end": 759,
                            "matchedPaperCorpusId": "174799390"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.958984375
                },
                {
                    "corpus_id": "226283949",
                    "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
                    "text": "Document summarization aims at producing a fluent, condensed summary for the given document or document set. It involves identifying important information and filtering out redundant information from input sources. While single-document summarization takes a single source document as input, multi-document summarization requires producing a summary from a cluster of thematically related documents. There are two primary methodologies for document summarization: extractive and abstractive. Extractive methods directly select important sentences from the original documents, which are relatively simple but face the drawbacks of information redundancy and incoherence between sentences. Abstractive methods enable generating new words, phrases, and sentences, which are able to generate better summaries with higher readability and conciseness. In this paper, we focus on abstractive document summarization.\n\nEmpowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017;Paulus et al., 2018;Tan et al., 2017;\u00c7 elikyilmaz et al., 2018). Compared with single-document summarization, annotated multi-document summarization datasets are often scarce. Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization.\n\nSingle-document and multi-document summarizations are very closely related in both task definition and solution method (Wan, 2010). Both tasks need to deal with document-level input, identify the important content of documents, and paraphrase the important information to generate the summary, while the main difference is that multi-document summarization involves summarizing multiple input documents. Since the two tasks are closely related, it is promising to learn for two summarization tasks jointly. Compared",
                    "score": 0.6140678498956181,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1101,
                            "end": 1119,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 1119,
                            "end": 1139,
                            "matchedPaperCorpusId": "21850704"
                        },
                        {
                            "start": 1139,
                            "end": 1156,
                            "matchedPaperCorpusId": "26698484"
                        },
                        {
                            "start": 1156,
                            "end": 1182,
                            "matchedPaperCorpusId": "4406182"
                        },
                        {
                            "start": 1628,
                            "end": 1650,
                            "matchedPaperCorpusId": "52053741"
                        },
                        {
                            "start": 1970,
                            "end": 1981,
                            "matchedPaperCorpusId": "17224077"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95654296875
                },
                {
                    "corpus_id": "226283949",
                    "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
                    "text": "level input, identify the important content of documents, and paraphrase the important information to generate the summary, while the main difference is that multi-document summarization involves summarizing multiple input documents. Since the two tasks are closely related, it is promising to learn for two summarization tasks jointly. Compared with single-document summarization, multi-document summarization needs to handle multiple input documents. A simple method is to concatenate multiple documents into a long flat text and treat it as a long sequence-tosequence task. However, it blurs the boundaries between documents and loses the hierarchy within the document cluster. It is natural to regard multidocument summarization as a two-stage process of summarizing every single document and then merging multiple summaries. Nevertheless, this process is quite trivial, and it is difficult to utilize multi-document summarization corpus to train the single-document summarization model. Furthermore, the synthesis of multiple summaries involves eliminating redundant parts and organizing related paragraphs or sentences, which are also challenges to be solved.\n\nIn this work, we propose a joint learning approach to improve neural abstractive multi-document summarization by using singledocument summarization corpus to address these issues. Our approach first uses a shared document encoder to encode each document in the document set, then uses a shared decoder to predict the word probabilities for each document, and finally applies a decoding controller to aggregate all output probabilities from the summary decoder to make the final prediction at each decoding step. The shared encoder and decoder are jointly trained on the single document summarization data. In this way, we can unify single-document and multi-document summarizations into one architecture simultaneously, and make better use of single-document and multi-document corpora, so that both tasks can benefit from joint learning, especially for the multidocument summarization task.\n\nWe evaluate our approach on the benchmark multi-document summarization datasets, Multi-News and DUC-04, and it brings substantial improvements over several strong baselines for multi-document summarization. We leverage CNN/DailyMail, a single-document summarization dataset, to perform joint learning with Multi-News",
                    "score": 0.7006636776838184,
                    "section_title": "Introduction",
                    "char_start_offset": 2036,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                },
                {
                    "corpus_id": "226283949",
                    "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
                    "text": "the benchmark multi-document summarization datasets, Multi-News and DUC-04, and it brings substantial improvements over several strong baselines for multi-document summarization. We leverage CNN/DailyMail, a single-document summarization dataset, to perform joint learning with Multi-News. We also test the performance on CNN/DailyMail test set, and joint learning also brings certain performance improvement for the single-document summarization baselines.\n\nIn summary, we make the following contributions in this paper:\n\n\u2022 To the best of our knowledge, we are the first to explore joint learning for neural abstractive single-document and multi-document summarizations.\n\n\u2022 We propose a unified model by fully sharing encoder and decoder and utilizing a decoding controller to aggregate the decoder's outputs for multiple input documents.\n\n\u2022 Experimental results show that our approach substantially outperforms several strong baselines, and single document summarization is verified to be very helpful to neural abstractive multi-document summarization. Our code is publicly available at https://github.com/ zhongxia96/MDS-and-SDS.\n\n2 Related Work",
                    "score": 0.5495265038076145,
                    "section_title": "Introduction",
                    "char_start_offset": 4124,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91796875
                }
            ],
            "relevance_judgement": 0.958984375,
            "relevance_judgment_input_expanded": "# Title: Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization\n# Venue: Findings\n# Authors: Hanqi Jin, Xiaojun Wan\n## Abstract\nSingle-document and multi-document summarizations are very closely related in both task definition and solution method. In this work, we propose to improve neural abstractive multi-document summarization by jointly learning an abstractive single-document summarizer. We build a unified model for single-document and multi-document summarizations by fully sharing the encoder and decoder and utilizing a decoding controller to aggregate the decoder\u2019s outputs for multiple input documents. We evaluate our model on two multi-document summarization datasets: Multi-News and DUC-04. Experimental results show the efficacy of our approach, and it can substantially outperform several strong baselines. We also verify the helpfulness of single-document summarization to abstractive multi-document summarization task.\n## Introduction\nDocument summarization aims at producing a fluent, condensed summary for the given document or document set. It involves identifying important information and filtering out redundant information from input sources. While single-document summarization takes a single source document as input, multi-document summarization requires producing a summary from a cluster of thematically related documents. There are two primary methodologies for document summarization: extractive and abstractive. Extractive methods directly select important sentences from the original documents, which are relatively simple but face the drawbacks of information redundancy and incoherence between sentences. Abstractive methods enable generating new words, phrases, and sentences, which are able to generate better summaries with higher readability and conciseness. In this paper, we focus on abstractive document summarization.\n\nEmpowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017;Paulus et al., 2018;Tan et al., 2017;\u00c7 elikyilmaz et al., 2018). Compared with single-document summarization, annotated multi-document summarization datasets are often scarce. Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization.\n\nSingle-document and multi-document summarizations are very closely related in both task definition and solution method (Wan, 2010). Both tasks need to deal with document-level input, identify the important content of documents, and paraphrase the important information to generate the summary, while the main difference is that multi-document summarization involves summarizing multiple input documents. Since the two tasks are closely related, it is promising to learn for two summarization tasks jointly. Compared\n...\nlevel input, identify the important content of documents, and paraphrase the important information to generate the summary, while the main difference is that multi-document summarization involves summarizing multiple input documents. Since the two tasks are closely related, it is promising to learn for two summarization tasks jointly. Compared with single-document summarization, multi-document summarization needs to handle multiple input documents. A simple method is to concatenate multiple documents into a long flat text and treat it as a long sequence-tosequence task. However, it blurs the boundaries between documents and loses the hierarchy within the document cluster. It is natural to regard multidocument summarization as a two-stage process of summarizing every single document and then merging multiple summaries. Nevertheless, this process is quite trivial, and it is difficult to utilize multi-document summarization corpus to train the single-document summarization model. Furthermore, the synthesis of multiple summaries involves eliminating redundant parts and organizing related paragraphs or sentences, which are also challenges to be solved.\n\nIn this work, we propose a joint learning approach to improve neural abstractive multi-document summarization by using singledocument summarization corpus to address these issues. Our approach first uses a shared document encoder to encode each document in the document set, then uses a shared decoder to predict the word probabilities for each document, and finally applies a decoding controller to aggregate all output probabilities from the summary decoder to make the final prediction at each decoding step. The shared encoder and decoder are jointly trained on the single document summarization data. In this way, we can unify single-document and multi-document summarizations into one architecture simultaneously, and make better use of single-document and multi-document corpora, so that both tasks can benefit from joint learning, especially for the multidocument summarization task.\n\nWe evaluate our approach on the benchmark multi-document summarization datasets, Multi-News and DUC-04, and it brings substantial improvements over several strong baselines for multi-document summarization. We leverage CNN/DailyMail, a single-document summarization dataset, to perform joint learning with Multi-News\n...\nthe benchmark multi-document summarization datasets, Multi-News and DUC-04, and it brings substantial improvements over several strong baselines for multi-document summarization. We leverage CNN/DailyMail, a single-document summarization dataset, to perform joint learning with Multi-News. We also test the performance on CNN/DailyMail test set, and joint learning also brings certain performance improvement for the single-document summarization baselines.\n\nIn summary, we make the following contributions in this paper:\n\n\u2022 To the best of our knowledge, we are the first to explore joint learning for neural abstractive single-document and multi-document summarizations.\n\n\u2022 We propose a unified model by fully sharing encoder and decoder and utilizing a decoding controller to aggregate the decoder's outputs for multiple input documents.\n\n\u2022 Experimental results show that our approach substantially outperforms several strong baselines, and single document summarization is verified to be very helpful to neural abstractive multi-document summarization. Our code is publicly available at https://github.com/ zhongxia96/MDS-and-SDS.\n\n2 Related Work\n\n## Multi-Document Summarization\nThe methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Recently, two multi-document summarization datasets have been proposed, one for very long input, aimed at generating Wikipedia  and another dedicated to generating a comprehensive summary of multiple real-time news (Fabbri et al., 2019). Several works have begun to explore abstractive multi-document summarization.  concatenated multiple source documents into a long flat text and modeled multidocument summarization as a long sequence-tosequence task. Liu and Lapata (2019) represented cross-document relationships via an attention mechanism that allows sharing information as opposed to simply concatenating text spans and processing them as a flat sequence. Fabbri et al. (2019) incorporated MMR into a hierarchical pointer-generator network to address the information redundancy in multi-document summarization. The above works were all trained and tested on multi-document summarization corpus.",
            "reference_string": "[226283949 | Jin et al. | 2020 | Citations: 15]"
        },
        {
            "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 29,
            "citation_count": 99,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.556.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.acl-main.556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491212422",
                    "name": "Hanqi Jin"
                },
                {
                    "authorId": "1751960",
                    "name": "Tian-ming Wang"
                },
                {
                    "authorId": "145078589",
                    "name": "Xiaojun Wan"
                }
            ],
            "abstract": "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.",
            "corpus_id": 220045815,
            "sentences": [
                {
                    "corpus_id": "220045815",
                    "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
                    "text": "Document summarization aims at producing a fluent, condensed summary for given documents. Single document summarization has shown promising results with sequence-to-sequence models that encode a source document and then decode it into a summary (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u00b8elikyilmaz et al., 2018). Multi-document summarization requires producing a summary from a cluster of thematically related documents, where the given documents complement and overlap each other. Multi-document summarization involves identifying important information and filtering out redundant information from multiple input sources. \n\nThere are two primary methodologies for multidocument summarization: extractive and abstractive. Extractive methods directly select important sentences from the original, which are relatively simple. Cao et al. (2015) rank sentences with a recursive neural network. Yasunaga et al. (2017) employ a Graph Convolutional Network (GCN) to incorporate sentence relation graphs to improve the performance for the extractive summarization. Abstractive methods can generate new words and new sentences, but it is technically more difficult than extractive methods. Some works on multidocument summarization simply concatenate multiple source documents into a long flat sequence and model multi-document summarization as a long sequence-to-sequence task (Liu et al., 2018;Fabbri et al., 2019). However, these approaches don't take the hierarchical structure of document clusters into account, while the too-long input often leads to the degradation in document summarization (Cohan et al., 2018;Liu and Lapata, 2019). Recently, hierarchical frameworks have shown their effectiveness on multi-document summarization (Zhang et al., 2018;Liu and Lapata, 2019). These approaches usually use multiple encoders to model hierarchical relationships in the discourse structure, but other methods to incorporate the structural semantic knowledge have not been explored. The combination of extractive and abstractive has been explored in single document summarization. Chen and Bansal (2018) use the extracted sentences as the input of the abstractive summarization.",
                    "score": 0.6273599763123026,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 89
                        },
                        {
                            "start": 90,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 502
                        },
                        {
                            "start": 503,
                            "end": 643
                        },
                        {
                            "start": 646,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1430
                        },
                        {
                            "start": 1431,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1794
                        },
                        {
                            "start": 1795,
                            "end": 1996
                        },
                        {
                            "start": 1997,
                            "end": 2094
                        },
                        {
                            "start": 2095,
                            "end": 2192
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 245,
                            "end": 263,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 263,
                            "end": 283,
                            "matchedPaperCorpusId": "21850704"
                        },
                        {
                            "start": 283,
                            "end": 305,
                            "matchedPaperCorpusId": "52144157"
                        },
                        {
                            "start": 305,
                            "end": 332,
                            "matchedPaperCorpusId": "4406182"
                        },
                        {
                            "start": 846,
                            "end": 863,
                            "matchedPaperCorpusId": "10675728"
                        },
                        {
                            "start": 912,
                            "end": 934,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 1409,
                            "end": 1429,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 1612,
                            "end": 1632,
                            "matchedPaperCorpusId": "4894594"
                        },
                        {
                            "start": 1632,
                            "end": 1653,
                            "matchedPaperCorpusId": "170079112"
                        },
                        {
                            "start": 1772,
                            "end": 1793,
                            "matchedPaperCorpusId": "170079112"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94775390625
                },
                {
                    "corpus_id": "220045815",
                    "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
                    "text": "The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u00b8elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model. Recently, two large scale multi-document summarization datasets have been proposed, one for very long input, aimed at generating Wikipedia (Liu et al., 2018) and another dedicated to generating a comprehensive summarization of multiple real-time news (Fabbri et al., 2019). Liu et al. (2018) concatenated multiple source documents into a long flat text and introduced a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder-decoder architectures. Liu and Lapata (2019) introduced intermediate document representations and simply add the document representations to word representations for modeling the cross-document relationships. Compared with our proposed multi-granularity method, Liu and Lapata (2019) inclined to the traditional bottomup hierarchical method and don't effectively utilize the hierarchical representations while ignoring the hierarchical relationships of sentences.",
                    "score": 0.6157933056981211,
                    "section_title": "Related Work",
                    "char_start_offset": 4798,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 104
                        },
                        {
                            "start": 105,
                            "end": 339
                        },
                        {
                            "start": 340,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 864
                        },
                        {
                            "start": 865,
                            "end": 1039
                        },
                        {
                            "start": 1040,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1535
                        },
                        {
                            "start": 1536,
                            "end": 1762
                        },
                        {
                            "start": 1763,
                            "end": 1948
                        },
                        {
                            "start": 1949,
                            "end": 2203
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 451,
                            "end": 474,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 499,
                            "end": 521,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 624,
                            "end": 642,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 642,
                            "end": 662,
                            "matchedPaperCorpusId": "21850704"
                        },
                        {
                            "start": 662,
                            "end": 684,
                            "matchedPaperCorpusId": "52144157"
                        },
                        {
                            "start": 684,
                            "end": 711,
                            "matchedPaperCorpusId": "4406182"
                        },
                        {
                            "start": 1040,
                            "end": 1062,
                            "matchedPaperCorpusId": "52053741"
                        },
                        {
                            "start": 1513,
                            "end": 1534,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 1763,
                            "end": 1784,
                            "matchedPaperCorpusId": "170079112"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93994140625
                }
            ],
            "relevance_judgement": 0.94775390625,
            "relevance_judgment_input_expanded": "# Title: Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Hanqi Jin, Tian-ming Wang, Xiaojun Wan\n## Abstract\nIn this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.\n## Introduction\nDocument summarization aims at producing a fluent, condensed summary for given documents. Single document summarization has shown promising results with sequence-to-sequence models that encode a source document and then decode it into a summary (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u00b8elikyilmaz et al., 2018). Multi-document summarization requires producing a summary from a cluster of thematically related documents, where the given documents complement and overlap each other. Multi-document summarization involves identifying important information and filtering out redundant information from multiple input sources. \n\nThere are two primary methodologies for multidocument summarization: extractive and abstractive. Extractive methods directly select important sentences from the original, which are relatively simple. Cao et al. (2015) rank sentences with a recursive neural network. Yasunaga et al. (2017) employ a Graph Convolutional Network (GCN) to incorporate sentence relation graphs to improve the performance for the extractive summarization. Abstractive methods can generate new words and new sentences, but it is technically more difficult than extractive methods. Some works on multidocument summarization simply concatenate multiple source documents into a long flat sequence and model multi-document summarization as a long sequence-to-sequence task (Liu et al., 2018;Fabbri et al., 2019). However, these approaches don't take the hierarchical structure of document clusters into account, while the too-long input often leads to the degradation in document summarization (Cohan et al., 2018;Liu and Lapata, 2019). Recently, hierarchical frameworks have shown their effectiveness on multi-document summarization (Zhang et al., 2018;Liu and Lapata, 2019). These approaches usually use multiple encoders to model hierarchical relationships in the discourse structure, but other methods to incorporate the structural semantic knowledge have not been explored. The combination of extractive and abstractive has been explored in single document summarization. Chen and Bansal (2018) use the extracted sentences as the input of the abstractive summarization.\n\n## Related Work\nThe methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u00b8elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model. Recently, two large scale multi-document summarization datasets have been proposed, one for very long input, aimed at generating Wikipedia (Liu et al., 2018) and another dedicated to generating a comprehensive summarization of multiple real-time news (Fabbri et al., 2019). Liu et al. (2018) concatenated multiple source documents into a long flat text and introduced a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder-decoder architectures. Liu and Lapata (2019) introduced intermediate document representations and simply add the document representations to word representations for modeling the cross-document relationships. Compared with our proposed multi-granularity method, Liu and Lapata (2019) inclined to the traditional bottomup hierarchical method and don't effectively utilize the hierarchical representations while ignoring the hierarchical relationships of sentences.",
            "reference_string": "[220045815 | Jin et al. | 2020 | Citations: 99]"
        },
        {
            "title": "Multi-Document Summarization By Sentence Extraction",
            "venue": "",
            "year": 2000,
            "reference_count": 38,
            "citation_count": 447,
            "influential_citation_count": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.3115/1117575.1117580",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/W00-0405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40561307",
                    "name": "J. Goldstein"
                },
                {
                    "authorId": "1751139",
                    "name": "Vibhu Mittal"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                },
                {
                    "authorId": "50404544",
                    "name": "M. Kantrowitz"
                }
            ],
            "abstract": "This paper discusses a text extraction approach to multi-document summarization that builds on single-document summarization methods by using additional, available information about the document set as a whole and the relationships between the documents. Multi-document summarization differs from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries. Our approach addresses these issues by using domain-independent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements.",
            "corpus_id": 8294822,
            "sentences": [
                {
                    "corpus_id": "8294822",
                    "title": "Multi-Document Summarization By Sentence Extraction",
                    "text": "Multidocument summarization -capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones -are likely to be essential in such situations. Ideally, multi-document summaries should contain the key shared relevant information among all the documents only once, plus other information unique to some of the individual documents that are directly relevant to the user's query. \n\nThough many of the same techniques used in singledocument summarization can also be used in multidocument summarization, there are at least four significant differences: \n\n1. The degree of redundancy in information contained within a group of topically-related articles is much higher than the degree of redundancy within an article, as each article is apt to describe the main point as well as necessary shared background. Hence anti-redundancy methods are more crucial. \n\n2. A group of articles may contain a temporal dimension, typical in a stream of news reports about an unfolding event. Here later information may override earlier more tentative or incomplete accounts. \n\n3. The compression ratio (i.e. the size of the summary with respect to the size of the document set) will typically be much smaller for collections of dozens or hundreds of topically related documents than for single document summaries. The SUMMAC evaluation (TIPSTER, 1998a) tested 10% compression summaries, but in our work summarizing 200document clusters, we find that compression to the 1% or 0.1% level is required. Summarization becomes significantly more difficult when compression demands increase. \n\n4. The co-reference problem in summarization presents even greater challenges for multidocument than for single-document summarization (Baldwin and Morton, 1998). \n\nThis paper discusses an approach to multi-document summarization that builds on previous work in single- \n\ndocument summarization by using additional, available information about the document set as a whole, the relationships between the documents, as well as individual documents.",
                    "score": 0.5882761847590517,
                    "section_title": "Introduction",
                    "char_start_offset": 1744,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 430
                        },
                        {
                            "start": 433,
                            "end": 602
                        },
                        {
                            "start": 605,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 904
                        },
                        {
                            "start": 907,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1108
                        },
                        {
                            "start": 1111,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1532
                        },
                        {
                            "start": 1533,
                            "end": 1618
                        },
                        {
                            "start": 1621,
                            "end": 1783
                        },
                        {
                            "start": 1786,
                            "end": 1890
                        },
                        {
                            "start": 1893,
                            "end": 2067
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1756,
                            "end": 1782,
                            "matchedPaperCorpusId": "15450389"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9462890625
                },
                {
                    "corpus_id": "8294822",
                    "title": "Multi-Document Summarization By Sentence Extraction",
                    "text": "This paper discusses a text extraction approach to multi-document summarization that builds on single-document summarization methods by using additional, available information about the document set as a whole and the relationships between the documents. Multi-document summarization differs from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries. Our approach addresses these issues by using domain-independent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements.",
                    "score": 0.6758557327388102,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92919921875
                },
                {
                    "corpus_id": "8294822",
                    "title": "Multi-Document Summarization By Sentence Extraction",
                    "text": "Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997;Goldstein and Carbonell, 1998;TIPSTER, 1998b;Radev and McKeown, 1998;Mani and Bloedorn, 1999;McKeown et al., .!999;Stein et al., 1999). These include comparing templates filled in by extracting information -using specialized, domain specific knowledge sources -from the doc-\"ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com--\u2022 paring named-entities -extracted using specialized lists between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related lexical items (identity mappings, synonyms, hypernyms, etc.) to extract text spans from the document set (Mani and Bloedern, 1997). Another system (Stein et al., 1999) creates a multi-document summary from multiple single document summaries, an approach that can be sub-optimal in some cases, due to the fact that the process of generating the final multi-document summary takes as input the individual summaries and not the complete documents. (Particularly if the single-document summaries can contain much overlapping information.) The Columbia University system (McKeown et al., 1999) creates a multi-document summary using machine learning and statistical techniques to identify similar sections and language generation to reformulate the summary. The focus of our approach is a multi-document system that can quickly summarize large clusters of similar documents (on the order of thousands) while providing the key relevant useful information or pointers to such information. Our system (1) primarily uses only domainindependent techniques, based mainly on fast, statistical processing, (2) explicitly deals with the issue of reducing redundancy without eliminating potential relevant information, and (3) contains parameterized modules, so that different genres or corpora characteristics can be taken into account easily.",
                    "score": 0.5876043317090431,
                    "section_title": "Background and Related Work",
                    "char_start_offset": 5081,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 939
                        },
                        {
                            "start": 940,
                            "end": 1252
                        },
                        {
                            "start": 1253,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1560
                        },
                        {
                            "start": 1561,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 2137
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 119,
                            "end": 144,
                            "matchedPaperCorpusId": "9177142"
                        },
                        {
                            "start": 144,
                            "end": 174,
                            "matchedPaperCorpusId": "60778976"
                        },
                        {
                            "start": 259,
                            "end": 278,
                            "matchedPaperCorpusId": "232822009"
                        },
                        {
                            "start": 913,
                            "end": 938,
                            "matchedPaperCorpusId": "9177142"
                        },
                        {
                            "start": 955,
                            "end": 975,
                            "matchedPaperCorpusId": "232822009"
                        },
                        {
                            "start": 1374,
                            "end": 1396,
                            "matchedPaperCorpusId": "8115414"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                },
                {
                    "corpus_id": "8294822",
                    "title": "Multi-Document Summarization By Sentence Extraction",
                    "text": "Conclusions and Future Work \n\nThis paper presented a statistical method of generating extraction based multi-document summaries. It builds upon previous work in single-document summarization and takes into account some of the major differences between single-document and multi-document summarization: (i) the need to carefully eliminate redundant information from multiple documents, and achieve high compression ratios, (ii) take into account information about document and passage similarities, and weight different passages accordingly, and (iii) take temporal information into account. \n\nOur approach differs from others in several ways: it is completely domain-independent, is based mainly on fast, statistical processing, it attempts to maximize the novelty of the information being selected, and different genres or corpora characteristics can be taken into account easily. Since our system is not based on the use of sophisticated natural language understanding or information extraction techniques, summaries lack co-reference resolution, passages may be disjoint from one another, and in some cases may have false implicature. \n\nIn future work, we will integrate work on multidocument summarization with work on clustering to provide summaries for clusters produced by topic detection and tracking. We also plan to investigate how to generate coherent temporally based event summaries. We will also investigate how users can effectively use multidocument summarization through interactive interfaces to browse and explore large document sets.",
                    "score": 0.5593343958724772,
                    "section_title": "7",
                    "char_start_offset": 25267,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 27
                        },
                        {
                            "start": 30,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 590
                        },
                        {
                            "start": 593,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 1137
                        },
                        {
                            "start": 1140,
                            "end": 1309
                        },
                        {
                            "start": 1310,
                            "end": 1396
                        },
                        {
                            "start": 1397,
                            "end": 1553
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.9462890625,
            "relevance_judgment_input_expanded": "# Title: Multi-Document Summarization By Sentence Extraction\n# Venue: \n# Authors: J. Goldstein, Vibhu Mittal, J. Carbonell, M. Kantrowitz\n## Abstract\nThis paper discusses a text extraction approach to multi-document summarization that builds on single-document summarization methods by using additional, available information about the document set as a whole and the relationships between the documents. Multi-document summarization differs from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries. Our approach addresses these issues by using domain-independent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements.\n## Introduction\nMultidocument summarization -capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones -are likely to be essential in such situations. Ideally, multi-document summaries should contain the key shared relevant information among all the documents only once, plus other information unique to some of the individual documents that are directly relevant to the user's query. \n\nThough many of the same techniques used in singledocument summarization can also be used in multidocument summarization, there are at least four significant differences: \n\n1. The degree of redundancy in information contained within a group of topically-related articles is much higher than the degree of redundancy within an article, as each article is apt to describe the main point as well as necessary shared background. Hence anti-redundancy methods are more crucial. \n\n2. A group of articles may contain a temporal dimension, typical in a stream of news reports about an unfolding event. Here later information may override earlier more tentative or incomplete accounts. \n\n3. The compression ratio (i.e. the size of the summary with respect to the size of the document set) will typically be much smaller for collections of dozens or hundreds of topically related documents than for single document summaries. The SUMMAC evaluation (TIPSTER, 1998a) tested 10% compression summaries, but in our work summarizing 200document clusters, we find that compression to the 1% or 0.1% level is required. Summarization becomes significantly more difficult when compression demands increase. \n\n4. The co-reference problem in summarization presents even greater challenges for multidocument than for single-document summarization (Baldwin and Morton, 1998). \n\nThis paper discusses an approach to multi-document summarization that builds on previous work in single- \n\ndocument summarization by using additional, available information about the document set as a whole, the relationships between the documents, as well as individual documents.\n\n## Background and Related Work\nSome of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997;Goldstein and Carbonell, 1998;TIPSTER, 1998b;Radev and McKeown, 1998;Mani and Bloedorn, 1999;McKeown et al., .!999;Stein et al., 1999). These include comparing templates filled in by extracting information -using specialized, domain specific knowledge sources -from the doc-\"ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com--\u2022 paring named-entities -extracted using specialized lists between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related lexical items (identity mappings, synonyms, hypernyms, etc.) to extract text spans from the document set (Mani and Bloedern, 1997). Another system (Stein et al., 1999) creates a multi-document summary from multiple single document summaries, an approach that can be sub-optimal in some cases, due to the fact that the process of generating the final multi-document summary takes as input the individual summaries and not the complete documents. (Particularly if the single-document summaries can contain much overlapping information.) The Columbia University system (McKeown et al., 1999) creates a multi-document summary using machine learning and statistical techniques to identify similar sections and language generation to reformulate the summary. The focus of our approach is a multi-document system that can quickly summarize large clusters of similar documents (on the order of thousands) while providing the key relevant useful information or pointers to such information. Our system (1) primarily uses only domainindependent techniques, based mainly on fast, statistical processing, (2) explicitly deals with the issue of reducing redundancy without eliminating potential relevant information, and (3) contains parameterized modules, so that different genres or corpora characteristics can be taken into account easily.\n\n## 7\nConclusions and Future Work \n\nThis paper presented a statistical method of generating extraction based multi-document summaries. It builds upon previous work in single-document summarization and takes into account some of the major differences between single-document and multi-document summarization: (i) the need to carefully eliminate redundant information from multiple documents, and achieve high compression ratios, (ii) take into account information about document and passage similarities, and weight different passages accordingly, and (iii) take temporal information into account. \n\nOur approach differs from others in several ways: it is completely domain-independent, is based mainly on fast, statistical processing, it attempts to maximize the novelty of the information being selected, and different genres or corpora characteristics can be taken into account easily. Since our system is not based on the use of sophisticated natural language understanding or information extraction techniques, summaries lack co-reference resolution, passages may be disjoint from one another, and in some cases may have false implicature. \n\nIn future work, we will integrate work on multidocument summarization with work on clustering to provide summaries for clusters produced by topic detection and tracking. We also plan to investigate how to generate coherent temporally based event summaries. We will also investigate how users can effectively use multidocument summarization through interactive interfaces to browse and explore large document sets.",
            "reference_string": "[8294822 | Goldstein et al. | 2000 | Citations: 447]"
        },
        {
            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.03978",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.03978, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064325789",
                    "name": "Aiswarya Sankar"
                },
                {
                    "authorId": "145934595",
                    "name": "Ankit R. Chadha"
                }
            ],
            "abstract": "Abstractive multi document summarization has evolved as a task through the basic sequence to sequence approaches to transformer and graph based techniques. Each of these approaches has primarily focused on the issues of multi document information synthe-sis and attention based approaches to extract salient information. A challenge that arises with multi document summarization which is not prevalent in single document summarization is the need to effectively summarize multiple documents that might have con\ufb02icting polarity, sentiment or subjective information about a given topic. In this paper we propose ACM, attribute conditioned multi document summarization,a model that incorporates attribute conditioning modules in order to decouple con\ufb02icting information by conditioning for a certain attribute in the output summary. This approach shows strong gains in ROUGE score over baseline multi document summarization approaches and shows gains in \ufb02uency, informativeness and reduction in repetitiveness as shown through a human annotation analysis study.",
            "corpus_id": 248571519,
            "sentences": [
                {
                    "corpus_id": "248571519",
                    "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                    "text": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013). These papers primarily aim to address de-duplicating information and learning relationships between the different topics shared across documents however none of these architectures are built to deal with conflicting information.",
                    "score": 0.5346926398026174,
                    "section_title": "Multi Document Summarization",
                    "char_start_offset": 8653,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1615
                        },
                        {
                            "start": 1616,
                            "end": 1844
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 205,
                            "end": 231,
                            "matchedPaperCorpusId": "577937"
                        },
                        {
                            "start": 241,
                            "end": 264,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 491,
                            "end": 508,
                            "matchedPaperCorpusId": "10112929"
                        },
                        {
                            "start": 1345,
                            "end": 1362,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1448,
                            "end": 1465,
                            "matchedPaperCorpusId": "218718706"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9453125
                },
                {
                    "corpus_id": "248571519",
                    "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                    "text": "Abstractive multi document summarization is the task of writing a single summary of the key points and content in multiple related documents. This task has evolved from research in single document abstractive and extractive summarization; however, it faces unique challenges due to input documents having duplicate and conflicting content as well a larger body of text. (Radev et al., 2000). This task has evolved from early approaches using sequence to sequence (Seq2Seq) neural architectures to transformer based architectures with the introduction of large-scale datasets (Liu et al., 2018), (Fabbri et al., 2019). Beyond the introduction of approaches now commonly used for single document abstractive summarization, cross document attention and graphs that capture relations between text in various documents have further improved the state of the art for multi document summarization tasks. (Liu et al., 2018), (Li et al., 2014). These graphs aim to better represent the inter dependencies between articles by representing text spans as nodes in the graph and capturing the relations between these sentences as edge weights. \n\nDespite the advances made with these approaches, a significant challenge remains in multi document summarization with respect to how to deal with contradictory information present in the multiple source documents. It is critical to both learn the relationships between different documents as well as to extract salient information that is consistent with the output viewpoint. This is a situation often faced with summarizing multiple news articles where different viewpoints on an issue can significantly change the semantic structure of the content present in each article making it challenging for the abstractive summarization model to learn the relationships between inconsistent or conflicting information. In this work we define conflicting opinions as a combined measure of the polarity and sentiment of text. By this definition, two pieces of text on the same topic that have a differing sentiment and/ or polarity are determined to have different viewpoints. This definition is used throughout the paper. \n\nThis paper proposes ACM, attribute conditioned multi document summarization, a novel approach to multi document summarization that incorporates an attribute conditioning module with an abstractive multi document summarization model in order to condition for a particular attribute when generating the multi document summary.",
                    "score": 0.5617418573685862,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 617
                        },
                        {
                            "start": 618,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1130
                        },
                        {
                            "start": 1133,
                            "end": 1346
                        },
                        {
                            "start": 1347,
                            "end": 1509
                        },
                        {
                            "start": 1510,
                            "end": 1845
                        },
                        {
                            "start": 1846,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2101
                        },
                        {
                            "start": 2102,
                            "end": 2147
                        },
                        {
                            "start": 2150,
                            "end": 2474
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 370,
                            "end": 390,
                            "matchedPaperCorpusId": "1320"
                        },
                        {
                            "start": 917,
                            "end": 934,
                            "matchedPaperCorpusId": "10112929"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                }
            ],
            "relevance_judgement": 0.9453125,
            "relevance_judgment_input_expanded": "# Title: ACM - Attribute Conditioning for Abstractive Multi Document Summarization\n# Venue: arXiv.org\n# Authors: Aiswarya Sankar, Ankit R. Chadha\n## Abstract\nAbstractive multi document summarization has evolved as a task through the basic sequence to sequence approaches to transformer and graph based techniques. Each of these approaches has primarily focused on the issues of multi document information synthe-sis and attention based approaches to extract salient information. A challenge that arises with multi document summarization which is not prevalent in single document summarization is the need to effectively summarize multiple documents that might have con\ufb02icting polarity, sentiment or subjective information about a given topic. In this paper we propose ACM, attribute conditioned multi document summarization,a model that incorporates attribute conditioning modules in order to decouple con\ufb02icting information by conditioning for a certain attribute in the output summary. This approach shows strong gains in ROUGE score over baseline multi document summarization approaches and shows gains in \ufb02uency, informativeness and reduction in repetitiveness as shown through a human annotation analysis study.\n## Introduction\nAbstractive multi document summarization is the task of writing a single summary of the key points and content in multiple related documents. This task has evolved from research in single document abstractive and extractive summarization; however, it faces unique challenges due to input documents having duplicate and conflicting content as well a larger body of text. (Radev et al., 2000). This task has evolved from early approaches using sequence to sequence (Seq2Seq) neural architectures to transformer based architectures with the introduction of large-scale datasets (Liu et al., 2018), (Fabbri et al., 2019). Beyond the introduction of approaches now commonly used for single document abstractive summarization, cross document attention and graphs that capture relations between text in various documents have further improved the state of the art for multi document summarization tasks. (Liu et al., 2018), (Li et al., 2014). These graphs aim to better represent the inter dependencies between articles by representing text spans as nodes in the graph and capturing the relations between these sentences as edge weights. \n\nDespite the advances made with these approaches, a significant challenge remains in multi document summarization with respect to how to deal with contradictory information present in the multiple source documents. It is critical to both learn the relationships between different documents as well as to extract salient information that is consistent with the output viewpoint. This is a situation often faced with summarizing multiple news articles where different viewpoints on an issue can significantly change the semantic structure of the content present in each article making it challenging for the abstractive summarization model to learn the relationships between inconsistent or conflicting information. In this work we define conflicting opinions as a combined measure of the polarity and sentiment of text. By this definition, two pieces of text on the same topic that have a differing sentiment and/ or polarity are determined to have different viewpoints. This definition is used throughout the paper. \n\nThis paper proposes ACM, attribute conditioned multi document summarization, a novel approach to multi document summarization that incorporates an attribute conditioning module with an abstractive multi document summarization model in order to condition for a particular attribute when generating the multi document summary.\n\n## Multi Document Summarization\nMulti document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013). These papers primarily aim to address de-duplicating information and learning relationships between the different topics shared across documents however none of these architectures are built to deal with conflicting information.",
            "reference_string": "[248571519 | Sankar et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Exploring Content Models for Multi-Document Summarization",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2009,
            "reference_count": 24,
            "citation_count": 560,
            "influential_citation_count": 47,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.5555/1620754.1620807",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N09-1041, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1761880",
                    "name": "A. Haghighi"
                },
                {
                    "authorId": "1909300",
                    "name": "Lucy Vanderwende"
                }
            ],
            "abstract": "We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HierSum, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions. At the task of producing generic DUC-style summaries, HierSum yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)'s state-of-the-art discriminative system. We also explore HierSum's capacity to produce multiple 'topical summaries' in order to facilitate content discovery and navigation.",
            "corpus_id": 678258,
            "sentences": [
                {
                    "corpus_id": "678258",
                    "title": "Exploring Content Models for Multi-Document Summarization",
                    "text": "Over the past several years, there has been much interest in the task of multi-document summarization. In the common Document Understanding Conference (DUC) formulation of the task, a system takes as input a document set as well as a short description of desired summary focus and outputs a word length limited summary. 1 To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content. 2 There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende,  2005), graph-based approaches (Radev, 2004; Wan  and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec  et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum\u00e9 III and  Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. \n\nIn this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei  et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2). We also contend that they provide convenient building blocks for adding more structure to a summarization model. In particular, we utilize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific 'subtopics' within a document set. The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics.",
                    "score": 0.5902543533425346,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1179
                        },
                        {
                            "start": 1182,
                            "end": 1484
                        },
                        {
                            "start": 1485,
                            "end": 1597
                        },
                        {
                            "start": 1598,
                            "end": 1756
                        },
                        {
                            "start": 1757,
                            "end": 1890
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94482421875
                }
            ],
            "relevance_judgement": 0.94482421875,
            "relevance_judgment_input_expanded": "# Title: Exploring Content Models for Multi-Document Summarization\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: A. Haghighi, Lucy Vanderwende\n## Abstract\nWe present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HierSum, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions. At the task of producing generic DUC-style summaries, HierSum yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)'s state-of-the-art discriminative system. We also explore HierSum's capacity to produce multiple 'topical summaries' in order to facilitate content discovery and navigation.\n## Introduction\nOver the past several years, there has been much interest in the task of multi-document summarization. In the common Document Understanding Conference (DUC) formulation of the task, a system takes as input a document set as well as a short description of desired summary focus and outputs a word length limited summary. 1 To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content. 2 There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende,  2005), graph-based approaches (Radev, 2004; Wan  and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec  et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum\u00e9 III and  Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. \n\nIn this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei  et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2). We also contend that they provide convenient building blocks for adding more structure to a summarization model. In particular, we utilize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific 'subtopics' within a document set. The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics.",
            "reference_string": "[678258 | Haghighi et al. | 2009 | Citations: 560]"
        },
        {
            "title": "Query Understanding via Intent Description Generation",
            "venue": "International Conference on Information and Knowledge Management",
            "year": 2020,
            "reference_count": 69,
            "citation_count": 17,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2008.10889",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2008.10889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109960367",
                    "name": "Ruqing Zhang"
                },
                {
                    "authorId": "70414094",
                    "name": "Jiafeng Guo"
                },
                {
                    "authorId": "7888704",
                    "name": "Yixing Fan"
                },
                {
                    "authorId": "37510256",
                    "name": "Yanyan Lan"
                },
                {
                    "authorId": "1717004",
                    "name": "Xueqi Cheng"
                }
            ],
            "abstract": "Query understanding is a fundamental problem in information retrieval (IR), which has attracted continuous attention through the past decades. Many different tasks have been proposed for understanding users' search queries, e.g., query classification or query clustering. However, it is not that precise to understand a search query at the intent class/cluster level due to the loss of many detailed information. As we may find in many benchmark datasets, e.g., TREC and SemEval, queries are often associated with a detailed description provided by human annotators which clearly describes its intent to help evaluate the relevance of the documents. If a system could automatically generate a detailed and precise intent description for a search query, like human annotators, that would indicate much better query understanding has been achieved. In this paper, therefore, we propose a novel Query-to-Intent-Description (Q2ID) task for query understanding. Unlike those existing ranking tasks which leverage the query and its description to compute the relevance of documents, Q2ID is a reverse task which aims to generate a natural language intent description based on both relevant and irrelevant documents of a given query. To address this new task, we propose a novel Contrastive Generation model, namely CtrsGen for short, to generate the intent description by contrasting the relevant documents with the irrelevant documents given a query. We demonstrate the effectiveness of our model by comparing with several state-of-the-art generation models on the Q2ID task. We discuss the potential usage of such Q2ID technique through an example application.",
            "corpus_id": 221293184,
            "sentences": [
                {
                    "corpus_id": "221293184",
                    "title": "Query Understanding via Intent Description Generation",
                    "text": "Query-based Multi-document Summarization. Query-based multi-document summarization is the process of automatically generating natural summaries of text documents in the context of a given query. An early work for extractive query-based multidocument summarization is presented by [22], which ranked sentences using a weighted combination of statistical and linguistic features. [16] presented to extract sentences based on the language model, Bayesian model, and graphical model. [40] introduced the graph information to look for relevant sentences. [51] used the multi-modality manifold-ranking algorithm to extract topic-focused summary from multiple documents. Recently, some works employ the encoder-decoder framework to produce the query-based summaries. [24] trained a pointer-generator model, and [3] incorporated relevance into a neural seq2seq models for query-based abstractive summarization. [43] introduced a new diversity based attention mechanism to alleviate the problem of repeating phrases.",
                    "score": 0.6102310289648089,
                    "section_title": "2.2.2",
                    "char_start_offset": 11789,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 41
                        },
                        {
                            "start": 42,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 759
                        },
                        {
                            "start": 760,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1007
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 280,
                            "end": 284,
                            "matchedPaperCorpusId": "11218013"
                        },
                        {
                            "start": 378,
                            "end": 382,
                            "matchedPaperCorpusId": "6241932"
                        },
                        {
                            "start": 550,
                            "end": 554,
                            "matchedPaperCorpusId": "22109805"
                        },
                        {
                            "start": 903,
                            "end": 907,
                            "matchedPaperCorpusId": "5673925"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9423828125
                }
            ],
            "relevance_judgement": 0.9423828125,
            "relevance_judgment_input_expanded": "# Title: Query Understanding via Intent Description Generation\n# Venue: International Conference on Information and Knowledge Management\n# Authors: Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, Xueqi Cheng\n## Abstract\nQuery understanding is a fundamental problem in information retrieval (IR), which has attracted continuous attention through the past decades. Many different tasks have been proposed for understanding users' search queries, e.g., query classification or query clustering. However, it is not that precise to understand a search query at the intent class/cluster level due to the loss of many detailed information. As we may find in many benchmark datasets, e.g., TREC and SemEval, queries are often associated with a detailed description provided by human annotators which clearly describes its intent to help evaluate the relevance of the documents. If a system could automatically generate a detailed and precise intent description for a search query, like human annotators, that would indicate much better query understanding has been achieved. In this paper, therefore, we propose a novel Query-to-Intent-Description (Q2ID) task for query understanding. Unlike those existing ranking tasks which leverage the query and its description to compute the relevance of documents, Q2ID is a reverse task which aims to generate a natural language intent description based on both relevant and irrelevant documents of a given query. To address this new task, we propose a novel Contrastive Generation model, namely CtrsGen for short, to generate the intent description by contrasting the relevant documents with the irrelevant documents given a query. We demonstrate the effectiveness of our model by comparing with several state-of-the-art generation models on the Q2ID task. We discuss the potential usage of such Q2ID technique through an example application.\n## 2.2.2\nQuery-based Multi-document Summarization. Query-based multi-document summarization is the process of automatically generating natural summaries of text documents in the context of a given query. An early work for extractive query-based multidocument summarization is presented by [22], which ranked sentences using a weighted combination of statistical and linguistic features. [16] presented to extract sentences based on the language model, Bayesian model, and graphical model. [40] introduced the graph information to look for relevant sentences. [51] used the multi-modality manifold-ranking algorithm to extract topic-focused summary from multiple documents. Recently, some works employ the encoder-decoder framework to produce the query-based summaries. [24] trained a pointer-generator model, and [3] incorporated relevance into a neural seq2seq models for query-based abstractive summarization. [43] introduced a new diversity based attention mechanism to alleviate the problem of repeating phrases.",
            "reference_string": "[221293184 | Zhang et al. | 2020 | Citations: 17]"
        },
        {
            "title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies",
            "venue": "arXiv.org",
            "year": 2000,
            "reference_count": 12,
            "citation_count": 584,
            "influential_citation_count": 45,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.3115/1117575.1117578",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/cs/0005020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                },
                {
                    "authorId": "40544823",
                    "name": "Hongyan Jing"
                },
                {
                    "authorId": "3166871",
                    "name": "M. Budzikowska"
                }
            ],
            "abstract": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",
            "corpus_id": 1320,
            "sentences": [
                {
                    "corpus_id": "1320",
                    "title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies",
                    "text": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",
                    "score": 0.5743949504933779,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94189453125
                },
                {
                    "corpus_id": "1320",
                    "title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies",
                    "text": "We presented a new multi-document summarizer, MEAD. It summarizes clusters of news articles automatically grouped by a topic detection system. MEAD uses information from the centroids of the clusters to select sentences that are most likely to be relevant to the cluster topic. \n\nWe used a new utility-based technique, CBSU, for the evaluation of MEAD and of summarizers in general. We found that MEAD produces summaries that are similar in quality to the ones produced by humans. We also compared MEAD's performance to an alternative method, multi-document lead, and showed how MEAD's sentence scoring weights can be modified to produce summaries significantly better than the alternatives. \n\nWe also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in [Carbonell and Goldstein, 1998]) and showed how it can be used in evaluating multidocument summaries. \n\nAll our findings are backed by the analysis of two experiments that we performed with human subjects. We found that the interjudge agreement on sentence utility is very high while the agreement on crosssentence subsumption is moderately low, although promising. \n\nIn the future, we would like to test our multidocument summarizer on a larger corpus and improve the summarization algorithm. We would also like to explore how the techniques we proposed here can be used for multiligual multidocument summarization.",
                    "score": 0.5570599652501886,
                    "section_title": "Contributions and future work",
                    "char_start_offset": 21158,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 51
                        },
                        {
                            "start": 52,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 277
                        },
                        {
                            "start": 280,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 691
                        },
                        {
                            "start": 694,
                            "end": 946
                        },
                        {
                            "start": 949,
                            "end": 1050
                        },
                        {
                            "start": 1051,
                            "end": 1210
                        },
                        {
                            "start": 1213,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1461
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 845,
                            "end": 876,
                            "matchedPaperCorpusId": "4508623"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93310546875
                }
            ],
            "relevance_judgement": 0.94189453125,
            "relevance_judgment_input_expanded": "# Title: Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies\n# Venue: arXiv.org\n# Authors: Dragomir R. Radev, Hongyan Jing, M. Budzikowska\n## Abstract\nWe present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.\n## Contributions and future work\nWe presented a new multi-document summarizer, MEAD. It summarizes clusters of news articles automatically grouped by a topic detection system. MEAD uses information from the centroids of the clusters to select sentences that are most likely to be relevant to the cluster topic. \n\nWe used a new utility-based technique, CBSU, for the evaluation of MEAD and of summarizers in general. We found that MEAD produces summaries that are similar in quality to the ones produced by humans. We also compared MEAD's performance to an alternative method, multi-document lead, and showed how MEAD's sentence scoring weights can be modified to produce summaries significantly better than the alternatives. \n\nWe also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in [Carbonell and Goldstein, 1998]) and showed how it can be used in evaluating multidocument summaries. \n\nAll our findings are backed by the analysis of two experiments that we performed with human subjects. We found that the interjudge agreement on sentence utility is very high while the agreement on crosssentence subsumption is moderately low, although promising. \n\nIn the future, we would like to test our multidocument summarizer on a larger corpus and improve the summarization algorithm. We would also like to explore how the techniques we proposed here can be used for multiligual multidocument summarization.",
            "reference_string": "[1320 | Radev et al. | 2000 | Citations: 584]"
        },
        {
            "title": "Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 49,
            "citation_count": 32,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.15.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.acl-long.15, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143853729",
                    "name": "G. Moro"
                },
                {
                    "authorId": "134327204",
                    "name": "Luca Ragazzi"
                },
                {
                    "authorId": "2132084411",
                    "name": "Lorenzo Valgimigli"
                },
                {
                    "authorId": "2165227434",
                    "name": "Davide Freddi"
                }
            ],
            "abstract": "Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can be vital. Others leverage linear model approximations to apply multi-input concatenation, worsening the results because all information is considered, even if it is conflicting or noisy with respect to a shared background. Despite the importance and social impact of medicine, there are no ad-hoc solutions for multi-document summarization. For this reason, we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization. Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews. Moreover, we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method.",
            "corpus_id": 248780330,
            "sentences": [
                {
                    "corpus_id": "248780330",
                    "title": "Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature",
                    "text": "We describe related works on multi-document summarization categorized on model architectures. \n\nFlat solutions. Flat concatenation is a simple yet powerful solution because the generation of the multi-document summary is treated as a singledocument summarization task, thus it can leverage state-of-the-art pre-trained summarization models. \n\nConsequently, processing all documents as a flat input requires models capable of handling long sequences. As previously experimented by DeYoung et al. (2021), Xiao et al. (2021) proposed to leverage the Longformer-Encoder-Decoder model (Beltagy et al., 2020) pre-trained with a novel multi-document summarization specific task. They proved that a long-range Transformer that encodes all documents is a straightforward yet effective solution, and they achieved new state-of-the-art results in several multi-document summarization datasets. However, such models may struggle to handle a massive cluster of topic-related documents since they need to truncate them because of architectural limits. Further, processing all documents in a cluster could be noisy if some of them are not relevant or factual with respect to the summary. \n\nHierarchical solutions. To better preserve crossdocument relations and obtain semantic-rich representations, hierarchical concatenation solutions leverage graph-based techniques to work from word and sentence-level (Wan and Yang, 2006;Liao et al., 2018;Nayeem et al., 2018;Antognini and Faltings, 2019;Li et al., 2020) to documentlevel (Amplayo and Lapata, 2021). Other hierarchical approaches include multi-head pooling and inter-paragraph attention architectures (Liu and Lapata, 2019a), attention models with maximal marginal relevance (Fabbri et al., 2019), and attention across different granularity representations (Jin et al., 2020). Such models are often datasetspecific because of the custom architecture, so they struggle to adapt to other datasets and effectively leverage pre-trained state-of-the-art Transformers. \n\nOur solution.",
                    "score": 0.5741903654961312,
                    "section_title": "Related Work",
                    "char_start_offset": 3656,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 96,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 340
                        },
                        {
                            "start": 343,
                            "end": 449
                        },
                        {
                            "start": 450,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 882
                        },
                        {
                            "start": 883,
                            "end": 1037
                        },
                        {
                            "start": 1038,
                            "end": 1172
                        },
                        {
                            "start": 1175,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1538
                        },
                        {
                            "start": 1539,
                            "end": 1815
                        },
                        {
                            "start": 1816,
                            "end": 2001
                        },
                        {
                            "start": 2004,
                            "end": 2017
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 480,
                            "end": 501,
                            "matchedPaperCorpusId": "233231380"
                        },
                        {
                            "start": 1390,
                            "end": 1410,
                            "matchedPaperCorpusId": "5457260"
                        },
                        {
                            "start": 1410,
                            "end": 1428,
                            "matchedPaperCorpusId": "49210924"
                        },
                        {
                            "start": 1428,
                            "end": 1448,
                            "matchedPaperCorpusId": "52011473"
                        },
                        {
                            "start": 1477,
                            "end": 1493,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1640,
                            "end": 1663,
                            "matchedPaperCorpusId": "170079112"
                        },
                        {
                            "start": 1714,
                            "end": 1735,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 1796,
                            "end": 1814,
                            "matchedPaperCorpusId": "220045815"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                }
            ],
            "relevance_judgement": 0.9375,
            "relevance_judgment_input_expanded": "# Title: Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: G. Moro, Luca Ragazzi, Lorenzo Valgimigli, Davide Freddi\n## Abstract\nAlthough current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can be vital. Others leverage linear model approximations to apply multi-input concatenation, worsening the results because all information is considered, even if it is conflicting or noisy with respect to a shared background. Despite the importance and social impact of medicine, there are no ad-hoc solutions for multi-document summarization. For this reason, we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization. Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews. Moreover, we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method.\n## Related Work\nWe describe related works on multi-document summarization categorized on model architectures. \n\nFlat solutions. Flat concatenation is a simple yet powerful solution because the generation of the multi-document summary is treated as a singledocument summarization task, thus it can leverage state-of-the-art pre-trained summarization models. \n\nConsequently, processing all documents as a flat input requires models capable of handling long sequences. As previously experimented by DeYoung et al. (2021), Xiao et al. (2021) proposed to leverage the Longformer-Encoder-Decoder model (Beltagy et al., 2020) pre-trained with a novel multi-document summarization specific task. They proved that a long-range Transformer that encodes all documents is a straightforward yet effective solution, and they achieved new state-of-the-art results in several multi-document summarization datasets. However, such models may struggle to handle a massive cluster of topic-related documents since they need to truncate them because of architectural limits. Further, processing all documents in a cluster could be noisy if some of them are not relevant or factual with respect to the summary. \n\nHierarchical solutions. To better preserve crossdocument relations and obtain semantic-rich representations, hierarchical concatenation solutions leverage graph-based techniques to work from word and sentence-level (Wan and Yang, 2006;Liao et al., 2018;Nayeem et al., 2018;Antognini and Faltings, 2019;Li et al., 2020) to documentlevel (Amplayo and Lapata, 2021). Other hierarchical approaches include multi-head pooling and inter-paragraph attention architectures (Liu and Lapata, 2019a), attention models with maximal marginal relevance (Fabbri et al., 2019), and attention across different granularity representations (Jin et al., 2020). Such models are often datasetspecific because of the custom architecture, so they struggle to adapt to other datasets and effectively leverage pre-trained state-of-the-art Transformers. \n\nOur solution.",
            "reference_string": "[248780330 | Moro et al. | 2022 | Citations: 32]"
        },
        {
            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 63,
            "citation_count": 136,
            "influential_citation_count": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.555.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.10043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48624966",
                    "name": "Wei Li"
                },
                {
                    "authorId": "2107521158",
                    "name": "Xinyan Xiao"
                },
                {
                    "authorId": null,
                    "name": "Jiachen Liu"
                },
                {
                    "authorId": "40354707",
                    "name": "Hua Wu"
                },
                {
                    "authorId": "144270731",
                    "name": "Haifeng Wang"
                },
                {
                    "authorId": "2117218629",
                    "name": "Junping Du"
                }
            ],
            "abstract": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
            "corpus_id": 218718706,
            "sentences": [
                {
                    "corpus_id": "218718706",
                    "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                    "text": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "score": 0.5368085336638815,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93505859375
                }
            ],
            "relevance_judgement": 0.93505859375,
            "relevance_judgment_input_expanded": "# Title: Leveraging Graph to Improve Abstractive Multi-Document Summarization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, Junping Du\n## Abstract\nGraphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.\n",
            "reference_string": "[218718706 | Li et al. | 2020 | Citations: 136]"
        },
        {
            "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach",
            "venue": "International Workshop on Semantic Evaluation",
            "year": 2015,
            "reference_count": 23,
            "citation_count": 17,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/S15-1020.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1507.02907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1728026",
                    "name": "Lu\u00eds Marujo"
                },
                {
                    "authorId": "145648625",
                    "name": "Ricardo Ribeiro"
                },
                {
                    "authorId": "4284219",
                    "name": "David Martins de Matos"
                },
                {
                    "authorId": "1723288",
                    "name": "J. Neto"
                },
                {
                    "authorId": "145001267",
                    "name": "A. Gershman"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                }
            ],
            "abstract": "The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.",
            "corpus_id": 9174081,
            "sentences": [
                {
                    "corpus_id": "9174081",
                    "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach",
                    "text": "The use of the Internet to fulfill generic information needs motivated pioneer multi-document summarization efforts as NewsInEssence (Radev et al., 2005) or Newsblaster (McKeown et al., 2002), online since 2001. In general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics. \n\nWe propose to extend a state-of-the-art single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013), capable of focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization. We explore two hierarchical strategies to perform this extension. This document is organized as follows: Sect. 2 addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper.",
                    "score": 0.6047676603932829,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 727
                        },
                        {
                            "start": 730,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1206
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 133,
                            "end": 153,
                            "matchedPaperCorpusId": "681677"
                        },
                        {
                            "start": 169,
                            "end": 191,
                            "matchedPaperCorpusId": "681677"
                        },
                        {
                            "start": 553,
                            "end": 579,
                            "matchedPaperCorpusId": "4508623"
                        },
                        {
                            "start": 822,
                            "end": 844,
                            "matchedPaperCorpusId": "681677"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93408203125
                },
                {
                    "corpus_id": "9174081",
                    "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach",
                    "text": "Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content). \n\nWhen adapting a single-document summarization method to perform multi-document summarization, a possible strategy is to combine the summaries of each document. To iteratively combine the summaries, we explore two different approaches: singlelayer hierarchical and waterfall. Given that the summarization method also uses as input a set of key phrases, we extract from each input document the required set of key phrases, join the extracted sets, and rank the key phrases using their frequency. To generate each summary, we use the top key phrases, excluding the ones that do not occur in the input document.",
                    "score": 0.6153341582966353,
                    "section_title": "Multi-Document Summarization",
                    "char_start_offset": 3618,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 413
                        },
                        {
                            "start": 416,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 909
                        },
                        {
                            "start": 910,
                            "end": 1023
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 126,
                            "end": 148,
                            "matchedPaperCorpusId": "681677"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9228515625
                }
            ],
            "relevance_judgement": 0.93408203125,
            "relevance_judgment_input_expanded": "# Title: Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach\n# Venue: International Workshop on Semantic Evaluation\n# Authors: Lu\u00eds Marujo, Ricardo Ribeiro, David Martins de Matos, J. Neto, A. Gershman, J. Carbonell\n## Abstract\nThe increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.\n## Introduction\nThe use of the Internet to fulfill generic information needs motivated pioneer multi-document summarization efforts as NewsInEssence (Radev et al., 2005) or Newsblaster (McKeown et al., 2002), online since 2001. In general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics. \n\nWe propose to extend a state-of-the-art single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013), capable of focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization. We explore two hierarchical strategies to perform this extension. This document is organized as follows: Sect. 2 addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper.\n\n## Multi-Document Summarization\nOur multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content). \n\nWhen adapting a single-document summarization method to perform multi-document summarization, a possible strategy is to combine the summaries of each document. To iteratively combine the summaries, we explore two different approaches: singlelayer hierarchical and waterfall. Given that the summarization method also uses as input a set of key phrases, we extract from each input document the required set of key phrases, join the extracted sets, and rank the key phrases using their frequency. To generate each summary, we use the top key phrases, excluding the ones that do not occur in the input document.",
            "reference_string": "[9174081 | Marujo et al. | 2015 | Citations: 17]"
        },
        {
            "title": "Entity-Aware Abstractive Multi-Document Summarization",
            "venue": "Findings",
            "year": 2021,
            "reference_count": 57,
            "citation_count": 28,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-acl.30.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.findings-acl.30, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Hao Zhou"
                },
                {
                    "authorId": "2053308860",
                    "name": "Weidong Ren"
                },
                {
                    "authorId": "150112803",
                    "name": "Gongshen Liu"
                },
                {
                    "authorId": "153253583",
                    "name": "Bo Su"
                },
                {
                    "authorId": "143844110",
                    "name": "Wei Lu"
                }
            ],
            "abstract": ",",
            "corpus_id": 236478143,
            "sentences": [
                {
                    "corpus_id": "236478143",
                    "title": "Entity-Aware Abstractive Multi-Document Summarization",
                    "text": "Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020). \n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018). \n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020). Such relations were shown useful in identifying the salient and redundant information from long documents, and can thus guide the summary generation process. However, while effective empirically, such approaches do not focus on explicitly modeling the underlying semantic information across documents. \n\nEntities and their mentions convey rich semantic information, and can be significant in summarization, especially when a specific entity is the topic under discussion for a set of documents. As shown in Figure 1, entity mentions frequently appear in the input article, and are playing unique roles that contribute towards the coherence and conciseness of the text. We believe that entities can be regarded as the indicator of saliency and can be used to reduce redundancy. This motivates us to propose an entity-aware abstractive multi-document summarization model that effectively encodes relations across documents with the help of entities, and explicitly solve the issues of saliency and redundancy. \n\nInspired by Wang et al. (2020a), we build a heterogeneous graph that consists of nodes that represent documents and entities. The entity nodes can serve as bridges that connect different documents -we can model the relations across documents through entity clusters.",
                    "score": 0.5814983055552646,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 287
                        },
                        {
                            "start": 290,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 796
                        },
                        {
                            "start": 799,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1126
                        },
                        {
                            "start": 1127,
                            "end": 1270
                        },
                        {
                            "start": 1273,
                            "end": 1463
                        },
                        {
                            "start": 1464,
                            "end": 1637
                        },
                        {
                            "start": 1638,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1976
                        },
                        {
                            "start": 1979,
                            "end": 2104
                        },
                        {
                            "start": 2105,
                            "end": 2245
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 733,
                            "end": 756,
                            "matchedPaperCorpusId": "52053741"
                        },
                        {
                            "start": 928,
                            "end": 951,
                            "matchedPaperCorpusId": "170079112"
                        },
                        {
                            "start": 951,
                            "end": 967,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1991,
                            "end": 2010,
                            "matchedPaperCorpusId": "216552978"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93408203125
                }
            ],
            "relevance_judgement": 0.93408203125,
            "relevance_judgment_input_expanded": "# Title: Entity-Aware Abstractive Multi-Document Summarization\n# Venue: Findings\n# Authors: Hao Zhou, Weidong Ren, Gongshen Liu, Bo Su, Wei Lu\n## Abstract\n,\n## Introduction\nMulti-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020). \n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018). \n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020). Such relations were shown useful in identifying the salient and redundant information from long documents, and can thus guide the summary generation process. However, while effective empirically, such approaches do not focus on explicitly modeling the underlying semantic information across documents. \n\nEntities and their mentions convey rich semantic information, and can be significant in summarization, especially when a specific entity is the topic under discussion for a set of documents. As shown in Figure 1, entity mentions frequently appear in the input article, and are playing unique roles that contribute towards the coherence and conciseness of the text. We believe that entities can be regarded as the indicator of saliency and can be used to reduce redundancy. This motivates us to propose an entity-aware abstractive multi-document summarization model that effectively encodes relations across documents with the help of entities, and explicitly solve the issues of saliency and redundancy. \n\nInspired by Wang et al. (2020a), we build a heterogeneous graph that consists of nodes that represent documents and entities. The entity nodes can serve as bridges that connect different documents -we can model the relations across documents through entity clusters.",
            "reference_string": "[236478143 | Zhou et al. | 2021 | Citations: 28]"
        },
        {
            "title": "Abstractive text summarization: State of the art, challenges, and improvements",
            "venue": "Neurocomputing",
            "year": 2024,
            "reference_count": 224,
            "citation_count": 24,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.02413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2300173312",
                    "name": "Hassan Shakil"
                },
                {
                    "authorId": "2313554804",
                    "name": "Ahmad Farooq"
                },
                {
                    "authorId": "2261083539",
                    "name": "Jugal K. Kalita"
                }
            ],
            "abstract": null,
            "corpus_id": 271525553,
            "sentences": [
                {
                    "corpus_id": "271525553",
                    "title": "Abstractive text summarization: State of the art, challenges, and improvements",
                    "text": "Although much research focuses on single-document summarization, multi-document summarization presents unique challenges [181,182]. Unlike its single-document counterpart, multi-document summarization involves synthesizing information from multiple sources, often necessitating the alignment of documents, identification, and resolution of redundancies, contradictions, and varying perspectives. These complexities introduce unique challenges such as ensuring coherence in the face of diverse inputs and maintaining a balanced representation of all source documents. The enormous amount of information that needs to be processed during multi-document summarization is one of the main challenges. The amount of data increases rapidly with many documents, causing computational difficulties and extending processing times [183]. The possibility of conflicting information across documents presents another challenge. Finding the most precise or relevant information can be troublesome, particularly if the source text comprises different authors or viewpoints [39]. Furthermore, the temporal part of the information can present difficulties. For instance, while summing up news articles, recent data may be more pertinent than older information, expecting models to have a sense of temporality [184]. Large Language Models (LLMs) can play a crucial role. Their ability to process large volumes of text and understand complex linguistic patterns makes them well-suited for tackling the challenges of multi-document summarization [185]. However, their application also introduces new dimensions to these challenges. For example, the computational resources required to process multiple lengthy documents using LLMs are significant, and the risk of perpetuating biases present in training data is heightened due to the models' extensive scope. \n\nTo address these issues, integrating knowledge graphs and structured knowledge representations has arisen as a promising strategy [186]. Knowledge graphs, with their interconnected nodes and connections, provide an organized system that can assist models, including LLMs, in understanding the connections between various text documents, recognizing key subjects, and generating summaries that capture the essence of the entire text document set [187]. Essentially, structured knowledge representations offer a deliberate method for coordinating and processing multi-document content, ensuring that the resulting summaries are comprehensive and well-structured.",
                    "score": 0.5713134900671631,
                    "section_title": "Multi-Document Summarization",
                    "char_start_offset": 109325,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 395
                        },
                        {
                            "start": 396,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 695
                        },
                        {
                            "start": 696,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 1063
                        },
                        {
                            "start": 1064,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1298
                        },
                        {
                            "start": 1299,
                            "end": 1352
                        },
                        {
                            "start": 1353,
                            "end": 1532
                        },
                        {
                            "start": 1533,
                            "end": 1611
                        },
                        {
                            "start": 1612,
                            "end": 1838
                        },
                        {
                            "start": 1841,
                            "end": 1977
                        },
                        {
                            "start": 1978,
                            "end": 2292
                        },
                        {
                            "start": 2293,
                            "end": 2501
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 121,
                            "end": 126,
                            "matchedPaperCorpusId": "3510042"
                        },
                        {
                            "start": 126,
                            "end": 130,
                            "matchedPaperCorpusId": "233948337"
                        },
                        {
                            "start": 1058,
                            "end": 1062,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1292,
                            "end": 1297,
                            "matchedPaperCorpusId": "532313"
                        },
                        {
                            "start": 1526,
                            "end": 1531,
                            "matchedPaperCorpusId": "253828067"
                        },
                        {
                            "start": 2286,
                            "end": 2291,
                            "matchedPaperCorpusId": "259360395"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.93359375,
            "relevance_judgment_input_expanded": "# Title: Abstractive text summarization: State of the art, challenges, and improvements\n# Venue: Neurocomputing\n# Authors: Hassan Shakil, Ahmad Farooq, Jugal K. Kalita\n## Abstract\nNone\n## Multi-Document Summarization\nAlthough much research focuses on single-document summarization, multi-document summarization presents unique challenges [181,182]. Unlike its single-document counterpart, multi-document summarization involves synthesizing information from multiple sources, often necessitating the alignment of documents, identification, and resolution of redundancies, contradictions, and varying perspectives. These complexities introduce unique challenges such as ensuring coherence in the face of diverse inputs and maintaining a balanced representation of all source documents. The enormous amount of information that needs to be processed during multi-document summarization is one of the main challenges. The amount of data increases rapidly with many documents, causing computational difficulties and extending processing times [183]. The possibility of conflicting information across documents presents another challenge. Finding the most precise or relevant information can be troublesome, particularly if the source text comprises different authors or viewpoints [39]. Furthermore, the temporal part of the information can present difficulties. For instance, while summing up news articles, recent data may be more pertinent than older information, expecting models to have a sense of temporality [184]. Large Language Models (LLMs) can play a crucial role. Their ability to process large volumes of text and understand complex linguistic patterns makes them well-suited for tackling the challenges of multi-document summarization [185]. However, their application also introduces new dimensions to these challenges. For example, the computational resources required to process multiple lengthy documents using LLMs are significant, and the risk of perpetuating biases present in training data is heightened due to the models' extensive scope. \n\nTo address these issues, integrating knowledge graphs and structured knowledge representations has arisen as a promising strategy [186]. Knowledge graphs, with their interconnected nodes and connections, provide an organized system that can assist models, including LLMs, in understanding the connections between various text documents, recognizing key subjects, and generating summaries that capture the essence of the entire text document set [187]. Essentially, structured knowledge representations offer a deliberate method for coordinating and processing multi-document content, ensuring that the resulting summaries are comprehensive and well-structured.",
            "reference_string": "[271525553 | Shakil et al. | 2024 | Citations: 24]"
        },
        {
            "title": "Learning Document-Level Semantic Properties from Free-Text Annotations",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2008,
            "reference_count": 55,
            "citation_count": 122,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/10598/25353",
                "status": "GOLD",
                "license": "publisher-specific-oa",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1401.3457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1741598",
                    "name": "S. Branavan"
                },
                {
                    "authorId": "3152698",
                    "name": "Harr Chen"
                },
                {
                    "authorId": "144154709",
                    "name": "Jacob Eisenstein"
                },
                {
                    "authorId": "1741283",
                    "name": "R. Barzilay"
                }
            ],
            "abstract": "This paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations. Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as \"a real bargain\" or \"good value.\" These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing. To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases. The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews. Our approach is implemented as a hierarchical Bayesian model with joint inference. We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties. Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.",
            "corpus_id": 152281,
            "sentences": [
                {
                    "corpus_id": "152281",
                    "title": "Learning Document-Level Semantic Properties from Free-Text Annotations",
                    "text": "This paper also relates to a large body of work in multi-document summarization. Researchers have long noted that a central challenge of multi-document summarization is identifying redundant information over input documents (Radev & McKeown, 1998;Carbonell & Goldstein, 1998;Mani & Bloedorn, 1997;Barzilay, McKeown, & Elhadad, 1999). This task is of crucial significance because multi-document summarizers operate over related documents that describe the same facts multiple times. In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999;Radev, Jing, & Budzikowska, 2000;Nenkova, Vanderwende, & McKeown, 2006). Many of these algorithms first cluster sentences together, and then extract or generate sentence representatives for the clusters. \n\nIdentification of repeated information is equally central in our approach -our multi-document summarization method only selects properties that are stated by a plurality of users, thereby eliminating rare and/or erroneous opinions. The key difference between our algorithm and existing summarization systems is the method for identifying repeated expressions of a single semantic property. Since most of the existing work on multi-document summarization focuses on topic-independent newspaper articles, redundancy is identified via sentence comparison. For instance, Radev et al. (2000) compare sentences using cosine similarity between corresponding word vectors. Alternatively, some methods compare sentences via alignment of their syntactic trees (Barzilay et al., 1999;Marsi & Krahmer, 2005). Both string-and tree-based comparison algorithms are augmented with lexico-semantic knowledge using resources such as WordNet. \n\nThe approach described in this paper does not perform comparisons at the sentence level. Instead, we first abstract reviews into a set of properties and then compare property overlap across different documents. This approach relates to domain-dependent approaches for text summarization (Radev & McKeown, 1998;White, Korelsky, Cardie, Ng, Pierce, & Wagstaff, 2001;Elhadad & McKeown, 2001). These methods identify the relations between documents by comparing their abstract representations.",
                    "score": 0.5630080966348785,
                    "section_title": "Multidocument Summarization",
                    "char_start_offset": 17535,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 80
                        },
                        {
                            "start": 81,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 481
                        },
                        {
                            "start": 482,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 827
                        },
                        {
                            "start": 830,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1219
                        },
                        {
                            "start": 1220,
                            "end": 1382
                        },
                        {
                            "start": 1383,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1626
                        },
                        {
                            "start": 1627,
                            "end": 1753
                        },
                        {
                            "start": 1756,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 1966
                        },
                        {
                            "start": 1967,
                            "end": 2145
                        },
                        {
                            "start": 2146,
                            "end": 2245
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 224,
                            "end": 247,
                            "matchedPaperCorpusId": "10019526"
                        },
                        {
                            "start": 247,
                            "end": 275,
                            "matchedPaperCorpusId": "4508623"
                        },
                        {
                            "start": 275,
                            "end": 297,
                            "matchedPaperCorpusId": "6025826"
                        },
                        {
                            "start": 624,
                            "end": 657,
                            "matchedPaperCorpusId": "1320"
                        },
                        {
                            "start": 657,
                            "end": 695,
                            "matchedPaperCorpusId": "86903"
                        },
                        {
                            "start": 1397,
                            "end": 1416,
                            "matchedPaperCorpusId": "1320"
                        },
                        {
                            "start": 1603,
                            "end": 1625,
                            "matchedPaperCorpusId": "2293515"
                        },
                        {
                            "start": 2043,
                            "end": 2066,
                            "matchedPaperCorpusId": "10019526"
                        },
                        {
                            "start": 2066,
                            "end": 2120,
                            "matchedPaperCorpusId": "1496402"
                        },
                        {
                            "start": 2120,
                            "end": 2144,
                            "matchedPaperCorpusId": "15641201"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.931640625
                }
            ],
            "relevance_judgement": 0.931640625,
            "relevance_judgment_input_expanded": "# Title: Learning Document-Level Semantic Properties from Free-Text Annotations\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: S. Branavan, Harr Chen, Jacob Eisenstein, R. Barzilay\n## Abstract\nThis paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations. Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as \"a real bargain\" or \"good value.\" These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing. To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases. The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews. Our approach is implemented as a hierarchical Bayesian model with joint inference. We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties. Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.\n## Multidocument Summarization\nThis paper also relates to a large body of work in multi-document summarization. Researchers have long noted that a central challenge of multi-document summarization is identifying redundant information over input documents (Radev & McKeown, 1998;Carbonell & Goldstein, 1998;Mani & Bloedorn, 1997;Barzilay, McKeown, & Elhadad, 1999). This task is of crucial significance because multi-document summarizers operate over related documents that describe the same facts multiple times. In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999;Radev, Jing, & Budzikowska, 2000;Nenkova, Vanderwende, & McKeown, 2006). Many of these algorithms first cluster sentences together, and then extract or generate sentence representatives for the clusters. \n\nIdentification of repeated information is equally central in our approach -our multi-document summarization method only selects properties that are stated by a plurality of users, thereby eliminating rare and/or erroneous opinions. The key difference between our algorithm and existing summarization systems is the method for identifying repeated expressions of a single semantic property. Since most of the existing work on multi-document summarization focuses on topic-independent newspaper articles, redundancy is identified via sentence comparison. For instance, Radev et al. (2000) compare sentences using cosine similarity between corresponding word vectors. Alternatively, some methods compare sentences via alignment of their syntactic trees (Barzilay et al., 1999;Marsi & Krahmer, 2005). Both string-and tree-based comparison algorithms are augmented with lexico-semantic knowledge using resources such as WordNet. \n\nThe approach described in this paper does not perform comparisons at the sentence level. Instead, we first abstract reviews into a set of properties and then compare property overlap across different documents. This approach relates to domain-dependent approaches for text summarization (Radev & McKeown, 1998;White, Korelsky, Cardie, Ng, Pierce, & Wagstaff, 2001;Elhadad & McKeown, 2001). These methods identify the relations between documents by comparing their abstract representations.",
            "reference_string": "[152281 | Branavan et al. | 2008 | Citations: 122]"
        },
        {
            "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.18454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350511520",
                    "name": "Aditi Godbole"
                },
                {
                    "authorId": "2301049091",
                    "name": "Jabin Geevarghese George"
                },
                {
                    "authorId": "48781397",
                    "name": "Smita Shandilya"
                }
            ],
            "abstract": "The rapid increase in unstructured data across various fields has made multi-document comprehension and summarization a critical task. Traditional approaches often fail to capture relevant context, maintain logical consistency, and extract essential information from lengthy documents. This paper explores the use of Long-context Large Language Models (LLMs) for multi-document summarization, demonstrating their exceptional capacity to grasp extensive connections, provide cohesive summaries, and adapt to various industry domains and integration with enterprise applications/systems. The paper discusses the workflow of multi-document summarization for effectively deploying long-context LLMs, supported by case studies in legal applications, enterprise functions such as HR, finance, and sourcing, as well as in the medical and news domains. These case studies show notable enhancements in both efficiency and accuracy. Technical obstacles, such as dataset diversity, model scalability, and ethical considerations like bias mitigation and factual accuracy, are carefully analyzed. Prospective research avenues are suggested to augment the functionalities and applications of long-context LLMs, establishing them as pivotal tools for transforming information processing across diverse sectors and enterprise applications.",
            "corpus_id": 272969413,
            "sentences": [
                {
                    "corpus_id": "272969413",
                    "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications",
                    "text": "In a multinational corporation, a team of analysts faces the daunting task of summarizing thousands of documents spanning financial reports, market analyses, and internal communications to inform a critical strategic decision. This scenario illustrates the challenge of multi-document summarization in enterprise settings, where the volume and diversity of information can overwhelm traditional analysis methods. \n\nThe exponential growth of unstructured text data across various sectors has made document summarization a critical task [1]. Multi-document summarization presents unique challenges due to the need for synthesizing information from diverse sources, which may contain redundant, complementary, or contradictory information across documents [4]. Variations in writing style and level of detail add complexity to the task. Determining the relevance and importance of information from each source is crucial for creating a coherent and comprehensive summary [5]. \n\nTraditional document summarization techniques often struggle with redundancy, inconsistency, lack of context understanding, scalability issues for multiple document summarization tasks, inability to capture cross-document relationships, difficulty handling diverse formats, and lack of domain adaptability [6,7,8]. These limitations highlight the need for more advanced approaches to multi-document summarization. \n\nThis paper addresses the following research question: How can long-context Large Language Models (LLMs) be leveraged to improve multi-document understanding and summarization in enterprise applications? \n\nWe investigate the use of Long-context LLMs for multi-document summarization, demonstrating their exceptional capacity to grasp extensive connections, provide cohesive summaries, and adapt to various industry domains and integration with enterprise applications/systems [14]. The paper discusses the workflow of multi-document summarization for effectively adopting long-context LLMs, supported by case studies in legal applications, enterprise functions such as HR, finance, and sourcing, as well as in the medical and news domains [46,49,53]. \n\nBy exploring the potential of Long-context LLMs in multi-document summarization, we aim to address the limitations of traditional methods and provide a more efficient and accurate approach to processing large volumes of unstructured data [13,18]. This research has significant implications for improving information processing across diverse sectors and enterprise applications.",
                    "score": 0.5632227759639081,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 412
                        },
                        {
                            "start": 415,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 757
                        },
                        {
                            "start": 758,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 972
                        },
                        {
                            "start": 975,
                            "end": 1289
                        },
                        {
                            "start": 1290,
                            "end": 1388
                        },
                        {
                            "start": 1391,
                            "end": 1593
                        },
                        {
                            "start": 1596,
                            "end": 1871
                        },
                        {
                            "start": 1872,
                            "end": 2140
                        },
                        {
                            "start": 2143,
                            "end": 2389
                        },
                        {
                            "start": 2390,
                            "end": 2521
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 753,
                            "end": 756,
                            "matchedPaperCorpusId": "8377315"
                        },
                        {
                            "start": 968,
                            "end": 971,
                            "matchedPaperCorpusId": "337730"
                        },
                        {
                            "start": 1281,
                            "end": 1284,
                            "matchedPaperCorpusId": "1296465"
                        },
                        {
                            "start": 1284,
                            "end": 1286,
                            "matchedPaperCorpusId": "269225"
                        },
                        {
                            "start": 1286,
                            "end": 1288,
                            "matchedPaperCorpusId": "19198109"
                        },
                        {
                            "start": 2136,
                            "end": 2139,
                            "matchedPaperCorpusId": "55461757"
                        },
                        {
                            "start": 2381,
                            "end": 2385,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 2385,
                            "end": 2388,
                            "matchedPaperCorpusId": "204960716"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93115234375
                }
            ],
            "relevance_judgement": 0.93115234375,
            "relevance_judgment_input_expanded": "# Title: Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications\n# Venue: arXiv.org\n# Authors: Aditi Godbole, Jabin Geevarghese George, Smita Shandilya\n## Abstract\nThe rapid increase in unstructured data across various fields has made multi-document comprehension and summarization a critical task. Traditional approaches often fail to capture relevant context, maintain logical consistency, and extract essential information from lengthy documents. This paper explores the use of Long-context Large Language Models (LLMs) for multi-document summarization, demonstrating their exceptional capacity to grasp extensive connections, provide cohesive summaries, and adapt to various industry domains and integration with enterprise applications/systems. The paper discusses the workflow of multi-document summarization for effectively deploying long-context LLMs, supported by case studies in legal applications, enterprise functions such as HR, finance, and sourcing, as well as in the medical and news domains. These case studies show notable enhancements in both efficiency and accuracy. Technical obstacles, such as dataset diversity, model scalability, and ethical considerations like bias mitigation and factual accuracy, are carefully analyzed. Prospective research avenues are suggested to augment the functionalities and applications of long-context LLMs, establishing them as pivotal tools for transforming information processing across diverse sectors and enterprise applications.\n## Introduction\nIn a multinational corporation, a team of analysts faces the daunting task of summarizing thousands of documents spanning financial reports, market analyses, and internal communications to inform a critical strategic decision. This scenario illustrates the challenge of multi-document summarization in enterprise settings, where the volume and diversity of information can overwhelm traditional analysis methods. \n\nThe exponential growth of unstructured text data across various sectors has made document summarization a critical task [1]. Multi-document summarization presents unique challenges due to the need for synthesizing information from diverse sources, which may contain redundant, complementary, or contradictory information across documents [4]. Variations in writing style and level of detail add complexity to the task. Determining the relevance and importance of information from each source is crucial for creating a coherent and comprehensive summary [5]. \n\nTraditional document summarization techniques often struggle with redundancy, inconsistency, lack of context understanding, scalability issues for multiple document summarization tasks, inability to capture cross-document relationships, difficulty handling diverse formats, and lack of domain adaptability [6,7,8]. These limitations highlight the need for more advanced approaches to multi-document summarization. \n\nThis paper addresses the following research question: How can long-context Large Language Models (LLMs) be leveraged to improve multi-document understanding and summarization in enterprise applications? \n\nWe investigate the use of Long-context LLMs for multi-document summarization, demonstrating their exceptional capacity to grasp extensive connections, provide cohesive summaries, and adapt to various industry domains and integration with enterprise applications/systems [14]. The paper discusses the workflow of multi-document summarization for effectively adopting long-context LLMs, supported by case studies in legal applications, enterprise functions such as HR, finance, and sourcing, as well as in the medical and news domains [46,49,53]. \n\nBy exploring the potential of Long-context LLMs in multi-document summarization, we aim to address the limitations of traditional methods and provide a more efficient and accurate approach to processing large volumes of unstructured data [13,18]. This research has significant implications for improving information processing across diverse sectors and enterprise applications.",
            "reference_string": "[272969413 | Godbole et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 46,
            "citation_count": 590,
            "influential_citation_count": 141,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1906.01749",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.01749, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46255971",
                    "name": "Alexander R. Fabbri"
                },
                {
                    "authorId": "46331602",
                    "name": "Irene Li"
                },
                {
                    "authorId": "2106009217",
                    "name": "Tianwei She"
                },
                {
                    "authorId": "50341789",
                    "name": "Suyi Li"
                },
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                }
            ],
            "abstract": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",
            "corpus_id": 174799390,
            "sentences": [
                {
                    "corpus_id": "174799390",
                    "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
                    "text": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",
                    "score": 0.5402305007395213,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9287109375
                },
                {
                    "corpus_id": "174799390",
                    "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
                    "text": "In this paper we introduce Multi-News, the first large-scale multi-document news summarization dataset. We hope that this dataset will promote work in multi-document summarization similar to the progress seen in the single-document case. Additionally, we introduce an end-to-end model which incorporates MMR into a pointer-generator network, which performs competitively compared to previous multi-document summarization models. We also benchmark methods on our dataset. In the future we plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.",
                    "score": 0.6558887524505488,
                    "section_title": "Conclusion",
                    "char_start_offset": 27298,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 104,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 428
                        },
                        {
                            "start": 429,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 609
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.9287109375,
            "relevance_judgment_input_expanded": "# Title: Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir R. Radev\n## Abstract\nAutomatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.\n## Conclusion\nIn this paper we introduce Multi-News, the first large-scale multi-document news summarization dataset. We hope that this dataset will promote work in multi-document summarization similar to the progress seen in the single-document case. Additionally, we introduce an end-to-end model which incorporates MMR into a pointer-generator network, which performs competitively compared to previous multi-document summarization models. We also benchmark methods on our dataset. In the future we plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.",
            "reference_string": "[174799390 | Fabbri et al. | 2019 | Citations: 590]"
        },
        {
            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
            "venue": "The Web Conference",
            "year": 2024,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651262",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3589335.3651262?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3589335.3651262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2163451228",
                    "name": "Yutong Qu"
                }
            ],
            "abstract": "With the development of information technology, a large amount of information and corpora has been incrementally sparked from the Web, stimulating an increasingly high demand for summarizing. Document Summarization is one of Natural Language Processing tasks, which aims to generate abridged versions of a given single or multiple documents as concise and coherent as possible while preserving salient information from the source texts. Recent research in the area has started to use knowledge graphs as they can capture more factual and applicable information from more facets along with source information, benefiting fact consistency and informativeness of generated summaries, rather than just from a linguistic perspective. However, there is no explicit investigation of the effects of different kinds of knowledge graphs on document summarization. The proposed method is to use structured informative and knowledgeable auxiliary information, especially knowledge graphs, into pre-trained summarization models, advancing summary qualities. Expected outcomes are exploring knowledge and knowledge graph incorporation for multi-document summarization, and achieving more informative, coherent, and factually consistent summaries.",
            "corpus_id": 269762702,
            "sentences": [
                {
                    "corpus_id": "269762702",
                    "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
                    "text": "Along with the prosperity of knowledge-aware research in the natural language processing field, more and more document summarization models attempted to incorporate knowledge graphs to enhance the quality of generated summaries.SDS with KG Gunel et al. [6]  MDS with KG Zhou et al. [26] presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. [18] presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News [5] and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model [13] with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. [18] utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
                    "score": 0.5553767710830457,
                    "section_title": "STATE OF THE ART",
                    "char_start_offset": 6022,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 228,
                            "end": 547
                        },
                        {
                            "start": 547,
                            "end": 740
                        },
                        {
                            "start": 740,
                            "end": 879
                        },
                        {
                            "start": 879,
                            "end": 1129
                        },
                        {
                            "start": 1129,
                            "end": 1335
                        },
                        {
                            "start": 1335,
                            "end": 1466
                        },
                        {
                            "start": 1466,
                            "end": 1729
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 253,
                            "end": 256,
                            "matchedPaperCorpusId": "204735695"
                        },
                        {
                            "start": 282,
                            "end": 286,
                            "matchedPaperCorpusId": "236478143"
                        },
                        {
                            "start": 895,
                            "end": 899,
                            "matchedPaperCorpusId": "235097309"
                        },
                        {
                            "start": 1112,
                            "end": 1115,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 1210,
                            "end": 1214,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 1525,
                            "end": 1529,
                            "matchedPaperCorpusId": "235097309"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                }
            ],
            "relevance_judgement": 0.92626953125,
            "relevance_judgment_input_expanded": "# Title: Leveraging Knowledge-aware Methodologies for Multi-document Summarization\n# Venue: The Web Conference\n# Authors: Yutong Qu\n## Abstract\nWith the development of information technology, a large amount of information and corpora has been incrementally sparked from the Web, stimulating an increasingly high demand for summarizing. Document Summarization is one of Natural Language Processing tasks, which aims to generate abridged versions of a given single or multiple documents as concise and coherent as possible while preserving salient information from the source texts. Recent research in the area has started to use knowledge graphs as they can capture more factual and applicable information from more facets along with source information, benefiting fact consistency and informativeness of generated summaries, rather than just from a linguistic perspective. However, there is no explicit investigation of the effects of different kinds of knowledge graphs on document summarization. The proposed method is to use structured informative and knowledgeable auxiliary information, especially knowledge graphs, into pre-trained summarization models, advancing summary qualities. Expected outcomes are exploring knowledge and knowledge graph incorporation for multi-document summarization, and achieving more informative, coherent, and factually consistent summaries.\n## STATE OF THE ART\nAlong with the prosperity of knowledge-aware research in the natural language processing field, more and more document summarization models attempted to incorporate knowledge graphs to enhance the quality of generated summaries.SDS with KG Gunel et al. [6]  MDS with KG Zhou et al. [26] presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. [18] presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News [5] and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model [13] with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. [18] utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
            "reference_string": "[269762702 | Qu | 2024 | Citations: 0]"
        },
        {
            "title": "Automatic Multi Document Summarization Approaches",
            "venue": "",
            "year": 2012,
            "reference_count": 36,
            "citation_count": 72,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://thescipub.com/pdf/jcssp.2012.133.140.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3844/JCSSP.2012.133.140?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3844/JCSSP.2012.133.140, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1734844",
                    "name": "Y. J. Kumar"
                },
                {
                    "authorId": "1680372",
                    "name": "N. Salim"
                }
            ],
            "abstract": "Problem statement: Text summarization can be of different nature ranging from indicative summary that identifies the topics of the document to informative summary which is meant to represent the concise description of the original document, providing an idea of what the whole content of document is all about. Approach: Single document summary seems to capture both the information well but it has not been the case for multi document summary where the overall comprehensive quality in presenting informative summary often lacks. It is found that most of the existing methods tend to focus on sentence scoring and less consideration is given to the contextual information content in multiple documents. Results: In this study, some survey on multi document summarization approaches has been presented. We will direct our focus notably on four well known approaches to multi document summarization namely the feature based method, cluster based method, graph based method and knowledge based method. The general ideas behind these methods have been described. Conclusion: Besides the general idea and concept, we discuss the benefits and limitations concerning these methods. With the aim of enhancing multi document summarization, specifically news documents, a novel type of approach is outlined to be developed in the future, taking into account the generic components of a news story in order to generate a better summary.",
            "corpus_id": 15926944,
            "sentences": [
                {
                    "corpus_id": "15926944",
                    "title": "Automatic Multi Document Summarization Approaches",
                    "text": "A number of research study have addressed multi document summarization in academia (Erkan and Radev, 2004a, Wan and Yang, 2008, Haribagiu and Lacatusu, 2010) and illustrated different types of approaches and available systems for multi document summarization. In this study we direct our focus notably on four well known approaches to multi document summarization. Our discussion will be based on the following pattern: For each method, we will first discuss its main idea. Following that, we will look at some research study from related literatures. Finally the benefits and limitations concerning each method are commented.",
                    "score": 0.5783245809756478,
                    "section_title": "Multi document summarization approaches:",
                    "char_start_offset": 3245,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 626
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 106,
                            "end": 126,
                            "matchedPaperCorpusId": "9849366"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                },
                {
                    "corpus_id": "15926944",
                    "title": "Automatic Multi Document Summarization Approaches",
                    "text": "The need of automatic text summarization has recently increased due to the proliferation of information on the Internet. With the availability and speed of internet, information search from online documents has been eased down to user's finger tips. However, it is not easy for users to manually summarize those large online documents. For example, when a user searches for information about earthquake which occurred in Sendai, Japan, the user will probably receive enormous articles related to that event. The user would definitely opt for a system that could summarize those articles. The goal of automatic text summarization is condensing the source text into a shorter version preserving its information content and overall meaning. \n\nThe objective and approach of summarization of documents explain the kind of summary that is generated. For example, it could be indicative of what a particular subject is about (closely related to a user query), or can be informative about what the whole content of document is all about. Besides that, approach towards text summarization can be either extractive or abstractive (Radev et al., 2002). In extractive type summarization, important sentences are identified and directly extracted from the original document, i.e. the final summary consists of original sentences. On the other hand, in abstractive type summarization (Ganesan et al., 2010) the sentences which are selected from the original document are further processed to restructure them before concatenating them into final summary. This process usually involves deep natural language analysis and sentence compression. \n\nBy understanding the type of summary i.e., indicative, informative, extractive and abstractive, we can then apply them to either single document or multi document. This study focuses mainly on informative and extractive type multi document text summarization. The distinct characteristics that make multi document summarization rather different from single document summarization is that multi document summarization problem involves multiple sources of information that overlap and supplement each other, being contradictory at occasions. So the key tasks are not only identifying and coping with redundancy across documents, but also ensuring that the final summary is both coherent and complete. \n\nThe contributions of this study can be summarized as follows: We discuss the four notable approaches of multi document summarization and present it with related research from literatures.",
                    "score": 0.6317346816068719,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 587
                        },
                        {
                            "start": 588,
                            "end": 737
                        },
                        {
                            "start": 740,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1540
                        },
                        {
                            "start": 1541,
                            "end": 1627
                        },
                        {
                            "start": 1630,
                            "end": 1793
                        },
                        {
                            "start": 1794,
                            "end": 1889
                        },
                        {
                            "start": 1890,
                            "end": 2169
                        },
                        {
                            "start": 2170,
                            "end": 2328
                        },
                        {
                            "start": 2331,
                            "end": 2518
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1120,
                            "end": 1140,
                            "matchedPaperCorpusId": "94818"
                        },
                        {
                            "start": 1370,
                            "end": 1392,
                            "matchedPaperCorpusId": "988010"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9052734375
                }
            ],
            "relevance_judgement": 0.9248046875,
            "relevance_judgment_input_expanded": "# Title: Automatic Multi Document Summarization Approaches\n# Venue: \n# Authors: Y. J. Kumar, N. Salim\n## Abstract\nProblem statement: Text summarization can be of different nature ranging from indicative summary that identifies the topics of the document to informative summary which is meant to represent the concise description of the original document, providing an idea of what the whole content of document is all about. Approach: Single document summary seems to capture both the information well but it has not been the case for multi document summary where the overall comprehensive quality in presenting informative summary often lacks. It is found that most of the existing methods tend to focus on sentence scoring and less consideration is given to the contextual information content in multiple documents. Results: In this study, some survey on multi document summarization approaches has been presented. We will direct our focus notably on four well known approaches to multi document summarization namely the feature based method, cluster based method, graph based method and knowledge based method. The general ideas behind these methods have been described. Conclusion: Besides the general idea and concept, we discuss the benefits and limitations concerning these methods. With the aim of enhancing multi document summarization, specifically news documents, a novel type of approach is outlined to be developed in the future, taking into account the generic components of a news story in order to generate a better summary.\n## INTRODUCTION\nThe need of automatic text summarization has recently increased due to the proliferation of information on the Internet. With the availability and speed of internet, information search from online documents has been eased down to user's finger tips. However, it is not easy for users to manually summarize those large online documents. For example, when a user searches for information about earthquake which occurred in Sendai, Japan, the user will probably receive enormous articles related to that event. The user would definitely opt for a system that could summarize those articles. The goal of automatic text summarization is condensing the source text into a shorter version preserving its information content and overall meaning. \n\nThe objective and approach of summarization of documents explain the kind of summary that is generated. For example, it could be indicative of what a particular subject is about (closely related to a user query), or can be informative about what the whole content of document is all about. Besides that, approach towards text summarization can be either extractive or abstractive (Radev et al., 2002). In extractive type summarization, important sentences are identified and directly extracted from the original document, i.e. the final summary consists of original sentences. On the other hand, in abstractive type summarization (Ganesan et al., 2010) the sentences which are selected from the original document are further processed to restructure them before concatenating them into final summary. This process usually involves deep natural language analysis and sentence compression. \n\nBy understanding the type of summary i.e., indicative, informative, extractive and abstractive, we can then apply them to either single document or multi document. This study focuses mainly on informative and extractive type multi document text summarization. The distinct characteristics that make multi document summarization rather different from single document summarization is that multi document summarization problem involves multiple sources of information that overlap and supplement each other, being contradictory at occasions. So the key tasks are not only identifying and coping with redundancy across documents, but also ensuring that the final summary is both coherent and complete. \n\nThe contributions of this study can be summarized as follows: We discuss the four notable approaches of multi document summarization and present it with related research from literatures.\n\n## Multi document summarization approaches:\nA number of research study have addressed multi document summarization in academia (Erkan and Radev, 2004a, Wan and Yang, 2008, Haribagiu and Lacatusu, 2010) and illustrated different types of approaches and available systems for multi document summarization. In this study we direct our focus notably on four well known approaches to multi document summarization. Our discussion will be based on the following pattern: For each method, we will first discuss its main idea. Following that, we will look at some research study from related literatures. Finally the benefits and limitations concerning each method are commented.",
            "reference_string": "[15926944 | Kumar et al. | 2012 | Citations: 72]"
        },
        {
            "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 58,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.16711, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "148149386",
                    "name": "Shiyin Tan"
                },
                {
                    "authorId": "2357102667",
                    "name": "Jaeeon Park"
                },
                {
                    "authorId": "2242195007",
                    "name": "Dongyuan Li"
                },
                {
                    "authorId": "2299193401",
                    "name": "Renhe Jiang"
                },
                {
                    "authorId": "2283854880",
                    "name": "Manabu Okumura"
                }
            ],
            "abstract": "In the field of multi-document summarization (MDS), transformer-based models have demonstrated remarkable success, yet they suffer an input length limitation. Current methods apply truncation after the retrieval process to fit the context length; however, they heavily depend on manually well-crafted queries, which are impractical to create for each document set for MDS. Additionally, these methods retrieve information at a coarse granularity, leading to the inclusion of irrelevant content. To address these issues, we propose a novel retrieval-based framework that integrates query selection and document ranking and shortening into a unified process. Our approach identifies the most salient elementary discourse units (EDUs) from input documents and utilizes them as latent queries. These queries guide the document ranking by calculating relevance scores. Instead of traditional truncation, our approach filters out irrelevant EDUs to fit the context length, ensuring that only critical information is preserved for summarization. We evaluate our framework on multiple MDS datasets, demonstrating consistent improvements in ROUGE metrics while confirming its scalability and flexibility across diverse model architectures. Additionally, we validate its effectiveness through an in-depth analysis, emphasizing its ability to dynamically select appropriate queries and accurately rank documents based on their relevance scores. These results demonstrate that our framework effectively addresses context-length constraints, establishing it as a robust and reliable solution for MDS.",
            "corpus_id": 278000561,
            "sentences": [
                {
                    "corpus_id": "278000561",
                    "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization",
                    "text": "Multi-document summarization (MDS) is a task that aims to generate concise and coherent summaries by synthesizing information from multiple documents on the same topic [21,29,36,40,44]. MDS can lead to diverse applications, such as news aggregation [7,13,23], scientific research [11,35,59], and legal document analysis [17,38,55]. Current MDS approaches can be categorized into two classes: Graph-based models [9,28,45,47,65] and pre-trained language models [2,46,61]. Graph-based models rely on auxiliary information (e.g., discourse structures) as an input graph to capture the cross-document relationships, while pre-trained language models use the attention mechanisms to capture them. \n\nAll these summarization models employ transformer [58] as a text encoder and are consequently constrained by a fixed input length, limiting the number of tokens they can process. A common solution is to apply truncation, dropping the last tokens of input documents to fit within the context length. However, this naive approach risks discarding critical information for summarization [37,60]. Thus, how to retain sufficient critical information within the length limitation has become a crucial issue for MDS to enhance the quality of summaries. Current approaches often follow a \"retrieve-then-summarize\" paradigm to alleviate this issue [1,12,15,64], as shown in Figure 1 (A). They use manually created queries as guidance to rank documents or sentences from input documents or external knowledge bases and retrieve the top ranked contents to generate summaries. For instance, Light-PAL [12] and DYLE [41] use dataset-provided queries to retrieve passages and sentences, respectively, for summarization. \n\nAlthough these frameworks effectively retrieve documents from a large document collection, they face two major issues in MDS, as shown in Figure 1 (B): (i) These retrieval methods [15] often heavily depend on manually well-crafted queries to guide the ranking of passages or documents. For example, queries require precise human-written topic statements to ensure accurate ranking.",
                    "score": 0.5748428056101271,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 331
                        },
                        {
                            "start": 332,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 690
                        },
                        {
                            "start": 693,
                            "end": 871
                        },
                        {
                            "start": 872,
                            "end": 991
                        },
                        {
                            "start": 992,
                            "end": 1085
                        },
                        {
                            "start": 1086,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1557
                        },
                        {
                            "start": 1558,
                            "end": 1698
                        },
                        {
                            "start": 1701,
                            "end": 1986
                        },
                        {
                            "start": 1987,
                            "end": 2082
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 168,
                            "end": 172,
                            "matchedPaperCorpusId": "220045815"
                        },
                        {
                            "start": 172,
                            "end": 175,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 175,
                            "end": 178,
                            "matchedPaperCorpusId": "235792259"
                        },
                        {
                            "start": 178,
                            "end": 181,
                            "matchedPaperCorpusId": "222090788"
                        },
                        {
                            "start": 181,
                            "end": 184,
                            "matchedPaperCorpusId": "235352668"
                        },
                        {
                            "start": 249,
                            "end": 252,
                            "matchedPaperCorpusId": "270371298"
                        },
                        {
                            "start": 252,
                            "end": 255,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 255,
                            "end": 258,
                            "matchedPaperCorpusId": "269756893"
                        },
                        {
                            "start": 280,
                            "end": 284,
                            "matchedPaperCorpusId": "233231380"
                        },
                        {
                            "start": 284,
                            "end": 287,
                            "matchedPaperCorpusId": "225075639"
                        },
                        {
                            "start": 287,
                            "end": 290,
                            "matchedPaperCorpusId": "269157041"
                        },
                        {
                            "start": 324,
                            "end": 327,
                            "matchedPaperCorpusId": "271114508"
                        },
                        {
                            "start": 327,
                            "end": 330,
                            "matchedPaperCorpusId": "249927023"
                        },
                        {
                            "start": 411,
                            "end": 414,
                            "matchedPaperCorpusId": "239050558"
                        },
                        {
                            "start": 414,
                            "end": 417,
                            "matchedPaperCorpusId": "257496469"
                        },
                        {
                            "start": 417,
                            "end": 420,
                            "matchedPaperCorpusId": "235097309"
                        },
                        {
                            "start": 420,
                            "end": 423,
                            "matchedPaperCorpusId": "269762702"
                        },
                        {
                            "start": 423,
                            "end": 426,
                            "matchedPaperCorpusId": "258378312"
                        },
                        {
                            "start": 462,
                            "end": 465,
                            "matchedPaperCorpusId": "251224184"
                        },
                        {
                            "start": 465,
                            "end": 468,
                            "matchedPaperCorpusId": "247519084"
                        },
                        {
                            "start": 743,
                            "end": 747,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1077,
                            "end": 1081,
                            "matchedPaperCorpusId": "226289939"
                        },
                        {
                            "start": 1081,
                            "end": 1084,
                            "matchedPaperCorpusId": "253098164"
                        },
                        {
                            "start": 1338,
                            "end": 1341,
                            "matchedPaperCorpusId": "258865156"
                        },
                        {
                            "start": 1341,
                            "end": 1344,
                            "matchedPaperCorpusId": "260332126"
                        },
                        {
                            "start": 1596,
                            "end": 1600,
                            "matchedPaperCorpusId": "239009689"
                        },
                        {
                            "start": 1881,
                            "end": 1885,
                            "matchedPaperCorpusId": "258865156"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.92041015625,
            "relevance_judgment_input_expanded": "# Title: A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization\n# Venue: arXiv.org\n# Authors: Shiyin Tan, Jaeeon Park, Dongyuan Li, Renhe Jiang, Manabu Okumura\n## Abstract\nIn the field of multi-document summarization (MDS), transformer-based models have demonstrated remarkable success, yet they suffer an input length limitation. Current methods apply truncation after the retrieval process to fit the context length; however, they heavily depend on manually well-crafted queries, which are impractical to create for each document set for MDS. Additionally, these methods retrieve information at a coarse granularity, leading to the inclusion of irrelevant content. To address these issues, we propose a novel retrieval-based framework that integrates query selection and document ranking and shortening into a unified process. Our approach identifies the most salient elementary discourse units (EDUs) from input documents and utilizes them as latent queries. These queries guide the document ranking by calculating relevance scores. Instead of traditional truncation, our approach filters out irrelevant EDUs to fit the context length, ensuring that only critical information is preserved for summarization. We evaluate our framework on multiple MDS datasets, demonstrating consistent improvements in ROUGE metrics while confirming its scalability and flexibility across diverse model architectures. Additionally, we validate its effectiveness through an in-depth analysis, emphasizing its ability to dynamically select appropriate queries and accurately rank documents based on their relevance scores. These results demonstrate that our framework effectively addresses context-length constraints, establishing it as a robust and reliable solution for MDS.\n## Introduction\nMulti-document summarization (MDS) is a task that aims to generate concise and coherent summaries by synthesizing information from multiple documents on the same topic [21,29,36,40,44]. MDS can lead to diverse applications, such as news aggregation [7,13,23], scientific research [11,35,59], and legal document analysis [17,38,55]. Current MDS approaches can be categorized into two classes: Graph-based models [9,28,45,47,65] and pre-trained language models [2,46,61]. Graph-based models rely on auxiliary information (e.g., discourse structures) as an input graph to capture the cross-document relationships, while pre-trained language models use the attention mechanisms to capture them. \n\nAll these summarization models employ transformer [58] as a text encoder and are consequently constrained by a fixed input length, limiting the number of tokens they can process. A common solution is to apply truncation, dropping the last tokens of input documents to fit within the context length. However, this naive approach risks discarding critical information for summarization [37,60]. Thus, how to retain sufficient critical information within the length limitation has become a crucial issue for MDS to enhance the quality of summaries. Current approaches often follow a \"retrieve-then-summarize\" paradigm to alleviate this issue [1,12,15,64], as shown in Figure 1 (A). They use manually created queries as guidance to rank documents or sentences from input documents or external knowledge bases and retrieve the top ranked contents to generate summaries. For instance, Light-PAL [12] and DYLE [41] use dataset-provided queries to retrieve passages and sentences, respectively, for summarization. \n\nAlthough these frameworks effectively retrieve documents from a large document collection, they face two major issues in MDS, as shown in Figure 1 (B): (i) These retrieval methods [15] often heavily depend on manually well-crafted queries to guide the ranking of passages or documents. For example, queries require precise human-written topic statements to ensure accurate ranking.",
            "reference_string": "[278000561 | Tan et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Hierarchical Transformers for Multi-Document Summarization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 46,
            "citation_count": 298,
            "influential_citation_count": 41,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1905.13164",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.13164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "39798499",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1747893",
                    "name": "Mirella Lapata"
                }
            ],
            "abstract": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.",
            "corpus_id": 170079112,
            "sentences": [
                {
                    "corpus_id": "170079112",
                    "title": "Hierarchical Transformers for Multi-Document Summarization",
                    "text": "In this paper we conceptualized abstractive multidocument summarization as a machine learning problem. We proposed a new model which is able to encode multiple input documents hierarchically, learn latent relations across them, and additionally incorporate structural information from well-known graph representations. We have also demonstrated the importance of a learning-based approach for selecting which documents to summarize. Experimental results show that our model produces summaries which are both fluent and in-formative outperforming competitive systems by a wide margin. In the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks.",
                    "score": 0.5418199205531851,
                    "section_title": "Conclusions",
                    "char_start_offset": 28299,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 432
                        },
                        {
                            "start": 433,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 708
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9189453125
                }
            ],
            "relevance_judgement": 0.9189453125,
            "relevance_judgment_input_expanded": "# Title: Hierarchical Transformers for Multi-Document Summarization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yang Liu, Mirella Lapata\n## Abstract\nIn this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.\n## Conclusions\nIn this paper we conceptualized abstractive multidocument summarization as a machine learning problem. We proposed a new model which is able to encode multiple input documents hierarchically, learn latent relations across them, and additionally incorporate structural information from well-known graph representations. We have also demonstrated the importance of a learning-based approach for selecting which documents to summarize. Experimental results show that our model produces summaries which are both fluent and in-formative outperforming competitive systems by a wide margin. In the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks.",
            "reference_string": "[170079112 | Liu et al. | 2019 | Citations: 298]"
        },
        {
            "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2019,
            "reference_count": 44,
            "citation_count": 22,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D19-5404.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.12231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "26399699",
                    "name": "Diego Antognini"
                },
                {
                    "authorId": "1735128",
                    "name": "B. Faltings"
                }
            ],
            "abstract": "Linking facts across documents is a challenging task, as the language used to express the same information in a sentence can vary significantly, which complicates the task of multi-document summarization. Consequently, existing approaches heavily rely on hand-crafted features, which are domain-dependent and hard to craft, or additional annotated data, which is costly to gather. To overcome these limitations, we present a novel method, which makes use of two types of sentence embeddings: universal embeddings, which are trained on a large unrelated corpus, and domain-specific embeddings, which are learned during training. To this end, we develop SemSentSum, a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of summary, consisting of 665 bytes and 100 words. Unlike other state-of-the-art models, neither hand-crafted features nor additional annotated data are necessary, and the method is easily adaptable for other tasks. To our knowledge, we are the first to use multiple sentence embeddings for the task of multi-document summarization.",
            "corpus_id": 202889056,
            "sentences": [
                {
                    "corpus_id": "202889056",
                    "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
                    "text": "The idea of using multiple embeddings has been employed at the word level. Kiela et al. (2018) use an attention mechanism to combine the embeddings for each word for the task of natural language inference. Xu et al. (2018); Bollegala et al. (2015) concatenate the embeddings of each word into a vector before feeding a neural network for the tasks of aspect extraction and sentiment analysis. To our knowledge, we are the first to combine multiple types of sentence embeddings. \n\nExtractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988). \n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges.",
                    "score": 0.5903775087637,
                    "section_title": "Related Work",
                    "char_start_offset": 23272,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 392
                        },
                        {
                            "start": 393,
                            "end": 477
                        },
                        {
                            "start": 480,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 614
                        },
                        {
                            "start": 615,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 873
                        },
                        {
                            "start": 874,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1042
                        },
                        {
                            "start": 1043,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1404
                        },
                        {
                            "start": 1407,
                            "end": 1772
                        },
                        {
                            "start": 1773,
                            "end": 1957
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 75,
                            "end": 94,
                            "matchedPaperCorpusId": "52166626"
                        },
                        {
                            "start": 206,
                            "end": 222,
                            "matchedPaperCorpusId": "44009215"
                        },
                        {
                            "start": 224,
                            "end": 247,
                            "matchedPaperCorpusId": "14116842"
                        },
                        {
                            "start": 615,
                            "end": 627,
                            "matchedPaperCorpusId": "10103200"
                        },
                        {
                            "start": 719,
                            "end": 741,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 945,
                            "end": 965,
                            "matchedPaperCorpusId": "5457260"
                        },
                        {
                            "start": 965,
                            "end": 989,
                            "matchedPaperCorpusId": "17064982"
                        },
                        {
                            "start": 991,
                            "end": 1010,
                            "matchedPaperCorpusId": "9849366"
                        },
                        {
                            "start": 1073,
                            "end": 1090,
                            "matchedPaperCorpusId": "11977708"
                        },
                        {
                            "start": 1209,
                            "end": 1234,
                            "matchedPaperCorpusId": "337730"
                        },
                        {
                            "start": 1378,
                            "end": 1403,
                            "matchedPaperCorpusId": "60514661"
                        },
                        {
                            "start": 1495,
                            "end": 1526,
                            "matchedPaperCorpusId": "4508623"
                        },
                        {
                            "start": 1665,
                            "end": 1682,
                            "matchedPaperCorpusId": "1207010"
                        },
                        {
                            "start": 1710,
                            "end": 1724,
                            "matchedPaperCorpusId": "5879376"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91748046875
                }
            ],
            "relevance_judgement": 0.91748046875,
            "relevance_judgment_input_expanded": "# Title: Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Diego Antognini, B. Faltings\n## Abstract\nLinking facts across documents is a challenging task, as the language used to express the same information in a sentence can vary significantly, which complicates the task of multi-document summarization. Consequently, existing approaches heavily rely on hand-crafted features, which are domain-dependent and hard to craft, or additional annotated data, which is costly to gather. To overcome these limitations, we present a novel method, which makes use of two types of sentence embeddings: universal embeddings, which are trained on a large unrelated corpus, and domain-specific embeddings, which are learned during training. To this end, we develop SemSentSum, a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of summary, consisting of 665 bytes and 100 words. Unlike other state-of-the-art models, neither hand-crafted features nor additional annotated data are necessary, and the method is easily adaptable for other tasks. To our knowledge, we are the first to use multiple sentence embeddings for the task of multi-document summarization.\n## Related Work\nThe idea of using multiple embeddings has been employed at the word level. Kiela et al. (2018) use an attention mechanism to combine the embeddings for each word for the task of natural language inference. Xu et al. (2018); Bollegala et al. (2015) concatenate the embeddings of each word into a vector before feeding a neural network for the tasks of aspect extraction and sentiment analysis. To our knowledge, we are the first to combine multiple types of sentence embeddings. \n\nExtractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988). \n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges.",
            "reference_string": "[202889056 | Antognini et al. | 2019 | Citations: 22]"
        },
        {
            "title": "An Exploration of Document Impact on Graph-Based Multi-Document Summarization",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2008,
            "reference_count": 24,
            "citation_count": 82,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D08-1079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145078589",
                    "name": "Xiaojun Wan"
                }
            ],
            "abstract": "The graph-based ranking algorithm has been recently exploited for multi-document summarization by making only use of the sentence-to-sentence relationships in the documents, under the assumption that all the sentences are indistinguishable. However, given a document set to be summarized, different documents are usually not equally important, and moreover, different sentences in a specific document are usually differently important. This paper aims to explore document impact on summarization performance. We propose a document-based graph model to incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Various methods are employed to evaluate the two factors. Experimental results on the DUC2001 and DUC2002 datasets demonstrate that the good effectiveness of the proposed model. Moreover, the results show the robustness of the proposed model.",
            "corpus_id": 17446655,
            "sentences": [
                {
                    "corpus_id": "17446655",
                    "title": "An Exploration of Document Impact on Graph-Based Multi-Document Summarization",
                    "text": "Multi-document summarization aims to produce a summary describing the main topic in a document set, without any prior knowledge. Multi-document summary can be used to facilitate users to quickly understand a document cluster. For example, a number of news services (e.g. NewsInEssence1 ) have been developed to group news articles into news topics, and then produce a short summary for each news topic. Users can easily understand the topic they have interest in by taking a look at the short summary, without looking into each individual article within the topic cluster. \n\nAutomated multi-document summarization has drawn much attention in recent years. In the communities of natural language processing and information retrieval, a series of workshops and conferences on automatic text summarization (e.g. NTCIR, DUC), special topic sessions in ACL, COLING, and SIGIR have advanced the summarization techniques and produced a couple of experimental online systems. \n\nA particular challenge for multi-document summarization is that a document set might contain diverse information, which is either related or unrelated to the main topic, and hence we need effective summarization methods to analyze the information stored in different documents and extract the globally important information to reflect the main topic. In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features. \n\nMost recently, the graph-based models have been successfully applied for multi-document summarization by making use of the \"voting\" or \"recommendations\" between sentences in the documents (Erkan and Radev, 2004;Mihalcea and Tarau, 2005;Wan and Yang, 2006). The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences. The sentences with large rank scores are chosen into the summary. However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-todocument relationship. Actually, given a document set, different documents are not equally important.",
                    "score": 0.6978495450687118,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 572
                        },
                        {
                            "start": 575,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 967
                        },
                        {
                            "start": 970,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1551
                        },
                        {
                            "start": 1554,
                            "end": 1810
                        },
                        {
                            "start": 1811,
                            "end": 2016
                        },
                        {
                            "start": 2017,
                            "end": 2082
                        },
                        {
                            "start": 2083,
                            "end": 2287
                        },
                        {
                            "start": 2288,
                            "end": 2366
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1742,
                            "end": 1765,
                            "matchedPaperCorpusId": "10418456"
                        },
                        {
                            "start": 1765,
                            "end": 1790,
                            "matchedPaperCorpusId": "8878897"
                        },
                        {
                            "start": 1790,
                            "end": 1809,
                            "matchedPaperCorpusId": "5457260"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9169921875
                }
            ],
            "relevance_judgement": 0.9169921875,
            "relevance_judgment_input_expanded": "# Title: An Exploration of Document Impact on Graph-Based Multi-Document Summarization\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xiaojun Wan\n## Abstract\nThe graph-based ranking algorithm has been recently exploited for multi-document summarization by making only use of the sentence-to-sentence relationships in the documents, under the assumption that all the sentences are indistinguishable. However, given a document set to be summarized, different documents are usually not equally important, and moreover, different sentences in a specific document are usually differently important. This paper aims to explore document impact on summarization performance. We propose a document-based graph model to incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Various methods are employed to evaluate the two factors. Experimental results on the DUC2001 and DUC2002 datasets demonstrate that the good effectiveness of the proposed model. Moreover, the results show the robustness of the proposed model.\n## Introduction\nMulti-document summarization aims to produce a summary describing the main topic in a document set, without any prior knowledge. Multi-document summary can be used to facilitate users to quickly understand a document cluster. For example, a number of news services (e.g. NewsInEssence1 ) have been developed to group news articles into news topics, and then produce a short summary for each news topic. Users can easily understand the topic they have interest in by taking a look at the short summary, without looking into each individual article within the topic cluster. \n\nAutomated multi-document summarization has drawn much attention in recent years. In the communities of natural language processing and information retrieval, a series of workshops and conferences on automatic text summarization (e.g. NTCIR, DUC), special topic sessions in ACL, COLING, and SIGIR have advanced the summarization techniques and produced a couple of experimental online systems. \n\nA particular challenge for multi-document summarization is that a document set might contain diverse information, which is either related or unrelated to the main topic, and hence we need effective summarization methods to analyze the information stored in different documents and extract the globally important information to reflect the main topic. In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features. \n\nMost recently, the graph-based models have been successfully applied for multi-document summarization by making use of the \"voting\" or \"recommendations\" between sentences in the documents (Erkan and Radev, 2004;Mihalcea and Tarau, 2005;Wan and Yang, 2006). The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences. The sentences with large rank scores are chosen into the summary. However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-todocument relationship. Actually, given a document set, different documents are not equally important.",
            "reference_string": "[17446655 | Wan | 2008 | Citations: 82]"
        },
        {
            "title": "Tractable & Coherent Multi-Document Summarization: Discrete Optimization of Multiple Neural Modeling Streams via Integer Linear Programming",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 20,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.emnlp-industry.24.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2022.emnlp-industry.24?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2022.emnlp-industry.24, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3456974",
                    "name": "Litton J. Kurisinkel"
                },
                {
                    "authorId": "2118768398",
                    "name": "Nancy F. Chen"
                }
            ],
            "abstract": "One key challenge in multi-document summarization is the generated summary is often less coherent compared to single document summarization due to the larger heterogeneity of the input source content. In this work, we propose a generic framework to jointly consider coherence and informativeness in multi-document summarization and offers provisions to replace individual components based on the domain of source text. In particular, the framework characterizes coherence through verb transitions and entity mentions and takes advantage of syntactic parse trees and neural modeling for intra-sentential noise pruning. The framework cast the entire problem as an integer linear programming optimization problem with neural and non-neural models as linear components. We evaluate our method in the news and legal domains. The proposed approach consistently performs better than competitive baselines for both objective metrics and human evaluation.",
            "corpus_id": 257806309,
            "sentences": [
                {
                    "corpus_id": "257806309",
                    "title": "Tractable & Coherent Multi-Document Summarization: Discrete Optimization of Multiple Neural Modeling Streams via Integer Linear Programming",
                    "text": "One key challenge in multi-document summarization is the generated summary is often less coherent compared to single document summarization due to the larger heterogeneity of the input source content. In this work, we propose a generic framework to jointly consider coherence and informativeness in multi-document summarization and offers provisions to replace individual components based on the domain of source text. In particular, the framework characterizes coherence through verb transitions and entity mentions and takes advantage of syntactic parse trees and neural modeling for intra-sentential noise pruning. The framework cast the entire problem as an integer linear programming optimization problem with neural and non-neural models as linear components. We evaluate our method in the news and legal domains. The proposed approach consistently performs better than competitive baselines for both objective metrics and human evaluation.",
                    "score": 0.5744817130677973,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.916015625
                }
            ],
            "relevance_judgement": 0.916015625,
            "relevance_judgment_input_expanded": "# Title: Tractable & Coherent Multi-Document Summarization: Discrete Optimization of Multiple Neural Modeling Streams via Integer Linear Programming\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Litton J. Kurisinkel, Nancy F. Chen\n## Abstract\nOne key challenge in multi-document summarization is the generated summary is often less coherent compared to single document summarization due to the larger heterogeneity of the input source content. In this work, we propose a generic framework to jointly consider coherence and informativeness in multi-document summarization and offers provisions to replace individual components based on the domain of source text. In particular, the framework characterizes coherence through verb transitions and entity mentions and takes advantage of syntactic parse trees and neural modeling for intra-sentential noise pruning. The framework cast the entire problem as an integer linear programming optimization problem with neural and non-neural models as linear components. We evaluate our method in the news and legal domains. The proposed approach consistently performs better than competitive baselines for both objective metrics and human evaluation.\n",
            "reference_string": "[257806309 | Kurisinkel et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Multi-Document Summarization with Determinantal Point Process Attention",
            "venue": "Journal of Artificial Intelligence Research",
            "year": 2021,
            "reference_count": 61,
            "citation_count": 29,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jair.org/index.php/jair/article/download/12522/26693",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1613/jair.1.12522?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1613/jair.1.12522, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1400959575",
                    "name": "Laura Perez-Beltrachini"
                },
                {
                    "authorId": "1747893",
                    "name": "Mirella Lapata"
                }
            ],
            "abstract": "The ability to convey relevant and diverse information is critical in multi-document summarization and yet remains elusive for neural seq-to-seq models whose outputs are often redundant and fail to correctly cover important details. In this work, we propose an attention mechanism which encourages greater focus on relevance and diversity. Attention weights are computed based on (proportional) probabilities given by Determinantal Point Processes (DPPs) defined on the set of content units to be summarized. DPPs have been successfully used in extractive summarisation, here we use them to select relevant and diverse content for neural abstractive summarisation. We integrate DPP-based attention with various seq-to-seq architectures ranging from CNNs to LSTMs, and Transformers. Experimental evaluation shows that our attention mechanism consistently improves summarization and delivers performance comparable with the state-of-the-art on the MultiNews dataset",
            "corpus_id": 236150987,
            "sentences": [
                {
                    "corpus_id": "236150987",
                    "title": "Multi-Document Summarization with Determinantal Point Process Attention",
                    "text": "Multi-Document Summarization Most previous solutions to multi-document summarization adopt non-neural, extractive methods (Carbonell & Goldstein, 1998;Radev, Jing, Sty\u015b, & Tam, 2004;Erkan & Radev, 2004;Barzilay, McKeown, & Elhadad, 1999). \n\nMore recently, various encoder-decoder architectures (Liu & Lapata, 2019;Fabbri et al., 2019;Perez-Beltrachini et al., 2019;Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, & Shazeer, 2018;Zhang, Tan, & Wan, 2018;Lebanoff, Song, & Liu, 2018) have been ported to this task thanks to the development of large-scale datasets for model training. Among these, two approaches are closely related to our work on account of handling redundancy explicitly. Lebanoff et al. (2018) first pre-train an abstractive summarization model on single-document data and then fine-tune it on smaller multi-document benchmarks. They use a separately trained Maximal Marginal Relevance (MMR, Carbonell & Goldstein, 1998) module to select a relevant and non-redundant sentence from the input documents for the generation of the next summary sentence. Fabbri et al. (2019) incorporate this MMR mechanism as hierarchical attention into an end-to-end trained Pointer-Generator network (See et al., 2017). \n\nOur proposal differs from both approaches in terms of granularity; they operate at the sentence level while our model operates at the word level, resembling more the phrase selection approach of Barzilay et al. (1999). Another important difference lies in the way previously selected content is modelled. Lebanoff et al. (2018) explicitly select distinct sentences from the input, while Fabbri et al. (2019) do not track previously selected content and compute diversity as self-attention among all source sentences at each time step. In contrast, our DPP guided attention computes diversity at each time step between input tokens and a summary of previous context decisions.",
                    "score": 0.54402856855123,
                    "section_title": "Related Work",
                    "char_start_offset": 6143,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 238
                        },
                        {
                            "start": 241,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 841
                        },
                        {
                            "start": 842,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1213
                        },
                        {
                            "start": 1216,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1520
                        },
                        {
                            "start": 1521,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1891
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 122,
                            "end": 151,
                            "matchedPaperCorpusId": "4508623"
                        },
                        {
                            "start": 151,
                            "end": 182,
                            "matchedPaperCorpusId": "6354619"
                        },
                        {
                            "start": 182,
                            "end": 202,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 202,
                            "end": 237,
                            "matchedPaperCorpusId": "7031344"
                        },
                        {
                            "start": 365,
                            "end": 425,
                            "matchedPaperCorpusId": "3608234"
                        },
                        {
                            "start": 425,
                            "end": 449,
                            "matchedPaperCorpusId": "53223447"
                        },
                        {
                            "start": 905,
                            "end": 933,
                            "matchedPaperCorpusId": "4508623"
                        },
                        {
                            "start": 1411,
                            "end": 1433,
                            "matchedPaperCorpusId": "7031344"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91552734375
                }
            ],
            "relevance_judgement": 0.91552734375,
            "relevance_judgment_input_expanded": "# Title: Multi-Document Summarization with Determinantal Point Process Attention\n# Venue: Journal of Artificial Intelligence Research\n# Authors: Laura Perez-Beltrachini, Mirella Lapata\n## Abstract\nThe ability to convey relevant and diverse information is critical in multi-document summarization and yet remains elusive for neural seq-to-seq models whose outputs are often redundant and fail to correctly cover important details. In this work, we propose an attention mechanism which encourages greater focus on relevance and diversity. Attention weights are computed based on (proportional) probabilities given by Determinantal Point Processes (DPPs) defined on the set of content units to be summarized. DPPs have been successfully used in extractive summarisation, here we use them to select relevant and diverse content for neural abstractive summarisation. We integrate DPP-based attention with various seq-to-seq architectures ranging from CNNs to LSTMs, and Transformers. Experimental evaluation shows that our attention mechanism consistently improves summarization and delivers performance comparable with the state-of-the-art on the MultiNews dataset\n## Related Work\nMulti-Document Summarization Most previous solutions to multi-document summarization adopt non-neural, extractive methods (Carbonell & Goldstein, 1998;Radev, Jing, Sty\u015b, & Tam, 2004;Erkan & Radev, 2004;Barzilay, McKeown, & Elhadad, 1999). \n\nMore recently, various encoder-decoder architectures (Liu & Lapata, 2019;Fabbri et al., 2019;Perez-Beltrachini et al., 2019;Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, & Shazeer, 2018;Zhang, Tan, & Wan, 2018;Lebanoff, Song, & Liu, 2018) have been ported to this task thanks to the development of large-scale datasets for model training. Among these, two approaches are closely related to our work on account of handling redundancy explicitly. Lebanoff et al. (2018) first pre-train an abstractive summarization model on single-document data and then fine-tune it on smaller multi-document benchmarks. They use a separately trained Maximal Marginal Relevance (MMR, Carbonell & Goldstein, 1998) module to select a relevant and non-redundant sentence from the input documents for the generation of the next summary sentence. Fabbri et al. (2019) incorporate this MMR mechanism as hierarchical attention into an end-to-end trained Pointer-Generator network (See et al., 2017). \n\nOur proposal differs from both approaches in terms of granularity; they operate at the sentence level while our model operates at the word level, resembling more the phrase selection approach of Barzilay et al. (1999). Another important difference lies in the way previously selected content is modelled. Lebanoff et al. (2018) explicitly select distinct sentences from the input, while Fabbri et al. (2019) do not track previously selected content and compute diversity as self-attention among all source sentences at each time step. In contrast, our DPP guided attention computes diversity at each time step between input tokens and a summary of previous context decisions.",
            "reference_string": "[236150987 | Perez-Beltrachini et al. | 2021 | Citations: 29]"
        },
        {
            "title": "Mining Both Commonality and Specificity From Multiple Documents for Multi-Document Summarization",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 30,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10499708.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.02677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2084599072",
                    "name": "Bing Ma"
                }
            ],
            "abstract": "The multi-document summarization task requires the designed summarizer to generate a short text that covers the important information of original multiple documents and satisfies content diversity. To fulfill the dual requirements of coverage and diversity in multi-document summarization, this study introduces a novel method. Initially, a class tree is constructed through hierarchical clustering of documents. Subsequently, a sentence selection method based on class tree is proposed for generating a summary. Specifically, a top-down traversal is performed on the class tree, during which sentences are selected from each node based on their similarity to the centroid of the documents within the node and their dissimilarity to the centroid of documents not belonging to the node. Sentences selected from the root node reflect the commonality of all document, and sentences selected from the sub nodes reflect the distinct specificity of the respective subclasses. Experimental results on standard text summarization datasets DUC\u20192002, DUC\u20192003, and DUC\u20192004 demonstrate that the proposed method significantly outperforms the variant method that considers only commonality of all documents, achieving average improvements of up to 1.54 and 1.42 in ROUGE-1 and ROUGE-L scores, respectively. Additionally, the method demonstrates significant superiority over another variant method that considers only the specificity of subclasses, achieving average improvements of up to 2.16 and 2.01 in ROUGE-1 and ROUGE-L scores, respectively. Furthermore, extensive experiments on DUC\u20192004 and Multi-News datasets show that the proposed method outperforms lots of competitive supervised and unsupervised multi-document summarization methods and yields considerable results.",
            "corpus_id": 257364970,
            "sentences": [
                {
                    "corpus_id": "257364970",
                    "title": "Mining Both Commonality and Specificity From Multiple Documents for Multi-Document Summarization",
                    "text": "Automatic text summarization is becoming much more important because of the exponential growth of digital textual information on the web. Multidocument summarization, which aims to generate a short text containing important and diverse information of original multiple documents, is a challenging focus of NLP research. A well-organized summary of multiple documents needs to cover the main information of all documents comprehensively and simultaneously satisfy content diversity. Extractive summarization approaches, which generate a summary by selecting a few important sentences from original documents, attract much attention because of its simplicity and robustness. This paper focuses on extractive multi-document summarization. \n\nMost extractive multi-document summarization approaches splice all the sentences contained in the original documents into a larger text, and then generate a summary by selecting sentences from the larger text (Lamsiyah et al., 2021;Yang et al., 2014;Erkan and Radev, 2004). However, the task of summarizing multiple documents is more difficult than the task of summarizing a single document. Simply transforming multi-document summarization task into summarizing a single larger text completely breaks the constraints of documents on their sentences and lacks comparisons between documents, which results in the inability to mine the relevant information between documents, including mining the common information (commonality) of all documents and the important specific information (specificity) of some subclasses of documents. \n\nThe centroid-based summarization approaches focus on the commonality of all documents or all sentences and they select sentences based on the centroid words of all documents (Radev et al., 2004;Rossiello et al., 2017) or the centroid embedding of all sentences (Lamsiyah et al., 2021). The clustering-based summarization approaches divide sentences into multiple groups and select sentences from each group (Yang et al., 2014;Sarkar, 2009). These approaches do not take into account the commonality and specificity of documents simultaneously. \n\nThink about the process of human summarizing multiple documents: we would first describe the common information of all documents and then the important specific information of some subclasses of these documents respectively to satisfy the coverage and diversity requirements of multi-document summarization.",
                    "score": 0.572888769790699,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 481
                        },
                        {
                            "start": 482,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 735
                        },
                        {
                            "start": 738,
                            "end": 1011
                        },
                        {
                            "start": 1012,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1568
                        },
                        {
                            "start": 1571,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 2011
                        },
                        {
                            "start": 2012,
                            "end": 2114
                        },
                        {
                            "start": 2117,
                            "end": 2424
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 947,
                            "end": 970,
                            "matchedPaperCorpusId": "228954621"
                        },
                        {
                            "start": 970,
                            "end": 988,
                            "matchedPaperCorpusId": "5792920"
                        },
                        {
                            "start": 988,
                            "end": 1010,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1765,
                            "end": 1788,
                            "matchedPaperCorpusId": "2346086"
                        },
                        {
                            "start": 1832,
                            "end": 1855,
                            "matchedPaperCorpusId": "228954621"
                        },
                        {
                            "start": 1978,
                            "end": 1997,
                            "matchedPaperCorpusId": "5792920"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91455078125
                }
            ],
            "relevance_judgement": 0.91455078125,
            "relevance_judgment_input_expanded": "# Title: Mining Both Commonality and Specificity From Multiple Documents for Multi-Document Summarization\n# Venue: IEEE Access\n# Authors: Bing Ma\n## Abstract\nThe multi-document summarization task requires the designed summarizer to generate a short text that covers the important information of original multiple documents and satisfies content diversity. To fulfill the dual requirements of coverage and diversity in multi-document summarization, this study introduces a novel method. Initially, a class tree is constructed through hierarchical clustering of documents. Subsequently, a sentence selection method based on class tree is proposed for generating a summary. Specifically, a top-down traversal is performed on the class tree, during which sentences are selected from each node based on their similarity to the centroid of the documents within the node and their dissimilarity to the centroid of documents not belonging to the node. Sentences selected from the root node reflect the commonality of all document, and sentences selected from the sub nodes reflect the distinct specificity of the respective subclasses. Experimental results on standard text summarization datasets DUC\u20192002, DUC\u20192003, and DUC\u20192004 demonstrate that the proposed method significantly outperforms the variant method that considers only commonality of all documents, achieving average improvements of up to 1.54 and 1.42 in ROUGE-1 and ROUGE-L scores, respectively. Additionally, the method demonstrates significant superiority over another variant method that considers only the specificity of subclasses, achieving average improvements of up to 2.16 and 2.01 in ROUGE-1 and ROUGE-L scores, respectively. Furthermore, extensive experiments on DUC\u20192004 and Multi-News datasets show that the proposed method outperforms lots of competitive supervised and unsupervised multi-document summarization methods and yields considerable results.\n## Introduction\nAutomatic text summarization is becoming much more important because of the exponential growth of digital textual information on the web. Multidocument summarization, which aims to generate a short text containing important and diverse information of original multiple documents, is a challenging focus of NLP research. A well-organized summary of multiple documents needs to cover the main information of all documents comprehensively and simultaneously satisfy content diversity. Extractive summarization approaches, which generate a summary by selecting a few important sentences from original documents, attract much attention because of its simplicity and robustness. This paper focuses on extractive multi-document summarization. \n\nMost extractive multi-document summarization approaches splice all the sentences contained in the original documents into a larger text, and then generate a summary by selecting sentences from the larger text (Lamsiyah et al., 2021;Yang et al., 2014;Erkan and Radev, 2004). However, the task of summarizing multiple documents is more difficult than the task of summarizing a single document. Simply transforming multi-document summarization task into summarizing a single larger text completely breaks the constraints of documents on their sentences and lacks comparisons between documents, which results in the inability to mine the relevant information between documents, including mining the common information (commonality) of all documents and the important specific information (specificity) of some subclasses of documents. \n\nThe centroid-based summarization approaches focus on the commonality of all documents or all sentences and they select sentences based on the centroid words of all documents (Radev et al., 2004;Rossiello et al., 2017) or the centroid embedding of all sentences (Lamsiyah et al., 2021). The clustering-based summarization approaches divide sentences into multiple groups and select sentences from each group (Yang et al., 2014;Sarkar, 2009). These approaches do not take into account the commonality and specificity of documents simultaneously. \n\nThink about the process of human summarizing multiple documents: we would first describe the common information of all documents and then the important specific information of some subclasses of these documents respectively to satisfy the coverage and diversity requirements of multi-document summarization.",
            "reference_string": "[257364970 | Ma | 2023 | Citations: 3]"
        },
        {
            "title": "Entropy-based Sampling for Abstractive Multi-document Summarization in Low-resource Settings",
            "venue": "International Conference on Natural Language Generation",
            "year": 2023,
            "reference_count": 40,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.inlg-main.9.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.inlg-main.9, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2121237",
                    "name": "Laura Mascarell"
                },
                {
                    "authorId": "1879120021",
                    "name": "Ribin Chalumattu"
                },
                {
                    "authorId": "2253607868",
                    "name": "Julien Heitmann"
                }
            ],
            "abstract": "Research in Multi-document Summarization (MDS) mostly focuses on the English language and depends on large MDS datasets that are not available for other languages. Some of these approaches concatenate the source documents, resulting in overlong model inputs. Existing transformer architectures are unable to process such long inputs entirely, omitting documents in the summarization process. Other solutions address this issue by implementing multi-stage approaches that also require changes in the model architecture. In this paper, we introduce various sampling approaches based on information entropy that allow us to perform MDS in a single stage. These approaches also consider all source documents without using MDS training data nor changing the model\u2019s architecture. Besides, we build a MDS test set of German news articles to assess the performance of our methods on abstractive multi-document summaries. Experimental results show that our entropy-based approaches outperform previous state-of-the-art on German MDS, while still remaining primarily abstractive. We release our code and MDS test set to encourage further research in German abstractive MDS.",
            "corpus_id": 263610015,
            "sentences": [
                {
                    "corpus_id": "263610015",
                    "title": "Entropy-based Sampling for Abstractive Multi-document Summarization in Low-resource Settings",
                    "text": "In light of the ever-growing volume of available information, it becomes essential to be able to automatically summarize information from several sources. Multi-document Summarization (MDS) aims at condensing the most important information from different documents. Despite the advances in single-document summarization (Zhang et al., 2020), summarizing multiple related documents remains a greater challenge due to its input length and the presence of redundant information (Fan et al., 2019;Song et al., 2022). Therefore, some research focuses on implementing multi-stage approaches that first identify the relevant information to then feed it into a summarization model (Lebanoff et al., 2018;Liu and Lapata, 2019a). More recent works utilize pre-trained language models (Lewis et al., 2020;Raffel et al., 2020;Xiao et al., 2022) finetuned for the summarization task and feed them with the source documents concatenated (Johner et al., 2021;Xiao et al., 2022). However, these approaches pose two major issues. First, concatenated inputs exceeding the length limit of the model are truncated, which might lead to the omission of entire documents. Second, they rely on multidocument datasets that are scarce or unavailable in languages other than English. Hokamp et al. (2020) introduce a decoding strategy that adapts single-to multi-document summarization without using additional training data nor applying changes to the single-input model architecture. At every decoding timestep, it averages the output probabilities of a single-document summarization model for each individual document, combining them into a single output. Instead of averaging all log-probabilities, which favours highly frequent tokens, we propose to make a more informed decision. In particular, we leverage entropy to measure the model confidence in the next token prediction and thus select the most informative output. We implement different entropy-based approaches and evaluate their performance on MDS of German text. Our main contributions are: \n\n\u2022 We present different entropy-based sampling approaches for the MDS task. These are specially well-suited for languages like German that have limited or unavailable MDS data.",
                    "score": 0.5719751260199148,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 512
                        },
                        {
                            "start": 513,
                            "end": 719
                        },
                        {
                            "start": 720,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1012
                        },
                        {
                            "start": 1013,
                            "end": 1148
                        },
                        {
                            "start": 1149,
                            "end": 1256
                        },
                        {
                            "start": 1257,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1631
                        },
                        {
                            "start": 1632,
                            "end": 1758
                        },
                        {
                            "start": 1759,
                            "end": 1899
                        },
                        {
                            "start": 1900,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2029
                        },
                        {
                            "start": 2032,
                            "end": 2106
                        },
                        {
                            "start": 2107,
                            "end": 2207
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 320,
                            "end": 340,
                            "matchedPaperCorpusId": "209405420"
                        },
                        {
                            "start": 475,
                            "end": 493,
                            "matchedPaperCorpusId": "202785778"
                        },
                        {
                            "start": 493,
                            "end": 511,
                            "matchedPaperCorpusId": "248512466"
                        },
                        {
                            "start": 673,
                            "end": 696,
                            "matchedPaperCorpusId": "52053741"
                        },
                        {
                            "start": 696,
                            "end": 718,
                            "matchedPaperCorpusId": "170079112"
                        },
                        {
                            "start": 774,
                            "end": 794,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 794,
                            "end": 814,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 814,
                            "end": 832,
                            "matchedPaperCorpusId": "247519084"
                        },
                        {
                            "start": 923,
                            "end": 944,
                            "matchedPaperCorpusId": "235258298"
                        },
                        {
                            "start": 944,
                            "end": 962,
                            "matchedPaperCorpusId": "247519084"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9130859375
                }
            ],
            "relevance_judgement": 0.9130859375,
            "relevance_judgment_input_expanded": "# Title: Entropy-based Sampling for Abstractive Multi-document Summarization in Low-resource Settings\n# Venue: International Conference on Natural Language Generation\n# Authors: Laura Mascarell, Ribin Chalumattu, Julien Heitmann\n## Abstract\nResearch in Multi-document Summarization (MDS) mostly focuses on the English language and depends on large MDS datasets that are not available for other languages. Some of these approaches concatenate the source documents, resulting in overlong model inputs. Existing transformer architectures are unable to process such long inputs entirely, omitting documents in the summarization process. Other solutions address this issue by implementing multi-stage approaches that also require changes in the model architecture. In this paper, we introduce various sampling approaches based on information entropy that allow us to perform MDS in a single stage. These approaches also consider all source documents without using MDS training data nor changing the model\u2019s architecture. Besides, we build a MDS test set of German news articles to assess the performance of our methods on abstractive multi-document summaries. Experimental results show that our entropy-based approaches outperform previous state-of-the-art on German MDS, while still remaining primarily abstractive. We release our code and MDS test set to encourage further research in German abstractive MDS.\n## Introduction\nIn light of the ever-growing volume of available information, it becomes essential to be able to automatically summarize information from several sources. Multi-document Summarization (MDS) aims at condensing the most important information from different documents. Despite the advances in single-document summarization (Zhang et al., 2020), summarizing multiple related documents remains a greater challenge due to its input length and the presence of redundant information (Fan et al., 2019;Song et al., 2022). Therefore, some research focuses on implementing multi-stage approaches that first identify the relevant information to then feed it into a summarization model (Lebanoff et al., 2018;Liu and Lapata, 2019a). More recent works utilize pre-trained language models (Lewis et al., 2020;Raffel et al., 2020;Xiao et al., 2022) finetuned for the summarization task and feed them with the source documents concatenated (Johner et al., 2021;Xiao et al., 2022). However, these approaches pose two major issues. First, concatenated inputs exceeding the length limit of the model are truncated, which might lead to the omission of entire documents. Second, they rely on multidocument datasets that are scarce or unavailable in languages other than English. Hokamp et al. (2020) introduce a decoding strategy that adapts single-to multi-document summarization without using additional training data nor applying changes to the single-input model architecture. At every decoding timestep, it averages the output probabilities of a single-document summarization model for each individual document, combining them into a single output. Instead of averaging all log-probabilities, which favours highly frequent tokens, we propose to make a more informed decision. In particular, we leverage entropy to measure the model confidence in the next token prediction and thus select the most informative output. We implement different entropy-based approaches and evaluate their performance on MDS of German text. Our main contributions are: \n\n\u2022 We present different entropy-based sampling approaches for the MDS task. These are specially well-suited for languages like German that have limited or unavailable MDS data.",
            "reference_string": "[263610015 | Mascarell et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Topic-Guided Abstractive Multi-Document Summarization",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 31,
            "citation_count": 41,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-emnlp.126.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.11207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143738684",
                    "name": "Peng Cui"
                },
                {
                    "authorId": "2109312896",
                    "name": "Le Hu"
                }
            ],
            "abstract": "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics.",
            "corpus_id": 239050558,
            "sentences": [
                {
                    "corpus_id": "239050558",
                    "title": "Topic-Guided Abstractive Multi-Document Summarization",
                    "text": "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics.",
                    "score": 0.56798364750425,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91259765625
                }
            ],
            "relevance_judgement": 0.91259765625,
            "relevance_judgment_input_expanded": "# Title: Topic-Guided Abstractive Multi-Document Summarization\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Peng Cui, Le Hu\n## Abstract\nA critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics.\n",
            "reference_string": "[239050558 | Cui et al. | 2021 | Citations: 41]"
        },
        {
            "title": "Do Multi-Document Summarization Models Synthesize?",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 88,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1162/tacl_a_00687",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.13844, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48727916",
                    "name": "Jay DeYoung"
                },
                {
                    "authorId": "2203750000",
                    "name": "Stephanie C. Martinez"
                },
                {
                    "authorId": "1808775",
                    "name": "I. Marshall"
                },
                {
                    "authorId": "1912476",
                    "name": "Byron C. Wallace"
                }
            ],
            "abstract": "Abstract Multi-document summarization entails producing concise synopses of collections of inputs. For some applications, the synopsis should accurately synthesize inputs with respect to a key aspect, e.g., a synopsis of film reviews written about a particular movie should reflect the average critic consensus. As a more consequential example, narrative summaries that accompany biomedical systematic reviews of clinical trial results should accurately summarize the potentially conflicting results from individual trials. In this paper we ask: To what extent do modern multi-document summarization models implicitly perform this sort of synthesis? We run experiments over opinion and evidence synthesis datasets using a suite of summarization models, from fine-tuned transformers to GPT-4. We find that existing models partially perform synthesis, but imperfectly: Even the best performing models are over-sensitive to changes in input ordering and under-sensitive to changes in input compositions (e.g., ratio of positive to negative reviews). We propose a simple, general, effective method for improving model synthesis capabilities by generating an explicitly diverse set of candidate outputs, and then selecting from these the string best aligned with the expected aggregate measure for the inputs, or abstaining when the model produces no good candidate.",
            "corpus_id": 256416214,
            "sentences": [
                {
                    "corpus_id": "256416214",
                    "title": "Do Multi-Document Summarization Models Synthesize?",
                    "text": "Automatic (multi-document) summarization (Nenkova and McKeown, 2011;Maybury, 1999) has been an active subfield within NLP for decades. We have focused our analysis on modern, neural abstractive models for conditional text generation (Bahdanau et al., 2015). In light of their empirical success, we have specifically evaluated a set of Transformer-based (Vaswani et al., 2017) models which have recently been used for multidocument summarization (Beltagy et al., 2020;Zhang et al., 2020;Xiao et al., 2022;Raffel et al., 2020). There has been some work on highlighting conflicting evidence in health literature specifically (Shah et al., 2021b,a), though this focused primarily on highlighting conflicting evidence and explicitly aggregating extracted content. \n\nMultiple works have attempted gauge the difficulty of multi-document summarization. Wolhandler et al. (2022) measures the difficulty of abstractive multi-document news summarization as a function of inputs necessary to produce a final summary; they find that two to four well-chosen documents can cover a news topic sufficiently for the summarizer. They also find systematic reviews are particularly ill-suited to this minimal covering approach. Giorgi et al. (2022) studies the impact of document retrieval behaviors on multidocument summarization performance, and find that models are sensitive to missing inputs. \n\nSentence fusion One view on synthesis might be that is a particular kind of sentence fusion (Barzilay and McKeown, 2005). However, past work on \"fusing\" sentences has assumed that the aim is to generate an output that contains the information common to similar sentences (Thadani and McKeown, 2013). This is intuitive in the context of, e.g., summarizing multiple news articles covering the same event. But here we are interested in the more challenging setting in which the output should reflect an aggregate measure of potentially conflicting evidence or opinions. \n\nReview and opinion summarization considers a similar task to ours: Aggregating (usually product) reviews and opinions into a single coherent text.",
                    "score": 0.5569913819153017,
                    "section_title": "Related Work",
                    "char_start_offset": 29821,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 758
                        },
                        {
                            "start": 761,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1206
                        },
                        {
                            "start": 1207,
                            "end": 1376
                        },
                        {
                            "start": 1379,
                            "end": 1500
                        },
                        {
                            "start": 1501,
                            "end": 1678
                        },
                        {
                            "start": 1679,
                            "end": 1781
                        },
                        {
                            "start": 1782,
                            "end": 1945
                        },
                        {
                            "start": 1948,
                            "end": 2094
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 233,
                            "end": 256,
                            "matchedPaperCorpusId": "11212020"
                        },
                        {
                            "start": 467,
                            "end": 486,
                            "matchedPaperCorpusId": "209405420"
                        },
                        {
                            "start": 504,
                            "end": 524,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 845,
                            "end": 869,
                            "matchedPaperCorpusId": "253098164"
                        },
                        {
                            "start": 1471,
                            "end": 1499,
                            "matchedPaperCorpusId": "16188305"
                        },
                        {
                            "start": 1650,
                            "end": 1677,
                            "matchedPaperCorpusId": "12635978"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91259765625
                }
            ],
            "relevance_judgement": 0.91259765625,
            "relevance_judgment_input_expanded": "# Title: Do Multi-Document Summarization Models Synthesize?\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Jay DeYoung, Stephanie C. Martinez, I. Marshall, Byron C. Wallace\n## Abstract\nAbstract Multi-document summarization entails producing concise synopses of collections of inputs. For some applications, the synopsis should accurately synthesize inputs with respect to a key aspect, e.g., a synopsis of film reviews written about a particular movie should reflect the average critic consensus. As a more consequential example, narrative summaries that accompany biomedical systematic reviews of clinical trial results should accurately summarize the potentially conflicting results from individual trials. In this paper we ask: To what extent do modern multi-document summarization models implicitly perform this sort of synthesis? We run experiments over opinion and evidence synthesis datasets using a suite of summarization models, from fine-tuned transformers to GPT-4. We find that existing models partially perform synthesis, but imperfectly: Even the best performing models are over-sensitive to changes in input ordering and under-sensitive to changes in input compositions (e.g., ratio of positive to negative reviews). We propose a simple, general, effective method for improving model synthesis capabilities by generating an explicitly diverse set of candidate outputs, and then selecting from these the string best aligned with the expected aggregate measure for the inputs, or abstaining when the model produces no good candidate.\n## Related Work\nAutomatic (multi-document) summarization (Nenkova and McKeown, 2011;Maybury, 1999) has been an active subfield within NLP for decades. We have focused our analysis on modern, neural abstractive models for conditional text generation (Bahdanau et al., 2015). In light of their empirical success, we have specifically evaluated a set of Transformer-based (Vaswani et al., 2017) models which have recently been used for multidocument summarization (Beltagy et al., 2020;Zhang et al., 2020;Xiao et al., 2022;Raffel et al., 2020). There has been some work on highlighting conflicting evidence in health literature specifically (Shah et al., 2021b,a), though this focused primarily on highlighting conflicting evidence and explicitly aggregating extracted content. \n\nMultiple works have attempted gauge the difficulty of multi-document summarization. Wolhandler et al. (2022) measures the difficulty of abstractive multi-document news summarization as a function of inputs necessary to produce a final summary; they find that two to four well-chosen documents can cover a news topic sufficiently for the summarizer. They also find systematic reviews are particularly ill-suited to this minimal covering approach. Giorgi et al. (2022) studies the impact of document retrieval behaviors on multidocument summarization performance, and find that models are sensitive to missing inputs. \n\nSentence fusion One view on synthesis might be that is a particular kind of sentence fusion (Barzilay and McKeown, 2005). However, past work on \"fusing\" sentences has assumed that the aim is to generate an output that contains the information common to similar sentences (Thadani and McKeown, 2013). This is intuitive in the context of, e.g., summarizing multiple news articles covering the same event. But here we are interested in the more challenging setting in which the output should reflect an aggregate measure of potentially conflicting evidence or opinions. \n\nReview and opinion summarization considers a similar task to ours: Aggregating (usually product) reviews and opinions into a single coherent text.",
            "reference_string": "[256416214 | DeYoung et al. | 2023 | Citations: 8]"
        },
        {
            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 32,
            "citation_count": 73,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-main.380.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.naacl-main.380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "10721120",
                    "name": "Ramakanth Pasunuru"
                },
                {
                    "authorId": "2940333",
                    "name": "Mengwen Liu"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "120209444",
                    "name": "Sujith Ravi"
                },
                {
                    "authorId": "40262269",
                    "name": "Markus Dreyer"
                }
            ],
            "abstract": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.",
            "corpus_id": 235097309,
            "sentences": [
                {
                    "corpus_id": "235097309",
                    "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                    "text": "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani and Bloedorn, 1997;Radev and McKeown, 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. Mani and Bloedorn (1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and McKeown (1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, Erkan and Radev (2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). Dasgupta et al. (2013) incorporated dependency graph features into their sentence relation graphs. Baralis et al. (2013) built graphs over sets of terms, rather than sentences. Li et al. (2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. Liu et al. (2015) and Liao et al. (2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com",
                    "score": 0.6589208309966629,
                    "section_title": "Related Work",
                    "char_start_offset": 4341,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 697
                        },
                        {
                            "start": 700,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1765
                        },
                        {
                            "start": 1768,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2011
                        },
                        {
                            "start": 2012,
                            "end": 2271
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 115,
                            "end": 140,
                            "matchedPaperCorpusId": "6025826"
                        },
                        {
                            "start": 140,
                            "end": 164,
                            "matchedPaperCorpusId": "10019526"
                        },
                        {
                            "start": 373,
                            "end": 397,
                            "matchedPaperCorpusId": "6025826"
                        },
                        {
                            "start": 558,
                            "end": 572,
                            "matchedPaperCorpusId": "10019526"
                        },
                        {
                            "start": 896,
                            "end": 918,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1289,
                            "end": 1311,
                            "matchedPaperCorpusId": "16587947"
                        },
                        {
                            "start": 1388,
                            "end": 1409,
                            "matchedPaperCorpusId": "15709889"
                        },
                        {
                            "start": 1466,
                            "end": 1482,
                            "matchedPaperCorpusId": "6788641"
                        },
                        {
                            "start": 1614,
                            "end": 1631,
                            "matchedPaperCorpusId": "5001921"
                        },
                        {
                            "start": 1636,
                            "end": 1654,
                            "matchedPaperCorpusId": "49210924"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91064453125
                }
            ],
            "relevance_judgement": 0.91064453125,
            "relevance_judgment_input_expanded": "# Title: Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Ramakanth Pasunuru, Mengwen Liu, Mohit Bansal, Sujith Ravi, Markus Dreyer\n## Abstract\nThis paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.\n## Related Work\nResearchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani and Bloedorn, 1997;Radev and McKeown, 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. Mani and Bloedorn (1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and McKeown (1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, Erkan and Radev (2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). Dasgupta et al. (2013) incorporated dependency graph features into their sentence relation graphs. Baralis et al. (2013) built graphs over sets of terms, rather than sentences. Li et al. (2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. Liu et al. (2015) and Liao et al. (2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com",
            "reference_string": "[235097309 | Pasunuru et al. | 2021 | Citations: 73]"
        },
        {
            "title": "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section",
            "venue": "Brazilian Conference on Intelligent Systems",
            "year": 2024,
            "reference_count": 20,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16444, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2318273161",
                    "name": "Leandro Car'isio Fernandes"
                },
                {
                    "authorId": "2317979625",
                    "name": "Gustavo Bartz Guedes"
                },
                {
                    "authorId": "2188833716",
                    "name": "T. Laitz"
                },
                {
                    "authorId": "2188833452",
                    "name": "Thales Sales Almeida"
                },
                {
                    "authorId": "2268315298",
                    "name": "R. Nogueira"
                },
                {
                    "authorId": "2256889299",
                    "name": "R.A. Lotufo"
                },
                {
                    "authorId": "2257137831",
                    "name": "Jayr Pereira"
                }
            ],
            "abstract": "Document summarization is a task to shorten texts into concise and informative summaries. This paper introduces a novel dataset designed for summarizing multiple scientific articles into a section of a survey. Our contributions are: (1) SurveySum, a new dataset addressing the gap in domain-specific summarization tools; (2) two specific pipelines to summarize scientific articles into a section of a survey; and (3) the evaluation of these pipelines using multiple metrics to compare their performance. Our results highlight the importance of high-quality retrieval stages and the impact of different configurations on the quality of generated summaries.",
            "corpus_id": 272146191,
            "sentences": [
                {
                    "corpus_id": "272146191",
                    "title": "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section",
                    "text": "Multi-document summarization (MDS) addresses the problem of extracting information that is spread across multiple documents, making it more challenging than single-document summarization. It is still an evolving field, with no single approach to solve this problem. The development of this field relies on datasets that serve as benchmarks for evaluating and comparing different summarization methods. The scientific literature includes some examples of such datasets, each varying in domain, structure, size, and summarization objective. These datasets are important for advancing research and improving summarization models. \n\nOutside the scientific domain, Multi-News [4] presents a large volume of news articles and summaries, focusing on the abstractive summarization of multiple documents in the journalistic context. Also in this context, Ghalandari et al. [5] proposed a large-scale dataset for multi-document summarization that contains concise human-written summaries of news events. In the legal field, Multi-LexSum tackles the summarization of legal cases, presenting a set of civil rights litigation summaries with multiple granularities. The summarization of instructional content is the focus of the HowSumm dataset, derived from WikiHow articles [1]. The FINDSum dataset focuses on the challenge of summarizing long text and multiple tables [10]. \n\nIn the scientific domain, the need to manage the vast amount of literature has generated interest in datasets specific to the summarization of scientific articles. Multi-XScience [12] is a dataset focused on the generation of \"related work\" sections by summarizing multiple scientific articles. BigSurvey [9] was designed to create structured summaries of academic articles, focusing on consolidating literature reviews. SumPubMed [6] and ScisummNet [19] aim the summarization of single scientific articles. \n\nSeveral other datasets can be used for multi-document summarization. Koh et al. [8] work is an extensive survey on long document summarization, including ten other MDS datasets.",
                    "score": 0.6620516618442351,
                    "section_title": "Related Work",
                    "char_start_offset": 3392,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 626
                        },
                        {
                            "start": 629,
                            "end": 823
                        },
                        {
                            "start": 824,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1362
                        },
                        {
                            "start": 1365,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1659
                        },
                        {
                            "start": 1660,
                            "end": 1785
                        },
                        {
                            "start": 1786,
                            "end": 1872
                        },
                        {
                            "start": 1875,
                            "end": 1943
                        },
                        {
                            "start": 1944,
                            "end": 2052
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1670,
                            "end": 1673,
                            "matchedPaperCorpusId": "250636132"
                        },
                        {
                            "start": 1796,
                            "end": 1799,
                            "matchedPaperCorpusId": "236772881"
                        },
                        {
                            "start": 1815,
                            "end": 1819,
                            "matchedPaperCorpusId": "58053521"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90966796875
                }
            ],
            "relevance_judgement": 0.90966796875,
            "relevance_judgment_input_expanded": "# Title: SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section\n# Venue: Brazilian Conference on Intelligent Systems\n# Authors: Leandro Car'isio Fernandes, Gustavo Bartz Guedes, T. Laitz, Thales Sales Almeida, R. Nogueira, R.A. Lotufo, Jayr Pereira\n## Abstract\nDocument summarization is a task to shorten texts into concise and informative summaries. This paper introduces a novel dataset designed for summarizing multiple scientific articles into a section of a survey. Our contributions are: (1) SurveySum, a new dataset addressing the gap in domain-specific summarization tools; (2) two specific pipelines to summarize scientific articles into a section of a survey; and (3) the evaluation of these pipelines using multiple metrics to compare their performance. Our results highlight the importance of high-quality retrieval stages and the impact of different configurations on the quality of generated summaries.\n## Related Work\nMulti-document summarization (MDS) addresses the problem of extracting information that is spread across multiple documents, making it more challenging than single-document summarization. It is still an evolving field, with no single approach to solve this problem. The development of this field relies on datasets that serve as benchmarks for evaluating and comparing different summarization methods. The scientific literature includes some examples of such datasets, each varying in domain, structure, size, and summarization objective. These datasets are important for advancing research and improving summarization models. \n\nOutside the scientific domain, Multi-News [4] presents a large volume of news articles and summaries, focusing on the abstractive summarization of multiple documents in the journalistic context. Also in this context, Ghalandari et al. [5] proposed a large-scale dataset for multi-document summarization that contains concise human-written summaries of news events. In the legal field, Multi-LexSum tackles the summarization of legal cases, presenting a set of civil rights litigation summaries with multiple granularities. The summarization of instructional content is the focus of the HowSumm dataset, derived from WikiHow articles [1]. The FINDSum dataset focuses on the challenge of summarizing long text and multiple tables [10]. \n\nIn the scientific domain, the need to manage the vast amount of literature has generated interest in datasets specific to the summarization of scientific articles. Multi-XScience [12] is a dataset focused on the generation of \"related work\" sections by summarizing multiple scientific articles. BigSurvey [9] was designed to create structured summaries of academic articles, focusing on consolidating literature reviews. SumPubMed [6] and ScisummNet [19] aim the summarization of single scientific articles. \n\nSeveral other datasets can be used for multi-document summarization. Koh et al. [8] work is an extensive survey on long document summarization, including ten other MDS datasets.",
            "reference_string": "[272146191 | Fernandes et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Multi-Document Abstractive Summarization Using ILP Based Multi-Sentence Compression",
            "venue": "International Joint Conference on Artificial Intelligence",
            "year": 2015,
            "reference_count": 28,
            "citation_count": 136,
            "influential_citation_count": 19,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.07034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2169453878",
                    "name": "Siddhartha Banerjee"
                },
                {
                    "authorId": "143930195",
                    "name": "P. Mitra"
                },
                {
                    "authorId": "3060386",
                    "name": "Kazunari Sugiyama"
                }
            ],
            "abstract": "Abstractive summarization is an ideal form of summarization since it can synthesize information from multiple documents to create concise informative summaries. In this work, we aim at developing an abstractive summarizer. First, our proposed approach identifies the most important document in the multi-document set. The sentences in the most important document are aligned to sentences in other documents to generate clusters of similar sentences. Second, we generate K-shortest paths from the sentences in each cluster using a word-graph structure. Finally, we select sentences from the set of shortest paths generated from all the clusters employing a novel integer linear programming (ILP) model with the objective of maximizing information content and readability of the final summary. Our ILP model represents the shortest paths as binary variables and considers the length of the path, information score and linguistic quality score in the objective function. Experimental results on the DUC 2004 and 2005 multi-document summarization datasets show that our proposed approach outperforms all the baselines and state-of-the-art extractive summarizers as measured by the ROUGE scores. Our method also outperforms a recent abstractive summarization technique. In manual evaluation, our approach also achieves promising results on informativeness and readability.",
            "corpus_id": 15795297,
            "sentences": [
                {
                    "corpus_id": "15795297",
                    "title": "Multi-Document Abstractive Summarization Using ILP Based Multi-Sentence Compression",
                    "text": "In multidocument summarization, all documents are not equally important; some documents contain more information on the main topics in the document set. Our first step estimates the importance of a document in the whole dataset using LexRank [Erkan and Radev, 2004], Pairwise Cosine Similarity and Overall Document Collection Similarity. Each sentence from the most important document are initialized into separate clusters. Thereafter, each sentence from the other documents are assigned to the cluster that has the highest similarity with the sentence. In the generation step, we first generate a word-graph structure from the sentences in each cluster and construct K shortest paths from the graph between the start and end nodes. We formulate a novel integer linear programming (ILP) problem that maximizes the information content and linguistic quality of the generated summary. Our ILP problem represents each of the K shortest paths as a binary variable. The coefficients of each variable in the objective function is obtained by combining the information score of the path and the linguistic quality score. We introduce several constraints into our ILP model. We ensure that only one sentence is generated from each cluster. Second, we avoid redundant sentences that carry the same or similar information from different clusters. The solution to the optimization problem decides the paths that would be included in the final abstractive summary. \n\nOn the DUC2004 and DUC2005 datasets, we demonstrate the effectiveness of our proposed method. Our proposed method outperforms not only some popular baselines but also the state-of-the-art extractive summarization systems. ROUGE scores [Lin, 2004] obtained by our system outperforms the best extractive summarizer on both the datasets. Our method also outperforms an abstractive summarizer based on multi-sentence compression [Filippova, 2010] when measured by ROUGE-2, ROUGE-L and ROUGE-SU4 scores. Further, manual evaluation by human judges shows that our technique produces summaries with acceptable linguistic quality and high informativeness.",
                    "score": 0.5342803727967043,
                    "section_title": "Introduction",
                    "char_start_offset": 2158,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 424
                        },
                        {
                            "start": 425,
                            "end": 554
                        },
                        {
                            "start": 555,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1453
                        },
                        {
                            "start": 1456,
                            "end": 1549
                        },
                        {
                            "start": 1550,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1790
                        },
                        {
                            "start": 1791,
                            "end": 1954
                        },
                        {
                            "start": 1955,
                            "end": 2102
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1691,
                            "end": 1702,
                            "matchedPaperCorpusId": "12299544"
                        },
                        {
                            "start": 1881,
                            "end": 1898,
                            "matchedPaperCorpusId": "14750088"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9091796875
                }
            ],
            "relevance_judgement": 0.9091796875,
            "relevance_judgment_input_expanded": "# Title: Multi-Document Abstractive Summarization Using ILP Based Multi-Sentence Compression\n# Venue: International Joint Conference on Artificial Intelligence\n# Authors: Siddhartha Banerjee, P. Mitra, Kazunari Sugiyama\n## Abstract\nAbstractive summarization is an ideal form of summarization since it can synthesize information from multiple documents to create concise informative summaries. In this work, we aim at developing an abstractive summarizer. First, our proposed approach identifies the most important document in the multi-document set. The sentences in the most important document are aligned to sentences in other documents to generate clusters of similar sentences. Second, we generate K-shortest paths from the sentences in each cluster using a word-graph structure. Finally, we select sentences from the set of shortest paths generated from all the clusters employing a novel integer linear programming (ILP) model with the objective of maximizing information content and readability of the final summary. Our ILP model represents the shortest paths as binary variables and considers the length of the path, information score and linguistic quality score in the objective function. Experimental results on the DUC 2004 and 2005 multi-document summarization datasets show that our proposed approach outperforms all the baselines and state-of-the-art extractive summarizers as measured by the ROUGE scores. Our method also outperforms a recent abstractive summarization technique. In manual evaluation, our approach also achieves promising results on informativeness and readability.\n## Introduction\nIn multidocument summarization, all documents are not equally important; some documents contain more information on the main topics in the document set. Our first step estimates the importance of a document in the whole dataset using LexRank [Erkan and Radev, 2004], Pairwise Cosine Similarity and Overall Document Collection Similarity. Each sentence from the most important document are initialized into separate clusters. Thereafter, each sentence from the other documents are assigned to the cluster that has the highest similarity with the sentence. In the generation step, we first generate a word-graph structure from the sentences in each cluster and construct K shortest paths from the graph between the start and end nodes. We formulate a novel integer linear programming (ILP) problem that maximizes the information content and linguistic quality of the generated summary. Our ILP problem represents each of the K shortest paths as a binary variable. The coefficients of each variable in the objective function is obtained by combining the information score of the path and the linguistic quality score. We introduce several constraints into our ILP model. We ensure that only one sentence is generated from each cluster. Second, we avoid redundant sentences that carry the same or similar information from different clusters. The solution to the optimization problem decides the paths that would be included in the final abstractive summary. \n\nOn the DUC2004 and DUC2005 datasets, we demonstrate the effectiveness of our proposed method. Our proposed method outperforms not only some popular baselines but also the state-of-the-art extractive summarization systems. ROUGE scores [Lin, 2004] obtained by our system outperforms the best extractive summarizer on both the datasets. Our method also outperforms an abstractive summarizer based on multi-sentence compression [Filippova, 2010] when measured by ROUGE-2, ROUGE-L and ROUGE-SU4 scores. Further, manual evaluation by human judges shows that our technique produces summaries with acceptable linguistic quality and high informativeness.",
            "reference_string": "[15795297 | Banerjee et al. | 2015 | Citations: 136]"
        },
        {
            "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization",
            "venue": "European Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316447969",
                    "name": "Ran Liu"
                },
                {
                    "authorId": "2287803134",
                    "name": "Ming Liu"
                },
                {
                    "authorId": "2316535336",
                    "name": "Min Yu"
                },
                {
                    "authorId": "152634491",
                    "name": "Jianguo Jiang"
                },
                {
                    "authorId": "1996030117",
                    "name": "Gang Li"
                },
                {
                    "authorId": "2316503511",
                    "name": "Dan Zhang"
                },
                {
                    "authorId": "2316432820",
                    "name": "Jingyuan Li"
                },
                {
                    "authorId": "2282447587",
                    "name": "Xiang Meng"
                },
                {
                    "authorId": "2282485475",
                    "name": "Weiqing Huang"
                }
            ],
            "abstract": "Pre-trained language models are increasingly being used in multi-document summarization tasks. However, these models need large-scale corpora for pre-training and are domain-dependent. Other non-neural unsupervised summarization approaches mostly rely on key sentence extraction, which can lead to information loss. To address these challenges, we propose a lightweight yet effective unsupervised approach called GLIMMER: a Graph and LexIcal features based unsupervised Multi-docuMEnt summaRization approach. It first constructs a sentence graph from the source documents, then automatically identifies semantic clusters by mining low-level features from raw texts, thereby improving intra-cluster correlation and the fluency of generated sentences. Finally, it summarizes clusters into natural sentences. Experiments conducted on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach outperforms existing unsupervised approaches. Furthermore, it surpasses state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally, human evaluations indicate that summaries generated by GLIMMER achieve high readability and informativeness scores. Our code is available at https://github.com/Oswald1997/GLIMMER.",
            "corpus_id": 271903777,
            "sentences": [
                {
                    "corpus_id": "271903777",
                    "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization",
                    "text": "Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise. Compared with single-document summarization, it's more challenging because an increasing number of input documents will make the source information more redundant and scattered. It also has practical significance, for example, key information of multiple news articles can be generated efficiently. Multi-document summarization approaches can also be applied in other scenarios, such as extracting opinions from social media. \n\nAs the application of MDS becomes more and more widespread, various approaches have been proposed. Non-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary. The main drawback of such approaches is that they retain only a subset of key sentences, which can lead to information loss and may not capture finegrained details. \n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020). Despite their capability in extracting abstract features, neural models are often resource-intensive and require large parallel training datasets. Moreover, recent studies (Pagnoni et al., 2021) have shown that they also suffer from factuality problems. \n\nPre-trained language models use large-scale cor- pora to optimize objective functions, allowing them to acquire more general feature representations. This capability enables them to require only a small amount of data to fine-tune downstream tasks and achieve impressive results.",
                    "score": 0.62387314117589,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 665
                        },
                        {
                            "start": 668,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 909
                        },
                        {
                            "start": 910,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1252
                        },
                        {
                            "start": 1255,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1662
                        },
                        {
                            "start": 1663,
                            "end": 1835
                        },
                        {
                            "start": 1836,
                            "end": 1982
                        },
                        {
                            "start": 1983,
                            "end": 2089
                        },
                        {
                            "start": 2092,
                            "end": 2241
                        },
                        {
                            "start": 2242,
                            "end": 2371
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 837,
                            "end": 860,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 860,
                            "end": 885,
                            "matchedPaperCorpusId": "577937"
                        },
                        {
                            "start": 885,
                            "end": 908,
                            "matchedPaperCorpusId": "2346086"
                        },
                        {
                            "start": 1508,
                            "end": 1529,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 1529,
                            "end": 1546,
                            "matchedPaperCorpusId": "222090788"
                        },
                        {
                            "start": 1546,
                            "end": 1563,
                            "matchedPaperCorpusId": "220633461"
                        },
                        {
                            "start": 1778,
                            "end": 1801,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 1801,
                            "end": 1818,
                            "matchedPaperCorpusId": "199466313"
                        },
                        {
                            "start": 1818,
                            "end": 1834,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 2008,
                            "end": 2030,
                            "matchedPaperCorpusId": "233407441"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90869140625
                }
            ],
            "relevance_judgement": 0.90869140625,
            "relevance_judgment_input_expanded": "# Title: GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization\n# Venue: European Conference on Artificial Intelligence\n# Authors: Ran Liu, Ming Liu, Min Yu, Jianguo Jiang, Gang Li, Dan Zhang, Jingyuan Li, Xiang Meng, Weiqing Huang\n## Abstract\nPre-trained language models are increasingly being used in multi-document summarization tasks. However, these models need large-scale corpora for pre-training and are domain-dependent. Other non-neural unsupervised summarization approaches mostly rely on key sentence extraction, which can lead to information loss. To address these challenges, we propose a lightweight yet effective unsupervised approach called GLIMMER: a Graph and LexIcal features based unsupervised Multi-docuMEnt summaRization approach. It first constructs a sentence graph from the source documents, then automatically identifies semantic clusters by mining low-level features from raw texts, thereby improving intra-cluster correlation and the fluency of generated sentences. Finally, it summarizes clusters into natural sentences. Experiments conducted on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach outperforms existing unsupervised approaches. Furthermore, it surpasses state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally, human evaluations indicate that summaries generated by GLIMMER achieve high readability and informativeness scores. Our code is available at https://github.com/Oswald1997/GLIMMER.\n## Introduction\nMulti-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise. Compared with single-document summarization, it's more challenging because an increasing number of input documents will make the source information more redundant and scattered. It also has practical significance, for example, key information of multiple news articles can be generated efficiently. Multi-document summarization approaches can also be applied in other scenarios, such as extracting opinions from social media. \n\nAs the application of MDS becomes more and more widespread, various approaches have been proposed. Non-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary. The main drawback of such approaches is that they retain only a subset of key sentences, which can lead to information loss and may not capture finegrained details. \n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020). Despite their capability in extracting abstract features, neural models are often resource-intensive and require large parallel training datasets. Moreover, recent studies (Pagnoni et al., 2021) have shown that they also suffer from factuality problems. \n\nPre-trained language models use large-scale cor- pora to optimize objective functions, allowing them to acquire more general feature representations. This capability enables them to require only a small amount of data to fine-tune downstream tasks and achieve impressive results.",
            "reference_string": "[271903777 | Liu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "SupMMD: A Sentence Importance Model for Extractive Summarisation using Maximum Mean Discrepancy",
            "venue": "Findings",
            "year": 2020,
            "reference_count": 50,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.367.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.02568, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2168767",
                    "name": "Umanga Bista"
                },
                {
                    "authorId": "3175685",
                    "name": "A. Mathews"
                },
                {
                    "authorId": "2844480",
                    "name": "A. Menon"
                },
                {
                    "authorId": "33650938",
                    "name": "Lexing Xie"
                }
            ],
            "abstract": "Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest (e.g., presenting readers with updates on an evolving news topic). In this work, we present SupMMD, a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. SupMMD combines both supervised learning for salience and unsupervised learning for coverage and diversity. Further, we adapt multiple kernel learning to make use of similarity across multiple information sources (e.g., text features and knowledge based concepts). We show the efficacy of SupMMD in both generic and update summarization tasks by meeting or exceeding the current state-of-the-art on the DUC-2004 and TAC-2009 datasets.",
            "corpus_id": 222140880,
            "sentences": [
                {
                    "corpus_id": "222140880",
                    "title": "SupMMD: A Sentence Importance Model for Extractive Summarisation using Maximum Mean Discrepancy",
                    "text": "Multi-document summarization is the problem of producing condensed digests of salient information from multiple sources, such as articles. Concretely, suppose we are given two sets of articles (denoted set A and set B) on a related topic (e.g., climate change, the COVID-19 pandemic), separated by publication timestamp or geographic region. We may then identify three possible instantiations of multi-document summarization (see Figure 1): \n\n(i) generic summarization, where the goal is to summarize a set (A or B) individually. (ii) comparative summarization, where the goal is to summarize a set (B) against another set (A) while highlighting the differences. (iii) update summarization, where the goal is both generic summarization of set A and comparative summarization of set B versus A. Most existing work on this topic has focused on the generic summarization task. However, update summarization is of equal practical interest. \n\nIntuitively, the comparative aspect of this setting aims to inform a user of new information on a topic they are already familiar with. \n\nMulti-document extractive summarization methods can be unsupervised or supervised. Unsupervised methods typically define salience (or coverage) using a global model of sentence-sentence similarity. Methods based on retrieval (Goldstein et al., 1999), centroids (Radev et al., 2004), graph centrality (Erkan and Radev, 2004), or utility maximization (Lin andBilmes, 2010, 2011;Gillick and Favre, 2009) have been well explored. However, sentence salience also depends on surface features (e.g., position, length, presence of cue words); effectively capturing these requires supervised models specific to the dataset and task. A body of work has incorporated such information through supervised learning, for example based on point processes (Kulesza and Taskar, 2012), learning important words (Hong and Nenkova, 2014), graph neural networks (Yasunaga et al., 2017), and support vector regression (Varma et al., 2009).",
                    "score": 0.7052627193009757,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 138
                        },
                        {
                            "start": 139,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 440
                        },
                        {
                            "start": 443,
                            "end": 529
                        },
                        {
                            "start": 530,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 873
                        },
                        {
                            "start": 874,
                            "end": 935
                        },
                        {
                            "start": 938,
                            "end": 1073
                        },
                        {
                            "start": 1076,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1699
                        },
                        {
                            "start": 1700,
                            "end": 1992
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1301,
                            "end": 1325,
                            "matchedPaperCorpusId": "11218013"
                        },
                        {
                            "start": 1376,
                            "end": 1399,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1425,
                            "end": 1433,
                            "matchedPaperCorpusId": "964287"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9072265625
                }
            ],
            "relevance_judgement": 0.9072265625,
            "relevance_judgment_input_expanded": "# Title: SupMMD: A Sentence Importance Model for Extractive Summarisation using Maximum Mean Discrepancy\n# Venue: Findings\n# Authors: Umanga Bista, A. Mathews, A. Menon, Lexing Xie\n## Abstract\nMost work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest (e.g., presenting readers with updates on an evolving news topic). In this work, we present SupMMD, a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. SupMMD combines both supervised learning for salience and unsupervised learning for coverage and diversity. Further, we adapt multiple kernel learning to make use of similarity across multiple information sources (e.g., text features and knowledge based concepts). We show the efficacy of SupMMD in both generic and update summarization tasks by meeting or exceeding the current state-of-the-art on the DUC-2004 and TAC-2009 datasets.\n## Introduction\nMulti-document summarization is the problem of producing condensed digests of salient information from multiple sources, such as articles. Concretely, suppose we are given two sets of articles (denoted set A and set B) on a related topic (e.g., climate change, the COVID-19 pandemic), separated by publication timestamp or geographic region. We may then identify three possible instantiations of multi-document summarization (see Figure 1): \n\n(i) generic summarization, where the goal is to summarize a set (A or B) individually. (ii) comparative summarization, where the goal is to summarize a set (B) against another set (A) while highlighting the differences. (iii) update summarization, where the goal is both generic summarization of set A and comparative summarization of set B versus A. Most existing work on this topic has focused on the generic summarization task. However, update summarization is of equal practical interest. \n\nIntuitively, the comparative aspect of this setting aims to inform a user of new information on a topic they are already familiar with. \n\nMulti-document extractive summarization methods can be unsupervised or supervised. Unsupervised methods typically define salience (or coverage) using a global model of sentence-sentence similarity. Methods based on retrieval (Goldstein et al., 1999), centroids (Radev et al., 2004), graph centrality (Erkan and Radev, 2004), or utility maximization (Lin andBilmes, 2010, 2011;Gillick and Favre, 2009) have been well explored. However, sentence salience also depends on surface features (e.g., position, length, presence of cue words); effectively capturing these requires supervised models specific to the dataset and task. A body of work has incorporated such information through supervised learning, for example based on point processes (Kulesza and Taskar, 2012), learning important words (Hong and Nenkova, 2014), graph neural networks (Yasunaga et al., 2017), and support vector regression (Varma et al., 2009).",
            "reference_string": "[222140880 | Bista et al. | 2020 | Citations: 2]"
        },
        {
            "title": "Information Fusion in the Context of Multi-Document Summarization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 1999,
            "reference_count": 23,
            "citation_count": 484,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.3115/1034678.1034760",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P99-1071, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1741283",
                    "name": "R. Barzilay"
                },
                {
                    "authorId": "145590324",
                    "name": "K. McKeown"
                },
                {
                    "authorId": "1754680",
                    "name": "Michael Elhadad"
                }
            ],
            "abstract": "We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents. Our approach is unique in its usage of language generation to reformulate the wording of the summary.",
            "corpus_id": 7031344,
            "sentences": [
                {
                    "corpus_id": "7031344",
                    "title": "Information Fusion in the Context of Multi-Document Summarization",
                    "text": "Information overload has created an acute need for summarization. Typically, the same information is described by many different online documents. Hence, summaries that synthesize common information across documents and emphasize the differences would significantly help readers. Such a summary would be beneficial, for example, to a user who follows a single event through several newswires. In this paper, we present research on the automatic fusion of similar information across multiple documents using language generation to produce a concise summary. \n\nWe propose a method for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for summary generation. \n\nMost research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997;Marcu, 1997;Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while ex-tracting some similar sentences could produce a summary biased towards some sources. \n\nInstead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be included in the summary and sentence generation to reformulate them as new text. Our work is part of a full summarization system (McKeown et al., 1999), which extracts sets of similax sentences, themes (Eskin et al., 1999), in the first stage for input to the components described here. \n\nOur model for multi-document summarization represents a number of departures from traditional language generation. Typically, language generation systems have access to a full semantic representation of the domain. A content planner selects and orders propositions from an underlying knowledge base to form text content. A sentence planner determines how to combine propositions into a single sentence, and a sentence generator realizes each set of combined propositions as a sentence, mapping from concepts to words and building syntactic structure. Our approach differs in the following ways: Content planning operates over full sentences, producing sentence fragments.",
                    "score": 0.6201722674166745,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 66,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 392
                        },
                        {
                            "start": 393,
                            "end": 556
                        },
                        {
                            "start": 559,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 767
                        },
                        {
                            "start": 768,
                            "end": 896
                        },
                        {
                            "start": 899,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1419
                        },
                        {
                            "start": 1422,
                            "end": 1634
                        },
                        {
                            "start": 1635,
                            "end": 1840
                        },
                        {
                            "start": 1843,
                            "end": 1957
                        },
                        {
                            "start": 1958,
                            "end": 2057
                        },
                        {
                            "start": 2058,
                            "end": 2163
                        },
                        {
                            "start": 2164,
                            "end": 2393
                        },
                        {
                            "start": 2394,
                            "end": 2514
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1036,
                            "end": 1056,
                            "matchedPaperCorpusId": "5519987"
                        },
                        {
                            "start": 1056,
                            "end": 1068,
                            "matchedPaperCorpusId": "11680756"
                        },
                        {
                            "start": 1068,
                            "end": 1088,
                            "matchedPaperCorpusId": "32296317"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90478515625
                }
            ],
            "relevance_judgement": 0.90478515625,
            "relevance_judgment_input_expanded": "# Title: Information Fusion in the Context of Multi-Document Summarization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: R. Barzilay, K. McKeown, Michael Elhadad\n## Abstract\nWe present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents. Our approach is unique in its usage of language generation to reformulate the wording of the summary.\n## Introduction\nInformation overload has created an acute need for summarization. Typically, the same information is described by many different online documents. Hence, summaries that synthesize common information across documents and emphasize the differences would significantly help readers. Such a summary would be beneficial, for example, to a user who follows a single event through several newswires. In this paper, we present research on the automatic fusion of similar information across multiple documents using language generation to produce a concise summary. \n\nWe propose a method for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for summary generation. \n\nMost research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997;Marcu, 1997;Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while ex-tracting some similar sentences could produce a summary biased towards some sources. \n\nInstead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be included in the summary and sentence generation to reformulate them as new text. Our work is part of a full summarization system (McKeown et al., 1999), which extracts sets of similax sentences, themes (Eskin et al., 1999), in the first stage for input to the components described here. \n\nOur model for multi-document summarization represents a number of departures from traditional language generation. Typically, language generation systems have access to a full semantic representation of the domain. A content planner selects and orders propositions from an underlying knowledge base to form text content. A sentence planner determines how to combine propositions into a single sentence, and a sentence generator realizes each set of combined propositions as a sentence, mapping from concepts to words and building syntactic structure. Our approach differs in the following ways: Content planning operates over full sentences, producing sentence fragments.",
            "reference_string": "[7031344 | Barzilay et al. | 1999 | Citations: 484]"
        },
        {
            "title": "Extractive Multi-Document Summarization: A Review of Progress in the Last Decade",
            "venue": "IEEE Access",
            "year": 2021,
            "reference_count": 81,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09536694.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2021.3112496?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2021.3112496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "70642020",
                    "name": "Zakia Jalil"
                },
                {
                    "authorId": "35560924",
                    "name": "Jamal Abdul Nasir"
                },
                {
                    "authorId": "2128893061",
                    "name": "Muhammad Nasir"
                }
            ],
            "abstract": "With the tremendous growth in the number of electronic documents, it is becoming challenging to manage the volume of information. Much research has focused on automatically summarizing the information available in the documents. Multi-Document Summarization (MDS) is one approach that aims to extract the information from the available documents in such a concise way that none of the important points are missed from the summary while avoiding the redundancy of information at the same time. This study presents an extensive survey of extractive MDS over the last decade to show the progress of research in this field. We present different techniques of extractive MDS and compare their strengths and weaknesses. Research work is presented by category and evaluated to help the reader understand the work in this field and to guide them in defining their own research directions. Benchmark datasets and standard evaluation techniques are also presented. This study concludes that most of the extractive MDS techniques are successful in developing salient and information-rich summaries of the documents provided.",
            "corpus_id": 238221709,
            "sentences": [
                {
                    "corpus_id": "238221709",
                    "title": "Extractive Multi-Document Summarization: A Review of Progress in the Last Decade",
                    "text": "For instance, the same contents may be covered from multiple sources, so at times, a number of documents may be available to gain an insight into the same event [5]. In this regard, a multi-document summary becomes a representation of the information contained in a cluster of documents which helps users understand the gist of those documents [9], [10]. A multi-document summary represents the information contained in the cluster of documents and helps users understand those documents [11]. \n\nThe task of multi-document summarization is much more complex than single-document summarization, even when the available single document is very large-sized. This difficulty is attributed to the inevitable diversity of themes within a large set of documents. \n\nA summary can be Abstractive or Extractive, depending on the method of summarization. Generally, an abstractive summary consists of concepts and ideas abstracted from the source document(s) and then represented in preferably different words. This method involves a thorough understanding of the meaning of the content. Two broad areas of Natural Language Processing (NLP) [12] that handle abstractive summary are semantic representation and natural language generation [4]. These involve various approaches, such as multimodal semantic models, information item-based methods, and semantic graph-based methods [13]. \n\nAn extractive summary is described as units of text extracted from the source document(s) and combined as a summary verbatim [14]. In this method, the important sentences of the documents under consideration are ranked and combined to form the summary [15]. Extractive summarization techniques can be divided into various categories, such as: query-based or generic and supervised or unsupervised methods. Generic summarization is based on preparing a summary of the main topic of the documents, whereas query-based summarization involves generating a summary related to the subject of the query asked by the user [9], [10], [12], [16]- [21]. \n\nIn order to gain a broader picture of research in this field, we have performed a systematic survey of the literature on extractive techniques of MDS. The survey may serve as a starting point for prospected researchers to identify gaps in current research.",
                    "score": 0.5724585970695696,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 2039,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 493
                        },
                        {
                            "start": 496,
                            "end": 654
                        },
                        {
                            "start": 655,
                            "end": 755
                        },
                        {
                            "start": 758,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1372
                        },
                        {
                            "start": 1375,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1632
                        },
                        {
                            "start": 1633,
                            "end": 1780
                        },
                        {
                            "start": 1781,
                            "end": 2017
                        },
                        {
                            "start": 2020,
                            "end": 2170
                        },
                        {
                            "start": 2171,
                            "end": 2276
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 161,
                            "end": 164,
                            "matchedPaperCorpusId": "45592507"
                        },
                        {
                            "start": 344,
                            "end": 347,
                            "matchedPaperCorpusId": "596849"
                        },
                        {
                            "start": 349,
                            "end": 353,
                            "matchedPaperCorpusId": "38686738"
                        },
                        {
                            "start": 488,
                            "end": 492,
                            "matchedPaperCorpusId": "4490918"
                        },
                        {
                            "start": 1130,
                            "end": 1134,
                            "matchedPaperCorpusId": "33913820"
                        },
                        {
                            "start": 1227,
                            "end": 1230,
                            "matchedPaperCorpusId": "36306586"
                        },
                        {
                            "start": 1367,
                            "end": 1371,
                            "matchedPaperCorpusId": "63740251"
                        },
                        {
                            "start": 1500,
                            "end": 1504,
                            "matchedPaperCorpusId": "32634196"
                        },
                        {
                            "start": 1627,
                            "end": 1631,
                            "matchedPaperCorpusId": "10353431"
                        },
                        {
                            "start": 1989,
                            "end": 1992,
                            "matchedPaperCorpusId": "596849"
                        },
                        {
                            "start": 1994,
                            "end": 1998,
                            "matchedPaperCorpusId": "38686738"
                        },
                        {
                            "start": 2000,
                            "end": 2004,
                            "matchedPaperCorpusId": "33913820"
                        },
                        {
                            "start": 2006,
                            "end": 2010,
                            "matchedPaperCorpusId": "3780782"
                        },
                        {
                            "start": 2012,
                            "end": 2016,
                            "matchedPaperCorpusId": "46073204"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.904296875
                }
            ],
            "relevance_judgement": 0.904296875,
            "relevance_judgment_input_expanded": "# Title: Extractive Multi-Document Summarization: A Review of Progress in the Last Decade\n# Venue: IEEE Access\n# Authors: Zakia Jalil, Jamal Abdul Nasir, Muhammad Nasir\n## Abstract\nWith the tremendous growth in the number of electronic documents, it is becoming challenging to manage the volume of information. Much research has focused on automatically summarizing the information available in the documents. Multi-Document Summarization (MDS) is one approach that aims to extract the information from the available documents in such a concise way that none of the important points are missed from the summary while avoiding the redundancy of information at the same time. This study presents an extensive survey of extractive MDS over the last decade to show the progress of research in this field. We present different techniques of extractive MDS and compare their strengths and weaknesses. Research work is presented by category and evaluated to help the reader understand the work in this field and to guide them in defining their own research directions. Benchmark datasets and standard evaluation techniques are also presented. This study concludes that most of the extractive MDS techniques are successful in developing salient and information-rich summaries of the documents provided.\n## I. INTRODUCTION\nFor instance, the same contents may be covered from multiple sources, so at times, a number of documents may be available to gain an insight into the same event [5]. In this regard, a multi-document summary becomes a representation of the information contained in a cluster of documents which helps users understand the gist of those documents [9], [10]. A multi-document summary represents the information contained in the cluster of documents and helps users understand those documents [11]. \n\nThe task of multi-document summarization is much more complex than single-document summarization, even when the available single document is very large-sized. This difficulty is attributed to the inevitable diversity of themes within a large set of documents. \n\nA summary can be Abstractive or Extractive, depending on the method of summarization. Generally, an abstractive summary consists of concepts and ideas abstracted from the source document(s) and then represented in preferably different words. This method involves a thorough understanding of the meaning of the content. Two broad areas of Natural Language Processing (NLP) [12] that handle abstractive summary are semantic representation and natural language generation [4]. These involve various approaches, such as multimodal semantic models, information item-based methods, and semantic graph-based methods [13]. \n\nAn extractive summary is described as units of text extracted from the source document(s) and combined as a summary verbatim [14]. In this method, the important sentences of the documents under consideration are ranked and combined to form the summary [15]. Extractive summarization techniques can be divided into various categories, such as: query-based or generic and supervised or unsupervised methods. Generic summarization is based on preparing a summary of the main topic of the documents, whereas query-based summarization involves generating a summary related to the subject of the query asked by the user [9], [10], [12], [16]- [21]. \n\nIn order to gain a broader picture of research in this field, we have performed a systematic survey of the literature on extractive techniques of MDS. The survey may serve as a starting point for prospected researchers to identify gaps in current research.",
            "reference_string": "[238221709 | Jalil et al. | 2021 | Citations: 14]"
        },
        {
            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 20,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.00548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152171283",
                    "name": "Ning Wang"
                },
                {
                    "authorId": "2140162523",
                    "name": "Han Liu"
                },
                {
                    "authorId": "1753376",
                    "name": "D. Klabjan"
                }
            ],
            "abstract": "We develop an abstractive summarization framework independent of labeled data for multiple heterogeneous documents. Unlike existing multi-document summarization methods, our framework processes documents telling different stories instead of documents on the same topic. We also enhance an existing sentence fusion method with a uni-directional language model to prioritize fused sentences with higher sentence probability with the goal of increasing readability. Lastly, we construct a total of twelve dataset variations based on CNN/Daily Mail and the NewsRoom datasets, where each document group contains a large and diverse collection of documents to evaluate the performance of our model in comparison with other baseline systems. Our experiments demonstrate that our framework outperforms current state-of-the-art methods in this more generic setting.",
            "corpus_id": 248496597,
            "sentences": [
                {
                    "corpus_id": "248496597",
                    "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
                    "text": "We develop an abstractive summarization framework independent of labeled data for multiple heterogeneous documents. Unlike existing multi-document summarization methods, our framework processes documents telling different stories instead of documents on the same topic. We also enhance an existing sentence fusion method with a uni-directional language model to prioritize fused sentences with higher sentence probability with the goal of increasing readability. Lastly, we construct a total of twelve dataset variations based on CNN/Daily Mail and the NewsRoom datasets, where each document group contains a large and diverse collection of documents to evaluate the performance of our model in comparison with other baseline systems. Our experiments demonstrate that our framework outperforms current state-of-the-art methods in this more generic setting.",
                    "score": 0.6381986655209576,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: Large-Scale Multi-Document Summarization with Information Extraction and Compression\n# Venue: arXiv.org\n# Authors: Ning Wang, Han Liu, D. Klabjan\n## Abstract\nWe develop an abstractive summarization framework independent of labeled data for multiple heterogeneous documents. Unlike existing multi-document summarization methods, our framework processes documents telling different stories instead of documents on the same topic. We also enhance an existing sentence fusion method with a uni-directional language model to prioritize fused sentences with higher sentence probability with the goal of increasing readability. Lastly, we construct a total of twelve dataset variations based on CNN/Daily Mail and the NewsRoom datasets, where each document group contains a large and diverse collection of documents to evaluate the performance of our model in comparison with other baseline systems. Our experiments demonstrate that our framework outperforms current state-of-the-art methods in this more generic setting.\n",
            "reference_string": "[248496597 | Wang et al. | 2022 | Citations: 1]"
        },
        {
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "venue": "PeerJ Computer Science",
            "year": 2023,
            "reference_count": 55,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.7717/peerj-cs.1737",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10773739, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2274342252",
                    "name": "Wu Su"
                },
                {
                    "authorId": "2274904693",
                    "name": "Jin Jiang"
                },
                {
                    "authorId": "2274639188",
                    "name": "Kaihui Huang"
                }
            ],
            "abstract": "The crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes. Last but not least, the model defines nodes with different levels of granularity\u2014ranging from documents and topics to sentences\u2014and these various nodes necessitate different propagation widths and depths for capturing intricate relationships in the information being disseminated. Adaptive measures are taken to learn the importance and correlation between nodes of different granularities in terms of both width and depth. Experimental evidence from two benchmark datasets highlights the superior performance of the proposed model, as assessed by ROUGE metrics, in comparison to existing approaches, even in the absence of pre-trained language models. Additionally, an ablation study confirms the positive impact of each individual module on the model's ROUGE scores.",
            "corpus_id": 266244733,
            "sentences": [
                {
                    "corpus_id": "266244733",
                    "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
                    "text": "In the context of multi-document summarization experiments, the model is configured for multi-document analysis, and multiple single documents are concatenated to form an extensive document. The task of multi-document summarization prohibits the use of triple blocking, as the task demands comprehension of several individual documents to generate a comprehensive summary. Results are displayed in Table 3. \n\nIt is observed that the model outperforms prior approaches in multi-document contexts, with particular improvements in various multi-document metrics. The incorporation of topic nodes and adaptive layers contributes positively to the performance of the multi-document summarization model. Concurrently, it is noted that even though document nodes are not directly linked to topic nodes, the designed message-passing mechanism for iterative updates enables topic nodes to convey information to sentence nodes. This information is subsequently transferred from sentence nodes to word nodes, culminating in the updating of document nodes based on the amalgamation of topic and sentence information. \n\nThis methodology enables enhanced capture of global semantic information within the text. Word nodes serve as conduits, aiding the flow of information from sentence and topic nodes to document nodes. This connectivity and message-passing technique contribute to an enriched modeling of textual context and relationships, thereby bolstering the model's capacity to address long-distance dependencies. Additionally, the growing number of documents exacerbates the problem of cross-sentence dependency if sentence nodes are not explicitly connected at the topic level. Note: \n\nThe answer on Multi-News.",
                    "score": 0.6821562201973334,
                    "section_title": "Results on multi-news",
                    "char_start_offset": 31730,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 191,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 406
                        },
                        {
                            "start": 409,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 697
                        },
                        {
                            "start": 698,
                            "end": 917
                        },
                        {
                            "start": 918,
                            "end": 1104
                        },
                        {
                            "start": 1107,
                            "end": 1196
                        },
                        {
                            "start": 1197,
                            "end": 1306
                        },
                        {
                            "start": 1307,
                            "end": 1506
                        },
                        {
                            "start": 1507,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1678
                        },
                        {
                            "start": 1681,
                            "end": 1706
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9033203125
                }
            ],
            "relevance_judgement": 0.9033203125,
            "relevance_judgment_input_expanded": "# Title: Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks\n# Venue: PeerJ Computer Science\n# Authors: Wu Su, Jin Jiang, Kaihui Huang\n## Abstract\nThe crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes. Last but not least, the model defines nodes with different levels of granularity\u2014ranging from documents and topics to sentences\u2014and these various nodes necessitate different propagation widths and depths for capturing intricate relationships in the information being disseminated. Adaptive measures are taken to learn the importance and correlation between nodes of different granularities in terms of both width and depth. Experimental evidence from two benchmark datasets highlights the superior performance of the proposed model, as assessed by ROUGE metrics, in comparison to existing approaches, even in the absence of pre-trained language models. Additionally, an ablation study confirms the positive impact of each individual module on the model's ROUGE scores.\n## Results on multi-news\nIn the context of multi-document summarization experiments, the model is configured for multi-document analysis, and multiple single documents are concatenated to form an extensive document. The task of multi-document summarization prohibits the use of triple blocking, as the task demands comprehension of several individual documents to generate a comprehensive summary. Results are displayed in Table 3. \n\nIt is observed that the model outperforms prior approaches in multi-document contexts, with particular improvements in various multi-document metrics. The incorporation of topic nodes and adaptive layers contributes positively to the performance of the multi-document summarization model. Concurrently, it is noted that even though document nodes are not directly linked to topic nodes, the designed message-passing mechanism for iterative updates enables topic nodes to convey information to sentence nodes. This information is subsequently transferred from sentence nodes to word nodes, culminating in the updating of document nodes based on the amalgamation of topic and sentence information. \n\nThis methodology enables enhanced capture of global semantic information within the text. Word nodes serve as conduits, aiding the flow of information from sentence and topic nodes to document nodes. This connectivity and message-passing technique contribute to an enriched modeling of textual context and relationships, thereby bolstering the model's capacity to address long-distance dependencies. Additionally, the growing number of documents exacerbates the problem of cross-sentence dependency if sentence nodes are not explicitly connected at the topic level. Note: \n\nThe answer on Multi-News.",
            "reference_string": "[266244733 | Su et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2019,
            "reference_count": 53,
            "citation_count": 102,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D19-1428.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.08435, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144270981",
                    "name": "Angela Fan"
                },
                {
                    "authorId": "1794075",
                    "name": "Claire Gardent"
                },
                {
                    "authorId": "2929661",
                    "name": "Chlo\u00e9 Braud"
                },
                {
                    "authorId": "1713934",
                    "name": "Antoine Bordes"
                }
            ],
            "abstract": "Query-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query, which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input, long-form question answering and multi-document summarization, feeding graph representations as input can achieve better performance than using retrieved text portions.",
            "corpus_id": 202785778,
            "sentences": [
                {
                    "corpus_id": "202785778",
                    "title": "Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs",
                    "text": "Previous work in multi-document summarization (Barzilay et al., 1999) applies various techniques to handle long input, including clustering to find similar information (Honarpisheh et al., 2008), extractive methods to select relevant sentences (Daum\u00e9 III and Marcu, 2002;Gillick and Favre, 2009;Berg-Kirkpatrick et al., 2011;Di Fabbrizio et al., 2014;Bing et al., 2015;Cao et al., 2017) including maximal marginal relevance (Fabbri et al., 2019), and incorporating queries (Baumel et al., 2018) and graphs (Ganesan et al., 2010;Yasunaga et al., 2017). However, there are few large scale multi-document summarization datasets and many approaches have focused on extractive selection or hybrid extractive-abstractive models. In this work, we use graph construction to re-structure multidocument input for abstractive generation.\n\nAdvancements in question answering have examined performance on datasets with multidocument input, such as TriviaQA (Joshi et al., 2017). Various approaches have been proposed, including leveraging TF-IDF and bigram hashing with an RNN to find relevant information (Chen et al., 2017), models that score individual paragraphs for sub-selection (Clark and Gardner, 2017), and nearest neighbor search with paragraph re-ranking (Das et al., 2018a). However, these approaches have been applied to extractive question answering tasks that require span identification, rather than abstractive text generation in an information synthesis setting.",
                    "score": 0.5578973897475901,
                    "section_title": "Multi-Document Input",
                    "char_start_offset": 2786,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 46,
                            "end": 69,
                            "matchedPaperCorpusId": "7031344"
                        },
                        {
                            "start": 168,
                            "end": 194,
                            "matchedPaperCorpusId": "15955042"
                        },
                        {
                            "start": 244,
                            "end": 271,
                            "matchedPaperCorpusId": "189898"
                        },
                        {
                            "start": 271,
                            "end": 295,
                            "matchedPaperCorpusId": "167874"
                        },
                        {
                            "start": 295,
                            "end": 325,
                            "matchedPaperCorpusId": "15467396"
                        },
                        {
                            "start": 325,
                            "end": 351,
                            "matchedPaperCorpusId": "12682781"
                        },
                        {
                            "start": 369,
                            "end": 386,
                            "matchedPaperCorpusId": "14651945"
                        },
                        {
                            "start": 506,
                            "end": 528,
                            "matchedPaperCorpusId": "988010"
                        },
                        {
                            "start": 944,
                            "end": 964,
                            "matchedPaperCorpusId": "26501419"
                        },
                        {
                            "start": 1093,
                            "end": 1112,
                            "matchedPaperCorpusId": "3618568"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90185546875
                }
            ],
            "relevance_judgement": 0.90185546875,
            "relevance_judgment_input_expanded": "# Title: Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Angela Fan, Claire Gardent, Chlo\u00e9 Braud, Antoine Bordes\n## Abstract\nQuery-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query, which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input, long-form question answering and multi-document summarization, feeding graph representations as input can achieve better performance than using retrieved text portions.\n## Multi-Document Input\nPrevious work in multi-document summarization (Barzilay et al., 1999) applies various techniques to handle long input, including clustering to find similar information (Honarpisheh et al., 2008), extractive methods to select relevant sentences (Daum\u00e9 III and Marcu, 2002;Gillick and Favre, 2009;Berg-Kirkpatrick et al., 2011;Di Fabbrizio et al., 2014;Bing et al., 2015;Cao et al., 2017) including maximal marginal relevance (Fabbri et al., 2019), and incorporating queries (Baumel et al., 2018) and graphs (Ganesan et al., 2010;Yasunaga et al., 2017). However, there are few large scale multi-document summarization datasets and many approaches have focused on extractive selection or hybrid extractive-abstractive models. In this work, we use graph construction to re-structure multidocument input for abstractive generation.\n\nAdvancements in question answering have examined performance on datasets with multidocument input, such as TriviaQA (Joshi et al., 2017). Various approaches have been proposed, including leveraging TF-IDF and bigram hashing with an RNN to find relevant information (Chen et al., 2017), models that score individual paragraphs for sub-selection (Clark and Gardner, 2017), and nearest neighbor search with paragraph re-ranking (Das et al., 2018a). However, these approaches have been applied to extractive question answering tasks that require span identification, rather than abstractive text generation in an information synthesis setting.",
            "reference_string": "[202785778 | Fan et al. | 2019 | Citations: 102]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273482385",
            "title": "From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization",
            "text": "Multi-document summarization is a broad and versatile multi-document NLP task that consists of generating a summary from multiple source documents, including opinion/reviews (Angelidis et al., 2021b;Iso et al., 2022), scientific articles (Lu et al.,",
            "score": 0.7515825203189789,
            "section_title": "Related Work",
            "char_start_offset": 29212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 199,
                    "matchedPaperCorpusId": "227745131"
                },
                {
                    "start": 199,
                    "end": 216,
                    "matchedPaperCorpusId": "238856696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.634765625
        },
        {
            "corpus_id": "254952632",
            "title": "Feature Based Automatic Text Summarization Methods: A Comprehensive State-of-the-Art Survey",
            "text": "In this type, a single summary is generated for multiple documents. It is more complex than single document text summarization as the documents may refer to different periods. In addition, different documents may cover different topics, which makes multi-document text summarization more challenging Ferreira et al. [23], Nguyen et al. [57], Barzilay and McKeown [47], Xu and Durrett [58], and Patel et al. [20] developed multi-document text summarizers as discussed in TABLE 20.",
            "score": 0.7100340923506873,
            "section_title": "2) MULTI-DOCUMENT",
            "char_start_offset": 28259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 479
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "14650402"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "3164844"
                },
                {
                    "start": 384,
                    "end": 388,
                    "matchedPaperCorpusId": "59599804"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "192571560"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "226965071",
            "title": "WikiAsp: A Dataset for Multi-domain Aspect-based Summarization",
            "text": "Extractive methods have shown effective for multi-document summarization in previous work (Nenkova et al., 2006;Cao et al., 2015;Yasunaga et al., 2017), but abstractive methods have increasingly adopted for the task (Lebanoff et al., 2018;Fabbri et al., 2019). Our task is based on the idea of (Liu et al., 2018) which treats references as source documents for the multi-document summarization task, and we experimented with both types of summarization models in our experiments.",
            "score": 0.7093269883299294,
            "section_title": "Multi-Document Summarization",
            "char_start_offset": 26869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 479
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 112,
                    "matchedPaperCorpusId": "86903"
                },
                {
                    "start": 112,
                    "end": 129,
                    "matchedPaperCorpusId": "10675728"
                },
                {
                    "start": 129,
                    "end": 151,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 216,
                    "end": 239,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 239,
                    "end": 259,
                    "matchedPaperCorpusId": "174799390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7451171875
        },
        {
            "corpus_id": "222140880",
            "title": "SupMMD: A Sentence Importance Model for Extractive Summarisation using Maximum Mean Discrepancy",
            "text": "Multi-document summarization is the problem of producing condensed digests of salient information from multiple sources, such as articles. Concretely, suppose we are given two sets of articles (denoted set A and set B) on a related topic (e.g., climate change, the COVID-19 pandemic), separated by publication timestamp or geographic region. We may then identify three possible instantiations of multi-document summarization (see Figure 1): \n\n(i) generic summarization, where the goal is to summarize a set (A or B) individually. (ii) comparative summarization, where the goal is to summarize a set (B) against another set (A) while highlighting the differences. (iii) update summarization, where the goal is both generic summarization of set A and comparative summarization of set B versus A. Most existing work on this topic has focused on the generic summarization task. However, update summarization is of equal practical interest. \n\nIntuitively, the comparative aspect of this setting aims to inform a user of new information on a topic they are already familiar with. \n\nMulti-document extractive summarization methods can be unsupervised or supervised. Unsupervised methods typically define salience (or coverage) using a global model of sentence-sentence similarity. Methods based on retrieval (Goldstein et al., 1999), centroids (Radev et al., 2004), graph centrality (Erkan and Radev, 2004), or utility maximization (Lin andBilmes, 2010, 2011;Gillick and Favre, 2009) have been well explored. However, sentence salience also depends on surface features (e.g., position, length, presence of cue words); effectively capturing these requires supervised models specific to the dataset and task. A body of work has incorporated such information through supervised learning, for example based on point processes (Kulesza and Taskar, 2012), learning important words (Hong and Nenkova, 2014), graph neural networks (Yasunaga et al., 2017), and support vector regression (Varma et al., 2009).",
            "score": 0.7052627193009757,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 1301,
                    "end": 1325,
                    "matchedPaperCorpusId": "11218013"
                },
                {
                    "start": 1376,
                    "end": 1399,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1425,
                    "end": 1433,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "226283949",
            "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
            "text": "The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Recently, two multi-document summarization datasets have been proposed, one for very long input, aimed at generating Wikipedia  and another dedicated to generating a comprehensive summary of multiple real-time news (Fabbri et al., 2019). Several works have begun to explore abstractive multi-document summarization.  concatenated multiple source documents into a long flat text and modeled multidocument summarization as a long sequence-tosequence task. Liu and Lapata (2019) represented cross-document relationships via an attention mechanism that allows sharing information as opposed to simply concatenating text spans and processing them as a flat sequence. Fabbri et al. (2019) incorporated MMR into a hierarchical pointer-generator network to address the information redundancy in multi-document summarization. The above works were all trained and tested on multi-document summarization corpus.",
            "score": 0.7051649907790778,
            "section_title": "Multi-Document Summarization",
            "char_start_offset": 5305,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 474,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 499,
                    "end": 521,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 738,
                    "end": 759,
                    "matchedPaperCorpusId": "174799390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "9174081",
            "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach",
            "text": "The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.",
            "score": 0.7012457726179616,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "226283949",
            "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
            "text": "level input, identify the important content of documents, and paraphrase the important information to generate the summary, while the main difference is that multi-document summarization involves summarizing multiple input documents. Since the two tasks are closely related, it is promising to learn for two summarization tasks jointly. Compared with single-document summarization, multi-document summarization needs to handle multiple input documents. A simple method is to concatenate multiple documents into a long flat text and treat it as a long sequence-tosequence task. However, it blurs the boundaries between documents and loses the hierarchy within the document cluster. It is natural to regard multidocument summarization as a two-stage process of summarizing every single document and then merging multiple summaries. Nevertheless, this process is quite trivial, and it is difficult to utilize multi-document summarization corpus to train the single-document summarization model. Furthermore, the synthesis of multiple summaries involves eliminating redundant parts and organizing related paragraphs or sentences, which are also challenges to be solved.\n\nIn this work, we propose a joint learning approach to improve neural abstractive multi-document summarization by using singledocument summarization corpus to address these issues. Our approach first uses a shared document encoder to encode each document in the document set, then uses a shared decoder to predict the word probabilities for each document, and finally applies a decoding controller to aggregate all output probabilities from the summary decoder to make the final prediction at each decoding step. The shared encoder and decoder are jointly trained on the single document summarization data. In this way, we can unify single-document and multi-document summarizations into one architecture simultaneously, and make better use of single-document and multi-document corpora, so that both tasks can benefit from joint learning, especially for the multidocument summarization task.\n\nWe evaluate our approach on the benchmark multi-document summarization datasets, Multi-News and DUC-04, and it brings substantial improvements over several strong baselines for multi-document summarization. We leverage CNN/DailyMail, a single-document summarization dataset, to perform joint learning with Multi-News",
            "score": 0.7006636776838184,
            "section_title": "Introduction",
            "char_start_offset": 2036,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "17446655",
            "title": "An Exploration of Document Impact on Graph-Based Multi-Document Summarization",
            "text": "Multi-document summarization aims to produce a summary describing the main topic in a document set, without any prior knowledge. Multi-document summary can be used to facilitate users to quickly understand a document cluster. For example, a number of news services (e.g. NewsInEssence1 ) have been developed to group news articles into news topics, and then produce a short summary for each news topic. Users can easily understand the topic they have interest in by taking a look at the short summary, without looking into each individual article within the topic cluster. \n\nAutomated multi-document summarization has drawn much attention in recent years. In the communities of natural language processing and information retrieval, a series of workshops and conferences on automatic text summarization (e.g. NTCIR, DUC), special topic sessions in ACL, COLING, and SIGIR have advanced the summarization techniques and produced a couple of experimental online systems. \n\nA particular challenge for multi-document summarization is that a document set might contain diverse information, which is either related or unrelated to the main topic, and hence we need effective summarization methods to analyze the information stored in different documents and extract the globally important information to reflect the main topic. In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features. \n\nMost recently, the graph-based models have been successfully applied for multi-document summarization by making use of the \"voting\" or \"recommendations\" between sentences in the documents (Erkan and Radev, 2004;Mihalcea and Tarau, 2005;Wan and Yang, 2006). The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences. The sentences with large rank scores are chosen into the summary. However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-todocument relationship. Actually, given a document set, different documents are not equally important.",
            "score": 0.6978495450687118,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1551
                },
                {
                    "start": 1554,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2287
                },
                {
                    "start": 2288,
                    "end": 2366
                }
            ],
            "ref_mentions": [
                {
                    "start": 1742,
                    "end": 1765,
                    "matchedPaperCorpusId": "10418456"
                },
                {
                    "start": 1765,
                    "end": 1790,
                    "matchedPaperCorpusId": "8878897"
                },
                {
                    "start": 1790,
                    "end": 1809,
                    "matchedPaperCorpusId": "5457260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9169921875
        },
        {
            "corpus_id": "266362657",
            "title": "Shaping Political Discourse using multi-source News Summarization",
            "text": "Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. The resulting summary allows individual users to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload. \n\nMulti-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together & outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. The multi-document summarization task is more complex than summarizing a single document, even a long one. The difficulty arises from thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and concision. An ideal multi-document summarization system not only shortens the source texts, but also presents information organized around the key aspects to represent diverse views. Success produces an overview of a given topic. \n\nIn an era of increasing political polarization of news agencies, as exemplified by the nature of discourse in the US and India, two of the largest democracies in the world, it has become imperative for the average citizen to be aware of both the sides of the coin before forming an opinion. There exist few news outlets, if at all, that consciously attempt to gather sources from both sides of the spectrum while reporting their stories. Even the ones that do, form the narrative in a manner that is conducive towards directing public discourse in the direction that supports their bias. \n\nIn this project, we have designed a machine learning model that takes multiple news documents pertaining to a specific topic as input and generates a concise summary of the topic as the output.",
            "score": 0.6961788668419397,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1694
                },
                {
                    "start": 1697,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2284
                },
                {
                    "start": 2287,
                    "end": 2480
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61474609375
        },
        {
            "corpus_id": "3011904",
            "title": "A survey for Multi-Document Summarization",
            "text": "Automatic Multi-Document summarization is still hard to realize. Under such circumstances, we believe, it is important to observe how humans are doing the same task, and look around for different strategies.We prepared 100 document sets similar to the ones used in the DUC multi-document summarization task. For each document set, several people prepared the following data and we conducted a survey.A) Free style summarizationB) Sentence Extraction type summarizationC) Axis (type of main topic)D) Table style summaryIn particular, we will describe the last two in detail, as these could lead to a new direction for multi-summarization research.",
            "score": 0.6847754539487594,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68359375
        },
        {
            "corpus_id": "52053741",
            "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization",
            "text": "Second, the attention mechanism is based on input word positions but not their semantics.It can lead to redundant content in the multi-document input being repeatedly used for summary generation.We conjecture that both aspects can be addressed by introducing an \"external\" model that selects representative sentences from multi-document inputs and dynamically adjusts the sentence importance to reduce summary redundancy.This external model is integrated with the encoder-decoder model to generate abstractive summaries using selected representative sentences.In the following section we present our adaptation method for multi-document summarization.",
            "score": 0.6840877594330963,
            "section_title": "Limits of the Encoder-Decoder Model",
            "char_start_offset": 12569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 195
                },
                {
                    "start": 195,
                    "end": 421
                },
                {
                    "start": 421,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 651
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "266244733",
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "text": "In the context of multi-document summarization experiments, the model is configured for multi-document analysis, and multiple single documents are concatenated to form an extensive document. The task of multi-document summarization prohibits the use of triple blocking, as the task demands comprehension of several individual documents to generate a comprehensive summary. Results are displayed in Table 3. \n\nIt is observed that the model outperforms prior approaches in multi-document contexts, with particular improvements in various multi-document metrics. The incorporation of topic nodes and adaptive layers contributes positively to the performance of the multi-document summarization model. Concurrently, it is noted that even though document nodes are not directly linked to topic nodes, the designed message-passing mechanism for iterative updates enables topic nodes to convey information to sentence nodes. This information is subsequently transferred from sentence nodes to word nodes, culminating in the updating of document nodes based on the amalgamation of topic and sentence information. \n\nThis methodology enables enhanced capture of global semantic information within the text. Word nodes serve as conduits, aiding the flow of information from sentence and topic nodes to document nodes. This connectivity and message-passing technique contribute to an enriched modeling of textual context and relationships, thereby bolstering the model's capacity to address long-distance dependencies. Additionally, the growing number of documents exacerbates the problem of cross-sentence dependency if sentence nodes are not explicitly connected at the topic level. Note: \n\nThe answer on Multi-News.",
            "score": 0.6821562201973334,
            "section_title": "Results on multi-news",
            "char_start_offset": 31730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1706
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "1465750",
            "title": "Using N-Grams To Understand the Nature of Summaries",
            "text": "Although single-document summarization is a well-studied task, the nature of multi-document summarization is only beginning to be studied in detail. While close attention has been paid to what technologies are necessary when moving from single to multi-document summarization, the properties of human-written multi-document summaries have not been quantified. In this paper, we empirically characterize human-written summaries provided in a widely used summarization corpus by attempting to answer the questions: Can multi-document summaries that are written by humans be characterized as extractive or generative? Are multi-document summaries less extractive than single-document summaries? Our results suggest that extraction-based techniques which have been successful for single-document summarization may not be sufficient when summarizing multiple documents.",
            "score": 0.6758691509030871,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.826171875
        },
        {
            "corpus_id": "8294822",
            "title": "Multi-Document Summarization By Sentence Extraction",
            "text": "This paper discusses a text extraction approach to multi-document summarization that builds on single-document summarization methods by using additional, available information about the document set as a whole and the relationships between the documents. Multi-document summarization differs from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries. Our approach addresses these issues by using domain-independent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements.",
            "score": 0.6758557327388102,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "266362657",
            "title": "Shaping Political Discourse using multi-source News Summarization",
            "text": "Multi-document summarization is the process of automatically generating a concise summary of multiple documents related to the same topic. This summary can help users quickly understand the key information from a large collection of documents. Multi-document summarization systems are more complex than single-document summarization systems due to the need to identify and combine information from multiple sources. In this paper, we have developed a machine learning model that generates a concise summary of a topic from multiple news documents. The model is designed to be unbiased by sampling its input equally from all the different aspects of the topic, even if the majority of the news sources lean one way.",
            "score": 0.6697290342418911,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79541015625
        },
        {
            "corpus_id": "272969413",
            "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications",
            "text": "While these traditional techniques can be effective for single documents, they face significant challenges when applied to multi-document summarization: \n\nRedundancy: Information is often repeated across multiple source documents, leading to repetitive summaries if not properly managed [4]. \n\nCoherence: Extracting sentences from different documents can result in disjointed summaries lacking logical flow [5]. \n\nContext preservation: It is challenging to maintain the broader context when combining information from diverse sources [6], leading to summaries that lack focus or miss key points [11]. \n\nScalability: Processing and synthesizing information from many documents increases computational complexity [10]. This challenge persists even with advanced models, as handling very long or numerous documents remains computationally intensive [14]. \n\nCross-document relationships: Capturing connections and contradictions between documents is difficult for methods focused on individual sentences or documents [7]. \n\nDomain adaptation: Traditional methods can be limited in their flexibility and adaptability to handle diverse document structures and writing styles [9]. Techniques optimized for one type of document may not generalize well to other domains or writing styles [12]. This challenge extends to ensuring models can handle varied document formats, styles, and topics [59]. \n\nFactual consistency: Ensuring that generated summaries remain faithful to the source material is crucial, especially for abstractive methods [66]. This challenge is particularly pronounced in abstractive summarization, where preventing \"hallucinations\" or factual errors remains an active area of research [65]. \n\nEthical considerations: As summarization techniques become more advanced, mitigating biases present in training data and ensuring fairness in generated summaries becomes increasingly important [63].",
            "score": 0.6685587496107229,
            "section_title": "Challenges in Multi-Document Summarization",
            "char_start_offset": 4172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 290,
                    "matchedPaperCorpusId": "8377315"
                },
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "337730"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "1296465"
                },
                {
                    "start": 595,
                    "end": 599,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 711,
                    "end": 715,
                    "matchedPaperCorpusId": "11730545"
                },
                {
                    "start": 1013,
                    "end": 1016,
                    "matchedPaperCorpusId": "269225"
                },
                {
                    "start": 1169,
                    "end": 1172,
                    "matchedPaperCorpusId": "2767900"
                },
                {
                    "start": 1279,
                    "end": 1283,
                    "matchedPaperCorpusId": "53028110"
                },
                {
                    "start": 1531,
                    "end": 1535,
                    "matchedPaperCorpusId": "196187162"
                },
                {
                    "start": 1696,
                    "end": 1700,
                    "matchedPaperCorpusId": "218487034"
                },
                {
                    "start": 1897,
                    "end": 1901,
                    "matchedPaperCorpusId": "235623756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "272146191",
            "title": "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section",
            "text": "Multi-document summarization (MDS) addresses the problem of extracting information that is spread across multiple documents, making it more challenging than single-document summarization. It is still an evolving field, with no single approach to solve this problem. The development of this field relies on datasets that serve as benchmarks for evaluating and comparing different summarization methods. The scientific literature includes some examples of such datasets, each varying in domain, structure, size, and summarization objective. These datasets are important for advancing research and improving summarization models. \n\nOutside the scientific domain, Multi-News [4] presents a large volume of news articles and summaries, focusing on the abstractive summarization of multiple documents in the journalistic context. Also in this context, Ghalandari et al. [5] proposed a large-scale dataset for multi-document summarization that contains concise human-written summaries of news events. In the legal field, Multi-LexSum tackles the summarization of legal cases, presenting a set of civil rights litigation summaries with multiple granularities. The summarization of instructional content is the focus of the HowSumm dataset, derived from WikiHow articles [1]. The FINDSum dataset focuses on the challenge of summarizing long text and multiple tables [10]. \n\nIn the scientific domain, the need to manage the vast amount of literature has generated interest in datasets specific to the summarization of scientific articles. Multi-XScience [12] is a dataset focused on the generation of \"related work\" sections by summarizing multiple scientific articles. BigSurvey [9] was designed to create structured summaries of academic articles, focusing on consolidating literature reviews. SumPubMed [6] and ScisummNet [19] aim the summarization of single scientific articles. \n\nSeveral other datasets can be used for multi-document summarization. Koh et al. [8] work is an extensive survey on long document summarization, including ten other MDS datasets.",
            "score": 0.6620516618442351,
            "section_title": "Related Work",
            "char_start_offset": 3392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1872
                },
                {
                    "start": 1875,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 1670,
                    "end": 1673,
                    "matchedPaperCorpusId": "250636132"
                },
                {
                    "start": 1796,
                    "end": 1799,
                    "matchedPaperCorpusId": "236772881"
                },
                {
                    "start": 1815,
                    "end": 1819,
                    "matchedPaperCorpusId": "58053521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "235097309",
            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
            "text": "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani and Bloedorn, 1997;Radev and McKeown, 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. Mani and Bloedorn (1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and McKeown (1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, Erkan and Radev (2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). Dasgupta et al. (2013) incorporated dependency graph features into their sentence relation graphs. Baralis et al. (2013) built graphs over sets of terms, rather than sentences. Li et al. (2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. Liu et al. (2015) and Liao et al. (2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com",
            "score": 0.6589208309966629,
            "section_title": "Related Work",
            "char_start_offset": 4341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 140,
                    "matchedPaperCorpusId": "6025826"
                },
                {
                    "start": 140,
                    "end": 164,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 373,
                    "end": 397,
                    "matchedPaperCorpusId": "6025826"
                },
                {
                    "start": 558,
                    "end": 572,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 896,
                    "end": 918,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1289,
                    "end": 1311,
                    "matchedPaperCorpusId": "16587947"
                },
                {
                    "start": 1388,
                    "end": 1409,
                    "matchedPaperCorpusId": "15709889"
                },
                {
                    "start": 1466,
                    "end": 1482,
                    "matchedPaperCorpusId": "6788641"
                },
                {
                    "start": 1614,
                    "end": 1631,
                    "matchedPaperCorpusId": "5001921"
                },
                {
                    "start": 1636,
                    "end": 1654,
                    "matchedPaperCorpusId": "49210924"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "273185717",
            "title": "GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization",
            "text": "Multi-document summarization (MDS) refers to the task of summarizing the text in multiple documents into a concise summary. Previous studies have delved into various approaches, encompassing extractive (Angelidis and Lapata, 2018;Zheng et al., 2019;Mao et al., 2020) and abstractive techniques (Gehrmann et al., 2018;Lebanoff et al., 2018;Zhang et al., 2018). And researchers mainly focus on reducing the redundancy among documents (Peyrard et al., 2017;Xiao and Carenini, 2020;Chen et al., 2021). Currently, there is a growing focus on MDS tasks in more diverse settings.",
            "score": 0.6562314277697568,
            "section_title": "Multi-document Summarization",
            "char_start_offset": 24800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 572
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 230,
                    "matchedPaperCorpusId": "52100878"
                },
                {
                    "start": 249,
                    "end": 266,
                    "matchedPaperCorpusId": "222090788"
                },
                {
                    "start": 294,
                    "end": 317,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 317,
                    "end": 339,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 339,
                    "end": 358,
                    "matchedPaperCorpusId": "53223447"
                },
                {
                    "start": 432,
                    "end": 454,
                    "matchedPaperCorpusId": "38926436"
                },
                {
                    "start": 454,
                    "end": 478,
                    "matchedPaperCorpusId": "227239144"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "174799390",
            "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
            "text": "In this paper we introduce Multi-News, the first large-scale multi-document news summarization dataset. We hope that this dataset will promote work in multi-document summarization similar to the progress seen in the single-document case. Additionally, we introduce an end-to-end model which incorporates MMR into a pointer-generator network, which performs competitively compared to previous multi-document summarization models. We also benchmark methods on our dataset. In the future we plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.",
            "score": 0.6558887524505488,
            "section_title": "Conclusion",
            "char_start_offset": 27298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 609
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "266599825",
            "title": "Summarization of Investment Reports Using Pre-trained Model",
            "text": "Related research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain [6]. Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization [7]. Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset [8]. Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing [10]. They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries [11]. Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries [12]. Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words [13]. Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature [14]. \n\nAs related work of extractive summarization, there is research by Cui et al. [15]. They proposed extractive summarization that can summarize long-form documents without content loss. Xu et al. proposed the neural network framework for extractive and compressive summarization [16]. \n\nAs related work of abstractive summarization, there is research by Nallapati et al. [17].",
            "score": 0.6543664239361845,
            "section_title": "VII. RELATED WORKS",
            "char_start_offset": 9491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 841,
                    "end": 845,
                    "matchedPaperCorpusId": "52011473"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1383,
                    "end": 1387,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1521,
                    "end": 1525,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1697,
                    "end": 1701,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "235097475"
                },
                {
                    "start": 1981,
                    "end": 1985,
                    "matchedPaperCorpusId": "59599804"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "225075639",
            "title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
            "text": "Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results\u2014using several state-of-the-art models trained on the Multi-XScience dataset\u2014reveal that Multi-XScience is well suited for abstractive models.",
            "score": 0.6513650600777274,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84814453125
        },
        {
            "corpus_id": "16263080",
            "title": "An Integrated Multi-document Summarization Approach based on Word Hierarchical Representation",
            "text": "Multi-document summarization requires creating a short summary from a set of documents which concentrate on the same topic. Sometimes an additional query is also given to specify the information need of the summary. Generally, an effective summary should be relevant, concise and fluent. It means that the summary should cover the most important concepts in the original document set, contain less redundant information and should be well-organized. \n\nCurrently, most successful multi-document summarization systems follow the extractive summarization framework. These systems first rank all the sentences in the original document set and then select the most salient sentences to compose summaries for a good coverage of the concepts. For the purpose of creating more concise and fluent summaries, some intensive post-processing approaches are also appended on the extracted sentences. For example, redundancy removal (Carbonell and Goldstein, 1998) and sentence compression (Knight and Marcu, 2000) approaches are used to make the summary more concise. Sentence re-ordering approaches (Barzilay et al., 2002) are used to make the summary more fluent. In most systems, these approaches are treated as independent steps. A sequential process is usually adopted in their implementation, applying the various approaches one after another. \n\nIn this paper, we suggest a new summarization framework aiming at integrating multiple objectives of multi-document summarization. The main idea of the approach is to employ a hierarchical summarization process which is motivated by the behavior of a human summarizer. While the document set may be very large in multi-document summarization, the length of the summary to be generated is usually limited. So there are always some concepts that can not be included in the summary. A natural thought is that more general concepts should be considered first. So, when a human summarizer faces a set of many documents, he may follow a general-specific principle to write the summary. The human summarizer may start with finding the core topic in a document set and write some sentences to describe this core topic. Next he may go to find the important sub-topics and cover the subtopics one by one in the summary, then the sub-sub-topics, sub-sub-sub-topics and so on. By this process, the written summary can convey the most salient concepts.",
            "score": 0.6506669718870125,
            "section_title": "Introduction and Background",
            "char_start_offset": 30,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2378
                }
            ],
            "ref_mentions": [
                {
                    "start": 919,
                    "end": 950,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 976,
                    "end": 1000,
                    "matchedPaperCorpusId": "9363872"
                },
                {
                    "start": 1087,
                    "end": 1110,
                    "matchedPaperCorpusId": "1312658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "1465750",
            "title": "Using N-Grams To Understand the Nature of Summaries",
            "text": "The explosion of available online text has made it necessary to be able to present information in a succinct, navigable manner. The increased accessibility of worldwide online news sources and the continually expanding size of the worldwide web place demands on users attempting to wade through vast amounts of text. Document clustering and multi-document summarization technologies working in tandem promise to ease some of the burden on users when browsing related documents. \n\nSummarizing a set of documents brings about challenges that are not present when summarizing a single document. One might expect that a good multidocument summary will present a synthesis of multiple views of the event being described over different documents, or present a high-level view of an event that is not explicitly reflected in any single document. A useful multi-document summary may also indicate the presence of new or distinct information contained within a set of documents describing the same topic (McKeown et. al., 1999, Mani andBloedorn, 1999). To meet these expectations, a multi-document summary is required to generalize, condense and merge information coming from multiple sources. \n\nAlthough single-document summarization is a wellstudied task (see Mani and Maybury, 1999 for an overview), multi-document summarization is only recently being studied closely (Marcu & Gerber 2001). While close attention has been paid to multi-document summarization technologies (Barzilay et al. 2002, Goldstein et al 2000), the inherent properties of humanwritten multi-document summaries have not yet been quantified. In this paper, we seek to empirically characterize ideal multi-document summaries in part by attempting to answer the questions: Can multi-document summaries that are written by humans be characterized as extractive or generative? \n\nAre multi-document summaries less extractive than single-document summaries? Our aim in answering these questions is to discover how the nature of multi-document summaries will impact our system requirements. \n\nWe have chosen to focus our experiments on the data provided for summarization evaluation during the Document Understanding Conference (DUC).",
            "score": 0.6497550175443978,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2048
                },
                {
                    "start": 2051,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 1466,
                    "end": 1487,
                    "matchedPaperCorpusId": "1312658"
                },
                {
                    "start": 1487,
                    "end": 1510,
                    "matchedPaperCorpusId": "8928636"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "235670190",
            "title": "Topic Modeling Based Extractive Text Summarization",
            "text": "The text summarization approach which generates a summary for a topic after considering multiple documents about the topic is called multi-document summarization. In this summarization technique, relevant sentences for the topic are either picked up (for extractive text summarization) or are rephrased (for abstractive text summarization) after performing multi-document analysis. The sentences are then combined to form a representative summary for the set of documents relevant to a topic. \n\nIn [18], Jian-Ping M. et al. developed a multi-document summarization framework to generate extractive summaries. They introduced two new features to calculate the scores of sentences in the summaries. The first being 'Exemplar', a feature to balance the relevance and coverage in their summaries and the second being 'Position', to measure the relative position of each sentence in a document. However, along with evaluating their model on the standard DUC datasets (DUC2004, DUC2005, DUC2006), they have used the ROUGE-2 and ROUGE-SU4 scores to compare their summaries with those generated by the other state-of-the-art techniques. Moreover, this comparison shows a very marginal increase in the performance of their approach. \n\nAn evolutionary algorithm is proposed by Rasim M. A. et al. [19] to perform summarization on multiple documents. They claim their model to have a superior correlation between sentences with a low rate of redundancy. Nevertheless, their model requires a large computational overhead and is experimented over the standard DUC2002 and DUC2004 datasets to get viable results. Aiming to have good topic coverage, this approach cannot be directly applied to WikiHow dataset for generating topic-based extractive summaries. \n\nA multi-document summarization technique to summarize documents containing events that have occurred in the past at different times was introduced in [20] by Giang T. et al. This simple timeline summarization approach aims to summarize prolonged events such as economic crises and war. Though it achieves good performance on timeline data, this method will not present equivalent results on the WikiHow dataset.",
            "score": 0.6488513700264703,
            "section_title": "B. Multi-Document Summarization",
            "char_start_offset": 8716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 498,
                    "end": 502,
                    "matchedPaperCorpusId": "695678"
                },
                {
                    "start": 1286,
                    "end": 1290,
                    "matchedPaperCorpusId": "4490918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "226283949",
            "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
            "text": "Multi-document summarization takes a document cluster D = {D 1 , D 2 , . . . , D I } as the input, and produces the summary Y , where I is the\n\nand Y = (y 1 , y 2 , . . . , y M ) is a sequence of M words. Compared with multi-document summarization, single-document summarization has only one input document. In order to unify the symbols, single-document summarization is regarded as a special input case of I = 1.\n\nAs illustrated in Figure 1, our model consists of a document encoder, a summary decoder, and a decoding controller. Different documents in multidocument summarization share document encoder and summary decoder. Single-document summarization also shares document encoder and summary decoder with multi-document summarization. A decoding controller is applied to aggregate the outputs of the summary decoder for multiple input documents.\n\nThe shared document encoder reads each input document D i and builds the contextual-level representations C i .\n\nIn each decoding step t, the shared summary decoder produces the vocabulary distribution of the next word given previously (predicted) words and each input document D i .\n\nNote that for multi-document summarization, the same sequence of previous words y 1:t\u22121 (i.e., partial summary) is used for decoding for every document of the multiple inputs.\n\nSince single-document summarization only summarizes one input document, the summary decoder can make the final prediction based on the output vocabulary distribution. While for multi-document summarization, a decoding controller is applied to aggregate multiple vocabulary distributions from the summary decoder for multiple input documents.\n\nHere z t i is the importance weight for each of the multiple vocabulary distributions in the t-th step.\n\nThe following sections will introduce the document encoder, the summary decoder, and the decoding controller, respectively.",
            "score": 0.6458007843942233,
            "section_title": "Overview and Notations",
            "char_start_offset": 7995,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "248496597",
            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
            "text": "We develop an abstractive summarization framework independent of labeled data for multiple heterogeneous documents. Unlike existing multi-document summarization methods, our framework processes documents telling different stories instead of documents on the same topic. We also enhance an existing sentence fusion method with a uni-directional language model to prioritize fused sentences with higher sentence probability with the goal of increasing readability. Lastly, we construct a total of twelve dataset variations based on CNN/Daily Mail and the NewsRoom datasets, where each document group contains a large and diverse collection of documents to evaluate the performance of our model in comparison with other baseline systems. Our experiments demonstrate that our framework outperforms current state-of-the-art methods in this more generic setting.",
            "score": 0.6381986655209576,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "3011904",
            "title": "A survey for Multi-Document Summarization",
            "text": "We reported a survey for multi-document summarization. We believe the results are encouraging for the pursuit of some novel strategies of multi-document summarization. \n\nOne of them is the notion of axis. As we observed that for some percentage of the document sets, the axis can be tagged with some certainty, we might be able to make an automatic system to find it. Once the axis is correctly found, it might be useful for multi document summarization. For example, if a set is \"single-person\" then the summary for the set should be centered on the person. This may suggest, for example, generating a summary of type 'biography' (Mani 2001). If a document set is found to be \"multi-event\", then the summary should focus on the differences of the events. \n\nThe other result found in the experiment is that a quite large percentage of document sets can be summarized in table format. As this is a preliminary experiment, there is incompleteness in the instruction and we believe further study on this topic is necessary. In addition to setting guidelines for the degree of detail, the style of cell contents shall be more uniform. Currently, cells contain words, phrases and sentences. We believe that by making more careful instructions for annotation, the comparison between different tables can be more systematized. In other words, a systematic evaluation may be possible.",
            "score": 0.6320316586276324,
            "section_title": "Discussion",
            "char_start_offset": 13117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "15926944",
            "title": "Automatic Multi Document Summarization Approaches",
            "text": "The need of automatic text summarization has recently increased due to the proliferation of information on the Internet. With the availability and speed of internet, information search from online documents has been eased down to user's finger tips. However, it is not easy for users to manually summarize those large online documents. For example, when a user searches for information about earthquake which occurred in Sendai, Japan, the user will probably receive enormous articles related to that event. The user would definitely opt for a system that could summarize those articles. The goal of automatic text summarization is condensing the source text into a shorter version preserving its information content and overall meaning. \n\nThe objective and approach of summarization of documents explain the kind of summary that is generated. For example, it could be indicative of what a particular subject is about (closely related to a user query), or can be informative about what the whole content of document is all about. Besides that, approach towards text summarization can be either extractive or abstractive (Radev et al., 2002). In extractive type summarization, important sentences are identified and directly extracted from the original document, i.e. the final summary consists of original sentences. On the other hand, in abstractive type summarization (Ganesan et al., 2010) the sentences which are selected from the original document are further processed to restructure them before concatenating them into final summary. This process usually involves deep natural language analysis and sentence compression. \n\nBy understanding the type of summary i.e., indicative, informative, extractive and abstractive, we can then apply them to either single document or multi document. This study focuses mainly on informative and extractive type multi document text summarization. The distinct characteristics that make multi document summarization rather different from single document summarization is that multi document summarization problem involves multiple sources of information that overlap and supplement each other, being contradictory at occasions. So the key tasks are not only identifying and coping with redundancy across documents, but also ensuring that the final summary is both coherent and complete. \n\nThe contributions of this study can be summarized as follows: We discuss the four notable approaches of multi document summarization and present it with related research from literatures.",
            "score": 0.6317346816068719,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1627
                },
                {
                    "start": 1630,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2328
                },
                {
                    "start": 2331,
                    "end": 2518
                }
            ],
            "ref_mentions": [
                {
                    "start": 1120,
                    "end": 1140,
                    "matchedPaperCorpusId": "94818"
                },
                {
                    "start": 1370,
                    "end": 1392,
                    "matchedPaperCorpusId": "988010"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9052734375
        },
        {
            "corpus_id": "237353454",
            "title": "SummerTime: Text Summarization Toolkit for Non-experts",
            "text": "For multi-document summarization, we adopt two popular single-document summarizers to complete the task, as this is shown to be effective in previous work (Fabbri et al., 2019). Combine-then-summarize is a pipeline method to handle multiple source documents, where the documents are concatenated and then a single document summarizer is used to produce the summary. Note that the length of the combined documents may exceed the input length limit for typical transformer-based models; Summarize-then-combine first summarizes each source document independently, then merges the resulting summaries. Compared to the combine-thensummarize method, it is not affected by overlong inputs. However, since each document is summarized separately, the final summary may contain redundant information (Carbonell and Goldstein, 1998).",
            "score": 0.6315161490417197,
            "section_title": "Multi-document Summarization",
            "char_start_offset": 9083,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 176,
                    "matchedPaperCorpusId": "58053521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7412109375
        },
        {
            "corpus_id": "9906673",
            "title": "MDSWriter: Annotation Tool for Creating High-Quality Multi-Document Summarization Corpora",
            "text": "Motivation. The need for automatic summarization systems has been rapidly increasing since the amount of textual information in the web and at large data centers became intractable for human readers. While single-document summarization systems can merely compress the information of one given text, multi-document summaries are even more important, because they can reduce the actual number of documents that require attention by a human. In fact, they enable users to acquire the most salient information about a topic without having to deal with the redundancy typically contained in a set of documents. Given that most search engine users only access the documents linked on the first result pages (cf. Jansen and Pooch, 2001), multi-document summaries even have the potential to radically influence our information access strategies to such textual data that remains unseen by most current search practices. \n\nAt the same time, automatic summarization is one of the most challenging natural language processing tasks. Successful approaches need to per-form several subtasks in a complex setup, including content selection, redundancy removal, and coherent writing. Training and evaluating such systems is extremely difficult and requires highquality reference corpora covering each subtask. \n\nCurrently available corpora are, however, still severely limited in terms of domains, genres, and languages covered. Most of them are additionally focused on the results of only one subtask, most often the final summaries, which prevents the training and evaluation of intermediate steps (e.g., redundancy detection). A major corpus creation issue is the lack of tool support for complex annotation setups. Existing annotation tools do not meet our demands, as they are limited to creating final summaries without storing intermediate results and user interactions or are not freely available or support only single document summarization.",
            "score": 0.6314920736010265,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 706,
                    "end": 729,
                    "matchedPaperCorpusId": "14020175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "220045815",
            "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
            "text": "Document summarization aims at producing a fluent, condensed summary for given documents. Single document summarization has shown promising results with sequence-to-sequence models that encode a source document and then decode it into a summary (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u00b8elikyilmaz et al., 2018). Multi-document summarization requires producing a summary from a cluster of thematically related documents, where the given documents complement and overlap each other. Multi-document summarization involves identifying important information and filtering out redundant information from multiple input sources. \n\nThere are two primary methodologies for multidocument summarization: extractive and abstractive. Extractive methods directly select important sentences from the original, which are relatively simple. Cao et al. (2015) rank sentences with a recursive neural network. Yasunaga et al. (2017) employ a Graph Convolutional Network (GCN) to incorporate sentence relation graphs to improve the performance for the extractive summarization. Abstractive methods can generate new words and new sentences, but it is technically more difficult than extractive methods. Some works on multidocument summarization simply concatenate multiple source documents into a long flat sequence and model multi-document summarization as a long sequence-to-sequence task (Liu et al., 2018;Fabbri et al., 2019). However, these approaches don't take the hierarchical structure of document clusters into account, while the too-long input often leads to the degradation in document summarization (Cohan et al., 2018;Liu and Lapata, 2019). Recently, hierarchical frameworks have shown their effectiveness on multi-document summarization (Zhang et al., 2018;Liu and Lapata, 2019). These approaches usually use multiple encoders to model hierarchical relationships in the discourse structure, but other methods to incorporate the structural semantic knowledge have not been explored. The combination of extractive and abstractive has been explored in single document summarization. Chen and Bansal (2018) use the extracted sentences as the input of the abstractive summarization.",
            "score": 0.6273599763123026,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 263,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 263,
                    "end": 283,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 283,
                    "end": 305,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 305,
                    "end": 332,
                    "matchedPaperCorpusId": "4406182"
                },
                {
                    "start": 846,
                    "end": 863,
                    "matchedPaperCorpusId": "10675728"
                },
                {
                    "start": 912,
                    "end": 934,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1409,
                    "end": 1429,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1612,
                    "end": 1632,
                    "matchedPaperCorpusId": "4894594"
                },
                {
                    "start": 1632,
                    "end": 1653,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1772,
                    "end": 1793,
                    "matchedPaperCorpusId": "170079112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "265220978",
            "title": "PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization",
            "text": "Multi-document summarization (MDS) poses unique challenges since their inputs are often lengthy, consisting of tens or hundreds of related articles, which necessitates computationally-efficient methods (Otmakhova et al., 2022;Pasunuru et al., 2021;Brazinskas et al., 2022). More importantly, unlike single-document inputs, these multidocuments contain a mix of complementary, redundant, or sometimes contradictory information (Hendrickx et al., 2009;Radev, 2000). Existing systems struggle with effectively aggregating knowledge from multiple documents, often overly favoring information from one document or overfitting to positional biases within the input (Wolhandler et al., 2022). We also note the lack of existing large-scale pre-training datasets for the MDS tasks, with existing multi-document modeling approaches confined to using relatively small or synthesized pre-training corpora (Caciularu et al., 2021;Xiao et al., 2022;Caciularu et al., 2023) or extends pre-trained models designed with SDS objective.",
            "score": 0.6263285360902904,
            "section_title": "Multi-Document Summarization",
            "char_start_offset": 6395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 1017
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 272,
                    "matchedPaperCorpusId": "248512730"
                },
                {
                    "start": 426,
                    "end": 450,
                    "matchedPaperCorpusId": "232963"
                },
                {
                    "start": 935,
                    "end": 958,
                    "matchedPaperCorpusId": "258865847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "46566435",
            "title": "NLU Methodologies for Capturing Non-redundant Information from Multi-documents - A Survey",
            "text": "The multi-document summarizer currently uses simple similarity scoring approaches but plans to replace them with better performing ones. \n\nNomoto and Matsumoto provide a method to exploit diversity of concepts in text in order to evaluate information based on how well source documents are represented in automatically generated summaries (Nomoto and Matsumoto, 2003).",
            "score": 0.6262985265935197,
            "section_title": "Disadvantages:",
            "char_start_offset": 23408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 368
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "271903777",
            "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization",
            "text": "Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise. Compared with single-document summarization, it's more challenging because an increasing number of input documents will make the source information more redundant and scattered. It also has practical significance, for example, key information of multiple news articles can be generated efficiently. Multi-document summarization approaches can also be applied in other scenarios, such as extracting opinions from social media. \n\nAs the application of MDS becomes more and more widespread, various approaches have been proposed. Non-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary. The main drawback of such approaches is that they retain only a subset of key sentences, which can lead to information loss and may not capture finegrained details. \n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020). Despite their capability in extracting abstract features, neural models are often resource-intensive and require large parallel training datasets. Moreover, recent studies (Pagnoni et al., 2021) have shown that they also suffer from factuality problems. \n\nPre-trained language models use large-scale cor- pora to optimize objective functions, allowing them to acquire more general feature representations. This capability enables them to require only a small amount of data to fine-tune downstream tasks and achieve impressive results.",
            "score": 0.62387314117589,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2089
                },
                {
                    "start": 2092,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 860,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 860,
                    "end": 885,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 885,
                    "end": 908,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 1508,
                    "end": 1529,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1529,
                    "end": 1546,
                    "matchedPaperCorpusId": "222090788"
                },
                {
                    "start": 1546,
                    "end": 1563,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 1778,
                    "end": 1801,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1801,
                    "end": 1818,
                    "matchedPaperCorpusId": "199466313"
                },
                {
                    "start": 1818,
                    "end": 1834,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 2008,
                    "end": 2030,
                    "matchedPaperCorpusId": "233407441"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "272969413",
            "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications",
            "text": "A typical multi-document summarization task follows the steps described below. Model Selection: A Long context LLM such as GPT-4 [22] or Claude 2.1 [23] can be chosen for multi-document summarization task. The final model is selected for its ability to capture long-range dependencies and handle large input sequences and generate coherent and fluent summaries [24]. An LLM's pre-training on diverse datasets allows for effective transfer learning to the multi-document summarization task. Data preparation: In data preparation step, multiple documents relevant for the summarization task having balanced representation of multiple sources such as news articles, academic papers, and reports [25] are collected to create a comprehensive dataset. The documents are filtered and to remove duplicates, irrelevant content and noise to ensure data quality. For data preprocessing the text is normalized, tokenized, and formatted for input to the LLMs.",
            "score": 0.6231177293590547,
            "section_title": "Methodology",
            "char_start_offset": 9202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 946
                }
            ],
            "ref_mentions": [
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "6532096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "269502531",
            "title": "Exploring News Summarization and Enrichment in a Highly Resource-Scarce Indian Language: A Case Study of Mizo",
            "text": "Multi-document Summarization (MDS) involves the generation of a brief and condensed summary that includes the essential information from a collection of interconnected documents.Recent studies in MDS have demonstrated promise in both extractive (Angelidis and Lapata, 2018;Narayan et al., 2018) and abstractive (Chu and Liu, 2018;Fabbri et al., 2019;Liu and Lapata, 2019) summarization techniques.",
            "score": 0.621107769373175,
            "section_title": "Multi Document Summarization",
            "char_start_offset": 4753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 178,
                    "end": 397
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 273,
                    "matchedPaperCorpusId": "52100878"
                },
                {
                    "start": 273,
                    "end": 294,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 311,
                    "end": 330,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 330,
                    "end": 350,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 350,
                    "end": 371,
                    "matchedPaperCorpusId": "170079112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "7031344",
            "title": "Information Fusion in the Context of Multi-Document Summarization",
            "text": "Information overload has created an acute need for summarization. Typically, the same information is described by many different online documents. Hence, summaries that synthesize common information across documents and emphasize the differences would significantly help readers. Such a summary would be beneficial, for example, to a user who follows a single event through several newswires. In this paper, we present research on the automatic fusion of similar information across multiple documents using language generation to produce a concise summary. \n\nWe propose a method for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for summary generation. \n\nMost research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997;Marcu, 1997;Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while ex-tracting some similar sentences could produce a summary biased towards some sources. \n\nInstead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be included in the summary and sentence generation to reformulate them as new text. Our work is part of a full summarization system (McKeown et al., 1999), which extracts sets of similax sentences, themes (Eskin et al., 1999), in the first stage for input to the components described here. \n\nOur model for multi-document summarization represents a number of departures from traditional language generation. Typically, language generation systems have access to a full semantic representation of the domain. A content planner selects and orders propositions from an underlying knowledge base to form text content. A sentence planner determines how to combine propositions into a single sentence, and a sentence generator realizes each set of combined propositions as a sentence, mapping from concepts to words and building syntactic structure. Our approach differs in the following ways: Content planning operates over full sentences, producing sentence fragments.",
            "score": 0.6201722674166745,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1840
                },
                {
                    "start": 1843,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2393
                },
                {
                    "start": 2394,
                    "end": 2514
                }
            ],
            "ref_mentions": [
                {
                    "start": 1036,
                    "end": 1056,
                    "matchedPaperCorpusId": "5519987"
                },
                {
                    "start": 1056,
                    "end": 1068,
                    "matchedPaperCorpusId": "11680756"
                },
                {
                    "start": 1068,
                    "end": 1088,
                    "matchedPaperCorpusId": "32296317"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "266362657",
            "title": "Shaping Political Discourse using multi-source News Summarization",
            "text": "Summarizing news in an unbiased fashion using a model operating on multiple sources poses a few interesting challenges over a models operating on single source: \n\n\u2022 Multi-document summarization is typically modeled as an extension of the single document version. Concatenating many documents into one produces an input size that is typically too large even for modern transformer based models to handle. Thus, information across documents needs to be condensed, and some recent approaches attempt to do that [4]. We propose selecting a subset of sentences from the source documents in order to handle this problem. \n\n\u2022 The degree of redundancy in information contained within a group of topically-related articles is much higher than the degree of redundancy within an article, as each article is apt to describe the main point as well as necessary shared background. Hence anti-redundancy methods are more crucial in multi-document summarization approaches. \n\n\u2022 A group of articles may contain a temporal dimension, typical in a stream of news reports about an unfolding event. Here later information may override earlier more tentative or incomplete accounts. Further, the co-reference problem in summarization presents even greater challenges for multi-document than for single-document summarization. \n\n\u2022 The model needs to be robust to imbalance in the coverage of the various aspects in the set of source documents pertaining to a topic. \n\n\u2022 Summarizing an entire aspect of a topic that has been handled in different ways by different sources is a challenging problem. \n\nFigure 1: BertSum model diagram as in [9]. \n\n\u2022 The final piece in the puzzle is combining summaries of the various aspects pertaining to a topic and the arguments therein. \n\nWe design two types of models which try to solve the above challenges as explained in sections 3.3 and 3.2",
            "score": 0.6195564381728265,
            "section_title": "Challenges",
            "char_start_offset": 8243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 163,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "267407058",
            "title": "Survey of transformers and towards ensemble learning using transformers for natural language processing",
            "text": "In today's digital age, the internet produces an ever-increasing volume of information, leading to the challenge of managing vast amounts of textual data. To address this issue, the task of text summarization comes to the forefront as a solution. Text summarization can be categorized into two main types based on the input data: single-document summarization and multi-document summarization. Single-document summarization involves generating concise summaries from individual documents, allowing users to extract key information from a single source. On the other hand, multi-document summarization goes a step further by creating summaries from a collection of documents related to a specific topic. Text summarization provides users with a concise overview of information from multiple sources, and help them understand complex topics. \n\nIn the text summarization task, we use the cnn_dailymail dataset [51]. The dataset includes two columns, article and highlights. The article is the main body of the news article, and the highlight is the summary information.",
            "score": 0.6194012562894419,
            "section_title": "CNN daily mail dataset [51] for text summarization task",
            "char_start_offset": 62469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "16188305",
            "title": "Sentence Fusion for Multidocument News Summarization",
            "text": "document summarization. Unlike extraction methods (used by the vast majority of summarization researchers), sentence fusion allows for the true synthesis of information from a set of input documents. It has been shown that combining information from several sources is a natural strategy for multidocument summarization. Analysis of human-written summaries reveals that most sentences combine information drawn from multiple documents (Banko and Vanderwende 2004). Sentence fusion achieves this goal automatically. Our evaluation shows that our approach is promising, with sentence fusion outperforming sentence extraction for the task of content selection.\n\nThis article focuses on the implementation and evaluation of the sentence fusion method within the multidocument summarization system MultiGen, which daily summarizes multiple news articles on the same event as part 1 of Columbia's news browsing system Newsblaster (http:/ /newsblaster.cs.columbia.edu/). In the next section, we provide an overview of MultiGen, focusing on components that produce input or operate over output of sentence fusion. In Section 3, we provide an overview of our fusion algorithm and detail on its main steps: identification of common information (Section 3.1), fusion lattice computation (Section 3.2), and lattice linearization (Section 3.3). Evaluation results and their analysis are presented in Section 4. Analysis of the system's output reveals the capabilities and the weaknesses of our textto-text generation method and identifies interesting challenges that will require new insights. An overview of related work and a discussion of future directions conclude the article.",
            "score": 0.6189412311883326,
            "section_title": "Introduction",
            "char_start_offset": 4285,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 435,
                    "end": 463,
                    "matchedPaperCorpusId": "1465750"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "8294822",
            "title": "Multi-Document Summarization By Sentence Extraction",
            "text": "When these 200 documents were added to a set of 4 other topics of 200 documents, yielding a document-set with 1000 documents, the query relevant multi-document summarization system produced exactly the same resuits. \n\nWe are currently working on constructing datasetsfor experimental evaluations of multi-document summarization. In order to construct these data sets, we attempted to categorize user's information seeking goals for multidocument summarization (see Section 3). As can be seen in Figure 2, the standard IR technique of using a query to extract relevant passages is no longer sufficient for multidocument summarization due to redundancy. In addition, query relevant extractions cannot capture temporal sequencing. The data sets will allow us to measure the effects of these, and other features, on multi-document summarization quality. \n\nSpecifically, we are constructing sets of 10 documents, \u2022 which either contain a snapshot of an event from multiple sources or the unfoldment of an event over time. From these sets we are performing two types of experiments. In the first, we are examining how users put sentences into pre-defined clusters and how they create sentence based multi-document summaries. The result will also serve as a gold standard for system generated summaries -do our systems pick the same summary sentences as humans and are they picking sentences from the same clusters as humans? The second type Of experiment is designed to determine how users perceive the output summary quality. In this experiment, users are asked to rate the output sentences from the summarizer as good, okay or bad. For the okay or bad sentences, they are asked to provide a summary sentence from the document set that is \"better\", i.e., that makes a better set of sentences to represent the information content of the document set. We are comparing our proposed summarizer #6 in Section 4 to summarizer #1, the common portions of the document sets with no anti-redundancy and summarizer #3, single document summary of a centroid document using our single document summarizer (Goldstein et al., 1999).",
            "score": 0.6184448198753522,
            "section_title": "documents_selected(Di, S) = ~ =",
            "char_start_offset": 23148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 2088,
                    "end": 2112,
                    "matchedPaperCorpusId": "11218013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "16259680",
            "title": "Improving Update Summarization by Revisiting the MMR Criterion",
            "text": "This paper describes a method for multi-document update summarization that relies on a double maximization criterion. A Maximal Marginal Relevance like criterion, modified and so called Smmr, is used to select sentences that are close to the topic and at the same time, distant from sentences used in already read documents. Summaries are then generated by assembling the high ranked material and applying some ruled-based linguistic post-processing in order to obtain length reduction and maintain coherency. Through a participation to the Text Analysis Conference (TAC) 2008 evaluation campaign, we have shown that our method achieves promising results.",
            "score": 0.6177492482184846,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "220045815",
            "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
            "text": "The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u00b8elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model. Recently, two large scale multi-document summarization datasets have been proposed, one for very long input, aimed at generating Wikipedia (Liu et al., 2018) and another dedicated to generating a comprehensive summarization of multiple real-time news (Fabbri et al., 2019). Liu et al. (2018) concatenated multiple source documents into a long flat text and introduced a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder-decoder architectures. Liu and Lapata (2019) introduced intermediate document representations and simply add the document representations to word representations for modeling the cross-document relationships. Compared with our proposed multi-granularity method, Liu and Lapata (2019) inclined to the traditional bottomup hierarchical method and don't effectively utilize the hierarchical representations while ignoring the hierarchical relationships of sentences.",
            "score": 0.6157933056981211,
            "section_title": "Related Work",
            "char_start_offset": 4798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 474,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 499,
                    "end": 521,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 624,
                    "end": 642,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 642,
                    "end": 662,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 662,
                    "end": 684,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 684,
                    "end": 711,
                    "matchedPaperCorpusId": "4406182"
                },
                {
                    "start": 1040,
                    "end": 1062,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 1513,
                    "end": 1534,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1763,
                    "end": 1784,
                    "matchedPaperCorpusId": "170079112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "229679767",
            "title": "On Generating Extended Summaries of Long Documents",
            "text": "Prior work in document summarization has mainly focused on generating short summaries of a document. While this type of summary helps get a high-level view of a given document, it is desirable in some cases to know more detailed information about its salient points that can't fit in a short summary. This is typically the case for longer documents such as a research paper, legal document, or a book. In this paper, we present a new method for generating extended summaries of long papers. Our method exploits hierarchical structure of the documents and incorporates it into an extractive summarization model through a multi-task learning approach. We then present our results on three long summarization datasets, arXiv-Long, PubMed-Long, and Longsumm. Our method outperforms or matches the performance of strong baselines. Furthermore, we perform a comprehensive analysis over the generated results, shedding insights on future research for long-form summary generation task. Our analysis shows that our multi-tasking approach can adjust extraction probability distribution to the favor of summary-worthy sentences across diverse sections. Our datasets, and codes are publicly available at this https URL",
            "score": 0.6157272022878915,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19482421875
        },
        {
            "corpus_id": "9174081",
            "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach",
            "text": "Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content). \n\nWhen adapting a single-document summarization method to perform multi-document summarization, a possible strategy is to combine the summaries of each document. To iteratively combine the summaries, we explore two different approaches: singlelayer hierarchical and waterfall. Given that the summarization method also uses as input a set of key phrases, we extract from each input document the required set of key phrases, join the extracted sets, and rank the key phrases using their frequency. To generate each summary, we use the top key phrases, excluding the ones that do not occur in the input document.",
            "score": 0.6153341582966353,
            "section_title": "Multi-Document Summarization",
            "char_start_offset": 3618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1023
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 148,
                    "matchedPaperCorpusId": "681677"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "264146402",
            "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
            "text": "Multi-document Single-document summarization and multi-document summarization are two distinct tasks within text summarization domain based on source document quantity. Single-document summarization focuses on generating a summary from a single input document, while multi-document summarization aims to create a summary by aggregating information from multiple related documents [41,111,149]. Within the single-document system, the objective is to condense the main ideas and essential information contained in that specific document. On the other hand, multi-document summarization tasks require identifying and combining the most relevant and nonredundant information from a set of documents, often covering the same topic or event. This means multi-document summarization has additional challenges, such as maintaining cross-document coherence, efficiently handling larger volumes of information, and processing redundancy across documents. These complexities make multi-document summarization generally more difficult than single-document summarization. \n\nTraditional approaches usually employ extractive techniques both in the context of single-document and multi-document summarization. Graph-based methods can be applied to both single-document and multi-document summarization by representing the relationships between sentences in one document or several documents as a graph, with sentences as nodes and the edges as the similarity between the sentences. The systems [46,96,141,171] then use algorithms like PageRank, HITS, or LexRank to identify the most important sentences in the graph, which are then extracted and combined to form the summary. Fig. 2. The weighted cosine similarity graph [46] was generated for the cluster, based on the subset of d1003t from DUC 2004, which is a dataset for multi-document summarization tasks. The notation used in the figure is as follows: 'd' represents document and 's' represents sentence. For instance, d2s3 denotes the third sentence of document 2. \n\nFor single-document summarization, position-based methods [44,82,95] exploit the position of sentences within the document to identify important content. Additionally, the TF-IDF approach [60,115,156] weighs the importance of words in main Surveying Text Summarization with Deep Learning 7 the document based on their frequency and rarity.",
            "score": 0.6141893544462675,
            "section_title": "Source Document Quantity: Single-document vs",
            "char_start_offset": 10281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2005
                },
                {
                    "start": 2008,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2347
                }
            ],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "243638031"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1482,
                    "end": 1485,
                    "matchedPaperCorpusId": "8014853"
                },
                {
                    "start": 1485,
                    "end": 1489,
                    "matchedPaperCorpusId": "269225"
                },
                {
                    "start": 1489,
                    "end": 1493,
                    "matchedPaperCorpusId": "21346422"
                },
                {
                    "start": 1705,
                    "end": 1709,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 2066,
                    "end": 2070,
                    "matchedPaperCorpusId": "1177942"
                },
                {
                    "start": 2070,
                    "end": 2073,
                    "matchedPaperCorpusId": "5775833"
                },
                {
                    "start": 2073,
                    "end": 2076,
                    "matchedPaperCorpusId": "40166767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.880859375
        },
        {
            "corpus_id": "226283949",
            "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
            "text": "Document summarization aims at producing a fluent, condensed summary for the given document or document set. It involves identifying important information and filtering out redundant information from input sources. While single-document summarization takes a single source document as input, multi-document summarization requires producing a summary from a cluster of thematically related documents. There are two primary methodologies for document summarization: extractive and abstractive. Extractive methods directly select important sentences from the original documents, which are relatively simple but face the drawbacks of information redundancy and incoherence between sentences. Abstractive methods enable generating new words, phrases, and sentences, which are able to generate better summaries with higher readability and conciseness. In this paper, we focus on abstractive document summarization.\n\nEmpowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017;Paulus et al., 2018;Tan et al., 2017;\u00c7 elikyilmaz et al., 2018). Compared with single-document summarization, annotated multi-document summarization datasets are often scarce. Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization.\n\nSingle-document and multi-document summarizations are very closely related in both task definition and solution method (Wan, 2010). Both tasks need to deal with document-level input, identify the important content of documents, and paraphrase the important information to generate the summary, while the main difference is that multi-document summarization involves summarizing multiple input documents. Since the two tasks are closely related, it is promising to learn for two summarization tasks jointly. Compared",
            "score": 0.6140678498956181,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1101,
                    "end": 1119,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1119,
                    "end": 1139,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 1139,
                    "end": 1156,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 1156,
                    "end": 1182,
                    "matchedPaperCorpusId": "4406182"
                },
                {
                    "start": 1628,
                    "end": 1650,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 1970,
                    "end": 1981,
                    "matchedPaperCorpusId": "17224077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "221293184",
            "title": "Query Understanding via Intent Description Generation",
            "text": "Query-based Multi-document Summarization. Query-based multi-document summarization is the process of automatically generating natural summaries of text documents in the context of a given query. An early work for extractive query-based multidocument summarization is presented by [22], which ranked sentences using a weighted combination of statistical and linguistic features. [16] presented to extract sentences based on the language model, Bayesian model, and graphical model. [40] introduced the graph information to look for relevant sentences. [51] used the multi-modality manifold-ranking algorithm to extract topic-focused summary from multiple documents. Recently, some works employ the encoder-decoder framework to produce the query-based summaries. [24] trained a pointer-generator model, and [3] incorporated relevance into a neural seq2seq models for query-based abstractive summarization. [43] introduced a new diversity based attention mechanism to alleviate the problem of repeating phrases.",
            "score": 0.6102310289648089,
            "section_title": "2.2.2",
            "char_start_offset": 11789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1007
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 284,
                    "matchedPaperCorpusId": "11218013"
                },
                {
                    "start": 378,
                    "end": 382,
                    "matchedPaperCorpusId": "6241932"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "22109805"
                },
                {
                    "start": 903,
                    "end": 907,
                    "matchedPaperCorpusId": "5673925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "46566435",
            "title": "NLU Methodologies for Capturing Non-redundant Information from Multi-documents - A Survey",
            "text": "documents, uses extraction to find main topics and organizes the resulting information for a logical presentation of a summary of multiple documents. This is an interactive approach that focuses on summarizing news line documents (reducing text to 15%) (Stein et al., 2000). \n\nAdvantages: This methodology both summarizes multi-document text and is designed to provide a smooth flow of the summary to the reader. It clusters single document representative summaries with similar topics to reduce redundancy. It orders the generated summary for multiple documents based on paragraph similarity to minimize the jerkiness of topic changes from paragraph to paragraph. The result is improved readability.",
            "score": 0.6084676442365666,
            "section_title": "Stein et al. provide a methodology that clusters",
            "char_start_offset": 22689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 700
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 273,
                    "matchedPaperCorpusId": "33748352"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "16263080",
            "title": "An Integrated Multi-document Summarization Approach based on Word Hierarchical Representation",
            "text": "This paper introduces a novel hierarchical summarization approach for automatic multi-document summarization. By creating a hierarchical representation of the words in the input document set, the proposed approach is able to incorporate various objectives of multi-document summarization through an integrated framework. The evaluation is conducted on the DUC 2007 data set.",
            "score": 0.6068676660213856,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "6532096",
            "title": "Graph-based Neural Multi-Document Summarization",
            "text": "Document summarization aims to produce fluent and coherent summaries covering salient information in the documents. Many previous summarization systems employ an extractive approach by identifying and concatenating the most salient text units (often whole sentences) in the document. \n\nTraditional extractive summarizers produce the summary in two steps: sentence ranking and sentence selection. First, they utilize humanengineered features such as sentence position and length (Radev et al., 2004a), word frequency and importance (Nenkova et al., 2006;Hong and Nenkova, 2014), among others, to rank sentence salience. Then, they select summary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (Mc-Donald, 2007;Gillick and Favre, 2009;Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003;Barzilay et al., 2001) can follow to improve coherence of the summary. \n\nRecently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015(Cao et al., , 2017)), all the sentences in the same document cluster are processed independently. Hence, the relationships between sentences and thus the relationships between different documents are ignored. However, Christensen et al. (2013) demonstrates the importance of considering discourse relations among sentences in multi-document summarization. \n\nThis work proposes a multi-document summarization system that exploits the representational power of deep neural networks and the sentence relation information encoded in graph representations of document clusters. Specifically, we apply Graph Convolutional Networks (Kipf and Welling, 2017) on sentence relation graphs. First, we discuss three different techniques to produce sentence relation graphs, where nodes represent sentences in a cluster and edges capture the connections between sentences.",
            "score": 0.6053424051470447,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 478,
                    "end": 499,
                    "matchedPaperCorpusId": "10151424"
                },
                {
                    "start": 531,
                    "end": 553,
                    "matchedPaperCorpusId": "86903"
                },
                {
                    "start": 716,
                    "end": 739,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 813,
                    "end": 837,
                    "matchedPaperCorpusId": "167874"
                },
                {
                    "start": 837,
                    "end": 853,
                    "matchedPaperCorpusId": "12090675"
                },
                {
                    "start": 884,
                    "end": 901,
                    "matchedPaperCorpusId": "1207010"
                },
                {
                    "start": 946,
                    "end": 960,
                    "matchedPaperCorpusId": "10135300"
                },
                {
                    "start": 960,
                    "end": 982,
                    "matchedPaperCorpusId": "144107"
                },
                {
                    "start": 1184,
                    "end": 1203,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 1238,
                    "end": 1262,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 1424,
                    "end": 1441,
                    "matchedPaperCorpusId": "10675728"
                },
                {
                    "start": 1441,
                    "end": 1462,
                    "matchedPaperCorpusId": "14651945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "9174081",
            "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach",
            "text": "The use of the Internet to fulfill generic information needs motivated pioneer multi-document summarization efforts as NewsInEssence (Radev et al., 2005) or Newsblaster (McKeown et al., 2002), online since 2001. In general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics. \n\nWe propose to extend a state-of-the-art single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013), capable of focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization. We explore two hierarchical strategies to perform this extension. This document is organized as follows: Sect. 2 addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper.",
            "score": 0.6047676603932829,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1206
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 153,
                    "matchedPaperCorpusId": "681677"
                },
                {
                    "start": 169,
                    "end": 191,
                    "matchedPaperCorpusId": "681677"
                },
                {
                    "start": 553,
                    "end": 579,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 822,
                    "end": 844,
                    "matchedPaperCorpusId": "681677"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "263713879",
            "title": "Exploring the Landscape of Automatic Text Summarization: A Comprehensive Survey",
            "text": "Multi-Document Summarization is the process of gathering important information and filtering out superfluous information from a series of documents to represent them with a short piece of text. Extractive and abstractive summarizing are two popular techniques for multi-document summarization [12]. Multi-Document is a useful information aggregation tool that creates an interesting and succinct summary from a collection of topic-related publications [23]. \n\n109822 VOLUME 11, 2023 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.",
            "score": 0.6041916202814844,
            "section_title": "B. MULTI-DOCUMENT SUMMARY",
            "char_start_offset": 14588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 594
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "235530816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7294921875
        },
        {
            "corpus_id": "3493705",
            "title": "Extractive Summarization: Limits, Compression, Generalized Model and Heuristics",
            "text": "Most of the summarization literature focuses on single-document and multidocument summarization algorithms and frameworks rather than limits on the performance of summarization systems. As pointed out by [8], competitive summarization systems are typically extractive, selecting representative sentences, concatenating them and often compressing them to squeeze in more sentences within the constraint. The summarization literature is vast, so we refer the reader to the recent survey [11], which is fairly comprehensive for summarization research until 2015. Here, we give a sampling of the literature and focus more on recent research and/or evaluation work. \n\nSingle-document extractive summarization. For single-document summarization, [22] explicitly model extraction and compression, but their results showed a wide variation on a subset of 140 documents from the DUC 2002 dataset, and [28] focused on topic coherence with a graphical structure with separate importance, coherence and topic coverage functions. In [28], the authors present results for single-document summarization on a subset of PLOS Medicine articles and DUC 2002 dataset without mentioning the number of articles used. An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17]. Several systems were compared against a newly-devised supervised method on a dataset from Yahoo in [24]. \n\nMulti-document extractive summarization. For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35]. Supervised and semi-supervised learning based extractive summarization was studied in [34]. Of course, single-document summarization can be considered as a special case, but no experimental results are presented for this important special case in the papers cited in this paragraph. \n\nAbstractive summarization. Abstractive summarization systems include [5,12,6,20,30,7]. \n\nFrameworks. Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36].",
            "score": 0.6028001065388855,
            "section_title": "Related Work",
            "char_start_offset": 3383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 660
                },
                {
                    "start": 663,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1940
                },
                {
                    "start": 1943,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2029
                },
                {
                    "start": 2032,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "26768540"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "24465182"
                },
                {
                    "start": 740,
                    "end": 744,
                    "matchedPaperCorpusId": "16148301"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "269225"
                },
                {
                    "start": 1020,
                    "end": 1024,
                    "matchedPaperCorpusId": "269225"
                },
                {
                    "start": 1317,
                    "end": 1319,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1319,
                    "end": 1322,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1322,
                    "end": 1325,
                    "matchedPaperCorpusId": "16531222"
                },
                {
                    "start": 1426,
                    "end": 1430,
                    "matchedPaperCorpusId": "2482257"
                },
                {
                    "start": 1637,
                    "end": 1641,
                    "matchedPaperCorpusId": "2798452"
                },
                {
                    "start": 1641,
                    "end": 1644,
                    "matchedPaperCorpusId": "167874"
                },
                {
                    "start": 1644,
                    "end": 1646,
                    "matchedPaperCorpusId": "15467396"
                },
                {
                    "start": 1646,
                    "end": 1648,
                    "matchedPaperCorpusId": "5060178"
                },
                {
                    "start": 1648,
                    "end": 1651,
                    "matchedPaperCorpusId": "10112929"
                },
                {
                    "start": 1651,
                    "end": 1653,
                    "matchedPaperCorpusId": "6171252"
                },
                {
                    "start": 1653,
                    "end": 1656,
                    "matchedPaperCorpusId": "12194143"
                },
                {
                    "start": 1744,
                    "end": 1748,
                    "matchedPaperCorpusId": "18517541"
                },
                {
                    "start": 2012,
                    "end": 2015,
                    "matchedPaperCorpusId": "8660413"
                },
                {
                    "start": 2015,
                    "end": 2018,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 2018,
                    "end": 2020,
                    "matchedPaperCorpusId": "9375250"
                },
                {
                    "start": 2020,
                    "end": 2023,
                    "matchedPaperCorpusId": "11345165"
                },
                {
                    "start": 2023,
                    "end": 2026,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 2026,
                    "end": 2028,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 2107,
                    "end": 2111,
                    "matchedPaperCorpusId": "624738"
                },
                {
                    "start": 2111,
                    "end": 2114,
                    "matchedPaperCorpusId": "2798452"
                },
                {
                    "start": 2114,
                    "end": 2117,
                    "matchedPaperCorpusId": "13274253"
                },
                {
                    "start": 2175,
                    "end": 2179,
                    "matchedPaperCorpusId": "18505561"
                },
                {
                    "start": 2179,
                    "end": 2182,
                    "matchedPaperCorpusId": "6181569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "258865156",
            "title": "Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval",
            "text": "Summarization is an NLP task that aims to generate accurate and coherent summaries for some given text automatically. Multi-document summarization (MDS) extends this task to provide multiple topicrelated documents as input, with the goal of summarizing salient information while avoiding redundancy. MDS is a popular research objective with many proposed approaches (Yasunaga et al., 2017;Liao et al., 2018;Liu and Lapata, 2019;Li et al., 2020;Jin et al., 2020;Mao et al., 2020;Zhang et al., 2020a;Pasunuru et al., 2021b;Xiao et al., 2022)  and important applications, e.g. the summarization of news articles (Fabbri et al., 2019;Gholipour Ghalandari et al., 2020), scientific literature (Lu et al., 2020;Wallace et al., 2021;DeYoung et al., 2021), and legal documents (Shen et al., 2022). \n\nExisting MDS task definitions, including queryfocused MDS (see \u00a73 for detailed comparison), assume a ground-truth, topic-related document set is provided at train and test time. This document set is often an artifact of the dataset curation process; in many practical settings, it is not available a priori and would need to be defined by an information need, expressed as a query.2 Documents relevant to the query would need to be retrieved from a large collection of (mostly irrelevant) documents and summarized (Figure 1); a setting we refer to as open-domain MDS. 3 Because even state-of-the-art information retrieval (IR) methods are imperfect, errors, like the retrieval of irrelevant documents, will occur. It is an open question how existing summarizers behave under this more challenging but realistic setting. Our major contributions are: \n\n\u2022 We formalize the task definition of open-domain MDS ( \u00a72), and bootstrap its study using existing datasets, retrievers and summarizers ( \u00a74);",
            "score": 0.6007455652140103,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1786
                }
            ],
            "ref_mentions": [
                {
                    "start": 366,
                    "end": 389,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 389,
                    "end": 407,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 407,
                    "end": 428,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 428,
                    "end": 444,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 444,
                    "end": 461,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 461,
                    "end": 478,
                    "matchedPaperCorpusId": "222090788"
                },
                {
                    "start": 478,
                    "end": 498,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 498,
                    "end": 521,
                    "matchedPaperCorpusId": "235097309"
                },
                {
                    "start": 521,
                    "end": 539,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 609,
                    "end": 630,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 630,
                    "end": 664,
                    "matchedPaperCorpusId": "218076648"
                },
                {
                    "start": 688,
                    "end": 705,
                    "matchedPaperCorpusId": "225075639"
                },
                {
                    "start": 705,
                    "end": 726,
                    "matchedPaperCorpusId": "221319573"
                },
                {
                    "start": 726,
                    "end": 747,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 769,
                    "end": 788,
                    "matchedPaperCorpusId": "249927023"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "8377315",
            "title": "Abstractive Multi-Document Summarization via Phrase Selection and Merging",
            "text": "Existing multi-document summarization (MDS) methods fall in three categories: extraction-based, compression-based and abstraction-based. Most summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004;Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000;Lin, 2003;Zajic et al., 2006;Harabagiu and Lacatusu, 2010;Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. \n\nIn fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013;Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008;Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. \n\nIn this paper, we propose an abstractive MDS framework that can construct new sentences by .",
            "score": 0.5982333363458762,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1793
                },
                {
                    "start": 1796,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 312,
                    "end": 329,
                    "matchedPaperCorpusId": "532313"
                },
                {
                    "start": 566,
                    "end": 590,
                    "matchedPaperCorpusId": "9363872"
                },
                {
                    "start": 590,
                    "end": 600,
                    "matchedPaperCorpusId": "7367421"
                },
                {
                    "start": 600,
                    "end": 619,
                    "matchedPaperCorpusId": "9011241"
                },
                {
                    "start": 619,
                    "end": 648,
                    "matchedPaperCorpusId": "15511124"
                },
                {
                    "start": 648,
                    "end": 664,
                    "matchedPaperCorpusId": "1803710"
                },
                {
                    "start": 1042,
                    "end": 1065,
                    "matchedPaperCorpusId": "15945816"
                },
                {
                    "start": 1279,
                    "end": 1306,
                    "matchedPaperCorpusId": "16188305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87548828125
        },
        {
            "corpus_id": "8294822",
            "title": "Multi-Document Summarization By Sentence Extraction",
            "text": "In the previous section we discussed the requirements for a multi-document summarization system. Depending on a user's information seeking goals, the user may want to create summaries that contain primarily the common portions of the documents (their intersection) or an overview of the entire cluster of documents (a sampling. of the space that the documents span). A user may also want to have a highly readable summary, an overview of pointers (sentences or word lists) to further information, \u2022 or a combination of the two. Following is a list of various methods of creating multi-document summaries by extraction:",
            "score": 0.5965377328671636,
            "section_title": "Types of Multi-Document Summarizers",
            "char_start_offset": 11564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 618
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69677734375
        },
        {
            "corpus_id": "10151113",
            "title": "System Combination for Multi-document Summarization",
            "text": "We present a novel framework of system combination for multi-document summarization. For each input set (input), we generate candidate summaries by combining whole sentences from the summaries generated by different systems. We show that the oracle among these candidates is much better than the summaries that we have combined. We then present a supervised model to select among the candidates. The model relies on a rich set of features that capture content importance from different perspectives. Our model performs better than the systems that we combined based on manual and automatic evaluations. We also achieve very competitive performance on six DUC/TAC datasets, comparable to the state-of-the-art on most datasets.",
            "score": 0.5954860369074206,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "corpus_id": "236460044",
            "title": "Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation",
            "text": "Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents to guide the summary generation process. \n\nWhile the multi-document summarization task aims to extract information shared by multiple documents, related work generation aims to compare and introduce the cited works in logic order.",
            "score": 0.5951431630269847,
            "section_title": "Related Work",
            "char_start_offset": 6577,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "238583377",
            "title": "Extending Multi-Text Sentence Fusion Resources via Pyramid Annotations",
            "text": "supplement the data with source document sentences ( \u00a73). Our contribution therefore is an extended sentencefusion dataset 1 , more than 4x times larger than its original, with 18% manually relabeled instances. We show that our final extended dataset better reflects challenges in multi-source summarization tasks ( \u00a74), with highly redundant salient content, originating in more representative sentences from the wild. In addition, we show ( \u00a75) that a contemporary generative model produces more abstractive output after training on our extended training set than on the original one. Similarly, it also outperforms the latter on the original test set. Given that sentence fusion was originally motivated as a step in modular multi-document summarization pipelines (Barzilay and McKeown, 2005;Marsi and Krahmer, 2005), we hope that progress on sentence fusion may contribute to broader contexts of multi-document consolidation and fusion tasks.",
            "score": 0.594448114724885,
            "section_title": "b.",
            "char_start_offset": 2989,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 767,
                    "end": 795,
                    "matchedPaperCorpusId": "16188305"
                },
                {
                    "start": 795,
                    "end": 819,
                    "matchedPaperCorpusId": "2293515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "235669622",
            "title": "TWAG: A Topic-Guided Wikipedia Abstract Generator",
            "text": "Multi-document summarization is a classic and challenging problem in natural language processing, which aims to distill an informative and coherent summary from a set of input documents. Compared with single-document summarization, the input documents may contain redundant or even contradictory information (Radev, 2000).\n\nEarly high-quality multi-document summarization datasets are annotated by humans, e.g., datasets for Document Understanding Conference (DUC) and Text Analysis Conference (TAC). These datasets are too small to build neural models, and most of the early works take an extractive method, attempting to build graphs with interparagraph relations and choose the most salient textual units. The graph could be built with various information, e.g., TF-IDF similarity (Erkan and Radev, 2004), discourse relation (Mihalcea and Tarau, 2004), document-sentence two-layer relations (Wan, 2008), multi-modal (Wan and Xiao, 2009) and query information (Cai and Li, 2012). Recently, there emerge attempts to incorporate neural models, e.g., Yasunaga et al. (2017) builds a discourse graph and represents textual units upon the graph convolutional network (GCN) (Kipf and Welling, 2017), and Yin et al. (2019) adopts the entity linking technique to capture global dependencies between sentences and ranks the sentences with a neural graph-based model.\n\nIn contrast, early abstractive models using sentence-fusion and paraphrasing (Filippova and Strube, 2008;Banerjee et al., 2015;Bing et al., 2015) achieve less success. Inspired by the recent success of single-document abstractive models (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;Huang et al., 2020), some works (Liu et al., 2018;Zhang et al., 2018) try to transfer single-document models to multi-document settings to alleviate the limitations of small-scale datasets. Specifically, Liu et al. (2018) defines Wikipedia generation problem and contributes the large-scale WikiSum dataset. Fabbri et al. (",
            "score": 0.5936176978479848,
            "section_title": "Multi-document Summarization",
            "char_start_offset": 4947,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 308,
                    "end": 321,
                    "matchedPaperCorpusId": "10103200"
                },
                {
                    "start": 784,
                    "end": 807,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 828,
                    "end": 854,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 894,
                    "end": 905,
                    "matchedPaperCorpusId": "17446655"
                },
                {
                    "start": 919,
                    "end": 938,
                    "matchedPaperCorpusId": "58509157"
                },
                {
                    "start": 962,
                    "end": 980,
                    "matchedPaperCorpusId": "14383376"
                },
                {
                    "start": 1050,
                    "end": 1072,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1200,
                    "end": 1217,
                    "matchedPaperCorpusId": "199466313"
                },
                {
                    "start": 1438,
                    "end": 1466,
                    "matchedPaperCorpusId": "14909308"
                },
                {
                    "start": 1466,
                    "end": 1488,
                    "matchedPaperCorpusId": "15795297"
                },
                {
                    "start": 1488,
                    "end": 1506,
                    "matchedPaperCorpusId": "8377315"
                },
                {
                    "start": 1598,
                    "end": 1616,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1616,
                    "end": 1636,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 1636,
                    "end": 1658,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1658,
                    "end": 1677,
                    "matchedPaperCorpusId": "218487279"
                },
                {
                    "start": 1690,
                    "end": 1708,
                    "matchedPaperCorpusId": "3608234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.857421875
        },
        {
            "corpus_id": "248496597",
            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
            "text": "The task of text summarization focuses on extracting the most salient information in one or a collection of documents and formulate them in a concise and readable way. While most of the existing works on multi-document summarization focus on homogeneous document groups with information overlap, a more generic and also practical case is to have heterogeneous document groups with more independent documents and only a minor information overlap. For example, in contrast to traditional multi-document methods capable of summarizing multiple reviews for a particular product, one use case of such a generic approach is to summarize a group of news articles in finance. Another case of heterogeneous multi-document summarization is summarizing research articles in a specific field, such as work studying COVID-19. In both cases, articles contain different stories or arguments with the possibility of overlapping information. \n\nThe nature of heterogeneous document groups necessitates different methodologies. If articles in a group discuss the same topic, then such overlapping information could be utilized to determine the content of the summary (Chu and Liu, 2019). Simple extractive algorithms developed upon graph-based ranking algorithms such as TextRank (Mihalcea and Tarau, 2004) can also be applied. However, if the articles in a group convey different stories, such methods built upon the assumption of overlapping information fail. Graph-based ranking algorithms for extracting salient sentences also become suboptimal if directly applied since they can lose focus on the information, provided there are multiple at once. In the case of summarizing a group of news articles, it is possible that some articles are conveying the same story. If the articles are summarized individually, the resulting group of summaries may still have overlapping and hence redundant information that should have been unified. If the articles are summarized based on the overlapping information, key information from other non-overlapping news articles is lost. Hence, it is important to balance the need of preserving useful information, while casting away redundant, overlapping information. \n\nThis work explores the problem of summarizing a more generic group of documents, keeping the important information while retaining the least amount of redundancy, by adopting a two-stage framework.",
            "score": 0.59264863463963,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2184
                },
                {
                    "start": 2187,
                    "end": 2384
                }
            ],
            "ref_mentions": [
                {
                    "start": 1148,
                    "end": 1167,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 1261,
                    "end": 1287,
                    "matchedPaperCorpusId": "577937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "202889056",
            "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
            "text": "The idea of using multiple embeddings has been employed at the word level. Kiela et al. (2018) use an attention mechanism to combine the embeddings for each word for the task of natural language inference. Xu et al. (2018); Bollegala et al. (2015) concatenate the embeddings of each word into a vector before feeding a neural network for the tasks of aspect extraction and sentiment analysis. To our knowledge, we are the first to combine multiple types of sentence embeddings. \n\nExtractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988). \n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges.",
            "score": 0.5903775087637,
            "section_title": "Related Work",
            "char_start_offset": 23272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1957
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 94,
                    "matchedPaperCorpusId": "52166626"
                },
                {
                    "start": 206,
                    "end": 222,
                    "matchedPaperCorpusId": "44009215"
                },
                {
                    "start": 224,
                    "end": 247,
                    "matchedPaperCorpusId": "14116842"
                },
                {
                    "start": 615,
                    "end": 627,
                    "matchedPaperCorpusId": "10103200"
                },
                {
                    "start": 719,
                    "end": 741,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 945,
                    "end": 965,
                    "matchedPaperCorpusId": "5457260"
                },
                {
                    "start": 965,
                    "end": 989,
                    "matchedPaperCorpusId": "17064982"
                },
                {
                    "start": 991,
                    "end": 1010,
                    "matchedPaperCorpusId": "9849366"
                },
                {
                    "start": 1073,
                    "end": 1090,
                    "matchedPaperCorpusId": "11977708"
                },
                {
                    "start": 1209,
                    "end": 1234,
                    "matchedPaperCorpusId": "337730"
                },
                {
                    "start": 1378,
                    "end": 1403,
                    "matchedPaperCorpusId": "60514661"
                },
                {
                    "start": 1495,
                    "end": 1526,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 1665,
                    "end": 1682,
                    "matchedPaperCorpusId": "1207010"
                },
                {
                    "start": 1710,
                    "end": 1724,
                    "matchedPaperCorpusId": "5879376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "678258",
            "title": "Exploring Content Models for Multi-Document Summarization",
            "text": "Over the past several years, there has been much interest in the task of multi-document summarization. In the common Document Understanding Conference (DUC) formulation of the task, a system takes as input a document set as well as a short description of desired summary focus and outputs a word length limited summary. 1 To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content. 2 There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende,  2005), graph-based approaches (Radev, 2004; Wan  and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec  et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum\u00e9 III and  Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. \n\nIn this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei  et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2). We also contend that they provide convenient building blocks for adding more structure to a summarization model. In particular, we utilize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific 'subtopics' within a document set. The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics.",
            "score": 0.5902543533425346,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1890
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "247958026",
            "title": "Automatic Text Summarization Methods: A Comprehensive Review",
            "text": "Based upon size of input source documents that are used to generate a summary, summarization can be divided in two types: \n\n\u2022 Single Document: Single document text summarization is automatic summarization of information a single document (Garner, 1982). \n\n\u2022 Multiple Document: Multi-document text summarization is an automatic summarization of information from multiple document (Ferreira et al., 2014). \n\nMulti-document summarization is important where we must put different types of opinions together, and each idea is written with multiple perspectives within a single document. Single document text summarization is easy to implement, but multi-document summarization is a complex task. Redundancy is one of the biggest problems in summarizing multiple documents. Carbonell & Goldstein (1998) has given MMR (Maximal Marginal Relevance) approach, which helps to reduce redundancy. Another main problem for multi-document summarization is heterogeneity within a large set of documents. It is very complex to summarize multiple documents with extractive methods where there are so many conflicts and biases in the real world. Here for multiple documents, abstractive summarization performs far better. However, multi-document summarization also brings issues like redundancy in output summary while working with a huge number of documents. Single document text summarization is used in a limited field like reading the given comprehension and giving an appropriate title or summary. In contrast, multi-document text summarization can be used in the field of news summarization from different sites, customer's product reviews from different vendors, Q&A systems and many more. \n\nSummCoder (Joshi et al., 2019) is a new methodology for generic extractive single document text summarization. The method creates a summary based on three criteria they developed: sentence content relevance, sentence novelty, and sentence position relevance. The novelty metric is produced by utilizing the similarity among sentences represented as embedding in a distributed semantic space, and the sentence content relevance is assessed using a deep auto-encoder network. The sentence position relevance metric is a custom feature that gives the initial few phrases more weight thanks to a dynamic weight calculation method controlled by the document length.",
            "score": 0.5897331538745769,
            "section_title": "Based on no. of Input documents",
            "char_start_offset": 5984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 124,
                    "end": 253
                },
                {
                    "start": 256,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 252,
                    "matchedPaperCorpusId": "144856356"
                },
                {
                    "start": 379,
                    "end": 402,
                    "matchedPaperCorpusId": "14650402"
                },
                {
                    "start": 768,
                    "end": 796,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 1690,
                    "end": 1710,
                    "matchedPaperCorpusId": "131782979"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "6721600",
            "title": "The Smart/Empire TIPSTER IR System",
            "text": "crucial when working with sentence based summaries.\n\nMulti-Document Summarization. Our current work includes extending context-dependent summarization techniques for use in multi-document, rather than single-document, summarization. Our work on duplicate information detection will also be critical for creating these more complicated summaries. We have no results to report for multi-document summarization at this time.",
            "score": 0.5895530986150743,
            "section_title": "CONTEXT-DEPENDENT SUMMARIZATION",
            "char_start_offset": 44630,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66748046875
        },
        {
            "corpus_id": "269762702",
            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
            "text": "As the development of information technology has incrementally sparked a large quantity of text data and corpus, there is an increasingly high demand for summarizing documents to assist users in gathering the most important and relevant information quickly and easily.Document Summarization (DS) is a Natural Language Processing (NLP) task of generating an abridged version of a given single or multiple documents as concise and coherent as possible while preserving salient and consistent information from the source text.According to the number of input source texts, the document summarization task can be categorized as single document summarization (SDS) task and multi-document summarization (MDS) task.In contrast to single document summarization, multi-document summarization can generate more comprehensive and objective digests, achieving a higher quality of generated summaries.That is because multi-document summarization targets generating a compressed and informative summary across a set of topic-related documents from diverse times, covering various perspectives [16].Accordingly, multi-document summarization tasks have effectively contributed to a wide range of real-world applications such as the generation of summative texts from news, scientific publications, emails, product reviews, lecture feedback, Wikipedia articles, medical documents, and software project activities [16].Consequently, with an increasing abundance of demands and requirements for significant information synthesis from academia and industries, improving multi-document summarization performance has been pumped into enormous research interests and attention.\n\nKnowledge-aware approaches have boosted a range of natural language processing applications over the last decades, such as question-answering [24] and recommendation systems [9].That is because knowledge can represent more applicable information from more facets with the source information, benefiting the informativeness and fact consistency of the generated textual results.With the gathered momentum, knowledge recently has gradually attracted considerable attention in research of document summarization.Generally, advanced knowledge-aware summarization methods utilize graph structures to capture and incorporate the knowledge into models.Identifying and proving the effectiveness of leveraging knowledge and knowledge graphs into document summarization tasks by empirical results is still a potential research direction in the document summarization field.",
            "score": 0.5883045203793976,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 523
                },
                {
                    "start": 523,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1402
                },
                {
                    "start": 1402,
                    "end": 1655
                },
                {
                    "start": 1657,
                    "end": 1835
                },
                {
                    "start": 1835,
                    "end": 2034
                },
                {
                    "start": 2034,
                    "end": 2166
                },
                {
                    "start": 2166,
                    "end": 2302
                },
                {
                    "start": 2302,
                    "end": 2520
                }
            ],
            "ref_mentions": [
                {
                    "start": 1080,
                    "end": 1084,
                    "matchedPaperCorpusId": "226289939"
                },
                {
                    "start": 1397,
                    "end": 1401,
                    "matchedPaperCorpusId": "226289939"
                },
                {
                    "start": 1799,
                    "end": 1803,
                    "matchedPaperCorpusId": "19135805"
                },
                {
                    "start": 1831,
                    "end": 1834,
                    "matchedPaperCorpusId": "211010433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "8294822",
            "title": "Multi-Document Summarization By Sentence Extraction",
            "text": "Multidocument summarization -capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones -are likely to be essential in such situations. Ideally, multi-document summaries should contain the key shared relevant information among all the documents only once, plus other information unique to some of the individual documents that are directly relevant to the user's query. \n\nThough many of the same techniques used in singledocument summarization can also be used in multidocument summarization, there are at least four significant differences: \n\n1. The degree of redundancy in information contained within a group of topically-related articles is much higher than the degree of redundancy within an article, as each article is apt to describe the main point as well as necessary shared background. Hence anti-redundancy methods are more crucial. \n\n2. A group of articles may contain a temporal dimension, typical in a stream of news reports about an unfolding event. Here later information may override earlier more tentative or incomplete accounts. \n\n3. The compression ratio (i.e. the size of the summary with respect to the size of the document set) will typically be much smaller for collections of dozens or hundreds of topically related documents than for single document summaries. The SUMMAC evaluation (TIPSTER, 1998a) tested 10% compression summaries, but in our work summarizing 200document clusters, we find that compression to the 1% or 0.1% level is required. Summarization becomes significantly more difficult when compression demands increase. \n\n4. The co-reference problem in summarization presents even greater challenges for multidocument than for single-document summarization (Baldwin and Morton, 1998). \n\nThis paper discusses an approach to multi-document summarization that builds on previous work in single- \n\ndocument summarization by using additional, available information about the document set as a whole, the relationships between the documents, as well as individual documents.",
            "score": 0.5882761847590517,
            "section_title": "Introduction",
            "char_start_offset": 1744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1618
                },
                {
                    "start": 1621,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1890
                },
                {
                    "start": 1893,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 1756,
                    "end": 1782,
                    "matchedPaperCorpusId": "15450389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "8294822",
            "title": "Multi-Document Summarization By Sentence Extraction",
            "text": "Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997;Goldstein and Carbonell, 1998;TIPSTER, 1998b;Radev and McKeown, 1998;Mani and Bloedorn, 1999;McKeown et al., .!999;Stein et al., 1999). These include comparing templates filled in by extracting information -using specialized, domain specific knowledge sources -from the doc-\"ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com--\u2022 paring named-entities -extracted using specialized lists between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related lexical items (identity mappings, synonyms, hypernyms, etc.) to extract text spans from the document set (Mani and Bloedern, 1997). Another system (Stein et al., 1999) creates a multi-document summary from multiple single document summaries, an approach that can be sub-optimal in some cases, due to the fact that the process of generating the final multi-document summary takes as input the individual summaries and not the complete documents. (Particularly if the single-document summaries can contain much overlapping information.) The Columbia University system (McKeown et al., 1999) creates a multi-document summary using machine learning and statistical techniques to identify similar sections and language generation to reformulate the summary. The focus of our approach is a multi-document system that can quickly summarize large clusters of similar documents (on the order of thousands) while providing the key relevant useful information or pointers to such information. Our system (1) primarily uses only domainindependent techniques, based mainly on fast, statistical processing, (2) explicitly deals with the issue of reducing redundancy without eliminating potential relevant information, and (3) contains parameterized modules, so that different genres or corpora characteristics can be taken into account easily.",
            "score": 0.5876043317090431,
            "section_title": "Background and Related Work",
            "char_start_offset": 5081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 144,
                    "matchedPaperCorpusId": "9177142"
                },
                {
                    "start": 144,
                    "end": 174,
                    "matchedPaperCorpusId": "60778976"
                },
                {
                    "start": 259,
                    "end": 278,
                    "matchedPaperCorpusId": "232822009"
                },
                {
                    "start": 913,
                    "end": 938,
                    "matchedPaperCorpusId": "9177142"
                },
                {
                    "start": 955,
                    "end": 975,
                    "matchedPaperCorpusId": "232822009"
                },
                {
                    "start": 1374,
                    "end": 1396,
                    "matchedPaperCorpusId": "8115414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "247593888",
            "title": "Read Top News First: A Document Reordering Approach for Multi-Document News Summarization",
            "text": "We conduct experiments on Multi-News and DUC-2004. Multi-News is the largest multi-document summarization dataset in the news domain. It contains 44,972/5,622/5,622 instances for training/validation/test. Each instance contains a set of news articles and an abstractive summary. The number of articles varies between 2 and 10. For evaluation, we compare the extracted summary to the gold abstractive summary. DUC-2004 contains 50 instances. Each instance has 10 documents and their abstractive summaries. Due to its small size, we use this dataset for out-of-domain evaluation only. We also use CNN DailyMail (CNNDM) to pretrain the base summarization model. It contains around 300K news articles and corresponding summaries from CNN and the Daily Mail.\n\nWe list the training details as follows. The training loss is optimized using Adam (Kingma and Ba, 2015) with a learning rate of 2 \u00d7 10 \u22123 and 10,000 training steps. We apply the warmup (Goyal et al., 2017) on the first 2,000 steps and the early stopping based on the ROUGE-1 score on the development set. The batch size is set as 6,000 tokens. Our model was trained on a single Quadro RTX 5000 GPU in 2 hours. Table 5: Sample summaries generated by our method and the baselines. MatchSum and PreSumm receives the documents as the original order, making them focus more on the top two documents. Our method first rearrange the documents as the order of {3, 4, 2, 1} and then create the summary. We highlight the contents of the generated summaries which are relevant to the referenced summary.",
            "score": 0.5870202033471128,
            "section_title": "B Training Details",
            "char_start_offset": 17722,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7275390625
        },
        {
            "corpus_id": "227336160",
            "title": "An Enhanced MeanSum Method For Generating Hotel Multi-Review Summarizations",
            "text": "Multi-document summaritazion is the process of taking multiple texts as input and producing a short summary text based on the content of input texts. Up until recently, multi-document summarizers are mostly supervised extractive. However, supervised methods require datasets of large, paired document-summary examples which are rare and expensive to produce. In 2018, an unsupervised multi-document abstractive summarization method(Meansum) was proposed by Chu and Liu, and demonstrated competitive performances comparing to extractive methods. Despite good evaluation results on automatic metrics, Meansum has multiple limitations, notably the inability of dealing with multiple aspects. The aim of this work was to use Multi-Aspect Masker(MAM) as content selector to address the issue with multi-aspect. Moreover, we propose a regularizer to control the length of the generated summaries. Through a series of experiments on the hotel dataset from Trip Advisor, we validate our assumption and show that our improved model achieves higher ROUGE, Sentiment Accuracy than the original Meansum method and also beats/ comprarable/close to the supervised baseline.",
            "score": 0.586919945543283,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74462890625
        },
        {
            "corpus_id": "270619335",
            "title": "Converging Dimensions: Information Extraction and Summarization through Multisource, Multimodal, and Multilingual Fusion",
            "text": "These facts could then be compiled using this multi-document summarization system into a comprehensive summary (Chen and Zhuge, 2014).Another literature investigates multi-document summarization techniques that evaluate various models, including graph-based, neural network-based, and hybrid approaches, proposing methods to effectively synthesize information from multiple papers (Agarwal et al., 2011).\n\nThe prevailing information extraction and summarization methodologies are predominantly characterized by their singular source dependence and a dearth of multi-modality.This constraint restricts the potential for optimal knowledge acquisition.This singular source dependence often leads to redundancy within extracted information and hinders the capture of diverse perspectives.To achieve a more comprehensive understanding of a subject, it is imperative to leverage information from a multiplicity of sources.To address these limitations, research efforts should be directed towards the development of robust multi-source information extraction and summarization techniques.\n\nThe proposed multifaceted approach should serve a dual purpose: it not only mitigates redundancy within the extracted data but also promotes the inclusion of diverse and potentially conflicting perspectives.This comprehensive methodology should enhance the overall quality of the data by optimizing its relevance and breadth.Minimizing repetitive information, ensures the capture of",
            "score": 0.5859226685819199,
            "section_title": "Related Work",
            "char_start_offset": 5798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 404
                },
                {
                    "start": 406,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 649
                },
                {
                    "start": 649,
                    "end": 784
                },
                {
                    "start": 784,
                    "end": 1081
                },
                {
                    "start": 1083,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1408
                },
                {
                    "start": 1408,
                    "end": 1465
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 132,
                    "matchedPaperCorpusId": "13216181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "232307817",
            "title": "Nutri-bullets: Summarizing Health Studies by Composing Segments",
            "text": "Multi-document Summarization Approaches in neural sequence-to-sequence learning (Rush, Chopra, and Weston 2015;Cheng and Lapata 2016;See, Liu, and Manning 2017) for document summarization have shown promise and have been adapted successfully for multi-document summarization (Zhang, Tan, and Wan 2018;Lebanoff, Song, and Liu 2018;Baumel, Eyal, and Elhadad 2018;Amplayo and Lapata 2019;Fabbri et al. 2019). Trained on large amounts of data, these methods have improved upon traditional extractive (Carbonell and Goldstein 1998;Radev and McKeown 1998;Haghighi and Vanderwende 2009) and abstractive approaches (Barzilay, McKeown, and Elhadad 1999;McKeown and Radev 1995;Ganesan, Zhai, and Han 2010). De-spite producing fluent text, these techniques also tend to generate false information which is not faithful to the original inputs (Puduppully, Dong, and Lapata 2019;Kry\u015bci\u0144ski et al. 2019). Side-information, such as citations in scientific domains (Qazvinian and Radev 2008;Qazvinian et al. 2013) or semantic representations (Liu et al. 2015), can be used to improve this (Sharma et al. 2019;Wenbo et al. 2019;Puduppully, Dong, and Lapata 2019;Koncel-Kedziorski et al. 2019a). However, such methods struggle in low resource scenarios. In this work, we are interested in producing faithful and fluent text in a technical domain where few parallel examples are available.\n\nText Fusion Traditionally, sentence fusion approaches (Barzilay and McKeown 2005) aid the concatenation of different text fragments for summarization. Recent language modeling approaches like Devlin et al. (2018); Stern et al. (2019) can also be extended for completion and fusion of partial text. These models have more flexibility than those trained on text fusion datasets (Narayan et al. 2017;Geva et al. 2019) that can combine two fragments only. In this work, we modify the Blank Language Model (Shen et al. 2020) to combine fragments coming from different source documents",
            "score": 0.5847097357945267,
            "section_title": "Related Work",
            "char_start_offset": 4677,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 111,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 111,
                    "end": 133,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 133,
                    "end": 160,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 275,
                    "end": 301,
                    "matchedPaperCorpusId": "53223447"
                },
                {
                    "start": 496,
                    "end": 526,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 526,
                    "end": 549,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 549,
                    "end": 579,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 607,
                    "end": 644,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 644,
                    "end": 667,
                    "matchedPaperCorpusId": "2446679"
                },
                {
                    "start": 667,
                    "end": 695,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 975,
                    "end": 997,
                    "matchedPaperCorpusId": "324527"
                },
                {
                    "start": 1026,
                    "end": 1043,
                    "matchedPaperCorpusId": "5001921"
                },
                {
                    "start": 1073,
                    "end": 1093,
                    "matchedPaperCorpusId": "202537639"
                },
                {
                    "start": 1426,
                    "end": 1453,
                    "matchedPaperCorpusId": "16188305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "1465750",
            "title": "Using N-Grams To Understand the Nature of Summaries",
            "text": "For our experiments we used data made available from the 2001 Document Understanding Conference (DUC), an annual large-scale evaluation of summarization systems sponsored by the National Institute of Standards and Technology (NIST). In this corpus, NIST has gathered documents describing 60 events, taken from the Associated Press, Wall Street Journal, FBIS San Jose Mercury, and LA Times newswires. An event is described by between 3 and 20 separate (but not necessarily unique) documents; on average a cluster contains 10 documents. Of the 60 available clusters, we used the portion specifically designated for training, which contains a total of 295 documents distributed over 30 clusters. \n\nAs part of the DUC 2001 summarization corpus, NIST also provides four hand-written summaries of different lengths for every document cluster, as well as 100-word summaries of each document. Since we wished to collectively compare single-document summaries against multi-document summaries, we used the 100-word multi-document summaries for our analysis. It is important to note that for each cluster, all summaries (50, 100, 200 and 400-word multi-document and 100-word per-document) have been written by the same author. NIST used a total of ten authors, each providing summaries for 3 of the 30 topics. The instructions provided did not differ per task; in both single and multi-document scenarios, the authors were directed to use complete sentences and told to feel free to use their own words (Over, 2004). \n\nTo compare the text of human-authored multidocument summaries to the full-text documents describing the events, we automatically broke the documents into sentences, and constructed a minimal tiling of each summary sentence. Specifically, for each sentence in the summary, we searched for all n-grams that are present in both the summary and the documents, placing no restrictions on the potential size of an n-gram. We then covered each summary sentence with the ngrams, optimizing to use as few n-grams as possible (i.e. favoring n-grams that are longer in length). For this experiment, we normalized the data by converting all terms to lowercase and removing punctuation.",
            "score": 0.5847054077103178,
            "section_title": "Data and Experiments",
            "char_start_offset": 6331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2182
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65185546875
        },
        {
            "corpus_id": "274964986",
            "title": "Multi-LLM Text Summarization",
            "text": "Large language models (LLMs) have been shown to have the potential to produce high-quality summaries (Chowdhery et al., 2022;Zhang et al., 2023;Goyal et al., 2023;Pu et al., 2023b). However, despite the remarkable progress in LLM-based summarization, limitations still exist for documents where useful information may be sparsely distributed throughout the text. Research by (Liu et al., 2023) highlights that a naive application of LLMs may overlook critical details or fail to grasp the holistic meaning of a document, indicating the need for more refined methods. \n\nTo address this, recent efforts have explored prompt-engineering techniques to guide LLMs towards producing better summaries (Adams et al., 2023). These techniques, while promising, still face limitations in consistently delivering high-quality summaries across different document types and structures. Instead of relying solely on a single model or simple prompt-engineering methods, we propose an approach novel to the summarization domain that focuses on aggregating the collective strengths of multiple LLMs. By combining the capabilities of multiple models with a diverse set of knowledge bases, we show it's possible to achieve more robust summaries across domains. \n\nSummary of Main Contributions. The main contributions of this work are as follows: \n\n\u2022 We propose the first framework for multi-LLM text summarization and investigate two topologies: centralized and decentralized. \u2022 We find that multi-LLM text summarization often performs better than using a single LLM for summarization, and we show that the best performing method in the framework aligns with human judgments. \u2022 We conduct experiments on how prompting, number of LLMs, and various combinations of generating and evaluating LLMs can affect quality of summaries in the multi-LLM setup. \n\n2 Related Work",
            "score": 0.5846673909712036,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1829
                },
                {
                    "start": 1832,
                    "end": 1846
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6328125
        },
        {
            "corpus_id": "245986626",
            "title": "Multi-Narrative Semantic Overlap Task: Evaluation and Benchmark",
            "text": "Summary length control for abstractive summarization has also been studied (Kikuchi et al., 2016;Fan et al., 2017;Liu et al., 2018;Fevry and Phang, 2018;Schumann, 2018;Makino et al., 2019). In general, multiple document summarization (Goldstein et al., 2000;Yasunaga et al., 2017;Zhao et al., 2020;Ma et al., 2020;Meena et al., 2014) is more challenging than single document summarization. However, MNSO task is different from traditional multi-document summarization tasks in that the goal here is to summarize content with an overlap constraint, i.e., the output should only contain the common information from both input narratives. \n\nAlternatively, one could aim to recover verb predicate-alignment structure (Roth and Frank, 2012;Xie et al., 2008;Wolfe et al., 2013) from a sentence and further, use this structure to compute the overlapping information (Wang and Zhang, 2009;Shibata and Kurohashi, 2012). Sentence Fusion is another related area which aims to combine the information from two given sentences with some additional constraints (Barzilay et al., 1999;Marsi and Krahmer, 2005;Krahmer et al., 2008;Thadani and McKeown, 2011). A related but simpler task is to retrieve parallel sentences (Cardon and Grabar, 2019;Nie et al., 1999;Murdock and Croft, 2005) without performing an actual intersection. However, these approaches are more targeted towards individual sentences and do not directly translate to arbitrarily long documents. Thus, MNSO task is still an open problem and there is no existing dataset, method or evaluation metric that have been systematically studied. \n\nAlong the evaluation dimension, ROUGE (Lin, 2004) is perhaps the most commonly used metric today for evaluating automated summarization techniques; due to its simplicity and automation. However, ROUGE has been criticized a lot for primarily relying on lexical overlap (Nenkova, 2006;Zhou et al., 2006;Cohan and Goharian, 2016) of n-grams.",
            "score": 0.5842062840337467,
            "section_title": "Related Works",
            "char_start_offset": 5409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1930
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 131,
                    "matchedPaperCorpusId": "53079938"
                },
                {
                    "start": 234,
                    "end": 258,
                    "matchedPaperCorpusId": "8294822"
                },
                {
                    "start": 258,
                    "end": 280,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 280,
                    "end": 298,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 298,
                    "end": 314,
                    "matchedPaperCorpusId": "196180065"
                },
                {
                    "start": 314,
                    "end": 333,
                    "matchedPaperCorpusId": "39340997"
                },
                {
                    "start": 713,
                    "end": 735,
                    "matchedPaperCorpusId": "10765592"
                },
                {
                    "start": 752,
                    "end": 771,
                    "matchedPaperCorpusId": "1147778"
                },
                {
                    "start": 859,
                    "end": 881,
                    "matchedPaperCorpusId": "144647"
                },
                {
                    "start": 881,
                    "end": 909,
                    "matchedPaperCorpusId": "1692766"
                },
                {
                    "start": 1047,
                    "end": 1070,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 1070,
                    "end": 1094,
                    "matchedPaperCorpusId": "2293515"
                },
                {
                    "start": 1094,
                    "end": 1115,
                    "matchedPaperCorpusId": "17303811"
                },
                {
                    "start": 1115,
                    "end": 1141,
                    "matchedPaperCorpusId": "2760413"
                },
                {
                    "start": 1204,
                    "end": 1229,
                    "matchedPaperCorpusId": "209092278"
                },
                {
                    "start": 1229,
                    "end": 1246,
                    "matchedPaperCorpusId": "12891906"
                },
                {
                    "start": 1246,
                    "end": 1270,
                    "matchedPaperCorpusId": "7188219"
                },
                {
                    "start": 1630,
                    "end": 1641,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "263320200",
            "title": "SDbQfSum: Query\u2010focused summarization framework based on diversity and text semantic analysis",
            "text": "This study falls in the category of extractive query-focused multi-document summarization as the developed approach is meant to answer complex user queries in the form of summaries extracted from related document sets. \n\nResearch on text summarization has achieved good progress in methods, applications, and languages since Luhn's pioneering work in 1958 (Luhn, 1958), but significant number of challenges still exist in the field to produce meaningful, coherent, easily readable summaries (El-Kassas et al., 2021). And unlike SDS (based on single source document), MDS (based on several related documents) has a number of additional challenges associated with it. This includes the differences and similarities across related document clusters, the high degree of redundancy inherent in extracts created from multiple sources, and the small source-to-summary compression ratio. This study attempts to address some of these challenges by investigating the following key research questions. \n\n1. What are the current challenges and limitations in extractive multi-document summarization, and how can we address them? This is discussed in sections 3 & 4 of this paper. \n\n2. Can the topic diversity of a multi-document summary be improved by reducing the information overlap at the pre-processing and summary extraction stages? This is addressed in Section 4 and the experiment part of this paper. \n\n3. Will the use of text semantic representation methods produce reliable features for scoring document sentences and establishing query relevance? This is addressed in Section 6 of this manuscript. \n\nEspecially, we propose a query-focused summarization framework built on effective text semantic representation techniques and a diversity principle in this paper. The framework uses the entire Wikipedia database as an external knowledge. Specifically, we apply a diversity-based approach to retrieve diverse concepts in the query reply summary, and semantic analysis techniques to enrich and improve sentence (and query) scoring features. The work is partially motivated by the improvements in measuring short text semantic similarities in our previous Wikipedia graph based single and multi-document text summarization approach (Mohamed, 2016;Mohamed & Oussalah, 2019). Besides, relevant literature confirms that using language semantics has the potential for building improved summarization approaches, which provides motivation for further research investigation in this area (Nenkova & McKeown, 2012).",
            "score": 0.5839598562826773,
            "section_title": "| INTRODUCTION",
            "char_start_offset": 2049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1167
                },
                {
                    "start": 1170,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1595
                },
                {
                    "start": 1598,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2503
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 368,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 491,
                    "end": 515,
                    "matchedPaperCorpusId": "14650402"
                },
                {
                    "start": 2242,
                    "end": 2267,
                    "matchedPaperCorpusId": "145993667"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8935546875
        },
        {
            "corpus_id": "277150718",
            "title": "Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer",
            "text": "Abstractive multi-document summarization (MDS) is the task of automatically summarizing information in multiple documents, from news articles to conversations with multiple speakers. The training approaches for current MDS models can be grouped into four approaches: end-to-end with special pre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and inference with GPT-style models. In this work, we evaluate MDS models across training approaches, domains, and dimensions (reference similarity, quality, and factuality), to analyze how and why models trained on one domain can fail to summarize documents from another (News, Science, and Conversation) in the zero-shot domain transfer setting. We define domain-transfer\"failure\"as a decrease in factuality, higher deviation from the target, and a general decrease in summary quality. In addition to exploring domain transfer for MDS models, we examine potential issues with applying popular summarization metrics out-of-the-box.",
            "score": 0.5837097665967214,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "2055498",
            "title": "A Text-Extraction Based Summarizer",
            "text": "Work and Future Work \n\nThe current summarizer is still undergoing improvement and adaptation in order to be able to summarize more than a single text news document at a time. At the same time we are investigating how summarization can be used in related but different problems. Both will be described below. \n\nA better and more flexible summarizer \n\nCurrently our summarizer is especially tuned for English one-document text-only news summarization. \n\nWhile we are still working on improving this, we also want the system to be able to summarize a wider variety of documents. Many challenges remain, including summarization of non-news documents, multimodal documents (such as web pages), foreign language documents and (small or large) groups of documents covering one or more topics. Typically, a user needs summarization the most when dealing with a large number of documents. Therefore, the next logical step is to summarize more than one documents at a time. At the moment we are focusing on multi-document (crossdocument) summarization of English text-only news documents. Just as for single-document summarization, multi-document summarization can be generic or topical and indicative or informative. Other factors that will influence the types of summary are the number of documents (a large versus a small set) and the variety of topics discussed by the documents (are the documents closely related or can they cover very different topics). Presentation of a multi-document offers a wide variety of choices. One could create one large text summary that gives an overview of all the main issues mentioned in all summaries. Or perhaps give different short summaries for similar documents. If the number of documents is very large it might be best to create nested summaries with high-level descriptions and the possibility to 'zoom in' on a subgroup with a more specific summary. A user will probably want to have the ability to trace information in a summary back to its original document; source information should be a part of the summary. If one views summarization in the context of tracking a topic, the main goal of the summary might be to show the new information every next document contains, while not repeating information already mentioned in previous documents.",
            "score": 0.5834958267286173,
            "section_title": "Related",
            "char_start_offset": 24838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 23,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2281
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.837890625
        },
        {
            "corpus_id": "273606761",
            "title": "Discrete Bat Algorithm for Efficient Multi-Document Summarization",
            "text": "The objective of multi-document summarization is to concise the content of multiple documents in a readable form without repetition of contents and with all vital information. All these three are three different objectives, and hence the computational factor of these three objectives to be carried out for every generated summary in different forms.",
            "score": 0.5826202096256216,
            "section_title": "Objective function",
            "char_start_offset": 18228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 350
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5693359375
        },
        {
            "corpus_id": "265068313",
            "title": "Non-Parametric Memory Guidance for Multi-Document Summarization",
            "text": "Multi-document summarization is performed using two methods: extractive (Wang et al., 2020;Liu et al., 2021) or abstractive (Jin et al., 2020;Xiao et al., 2022). So-called extractive methods rank sentences from source documents that best summarize them. These methods reuse important information well to construct a good summary but they lack coherence between sentences. To overcome this issue, abstractive methods are studied to imitate human writing behavior. They show great performance in human writing style but they often miss key information. \n\nTo make abstractive models aware of essential information, (Dou et al., 2021) guides their model with additional information like a set of keywords, graph triples, highlighted sentences of source documents, or retrieved similar summaries. Their method, which uses every guidance previously mentioned, improves summary quality and controllability compared with unguided models. However, guidances require specific training data, especially for keywords, graph triples, and highlighted sentences. \n\nOur proposal is that by guiding with pre-existing summaries, the model can draw inspiration from the summary as a whole. But also be able to extract keywords and phrases using a copy mechanism. Consequently, this work focuses on guidance by similar summaries extracted from a knowledge base using a similarity metric between source documents and pre-existing summaries. The model, inspired by RAG (Lewis et al., 2020), is fully differentiable. In addition, the model generator uses a copy mechanism on the candidates returned from the knowledge base, inspired by (Cai et al., 2021). The findings of these two studies motivated the development of our model for the multi-document text summarization task. \n\nWe demonstrate the potential of our method on MultiXScience (Lu et al., 2020). This dataset gathers scientific articles where we have to generate the \"related work\" part with the \"abstract\" of the source article and the \"abstracts\" of the citations. In the case of scientific articles, we believe that the source documents are insufficient to generate the \"related work\" part because external knowledge is necessary to write such a paragraph. \n\nIn this work, we investigate a sequence-tosequence model guided by a memory retriever of similar summaries.",
            "score": 0.5821148430888489,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2198
                },
                {
                    "start": 2201,
                    "end": 2308
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 91,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 91,
                    "end": 108,
                    "matchedPaperCorpusId": "238744469"
                },
                {
                    "start": 124,
                    "end": 142,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 142,
                    "end": 160,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 612,
                    "end": 630,
                    "matchedPaperCorpusId": "223953416"
                },
                {
                    "start": 1447,
                    "end": 1467,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1613,
                    "end": 1631,
                    "matchedPaperCorpusId": "235166182"
                },
                {
                    "start": 1816,
                    "end": 1833,
                    "matchedPaperCorpusId": "225075639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "1156692",
            "title": "Automated Text Summarization Base on Lexicales Chain and graph Using of WordNet and Wikipedia Knowledge Base",
            "text": "The technology of automatic document summarization is maturing and may provide a solution to the information overload problem. Nowadays, document summarization plays an important role in information retrieval (IR). With a large volume of documents, presenting the user with a summary of each document greatly facilitates the task of finding the desired documents. Text summarization is the process of automatically creating a compressed version of a given text that provides useful information to users, and multi-document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic [14]. Authors of the paper [10] provide the following definition for a summary: \"A summary can be loosely defined as a text that is produced from one or more texts that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually significantly less than that. Text here is used rather loosely and can refer to speech, multimedia documents, hypertext, etc. The main goal of a summary is to present the main ideas in a document in less space. If all sentences in a text document were of equal importance, producing a summary would not be very effective, as any reduction in the size of a document would carry a proportional decrease in its in formativeness. Luckily, information content in a document appears in bursts, and one can therefore distinguish between more and less informative segments. Identifying the informative segments at the expense of the rest is the main challenge in summarization\". assumes a tripartite processing model distinguishing three stages: source text interpretation to obtain a source representation, source representation transformation to summary representation, and summary text generation from the summary representation. A variety of document summarization methods have been developed recently. The paper [4] reviews research on automatic summarizing over the last decade. This paper reviews salient notions and developments, and seeks to assess the state-of-the-art for this challenging natural language processing (NLP) task. The review shows that some useful summarizing for various purposes can already be done but also, not surprisingly, that there is a huge amount more to do. Sentence based extractive summarization techniques are commonly used in automatic summarization to produce extractive summaries.",
            "score": 0.5815444336145754,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2196
                },
                {
                    "start": 2197,
                    "end": 2351
                },
                {
                    "start": 2352,
                    "end": 2480
                }
            ],
            "ref_mentions": [
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "36721285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.578125
        },
        {
            "corpus_id": "236478143",
            "title": "Entity-Aware Abstractive Multi-Document Summarization",
            "text": "Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020). \n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018). \n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020). Such relations were shown useful in identifying the salient and redundant information from long documents, and can thus guide the summary generation process. However, while effective empirically, such approaches do not focus on explicitly modeling the underlying semantic information across documents. \n\nEntities and their mentions convey rich semantic information, and can be significant in summarization, especially when a specific entity is the topic under discussion for a set of documents. As shown in Figure 1, entity mentions frequently appear in the input article, and are playing unique roles that contribute towards the coherence and conciseness of the text. We believe that entities can be regarded as the indicator of saliency and can be used to reduce redundancy. This motivates us to propose an entity-aware abstractive multi-document summarization model that effectively encodes relations across documents with the help of entities, and explicitly solve the issues of saliency and redundancy. \n\nInspired by Wang et al. (2020a), we build a heterogeneous graph that consists of nodes that represent documents and entities. The entity nodes can serve as bridges that connect different documents -we can model the relations across documents through entity clusters.",
            "score": 0.5814983055552646,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1976
                },
                {
                    "start": 1979,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 733,
                    "end": 756,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 928,
                    "end": 951,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 951,
                    "end": 967,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1991,
                    "end": 2010,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "248780330",
            "title": "Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature",
            "text": "The task of multi-document summarization aims to generate a compact and informative summary from a cluster of topic-related documents, which represents a very challenging natural language processing (NLP) application due to the presence of redundant and sometimes conflicting information among documents (Radev, 2000). In the medical domain, in which machine learning plays an increasingly significant role (Domeniconi et al., 2014a;di Lena et al., 2015), multi-document summarization finds application in the generation of systematic literature reviews, a biomedical paper that summarizes results across many studies (Khan et al., 2003). DeYoung et al. (2021) are the first that address this task, showing the related issues. \n\nState-of-the-art approaches leverage two leading solutions: hierarchical networks that capture crossdocument relations via graph encodings (Wan and Yang, 2006;Liao et al., 2018;Li et al., 2020;Pasunuru et al., 2021) or hidden states aggregation (Fabbri et al., 2019;Liu and Lapata, 2019a;Jin et al., 2020), and long-range neural models that apply multi-input concatenation (Xiao et al., 2021). While effective, these solutions struggle to process clusters of many topic-related documents in low computational resource scenarios (Moro and Ragazzi, 2022) because they need to truncate the inputs. Moreover, pre-trained state-of-the-art Transformers are not leveraged despite showing strong performance when fine-tuned in downstream tasks such as single-document summarization (Liu and Lapata, 2019b;Lewis et al., 2020a;Raffel et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). \n\nMulti-document summarization requires models to have more robust capabilities for analyzing the cluster to discriminate the correct information from noise and merge it consistently. In this work, we propose a discriminative marginalized probabilistic neural method (DAMEN) that selects worthy documents in the cluster with respect to a shared background and generates the summary via token probability marginalization.",
            "score": 0.5810641837361334,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 317,
                    "matchedPaperCorpusId": "10103200"
                },
                {
                    "start": 407,
                    "end": 433,
                    "matchedPaperCorpusId": "33320463"
                },
                {
                    "start": 433,
                    "end": 454,
                    "matchedPaperCorpusId": "6271496"
                },
                {
                    "start": 618,
                    "end": 637,
                    "matchedPaperCorpusId": "22459429"
                },
                {
                    "start": 639,
                    "end": 660,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 868,
                    "end": 888,
                    "matchedPaperCorpusId": "5457260"
                },
                {
                    "start": 888,
                    "end": 906,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 906,
                    "end": 922,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 922,
                    "end": 944,
                    "matchedPaperCorpusId": "235097309"
                },
                {
                    "start": 974,
                    "end": 995,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 995,
                    "end": 1017,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1017,
                    "end": 1034,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1257,
                    "end": 1281,
                    "matchedPaperCorpusId": "250304727"
                },
                {
                    "start": 1503,
                    "end": 1526,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1526,
                    "end": 1546,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1546,
                    "end": 1566,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1587,
                    "end": 1607,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "189209",
            "title": "Multi-Document Summarization using Sentence-based Topic Models",
            "text": "Most of the existing multi-document summarization methods decompose the documents into sentences and work directly in the sentence space using a term-sentence matrix. However, the knowledge on the document side, i.e. the topics embedded in the documents, can help the context understanding and guide the sentence selection in the summarization procedure. In this paper, we propose a new Bayesian sentence-based topic model for summarization by making use of both the term-document and term-sentence associations. An efficient variational Bayesian algorithm is derived for model parameter estimation. Experimental results on benchmark data sets show the effectiveness of the proposed model for the multi-document summarization task.",
            "score": 0.580786176032831,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "270000888",
            "title": "Advancing automatic text summarization: Unleashing enhanced binary multi-objective grey wolf optimization with mutation",
            "text": "\u2022 In the case of multi-document input, attention is given to the coherence of the text, ensuring a smooth flow of ideas for the reader.\n\n\u2022 The resulting summary must effectively represent the main idea, sparing the reader from the need to peruse the entire document(s).\n\nHistorically, researchers have explored various avenues to develop efficient ATS systems.The initial foray involved statistical-based methods, which, while straightforward to comprehend and implement, suffered from limited processing power.Graph-based approaches were subsequently introduced to enhance sentence relevance, yet faced challenges related to language independence and resource-intensive processing.Linguistic approaches yielded improved results compared to statistical methods but were hindered by a lack of suitable natural language processing tools [4].\n\nAddressing the aforementioned limitations, machine learning techniques emerged as a solution due to their capacity to automatically deduce rules.In the discussed techniques, traditional algorithms treated ATS generation as a single objective task.Recognizing the need for a robust and effective algorithm that approaches human-level summary development, our research advocates for considering multiple objectives simultaneously.\n\nOur proposed approach involves an extractive multi-document ATS with the dual objectives of maximizing coverage and minimizing redundancy.To achieve this, we introduce the BMOGWO-M, specifically designed for multi-objective (MO) ATS.Experiments were conducted using the widely utilized DUC 2002 dataset in text summarization tasks.The model's performance was evaluated using ROUGE-2 and ROUGE-L, with results compared against benchmark methods such as Adaptive DE, NSGA-II, and MOABC.\n\nThe significance of this research is that it eliminates the process of navigation through an immense volume of data by extracting specific information from lengthy documents or multiple sources; therefore, ensuring that the users invest their previous time in reading accurate and required materials.The under-considered text summarization system emerges as a crucial solution to distill the essence and central ideas from extensive documents.Manual condensation of lengthy documents is arduous and time-consuming, hence our research serves as the indispensable solution to address this issue.The primary significance of this research is to develop a concise summary that is grammatically accurate and encapsulates all the crucial data while maintaining the right flow of information.",
            "score": 0.5806997833077256,
            "section_title": "Introduction",
            "char_start_offset": 4415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 137,
                    "end": 269
                },
                {
                    "start": 271,
                    "end": 360
                },
                {
                    "start": 360,
                    "end": 511
                },
                {
                    "start": 511,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 839
                },
                {
                    "start": 841,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1269
                },
                {
                    "start": 1271,
                    "end": 1409
                },
                {
                    "start": 1409,
                    "end": 1504
                },
                {
                    "start": 1504,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1755
                },
                {
                    "start": 1757,
                    "end": 2057
                },
                {
                    "start": 2057,
                    "end": 2200
                },
                {
                    "start": 2200,
                    "end": 2350
                },
                {
                    "start": 2350,
                    "end": 2541
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58740234375
        },
        {
            "corpus_id": "2503061",
            "title": "Combining Optimal Clustering and Hidden Markov Models for Extractive Summarization",
            "text": "Multi-document summarization (MDS) is the summarization of a collection of related documents (Mani (1999)). Its application includes the summarization of a news story from different sources where document sources are related by the theme or topic of the story. Another application is the tracking of news stories from the single source over different time frame. In this case, documents are related by topic over time. \n\nMulti-document summarization is also an extension of single document summarization. One of the most robust and domain-independent summarization approaches is extraction-based or shallow summarization (Mani (1999)). In extraction-based summarization, salient sentences are automatically extracted to form a summary directly (Kupiec et. al, (1995), Myaeng & Jang (1999), Jing et. al, (2000), Nomoto & Matsumoto (2001,2002), Zha (2002), Osborne (2002)), or followed by a synthesis stage to generate a more natural summary (McKeown & Radev (1999), Hovy & Lin (1999)). Summarization therefore involves some theme or topic identification and then extraction of salient segments in a document. \n\nStory segmentation, document and sentence and classification can often be accomplished by unsupervised, clustering methods, with little or no requirement of human labeled data (Deerwester (1991), White & Cardie (2002), Jing et. al (2000)). \n\nUnsupervised methods or hybrids of supervised and unsupervised methods for extractive summarization have been found to yield promising results that are either comparable or superior to supervised methods (Nomoto & Matsumoto (2001,2002)). In these works, vector space models are used and document or sentence vectors are clustered together according to some similarity measure (Deerwester (1991), Dagan et al. (1997)). \n\nThe disadvantage of clustering methods lies in their ad hoc nature. Since sentence vectors are considered to be independent sample points, the sentence order information is lost.",
            "score": 0.5801392094277767,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1349
                },
                {
                    "start": 1352,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 105,
                    "matchedPaperCorpusId": "9177142"
                },
                {
                    "start": 621,
                    "end": 633,
                    "matchedPaperCorpusId": "9177142"
                },
                {
                    "start": 744,
                    "end": 766,
                    "matchedPaperCorpusId": "5775833"
                },
                {
                    "start": 811,
                    "end": 835,
                    "matchedPaperCorpusId": "6460414"
                },
                {
                    "start": 843,
                    "end": 853,
                    "matchedPaperCorpusId": "2036813"
                },
                {
                    "start": 855,
                    "end": 869,
                    "matchedPaperCorpusId": "7353825"
                },
                {
                    "start": 965,
                    "end": 982,
                    "matchedPaperCorpusId": "2521538"
                },
                {
                    "start": 1306,
                    "end": 1327,
                    "matchedPaperCorpusId": "2608602"
                },
                {
                    "start": 1556,
                    "end": 1581,
                    "matchedPaperCorpusId": "6460414"
                },
                {
                    "start": 1748,
                    "end": 1767,
                    "matchedPaperCorpusId": "2480472"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "53148260",
            "title": "Unsupervised Dual-Cascade Learning with Pseudo-Feedback Distillation for Query-based Extractive Summarization",
            "text": "We address the query-focused, multi-document summarization task. Formally, let q denote some user information need for documents summarization, which may be expressed by one or more queries. Let D denote a set of one or more matching documents to be summarized and L max be the maximum allowed summary length (in words). \n\nWe implement an extractive summarization approach. Our goal is to produce a length-limited summary S by extracting salient content parts in D which are further relevant (focused) to q. \n\nFollowing [6], we now cast the summarization task as a sentence subset selection problem. To this end, we produce summary S (with maximum length L max ) by choosing a subset of sentences s \u2208 D which maximizes a given quality target Q(S|q, D).",
            "score": 0.5799821750513667,
            "section_title": "Summarization task",
            "char_start_offset": 5610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 752
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "11730545"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.806640625
        },
        {
            "corpus_id": "219616694",
            "title": "Modern Multi-Document Text Summarization Techniques",
            "text": "The authors. [75] have introduced a novel idea for multidocument summarization viz. As it is becoming increasingly common to get access to large quantities of data on any topic that is of interest to you, it is also becoming more and more compulsory to have a summarization tool under your armour in order to understand and obtain the information that you require without getting overwhelmed by the immense display of all the data. There are many types of summarization. Extractive summarization effectively just extracts most of the most relevant content and produces the summary output. Abstractive summarization involves actually understanding the content and building new sentences to cover all the topics and then producing the summary output. This paper is proposing a method for multi-document extractive summarization.",
            "score": 0.5796940606098714,
            "section_title": "Introduction",
            "char_start_offset": 31943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 826
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "56481806"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "61134683",
            "title": "Explora\u00e7\u00e3o de m\u00e9todos de sumariza\u00e7\u00e3o autom\u00e1tica multidocumento com base em conhecimento sem\u00e2ntico-discursivo",
            "text": "The multi-document summarization aims at producing a summary from a set of related texts to be used for an individual or/and a particular task. Nowadays, with the exponential growth of available information and the people\u201fs need to obtain information in a short time, the task of automatic summarization has received wide attention. It is known that in a set of related texts there are pieces of redundant, contradictory and complementary information that represent the multi-document phenomenon. In each source text, the main subject is described in a sequence of subtopics. Furthermore, some sentences in the same text are more relevant than others. Considering this context, it is expected that a multi-document summary consists of relevant information that represents a set of texts. However, strategies for automatic multidocument summarization adopted until now have used only the relationships between texts and dismissed the analysis of textual structure of each source text, resulting in summaries that are less representative of subtopics and less informative than they could be. In order to properly treat the relevance of information, multi-document phenomena and distribution of subtopics, in this thesis, we investigated how to model the summarization process using the semantic-discursive knowledge and its impact for producing more informative and representative summaries from source texts. In order to formalize the semantic-discursive knowledge, we adopted RST (Rhetorical Structure Theory) and CST (Cross-document Structure Theory) theories. To support the work, a multi-document corpus was annotated with RST and subtopics, consisting of a new resource available for other researchers. From the corpus analysis, 10 methods for subtopic segmentation and 13 orignal methods for automatic summarization were proposed. The assessment of methods for subtopic segmentation showed that there is a strong relationship between the subtopics structure and the rhetorical analysis of a text. In regards to the assessment of the methods for automatic summarization, the results indicate that the use of semantic-discursive knowledge in good strategies for content selection affects positively the production of informative summaries.",
            "score": 0.5791401841546578,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "15926944",
            "title": "Automatic Multi Document Summarization Approaches",
            "text": "A number of research study have addressed multi document summarization in academia (Erkan and Radev, 2004a, Wan and Yang, 2008, Haribagiu and Lacatusu, 2010) and illustrated different types of approaches and available systems for multi document summarization. In this study we direct our focus notably on four well known approaches to multi document summarization. Our discussion will be based on the following pattern: For each method, we will first discuss its main idea. Following that, we will look at some research study from related literatures. Finally the benefits and limitations concerning each method are commented.",
            "score": 0.5783245809756478,
            "section_title": "Multi document summarization approaches:",
            "char_start_offset": 3245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 626
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 126,
                    "matchedPaperCorpusId": "9849366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "64353851",
            "title": "Empirical Analysis of Single and Multi Document Summarization using Clustering Algorithms",
            "text": "The extensive use of Internet has caused a vast growth in the usage of digital information. People use online information services, like social media, every day resulting to the availability of a huge amount of unstructured digital information. This information is directly accessible to a large number of end-users [1][2]. The user accesses this information through queries, but the improvement of precision and speed is always an issue. The information retrieval (IR) systems have resolved this to some extent. This information overload problem is more sensitive when there is a need of taking a decision or of deep understanding of a problem The IR systems solve this through user issued queries The obtained result most of the times overwhelms users with too many answers, and provided documents that may not be relevant to the topic asked. The multi document summarization has an ability to summarize a complete document set. Ideally it is a process of query shared information extraction through a set of multiple text documents. The techniques used in single-document summarization can also be used in multi-document summarization [3]. The comparison of single and multidocument summarization is presented in Table I. \n\nWeb information retrieval relevant to the issued query is a tedious task. Information retrieval tools can be used for retrieval relevant to the topic specified by the query. The results obtained sometimes may not preserve the required content. Summary generation or automatic text summarization is the creation of abstracts or summaries, with the help of a computer program, from one or more documents. There are specifically two types of text summarization techniques, generic and query specific [4]. It becomes a difficult task for the user to go through a large number of retrieved documents [5]. This difficulty can be resolved with the use of query specific document summary generation. The generated summary or abstract must preserve the semantics and central idea of an input text [6]. Below we present the main existing approaches to multi document summarization:",
            "score": 0.5780851928268763,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "33207284"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "5428641"
                },
                {
                    "start": 1138,
                    "end": 1141,
                    "matchedPaperCorpusId": "8294822"
                },
                {
                    "start": 1724,
                    "end": 1727,
                    "matchedPaperCorpusId": "15926944"
                },
                {
                    "start": 1822,
                    "end": 1825,
                    "matchedPaperCorpusId": "16534788"
                },
                {
                    "start": 2015,
                    "end": 2018,
                    "matchedPaperCorpusId": "12808608"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74853515625
        },
        {
            "corpus_id": "2791077",
            "title": "A new graph based text segmentation using Wikipedia for automatic text summarization",
            "text": "The technology of automatic document summarization is maturing and may provide a solution to the information overload problem. Nowadays, document summarization plays an important role in information retrieval. With a large volume of documents, presenting the user with a summary of each document greatly facilitates the task of finding the desired documents. Document summarization is a process of automatically creating a compressed version of a given document that provides useful information to users, and multi-document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic. According to the input text, in this paper we use the knowledge base of Wikipedia and the words of the main text to create independent graphs. We will then determine the important of graphs. Then we are specified importance of graph and sentences that have topics with high importance. Finally, we extract sentences with high importance. The experimental results on an open benchmark datasets from DUC01 and DUC02 show that our proposed approach can improve the performance compared to state-of-the-art summarization approaches.",
            "score": 0.5776791044253219,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57763671875
        },
        {
            "corpus_id": "269762702",
            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
            "text": "Identifying and proving the effectiveness of leveraging knowledge and knowledge graphs into document summarization tasks by empirical results is still a potential research direction in the document summarization field.\n\nRelatively, synthesizing a large quantity of topic-related documents would lead to content complementary, overlapping, and conflicting problems, bringing out model degradation and abridgment quality reduction [11].To improve the summarization performance for multiple input documents, a multi-document summarization model highly requires some model capabilities such as (1) analytically processing voluminous documents; (2) faithfully recognizing salient information; and (3) efficiently evaluating and integrating factually consistent snippets [16].Some recent key challenges of multi-document summarization are concentrated on topics about fact-awareness and summary logicality, including the lack of proper inter-document content-aware information, improper logical flow of information, and the need for external deep context representations [18].Therefore, it is still a worthy research direction for multi-document summarization to explore more precise ways with adequate information that can generate high-quality and comprehensive summaries across massive input documents while preserving the coherence, succinctness, and factual consistency of generated summaries.",
            "score": 0.5767007521165921,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 220,
                    "end": 434
                },
                {
                    "start": 434,
                    "end": 770
                },
                {
                    "start": 770,
                    "end": 1070
                },
                {
                    "start": 1070,
                    "end": 1392
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "214303164"
                },
                {
                    "start": 765,
                    "end": 769,
                    "matchedPaperCorpusId": "226289939"
                },
                {
                    "start": 1065,
                    "end": 1069,
                    "matchedPaperCorpusId": "235097309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "2791077",
            "title": "A new graph based text segmentation using Wikipedia for automatic text summarization",
            "text": "The technology of automatic document summarization is maturing and may provide a solution to the information overload problem. Nowadays, document summarization plays an important role in information retrieval (IR). With a large volume of documents, presenting the user with a summary of each document greatly facilitates the task of finding the desired documents. Text summarization is the process of automatically creating a compressed version of a given text that provides useful information to users, and multi-document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic [14]. Authors of the paper [10] provide the following definition for a summary: \"A summary can be loosely defined as a text that is produced from one or more texts that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually significantly less than that. Text here is used rather loosely and can refer to speech, multimedia documents, hypertext, etc. The main goal of a summary is to present the main ideas in a document in less space. If all sentences in a text document were of equal importance, producing a summary would not be very effective, as any reduction in the size of a document would carry a proportional decrease in its in formativeness. Luckily, information content in a document appears in bursts, and one can therefore distinguish between more and less informative segments. Identifying the informative segments at the expense of the rest is the main challenge in summarization\". Assume a tripartite processing model distinguishing three stages: source text interpretation to obtain a source representation, source representation transformation to summary representation, and summary text generation from the summary representation. A variety of document summarization methods have been developed recently. \n\nThe paper [4] reviews research on automatic summarizing over the last decade. This paper reviews salient notions and developments, and seeks to assess the state of-the-art for this challenging natural language processing (NLP) task. The review shows that some useful summarizing for various purposes can already be done but also, not surprisingly, that there is a huge amount more to do. Sentence based extractive summarization techniques are commonly used in automatic summarization to produce extractive summaries.",
            "score": 0.5764108790937219,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2197
                },
                {
                    "start": 2198,
                    "end": 2352
                },
                {
                    "start": 2353,
                    "end": 2481
                }
            ],
            "ref_mentions": [
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "36721285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "7096616",
            "title": "Topic-Focused Multi-Document Summarization Using an Approximate Oracle Score",
            "text": "We consider the problem of producing a multidocument summary given a collection of documents.\n\nMost automatic methods of multidocument summarization are largely extractive. This mimics the behavior of humans for single document summarization; (Kupiec, Pendersen, and Chen 1995) reported that 79% of the sentences in a human-generated abstract were a \"direct match\" to a sentence in a document. In contrast, for multi-document summarization, (Copeck and Szpakowicz 2004) report that no more than 55% of the vocabulary contained in human-generated abstracts can be found in the given documents. Furthermore, multiple human summaries on the same collection of documents often have little agreement. For example, (Hovy and Lin 2002) report that unigram overlap is around 40%. (Teufel and van Halteren 2004) used a \"factoid\" agreement analysis of human summaries for a single document and concluded that a resulting consensus summary is stable only if 30-40 summaries are collected.\n\nIn light of the strong evidence that nearly half of the terms in human-generated multi-document abstracts are not from the original documents, and that agreement of vocabulary among human abstracts is only about 40%, we pose two coupled questions about the quality of summaries that can be attained by document extraction:\n\n1. Given the sets of unigrams used by four human summarizers, can we produce an extract summary that is statistically indistinguishable from the human abstracts when measured by current automatic evaluation methods such as ROUGE?\n\n2. If such unigram information can produce good summaries, can we replace this information by a statistical model and still produce good summaries?\n\nWe will show that the answer to the first question is, indeed, yes and, in fact, the unigram set information gives rise to extract summaries that usually score better than the 4 human abstractors! Secondly, we give a method to statistically approximate the set of unigrams and find it produces extracts of the DUC 05 data which outperform all known evaluated machine entries. We conclude with experiments on the extent that redundancy removal improves extracts, as well as a method of moving beyond simple extracting by employing shallow parsing techniques to short",
            "score": 0.5750586098455229,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 709,
                    "end": 728,
                    "matchedPaperCorpusId": "40166767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "278000561",
            "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization",
            "text": "Multi-document summarization (MDS) is a task that aims to generate concise and coherent summaries by synthesizing information from multiple documents on the same topic [21,29,36,40,44]. MDS can lead to diverse applications, such as news aggregation [7,13,23], scientific research [11,35,59], and legal document analysis [17,38,55]. Current MDS approaches can be categorized into two classes: Graph-based models [9,28,45,47,65] and pre-trained language models [2,46,61]. Graph-based models rely on auxiliary information (e.g., discourse structures) as an input graph to capture the cross-document relationships, while pre-trained language models use the attention mechanisms to capture them. \n\nAll these summarization models employ transformer [58] as a text encoder and are consequently constrained by a fixed input length, limiting the number of tokens they can process. A common solution is to apply truncation, dropping the last tokens of input documents to fit within the context length. However, this naive approach risks discarding critical information for summarization [37,60]. Thus, how to retain sufficient critical information within the length limitation has become a crucial issue for MDS to enhance the quality of summaries. Current approaches often follow a \"retrieve-then-summarize\" paradigm to alleviate this issue [1,12,15,64], as shown in Figure 1 (A). They use manually created queries as guidance to rank documents or sentences from input documents or external knowledge bases and retrieve the top ranked contents to generate summaries. For instance, Light-PAL [12] and DYLE [41] use dataset-provided queries to retrieve passages and sentences, respectively, for summarization. \n\nAlthough these frameworks effectively retrieve documents from a large document collection, they face two major issues in MDS, as shown in Figure 1 (B): (i) These retrieval methods [15] often heavily depend on manually well-crafted queries to guide the ranking of passages or documents. For example, queries require precise human-written topic statements to ensure accurate ranking.",
            "score": 0.5748428056101271,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 172,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 175,
                    "end": 178,
                    "matchedPaperCorpusId": "235792259"
                },
                {
                    "start": 178,
                    "end": 181,
                    "matchedPaperCorpusId": "222090788"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "235352668"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "270371298"
                },
                {
                    "start": 252,
                    "end": 255,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 255,
                    "end": 258,
                    "matchedPaperCorpusId": "269756893"
                },
                {
                    "start": 280,
                    "end": 284,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "225075639"
                },
                {
                    "start": 287,
                    "end": 290,
                    "matchedPaperCorpusId": "269157041"
                },
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "271114508"
                },
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "249927023"
                },
                {
                    "start": 411,
                    "end": 414,
                    "matchedPaperCorpusId": "239050558"
                },
                {
                    "start": 414,
                    "end": 417,
                    "matchedPaperCorpusId": "257496469"
                },
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "235097309"
                },
                {
                    "start": 420,
                    "end": 423,
                    "matchedPaperCorpusId": "269762702"
                },
                {
                    "start": 423,
                    "end": 426,
                    "matchedPaperCorpusId": "258378312"
                },
                {
                    "start": 462,
                    "end": 465,
                    "matchedPaperCorpusId": "251224184"
                },
                {
                    "start": 465,
                    "end": 468,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 743,
                    "end": 747,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "226289939"
                },
                {
                    "start": 1081,
                    "end": 1084,
                    "matchedPaperCorpusId": "253098164"
                },
                {
                    "start": 1338,
                    "end": 1341,
                    "matchedPaperCorpusId": "258865156"
                },
                {
                    "start": 1341,
                    "end": 1344,
                    "matchedPaperCorpusId": "260332126"
                },
                {
                    "start": 1596,
                    "end": 1600,
                    "matchedPaperCorpusId": "239009689"
                },
                {
                    "start": 1881,
                    "end": 1885,
                    "matchedPaperCorpusId": "258865156"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "248496597",
            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
            "text": "Existing datasets on training or evaluating multi-document summarization systems such as Multi-News, Opinosis and DUC2004 have one major flaw -each document group contains texts about a shared topic. Whereas they still provide insights into the performance of many multi-document summarization systems, they fail to assess the case where a document group contains various stories. In order to evaluate the efficiency of our system at summarizing multiple diverse documents, we propose two variations of existing multi-document summarization datasets that are built on top of existing datasets: the CNN/Daily Mail and Newsroom datasets. \n\nThe CNN/Daily Mail dataset (Nallapati et al., 2016) contains 311,672 online news articlesummary pairs, where on average each article has 766 words, and each summary includes 53 words. The Newsroom dataset (Grusky et al., 2018) has in total 1,321,995 article-summary pairs, where on average each article contains 658.6 words and each summary is composed of 26.7 words. \n\nThe generation process is described as follows. Given a collection of documents D with n documents, we triple the number of documents and obtain a new collection D. The reason why we triple the amount of original documents is that we want to mimic real world cases where a large batch of documents may contain a few documents discussing similar topics. In our case, documents discussing similar topics are the potentially duplicated documents. Then, we generate m groups of documents by randomly drawing s documents from D at a time. Here s denotes the scale, i.e. how many documents each group has. Lastly, for each group, we generate the summary by concatenating the summary of each corresponding document. However, even though there can be duplicated documents in a group, we only keep one copy of the summary of such documents. This step is taken to ensure that summarization systems being evaluated can identify redundant information and avoid including such information twice in the summary. For each variation, CNN/Daily Mail and Newsroom, we randomly generate three batches of datasets with three different random seeds, and each batch has two different scales: s of 10 and 100. Therefore, there are in total 2 \u2022 3 \u2022 2 = 12 datasets.",
            "score": 0.574642563088585,
            "section_title": "Dataset Generation",
            "char_start_offset": 21298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 665,
                    "end": 689,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 843,
                    "end": 864,
                    "matchedPaperCorpusId": "13752552"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "257806309",
            "title": "Tractable & Coherent Multi-Document Summarization: Discrete Optimization of Multiple Neural Modeling Streams via Integer Linear Programming",
            "text": "One key challenge in multi-document summarization is the generated summary is often less coherent compared to single document summarization due to the larger heterogeneity of the input source content. In this work, we propose a generic framework to jointly consider coherence and informativeness in multi-document summarization and offers provisions to replace individual components based on the domain of source text. In particular, the framework characterizes coherence through verb transitions and entity mentions and takes advantage of syntactic parse trees and neural modeling for intra-sentential noise pruning. The framework cast the entire problem as an integer linear programming optimization problem with neural and non-neural models as linear components. We evaluate our method in the news and legal domains. The proposed approach consistently performs better than competitive baselines for both objective metrics and human evaluation.",
            "score": 0.5744817130677973,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "1320",
            "title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies",
            "text": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",
            "score": 0.5743949504933779,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "248780330",
            "title": "Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature",
            "text": "We describe related works on multi-document summarization categorized on model architectures. \n\nFlat solutions. Flat concatenation is a simple yet powerful solution because the generation of the multi-document summary is treated as a singledocument summarization task, thus it can leverage state-of-the-art pre-trained summarization models. \n\nConsequently, processing all documents as a flat input requires models capable of handling long sequences. As previously experimented by DeYoung et al. (2021), Xiao et al. (2021) proposed to leverage the Longformer-Encoder-Decoder model (Beltagy et al., 2020) pre-trained with a novel multi-document summarization specific task. They proved that a long-range Transformer that encodes all documents is a straightforward yet effective solution, and they achieved new state-of-the-art results in several multi-document summarization datasets. However, such models may struggle to handle a massive cluster of topic-related documents since they need to truncate them because of architectural limits. Further, processing all documents in a cluster could be noisy if some of them are not relevant or factual with respect to the summary. \n\nHierarchical solutions. To better preserve crossdocument relations and obtain semantic-rich representations, hierarchical concatenation solutions leverage graph-based techniques to work from word and sentence-level (Wan and Yang, 2006;Liao et al., 2018;Nayeem et al., 2018;Antognini and Faltings, 2019;Li et al., 2020) to documentlevel (Amplayo and Lapata, 2021). Other hierarchical approaches include multi-head pooling and inter-paragraph attention architectures (Liu and Lapata, 2019a), attention models with maximal marginal relevance (Fabbri et al., 2019), and attention across different granularity representations (Jin et al., 2020). Such models are often datasetspecific because of the custom architecture, so they struggle to adapt to other datasets and effectively leverage pre-trained state-of-the-art Transformers. \n\nOur solution.",
            "score": 0.5741903654961312,
            "section_title": "Related Work",
            "char_start_offset": 3656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 96,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2001
                },
                {
                    "start": 2004,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 501,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 1390,
                    "end": 1410,
                    "matchedPaperCorpusId": "5457260"
                },
                {
                    "start": 1410,
                    "end": 1428,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 1428,
                    "end": 1448,
                    "matchedPaperCorpusId": "52011473"
                },
                {
                    "start": 1477,
                    "end": 1493,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1640,
                    "end": 1663,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1714,
                    "end": 1735,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1796,
                    "end": 1814,
                    "matchedPaperCorpusId": "220045815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "13748058",
            "title": "Towards a Neural Network Approach to Abstractive Multi-Document Summarization",
            "text": "Document summarization is a task of automatically producing a summary for given documents. Different from Single Document Summarization (SDS) which generates a summary for each given document, Multi-Document Summarization (MDS) aims to generate a summary for a set of topic-related documents. Previous approaches to document summarization can be generally categorized to extractive methods and abstractive methods. Extractive methods produce a summary by extracting and merging sentences from the original document(s), while abstractive methods generate a summary using arbitrary words and expressions based on understanding the document(s). Due to the difficulty of natural language understanding and generation, previous research on document summarization is more focused on extrac-tive methods (Yao et al., 2017). However, extractive methods suffer from the inherent drawbacks of discourse incoherence and long, redundant sentences, which hampers its application in reality (Tan et al., 2017). Recently, with the success of sequence-to-sequence (seq2seq) models in natural language generation tasks including machine translation (Bahdanau et al., 2014) and dialog systems (Mou et al., 2016), abstractive summarization methods has received increasing attention. With the resource of large-scale corpus of human summaries, it is able to train an abstractive summarization model in an end-to-end framework. Neural abstractive summarization models (See et al., 2017;Tan et al., 2017) have surpass the performance of extractive methods on single document summarization task with abundant training data. \n\nUnfortunately, the extension of seq2seq models to MDS is not straightforward. Neural abstractive summarization models are usually trained on about hundreds of thousands of gold summaries, but there are usually very few human summaries available for the MDS task. More specifically, in the news domain, there is only a few hundred multi-document summaries provided by DUC and TAC conferences in total, which are largely insufficient for training neural abstractive models. Apart from insufficient training data, neural models for abstractive MDS also face the challenge of much more input content, and the study is still in the primary stage.",
            "score": 0.5740272533719633,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 815,
                    "matchedPaperCorpusId": "16506604"
                },
                {
                    "start": 977,
                    "end": 995,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 1175,
                    "end": 1193,
                    "matchedPaperCorpusId": "5165773"
                },
                {
                    "start": 1447,
                    "end": 1465,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1465,
                    "end": 1482,
                    "matchedPaperCorpusId": "26698484"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.873046875
        },
        {
            "corpus_id": "53223447",
            "title": "Adapting Neural Single-Document Summarization Model for Abstractive Multi-Document Summarization: A Pilot Study",
            "text": "Document summarization is a task of automatically producing a summary for given documents. Different from Single Document Summarization (SDS) which generates a summary for each given document, Multi-Document Summarization (MDS) aims to generate a summary for a set of topic-related documents. Previous approaches to document summarization can be generally categorized to extractive methods and abstractive methods. Extractive methods produce a summary by extracting and merging sentences from the original document(s), while abstractive methods generate a summary using arbitrary words and expressions based on understanding the document(s). Due to the difficulty of natural language understanding and generation, previous research on document summarization is more focused on extrac-tive methods (Yao et al., 2017). However, extractive methods suffer from the inherent drawbacks of discourse incoherence and long, redundant sentences, which hampers its application in reality (Tan et al., 2017). Recently, with the success of sequence-to-sequence (seq2seq) models in natural language generation tasks including machine translation (Bahdanau et al., 2014) and dialog systems (Mou et al., 2016), abstractive summarization methods has received increasing attention. With the resource of large-scale corpus of human summaries, it is able to train an abstractive summarization model in an end-to-end framework. Neural abstractive summarization models (See et al., 2017;Tan et al., 2017) have surpass the performance of extractive methods on single document summarization task with abundant training data. \n\nUnfortunately, the extension of seq2seq models to MDS is not straightforward. Neural abstractive summarization models are usually trained on about hundreds of thousands of gold summaries, but there are usually very few human summaries available for the MDS task. More specifically, in the news domain, there is only a few hundred multi-document summaries provided by DUC and TAC conferences in total, which are largely insufficient for training neural abstractive models. Apart from insufficient training data, neural models for abstractive MDS also face the challenge of much more input content, and the study is still in the primary stage.",
            "score": 0.5739944165347894,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 815,
                    "matchedPaperCorpusId": "16506604"
                },
                {
                    "start": 977,
                    "end": 995,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 1132,
                    "end": 1155,
                    "matchedPaperCorpusId": "8599292"
                },
                {
                    "start": 1175,
                    "end": 1193,
                    "matchedPaperCorpusId": "5165773"
                },
                {
                    "start": 1447,
                    "end": 1465,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1465,
                    "end": 1482,
                    "matchedPaperCorpusId": "26698484"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87548828125
        },
        {
            "corpus_id": "16149757",
            "title": "Document Fusion for Comprehensive Event Description",
            "text": "On the contrary, we are not aiming for the shortest description containing the most relevant information, but for the shortest description containing all information. For instance, even historic background information is included, as long as it allows the reader to get a more comprehensive description of an event. \n\nAlthough the techniques that are used for multi-document fusion and multi-document summarization are similar, the task of fusion is complementary to the summarization task. They differ in the way that, roughly speaking, multidocument summarization is the intersection of information within a topic, whereas multidocument fusion is the union of information. They are similar to the extent that in both cases nearly equivalent information stemming from different documents within the topic has to be identified as such. \n\nThe remainder of this paper is structured as follows: Section 2 introduces the main components and challenges of implementing a document fusion system. Issues of evaluating document fusion and some preliminary evaluation of our system are presented in Section 3. In Section 4, some conclusions and prospects on future work are given.",
            "score": 0.5730946925346425,
            "section_title": "Introduction",
            "char_start_offset": 2375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1171
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4931640625
        },
        {
            "corpus_id": "257364970",
            "title": "Mining Both Commonality and Specificity From Multiple Documents for Multi-Document Summarization",
            "text": "Automatic text summarization is becoming much more important because of the exponential growth of digital textual information on the web. Multidocument summarization, which aims to generate a short text containing important and diverse information of original multiple documents, is a challenging focus of NLP research. A well-organized summary of multiple documents needs to cover the main information of all documents comprehensively and simultaneously satisfy content diversity. Extractive summarization approaches, which generate a summary by selecting a few important sentences from original documents, attract much attention because of its simplicity and robustness. This paper focuses on extractive multi-document summarization. \n\nMost extractive multi-document summarization approaches splice all the sentences contained in the original documents into a larger text, and then generate a summary by selecting sentences from the larger text (Lamsiyah et al., 2021;Yang et al., 2014;Erkan and Radev, 2004). However, the task of summarizing multiple documents is more difficult than the task of summarizing a single document. Simply transforming multi-document summarization task into summarizing a single larger text completely breaks the constraints of documents on their sentences and lacks comparisons between documents, which results in the inability to mine the relevant information between documents, including mining the common information (commonality) of all documents and the important specific information (specificity) of some subclasses of documents. \n\nThe centroid-based summarization approaches focus on the commonality of all documents or all sentences and they select sentences based on the centroid words of all documents (Radev et al., 2004;Rossiello et al., 2017) or the centroid embedding of all sentences (Lamsiyah et al., 2021). The clustering-based summarization approaches divide sentences into multiple groups and select sentences from each group (Yang et al., 2014;Sarkar, 2009). These approaches do not take into account the commonality and specificity of documents simultaneously. \n\nThink about the process of human summarizing multiple documents: we would first describe the common information of all documents and then the important specific information of some subclasses of these documents respectively to satisfy the coverage and diversity requirements of multi-document summarization.",
            "score": 0.572888769790699,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2114
                },
                {
                    "start": 2117,
                    "end": 2424
                }
            ],
            "ref_mentions": [
                {
                    "start": 947,
                    "end": 970,
                    "matchedPaperCorpusId": "228954621"
                },
                {
                    "start": 970,
                    "end": 988,
                    "matchedPaperCorpusId": "5792920"
                },
                {
                    "start": 988,
                    "end": 1010,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1765,
                    "end": 1788,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 1832,
                    "end": 1855,
                    "matchedPaperCorpusId": "228954621"
                },
                {
                    "start": 1978,
                    "end": 1997,
                    "matchedPaperCorpusId": "5792920"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "220633461",
            "title": "SummPip: Unsupervised Multi-Document Summarization with Sentence Graph Compression",
            "text": "Obtaining training data for multi-document Summarization (MDS) is time consuming and resource-intensive, so recent neural models can only be trained for limited domains. In this paper, we propose SummPip: an unsupervised method for multi-document summarization, in which we convert the original documents to a sentence graph, taking both linguistic and deep representation into account, then apply spectral clustering to obtain multiple clusters of sentences, and finally compress each cluster to generate the final summary. Experiments on Multi-News and DUC-2004 datasets show that our method is competitive to previous unsupervised methods and is even comparable to the neural supervised approaches. In addition, human evaluation shows our system produces consistent and complete summaries compared to human written ones.",
            "score": 0.5725536745211107,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "238221709",
            "title": "Extractive Multi-Document Summarization: A Review of Progress in the Last Decade",
            "text": "For instance, the same contents may be covered from multiple sources, so at times, a number of documents may be available to gain an insight into the same event [5]. In this regard, a multi-document summary becomes a representation of the information contained in a cluster of documents which helps users understand the gist of those documents [9], [10]. A multi-document summary represents the information contained in the cluster of documents and helps users understand those documents [11]. \n\nThe task of multi-document summarization is much more complex than single-document summarization, even when the available single document is very large-sized. This difficulty is attributed to the inevitable diversity of themes within a large set of documents. \n\nA summary can be Abstractive or Extractive, depending on the method of summarization. Generally, an abstractive summary consists of concepts and ideas abstracted from the source document(s) and then represented in preferably different words. This method involves a thorough understanding of the meaning of the content. Two broad areas of Natural Language Processing (NLP) [12] that handle abstractive summary are semantic representation and natural language generation [4]. These involve various approaches, such as multimodal semantic models, information item-based methods, and semantic graph-based methods [13]. \n\nAn extractive summary is described as units of text extracted from the source document(s) and combined as a summary verbatim [14]. In this method, the important sentences of the documents under consideration are ranked and combined to form the summary [15]. Extractive summarization techniques can be divided into various categories, such as: query-based or generic and supervised or unsupervised methods. Generic summarization is based on preparing a summary of the main topic of the documents, whereas query-based summarization involves generating a summary related to the subject of the query asked by the user [9], [10], [12], [16]- [21]. \n\nIn order to gain a broader picture of research in this field, we have performed a systematic survey of the literature on extractive techniques of MDS. The survey may serve as a starting point for prospected researchers to identify gaps in current research.",
            "score": 0.5724585970695696,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2276
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "45592507"
                },
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "596849"
                },
                {
                    "start": 349,
                    "end": 353,
                    "matchedPaperCorpusId": "38686738"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "4490918"
                },
                {
                    "start": 1130,
                    "end": 1134,
                    "matchedPaperCorpusId": "33913820"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "36306586"
                },
                {
                    "start": 1367,
                    "end": 1371,
                    "matchedPaperCorpusId": "63740251"
                },
                {
                    "start": 1500,
                    "end": 1504,
                    "matchedPaperCorpusId": "32634196"
                },
                {
                    "start": 1627,
                    "end": 1631,
                    "matchedPaperCorpusId": "10353431"
                },
                {
                    "start": 1989,
                    "end": 1992,
                    "matchedPaperCorpusId": "596849"
                },
                {
                    "start": 1994,
                    "end": 1998,
                    "matchedPaperCorpusId": "38686738"
                },
                {
                    "start": 2000,
                    "end": 2004,
                    "matchedPaperCorpusId": "33913820"
                },
                {
                    "start": 2006,
                    "end": 2010,
                    "matchedPaperCorpusId": "3780782"
                },
                {
                    "start": 2012,
                    "end": 2016,
                    "matchedPaperCorpusId": "46073204"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.904296875
        },
        {
            "corpus_id": "189209",
            "title": "Multi-Document Summarization using Sentence-based Topic Models",
            "text": "With the continuing growth of online text resources, document summarization has found wide-ranging applications in information retrieval and web search. Many multi-document summarization methods have been developed to extract the most important sentences from the documents. These methods usually represent the documents as term-sentence matrices (where each row represents a sentence and each column represents a term) or graphs (where each node is a sentence and each edge represents the pairwise relationship among corresponding sentences), and ranks the sentences according to their scores calculated by a set of predefined features, such as term frequencyinverse sentence frequency (TF-ISF) (Radev et al., 2004;Lin and Hovy, 2002), sentence or term position (Yih et al., 2007), and number of key-words (Yih et al., 2007). Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al., 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al., 2007), and LSA based methods (Gong and Liu, 2001). \n\nThere are two limitations with most of the existing multi-document summarization methods: (1) They work directly in the sentence space and many methods treat the sentences as independent of each other. Although few work tries to analyze the context or sequence information of the sentences, the document side knowledge, i.e. the topics embedded in the documents are ignored. (2) Another limitation is that the sentence scores calculated from existing methods usually do not have very clear and rigorous probabilistic interpretations. Many if not all of the sentence scores are computed using various heuristics as few research efforts have been reported on using generative models for document summarization. \n\nIn this paper, to address the above issues, we propose a new Bayesian sentence-based topic model for multi-document summarization by making use of both the term-document and termsentence associations.",
            "score": 0.5723713565543418,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1916
                },
                {
                    "start": 1919,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 716,
                    "end": 735,
                    "matchedPaperCorpusId": "8448956"
                },
                {
                    "start": 763,
                    "end": 781,
                    "matchedPaperCorpusId": "7176707"
                },
                {
                    "start": 807,
                    "end": 825,
                    "matchedPaperCorpusId": "7176707"
                },
                {
                    "start": 1141,
                    "end": 1160,
                    "matchedPaperCorpusId": "126818"
                },
                {
                    "start": 1184,
                    "end": 1204,
                    "matchedPaperCorpusId": "7003631"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "263610015",
            "title": "Entropy-based Sampling for Abstractive Multi-document Summarization in Low-resource Settings",
            "text": "In light of the ever-growing volume of available information, it becomes essential to be able to automatically summarize information from several sources. Multi-document Summarization (MDS) aims at condensing the most important information from different documents. Despite the advances in single-document summarization (Zhang et al., 2020), summarizing multiple related documents remains a greater challenge due to its input length and the presence of redundant information (Fan et al., 2019;Song et al., 2022). Therefore, some research focuses on implementing multi-stage approaches that first identify the relevant information to then feed it into a summarization model (Lebanoff et al., 2018;Liu and Lapata, 2019a). More recent works utilize pre-trained language models (Lewis et al., 2020;Raffel et al., 2020;Xiao et al., 2022) finetuned for the summarization task and feed them with the source documents concatenated (Johner et al., 2021;Xiao et al., 2022). However, these approaches pose two major issues. First, concatenated inputs exceeding the length limit of the model are truncated, which might lead to the omission of entire documents. Second, they rely on multidocument datasets that are scarce or unavailable in languages other than English. Hokamp et al. (2020) introduce a decoding strategy that adapts single-to multi-document summarization without using additional training data nor applying changes to the single-input model architecture. At every decoding timestep, it averages the output probabilities of a single-document summarization model for each individual document, combining them into a single output. Instead of averaging all log-probabilities, which favours highly frequent tokens, we propose to make a more informed decision. In particular, we leverage entropy to measure the model confidence in the next token prediction and thus select the most informative output. We implement different entropy-based approaches and evaluate their performance on MDS of German text. Our main contributions are: \n\n\u2022 We present different entropy-based sampling approaches for the MDS task. These are specially well-suited for languages like German that have limited or unavailable MDS data.",
            "score": 0.5719751260199148,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2029
                },
                {
                    "start": 2032,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 340,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 475,
                    "end": 493,
                    "matchedPaperCorpusId": "202785778"
                },
                {
                    "start": 493,
                    "end": 511,
                    "matchedPaperCorpusId": "248512466"
                },
                {
                    "start": 673,
                    "end": 696,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 696,
                    "end": 718,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 774,
                    "end": 794,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 794,
                    "end": 814,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 814,
                    "end": 832,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 923,
                    "end": 944,
                    "matchedPaperCorpusId": "235258298"
                },
                {
                    "start": 944,
                    "end": 962,
                    "matchedPaperCorpusId": "247519084"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9130859375
        },
        {
            "corpus_id": "271525553",
            "title": "Abstractive text summarization: State of the art, challenges, and improvements",
            "text": "Although much research focuses on single-document summarization, multi-document summarization presents unique challenges [181,182]. Unlike its single-document counterpart, multi-document summarization involves synthesizing information from multiple sources, often necessitating the alignment of documents, identification, and resolution of redundancies, contradictions, and varying perspectives. These complexities introduce unique challenges such as ensuring coherence in the face of diverse inputs and maintaining a balanced representation of all source documents. The enormous amount of information that needs to be processed during multi-document summarization is one of the main challenges. The amount of data increases rapidly with many documents, causing computational difficulties and extending processing times [183]. The possibility of conflicting information across documents presents another challenge. Finding the most precise or relevant information can be troublesome, particularly if the source text comprises different authors or viewpoints [39]. Furthermore, the temporal part of the information can present difficulties. For instance, while summing up news articles, recent data may be more pertinent than older information, expecting models to have a sense of temporality [184]. Large Language Models (LLMs) can play a crucial role. Their ability to process large volumes of text and understand complex linguistic patterns makes them well-suited for tackling the challenges of multi-document summarization [185]. However, their application also introduces new dimensions to these challenges. For example, the computational resources required to process multiple lengthy documents using LLMs are significant, and the risk of perpetuating biases present in training data is heightened due to the models' extensive scope. \n\nTo address these issues, integrating knowledge graphs and structured knowledge representations has arisen as a promising strategy [186]. Knowledge graphs, with their interconnected nodes and connections, provide an organized system that can assist models, including LLMs, in understanding the connections between various text documents, recognizing key subjects, and generating summaries that capture the essence of the entire text document set [187]. Essentially, structured knowledge representations offer a deliberate method for coordinating and processing multi-document content, ensuring that the resulting summaries are comprehensive and well-structured.",
            "score": 0.5713134900671631,
            "section_title": "Multi-Document Summarization",
            "char_start_offset": 109325,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2292
                },
                {
                    "start": 2293,
                    "end": 2501
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 126,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 126,
                    "end": 130,
                    "matchedPaperCorpusId": "233948337"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1292,
                    "end": 1297,
                    "matchedPaperCorpusId": "532313"
                },
                {
                    "start": 1526,
                    "end": 1531,
                    "matchedPaperCorpusId": "253828067"
                },
                {
                    "start": 2286,
                    "end": 2291,
                    "matchedPaperCorpusId": "259360395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "261660414",
            "title": "Unsupervised Multi-document Summarization with Holistic Inference",
            "text": "The multi-document summarization (MDS) is one of the essential tools to obtain core information from a collection of documents written for the same topic. It seeks to find the main ideas from multiple sources with diversified messages. In spite of recent advances in MDS system designs (Mihalcea and Tarau, 2004;Liu and Lapata, 2019a;Xiao et al., 2022), three major challenges hinder its development: \n\nFirst, existing extractive multi-document summarization systems rely on optimization with individual scoring. It becomes sub-optimal when we need to extract multiple summary sentences (Zhong et al., 2020). A typical individual system scores each candidate summary with only measurements of the newly added sentences during inference. In contrast, the holistic system simultaneously measures all summary sentences and the relations among them. Despite recent efforts in holistic methods on a single document (An et al., 2022;Zhong et al., 2020), how to extract sentences holistically for multi-document summarization remains open. In this work, we propose an inference method that holistically optimizes the extractive summary under multi-document setting. \n\nSecond, multi-document summarization naturally contains excessively redundant information (Lebanoff et al., 2018). An ideal summary should provide important information with diversified perspectives (Nenkova and McKeown, 2011). \n\nIn Figure 1, we show a salient and diversified summary versus a salient but redundant summary. A salient and diversified summary often covers the information thoroughly, while the salient but redundant summary is usually incomplete. Different from existing approaches (Suzuki and Nagata, 2017;Cho et al., 2019b;Xiao and Carenini, 2020) for limiting the repetitions, we introduce Subset Representative Index (SRI), a holistically balanced measurement between importance and diversity for extractive multi-document summarization. \n\nFinally, recent deep learning-based supervised summarization methods are data-driven and require a massive number of high-quality summaries in the training data. Nevertheless, hiring humans to write summaries is always expensive, time-consuming, and thus hard to scale up. This problem becomes more severe for multi-document summarization, since it requires more effort to read more documents.",
            "score": 0.5706404444089255,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1918
                },
                {
                    "start": 1921,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2314
                }
            ],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 312,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 312,
                    "end": 334,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 334,
                    "end": 352,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 910,
                    "end": 927,
                    "matchedPaperCorpusId": "252596098"
                },
                {
                    "start": 1659,
                    "end": 1684,
                    "matchedPaperCorpusId": "1207251"
                },
                {
                    "start": 1684,
                    "end": 1702,
                    "matchedPaperCorpusId": "204901346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "225075639",
            "title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
            "text": "The lack of large-scale dataset has slowed the progress of multi-document summarization (MDS) research. We introduce Multi-XScience, a large-scale dataset for MDS using scientific articles. Multi-XScience is better suited to abstractive summarization than previous MDS datasets, since it requires summarization models to exhibit high text understanding and abstraction capabilities. Experimental results show that our dataset is amenable to abstractive summarization models and is challenging for current models.",
            "score": 0.5698769183630928,
            "section_title": "Conclusion",
            "char_start_offset": 16886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 512
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73779296875
        },
        {
            "corpus_id": "266374529",
            "title": "Survey on Multi-Document Summarization: Systematic Literature Review",
            "text": "In figure 10, the main challenges that occur in multi-document summarization are discussed: Redundancy, Performance, Quality of content selection, Minimize error rate, Efficiency, slow coverage, Readability and Human written summaries Papers use in this survey handle all these challenges and gave better results. Some papers reduce the redundancy from the summary, some papers improve the quality of content selection, some papers minimize the error rate and some generate a consistent and complete summary compared to human-written ones. In \n\nIn table 8, discuss the research gaps.",
            "score": 0.5691751991765569,
            "section_title": "Figure 10 Analysis of problems addressed by scheme listed in literature",
            "char_start_offset": 40891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 583
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "1260503",
            "title": "A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization",
            "text": "The explosion of the Internet clearly warrants the development of techniques for organizing and presenting information to users in an effective way. Query-focused multi-document summarization (MDS) methods have been proposed as one such technique and have attracted significant attention in recent years. The goal of query-focused MDS is to synthesize a brief (often fixed-length) and well-organized summary from a set of topicrelated documents that answer a complex question or address a topic statement. The resulting summaries, in turn, can support a number of information analysis applications including openended question answering, recommender systems, and summarization of search engine results. As further evidence of its importance, the Document Understanding Conference (DUC) has used queryfocused MDS as its main task since 2004 to foster new research on automatic summarization in the context of users' needs. \n\nTo date, most top-performing systems for multi-document summarization-whether queryspecific or not-remain largely extractive: their summaries are comprised exclusively of sentences selected directly from the documents to be summarized (Erkan and Radev, 2004;Haghighi and Vanderwende, 2009;Celikyilmaz and Hakkani-T\u00fcr, 2011). Despite their simplicity, extractive approaches have some disadvantages. First, lengthy sentences that are partly relevant are either excluded from the summary or (if selected) can block the selection of other important sentences, due to summary length constraints. In addition, when people write summaries, they tend to abstract the content and seldom use entire sentences taken verbatim from the original documents. In news articles, for example, most sentences are lengthy and contain both potentially useful information for a summary as well as unnecessary details that are better omitted. Consider the following DUC query as input for a MDS system: 1 \"In what ways have stolen artworks been recovered? How often are suspects arrested or prosecuted for the thefts?\" One manually generated summary includes the following sentence but removes the bracketed words in gray: In this example, the compressed sentence is rela-1 From DUC 2005, query for topic d422g. arXiv:1606.07548v1",
            "score": 0.568242061356395,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2211
                },
                {
                    "start": 2212,
                    "end": 2230
                }
            ],
            "ref_mentions": [
                {
                    "start": 1159,
                    "end": 1182,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "266244733",
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "text": "In multi-document summarization, the interrelationships between document-level nodes play an indispensable role for the model's ability to discern core topics and prioritize content across multiple documents. Yet, many extant models neglect this hierarchical organization, opting instead for a flat, sequential arrangement of multiple documents (Liu et al., 2018). Alternative approaches attempt to model these document-level relationships through attention-based, fully-connected graphs, or by leveraging similarity and statement relationships (Liu & Lapata, 2019a). \n\nTo establish document-level relationships analogously to sentence-level counterparts, the model can be effortlessly adapted for multi-document summarization by incorporating super-nodes (as shown in Fig. 4) for each document. Specifically, the heterogeneous graph is augmented with four distinct types of nodes: \n\nAs illustrated in Fig. 4, word nodes serve as the intermediary between sentence and document nodes, with relationships being established based on content similarity. Topic nodes directly link with sentence nodes according to the document's importance. \n\nA document node, which is essentially a specialized form of a sentence node but with extended content, associates with its corresponding word node, utilizing the TF-IDF value as the edge weight. The updating procedure for document nodes parallels that of sentence nodes, with the key distinction being in initialization; the document node is initialized via mean-pooling of the sentence node features. During the sentence node selection process in multi-document mode, the representation of each sentence node is concatenated with the document representation, leading to the final score post multi-document aggregation.",
            "score": 0.5681199416272094,
            "section_title": "Multi-document summary",
            "char_start_offset": 20848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1135
                },
                {
                    "start": 1138,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1757
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "239050558",
            "title": "Topic-Guided Abstractive Multi-Document Summarization",
            "text": "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics.",
            "score": 0.56798364750425,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "52053741",
            "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization",
            "text": "Neural abstractive summarization has primarily focused on summarizing short texts written by single authors.For example, sentence summarization seeks to reduce the first sentence of a news article to a title-like summary (Rush et al., 2015;Nallapati et al., 2016;Takase et al., 2016;Song et al., 2018); single-document summarization (SDS) focuses on condensing a news article to a handful of bullet points (Paulus et al., 2017;See et al., 2017).These summarization studies are empowered by large parallel datasets automatically harvested from online news outlets, including Gigaword (Rush et al., 2015), CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008), and Newsroom (Grusky et al., 2018).\n\nTo date, multi-document summarization (MDS) has not yet fully benefited from the development\n\nGigaword the first sentence 8.3 words 4 Million (Rush et al., 2015) of a news article title-like CNN/Daily Mail a news article 56 words 312 K (Hermann et al., 2015) multi-sent TAC (08-11) 10 news articles 100 words 728 (Dang et al., 2008) related to a topic multi-sent DUC (03-04) 10 news articles 100 words 320 (Over and Yen, 2004) related to a topic multi-sent\n\nTable 1: A comparison of datasets available for sent.summarization (Gigaword), single-doc (CNN/DM) and multi-doc summarization (DUC/TAC).The labelled data for multi-doc summarization are much less.\n\nof neural encoder-decoder models.MDS seeks to condense a set of documents likely written by multiple authors to a short and informative summary.It has practical applications, such as summarizing product reviews (Gerani et al., 2014), student responses to post-class questionnaires (Luo and Litman, 2015;Luo et al., 2016), and sets of news articles discussing certain topics (Hong et al., 2014).",
            "score": 0.5679043055989869,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 700
                },
                {
                    "start": 702,
                    "end": 794
                },
                {
                    "start": 796,
                    "end": 1158
                },
                {
                    "start": 1160,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1297
                },
                {
                    "start": 1297,
                    "end": 1357
                },
                {
                    "start": 1359,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1753
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 263,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 263,
                    "end": 283,
                    "matchedPaperCorpusId": "5450302"
                },
                {
                    "start": 283,
                    "end": 301,
                    "matchedPaperCorpusId": "46936631"
                },
                {
                    "start": 406,
                    "end": 427,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 427,
                    "end": 444,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 619,
                    "end": 641,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 678,
                    "end": 699,
                    "matchedPaperCorpusId": "13752552"
                },
                {
                    "start": 938,
                    "end": 960,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 1015,
                    "end": 1034,
                    "matchedPaperCorpusId": "26768540"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53466796875
        },
        {
            "corpus_id": "242549720",
            "title": "Exploiting Semantic Term Relations in Text Summarization",
            "text": "The traditional frequency based approach to creating multi-document extractive summary ranks sentences based on scores computed by summing up TF*IDF weights of words contained in the sentences. In this approach, TF or term frequency is calculated based on how frequently a term (word) occurs in the input and TF calculated in this way does not take into account the semantic relations among terms. In this paper, we propose methods that exploits semantic term relations for improving sentence ranking and redundancy removal steps of a summarization system. Our proposed summarization system has been tested on DUC 2003 and DUC 2004 benchmark multi-document summarization datasets. The experimental results reveal that performance of our multi-document text summarizer is significantly improved when the distributional term similarity measure is used for finding semantic term relations. Our multi-document text summarizer also outperforms some well known summarization baselines to which it is compared.",
            "score": 0.5655926175431025,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75927734375
        },
        {
            "corpus_id": "220936463",
            "title": "Experiments in Extractive Summarization: Integer Linear Programming, Term/Sentence Scoring, and Title-driven Models",
            "text": "Manual summarization of text documents is both expensive and time consuming (Lin, 2004). Thus development of automated systems for text summarization is a topic of considerable interest among the research community. Research on automated summarizers began in the late 50s (Luhn, 1958), with a system that uses term frequencies for assigning weights to sentences to build a summary. \n\nSummarization research has come a long way since then with automated summarizers aiming to generate the best possible summary. Summaries can be classified into two groups -(a) Abstractive: These are generally created by using lexically similar words and phrases to report the important information in the original article. Such a summary avoids using sentences verbatim from the source. It could also use other vehicles such as word graphs for displaying the important content. (b) Extractive: Such a summary is a collection of sentences from the source document purportedly containing the most relevant information. \n\nSummaries can be single or multi-document based on the number of documents used as the source material. With the DUC summarization conference, extractive multi-document summarization gained more popularity over single document summarization right after 2001-2002. A reason for this sudden drop was the absence of a system that could beat the lead-based baseline summaries (Svore et al., 2007) for news articles. Thus, finding innovative techniques for single document summarization has a set of challenges different from that of multi-document summarization (Barrera and Verma, 2012). \n\nOur research returns to unsupervised single document summarization. We focus on unsupervised summarization, since: (i) it does not need annotated datasets, and (ii) it can be orders-of-magnitude faster as neither training nor hyper-parameter optimization phases are necessary. We investigate several new aspects of this topic and make multiple contributions: \n\n1. We parameterize the most promising word scoring methods to improve ranking of words and also sentence score normalization (Section 3.1). By parameterizing the scoring methods, we show that the optimal scores are achieved when parameter values are rarely equal to 1, which is how they are often used in previous literature (Section 5).",
            "score": 0.5651920951846394,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1948
                },
                {
                    "start": 1951,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 87,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 272,
                    "end": 284,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 1561,
                    "end": 1586,
                    "matchedPaperCorpusId": "18529031"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.578125
        },
        {
            "corpus_id": "3011904",
            "title": "A survey for Multi-Document Summarization",
            "text": "Automatic Multi-Document summarization is still hard to realize. Like single document summarization for newspaper articles, where we don't have a notably better automatic summarization algorithm than a simple lead based method, automatic multi-document summarization faces very difficult challenges. Under such circumstances, we believe, it is important to observe how humans are doing on the same task, and look for possible different strategies. \n\nAssume you are given several documents talking about the same topic, and are asked to summarize them, what might you do. The authors tried this by themselves. First we used a marker to mark the important phrases or sentences. Then we tried to connect them, in some cases by figuring out the main or common topics in the marked sentences, or in some cases, by making a list or a table to figure out the overview of the documents. When we looked at the result at this stage we noticed that these are very good summaries, even if they are not summaries in the conventional sense (a set of sentences to be read). The main topics are good to understand the overall issues in the document set and the table is a good digest of the issues throughout the document set. If we can automatically create such data from document sets, we might be able to make a good summary. The questions arising here are what kinds of \"main topic\" we can make in general, and what percentage of document sets are suitable for table-style summarization. \n\nThe main topics we created in our hand summary experiment were like lists of keywords, but we found that there are more general types like \"these documents are talking about a single person\". As keyword extraction has been one of the techniques in summarization, we will focus on the types of the main topics in the following experiments. \n\nWe will describe the definition of our types and report on the experiment of manually creating table-style summarization, as well as analyses of free style summaries and sentence extraction type summaries. We prepared 100 document sets similar to the ones used in the DUC multi-document summarization task (DUC homepage). For each document set, annotators prepared the following data.",
            "score": 0.5642637474857634,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 447
                },
                {
                    "start": 450,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1816
                },
                {
                    "start": 1819,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2203
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "231733241",
            "title": "A Framework for Generating Extractive Summary from Multiple Malayalam Documents",
            "text": "Nowadays, the amount of data on the web is growing exponentially on any topic. The volume of data circulating in the digital space, generally the unstructured textual data, demands building automated text summarization tools to get insights from them quickly. Document summaries provide users the briefing of the most notable information contained in the document. Automatic document summarization is one of the most challenging and exciting issues in Natural Language Processing (NLP). \n\nThe automatic text summarization system has attracted substantial interest in providing relevant information in less time [1]. Text summarization is a process used to generate a simplified version of the original document. There are several types of summarization methods. Considering the whole or particular part of a text, summarization is categorized into generic and query relevant summarization. A generic summary presents an overall sense of the document's content, while a query-focused summary shows the document's content related to the user query [2,3]. \n\nBased on the number of source documents, there is single document summarization (SDS) and multi-document summarization (MDS). A single document produces a summary that is obtained from one source document where the content is sourced around a single topic [4]. At the same time, the multi-document summarization is taken from various sources or documents that discuss the same topic [5][6][7]. However, the task of summarizing multiple documents is more complicated than the job of summarizing a single document. The challenges associated with summarizing multiple documents are redundancy and cohesion. Initially, most of the research was conducted on single-document summarization. Multi-document summarization studies started by extending the single document techniques to more than one textual documents. \n\nBased on the approach followed for generating the summary, text summarization can be extractive or abstractive. An extractive summary is generated by concatenating meaningful sentences extracted from the document to be summarized. On the other hand, an abstractive summary conveys the primary information from the documents [8]. Abstractive summarization requires extensive natural language processing. Therefore, it is more complicated than extractive summarization.",
            "score": 0.5640912526153706,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1863
                },
                {
                    "start": 1866,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 1049,
                    "end": 1051,
                    "matchedPaperCorpusId": "36267198"
                },
                {
                    "start": 1311,
                    "end": 1314,
                    "matchedPaperCorpusId": "12666673"
                },
                {
                    "start": 1438,
                    "end": 1441,
                    "matchedPaperCorpusId": "3780782"
                },
                {
                    "start": 1441,
                    "end": 1444,
                    "matchedPaperCorpusId": "36089827"
                },
                {
                    "start": 1444,
                    "end": 1447,
                    "matchedPaperCorpusId": "13741028"
                },
                {
                    "start": 2190,
                    "end": 2193,
                    "matchedPaperCorpusId": "28442272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "9906673",
            "title": "MDSWriter: Annotation Tool for Creating High-Quality Multi-Document Summarization Corpora",
            "text": "We introduced MDSWriter, a tool for constructing multi-document summaries. Our software fills an important gap as high-quality summarization corpora are urgently needed to train and evaluate automatic summarization systems. Previously available tools are not well-suited for this task, as they do not support cross-document annotations, the modeling of complex tasks with a number of distinct steps, and reusing the tools under free licenses. As a key property of our tool, we store all intermediate annotation results and record the user-system interaction data. We argued that this enables next-generation summarization methods by learning from human summarization strategies and evaluating individual components of a system. \n\nIn future work, we plan to create and evaluate an actual corpus for multi-document summarization using our tool. We also plan to provide monitoring components in MDSWriter, such as computing inter-annotator agreement in real-time.",
            "score": 0.5634473830209927,
            "section_title": "Conclusion and future work",
            "char_start_offset": 16940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 960
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.888671875
        },
        {
            "corpus_id": "272969413",
            "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications",
            "text": "In a multinational corporation, a team of analysts faces the daunting task of summarizing thousands of documents spanning financial reports, market analyses, and internal communications to inform a critical strategic decision. This scenario illustrates the challenge of multi-document summarization in enterprise settings, where the volume and diversity of information can overwhelm traditional analysis methods. \n\nThe exponential growth of unstructured text data across various sectors has made document summarization a critical task [1]. Multi-document summarization presents unique challenges due to the need for synthesizing information from diverse sources, which may contain redundant, complementary, or contradictory information across documents [4]. Variations in writing style and level of detail add complexity to the task. Determining the relevance and importance of information from each source is crucial for creating a coherent and comprehensive summary [5]. \n\nTraditional document summarization techniques often struggle with redundancy, inconsistency, lack of context understanding, scalability issues for multiple document summarization tasks, inability to capture cross-document relationships, difficulty handling diverse formats, and lack of domain adaptability [6,7,8]. These limitations highlight the need for more advanced approaches to multi-document summarization. \n\nThis paper addresses the following research question: How can long-context Large Language Models (LLMs) be leveraged to improve multi-document understanding and summarization in enterprise applications? \n\nWe investigate the use of Long-context LLMs for multi-document summarization, demonstrating their exceptional capacity to grasp extensive connections, provide cohesive summaries, and adapt to various industry domains and integration with enterprise applications/systems [14]. The paper discusses the workflow of multi-document summarization for effectively adopting long-context LLMs, supported by case studies in legal applications, enterprise functions such as HR, finance, and sourcing, as well as in the medical and news domains [46,49,53]. \n\nBy exploring the potential of Long-context LLMs in multi-document summarization, we aim to address the limitations of traditional methods and provide a more efficient and accurate approach to processing large volumes of unstructured data [13,18]. This research has significant implications for improving information processing across diverse sectors and enterprise applications.",
            "score": 0.5632227759639081,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 972
                },
                {
                    "start": 975,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2140
                },
                {
                    "start": 2143,
                    "end": 2389
                },
                {
                    "start": 2390,
                    "end": 2521
                }
            ],
            "ref_mentions": [
                {
                    "start": 753,
                    "end": 756,
                    "matchedPaperCorpusId": "8377315"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "337730"
                },
                {
                    "start": 1281,
                    "end": 1284,
                    "matchedPaperCorpusId": "1296465"
                },
                {
                    "start": 1284,
                    "end": 1286,
                    "matchedPaperCorpusId": "269225"
                },
                {
                    "start": 1286,
                    "end": 1288,
                    "matchedPaperCorpusId": "19198109"
                },
                {
                    "start": 2136,
                    "end": 2139,
                    "matchedPaperCorpusId": "55461757"
                },
                {
                    "start": 2381,
                    "end": 2385,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 2385,
                    "end": 2388,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "6776899",
            "title": "AllSummarizer system at MultiLing 2015: Multilingual single and multi-document summarization",
            "text": "Besides our system (AllSummarizer), there are 4 systems that participated with all the 10 languages. Table 4 shows a comparison between our system and the other systems in multi-document task, using the relative improvement. We used the parameters fixed for single document summarization to see if the same parameters are applicable for both single and multi-document summarizations. \n\nLooking to the results, our system took the seventh place out of ten participants. When we use single document parameters, we can see that it doesn't outperform the results when using the parameters fixed for multi-document summarization. This shows that we can't use the same parameters for both single and multi-document summarization.",
            "score": 0.5630545051278515,
            "section_title": "Multi-document summarization",
            "char_start_offset": 16331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 723
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "152281",
            "title": "Learning Document-Level Semantic Properties from Free-Text Annotations",
            "text": "This paper also relates to a large body of work in multi-document summarization. Researchers have long noted that a central challenge of multi-document summarization is identifying redundant information over input documents (Radev & McKeown, 1998;Carbonell & Goldstein, 1998;Mani & Bloedorn, 1997;Barzilay, McKeown, & Elhadad, 1999). This task is of crucial significance because multi-document summarizers operate over related documents that describe the same facts multiple times. In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999;Radev, Jing, & Budzikowska, 2000;Nenkova, Vanderwende, & McKeown, 2006). Many of these algorithms first cluster sentences together, and then extract or generate sentence representatives for the clusters. \n\nIdentification of repeated information is equally central in our approach -our multi-document summarization method only selects properties that are stated by a plurality of users, thereby eliminating rare and/or erroneous opinions. The key difference between our algorithm and existing summarization systems is the method for identifying repeated expressions of a single semantic property. Since most of the existing work on multi-document summarization focuses on topic-independent newspaper articles, redundancy is identified via sentence comparison. For instance, Radev et al. (2000) compare sentences using cosine similarity between corresponding word vectors. Alternatively, some methods compare sentences via alignment of their syntactic trees (Barzilay et al., 1999;Marsi & Krahmer, 2005). Both string-and tree-based comparison algorithms are augmented with lexico-semantic knowledge using resources such as WordNet. \n\nThe approach described in this paper does not perform comparisons at the sentence level. Instead, we first abstract reviews into a set of properties and then compare property overlap across different documents. This approach relates to domain-dependent approaches for text summarization (Radev & McKeown, 1998;White, Korelsky, Cardie, Ng, Pierce, & Wagstaff, 2001;Elhadad & McKeown, 2001). These methods identify the relations between documents by comparing their abstract representations.",
            "score": 0.5630080966348785,
            "section_title": "Multidocument Summarization",
            "char_start_offset": 17535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 247,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 247,
                    "end": 275,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 275,
                    "end": 297,
                    "matchedPaperCorpusId": "6025826"
                },
                {
                    "start": 624,
                    "end": 657,
                    "matchedPaperCorpusId": "1320"
                },
                {
                    "start": 657,
                    "end": 695,
                    "matchedPaperCorpusId": "86903"
                },
                {
                    "start": 1397,
                    "end": 1416,
                    "matchedPaperCorpusId": "1320"
                },
                {
                    "start": 1603,
                    "end": 1625,
                    "matchedPaperCorpusId": "2293515"
                },
                {
                    "start": 2043,
                    "end": 2066,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 2066,
                    "end": 2120,
                    "matchedPaperCorpusId": "1496402"
                },
                {
                    "start": 2120,
                    "end": 2144,
                    "matchedPaperCorpusId": "15641201"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "250420837",
            "title": "Knowledge-aware document summarization: A survey of knowledge, embedding methods and architectures",
            "text": "With the exponential burst of textual data, demands in condensing voluminous text contents have been ubiquitous, bringing document summarization one of the most immensely researched fields in Natural Language Processing (NLP).Document Summarization (DS) aims to generate an abridged version of single or multiple topic-related texts as concise and coherent as possible while preserving the salient and factually consistent information [1].The document summarization task with a single input document is known as the Single Document Summarization (SDS).By contrast, the Multi-Document Summarization (MDS) task emphasizes synthesizing a large number of topic-related documents to generate a compressed summary from various times and perspectives.In addition, there are two general methods in document summarization: 1) the Extractive Document Summarization (EDS) method respects the lexicon of the original text, regarding the summary formation is verbatim by key words and phrases selected from the source corpus; and 2) the Abstractive Document Summarization (ADS) method respects the semantics of the original text, regarding the summary construction is by rephrasing texts according to the comprehension of text substances.Generally, a document summarization model is to achieve the following goals [2]:\n\n\u2022 G1.Coverage: A document summarization model aims to generate a comprehensive summary that covers all the main and noteworthy contents of the input text(s);\n\n\u2022 G2.Non-redundancy: A document summarization model aims to generate a precise and concise summary without any redundant or meaninglessly repeated information;\n\n\u2022 G3.Readability: A document summarization model aims to generate a smooth and logical summary composed by human-readable and coherent sentences to the viewer; For multi-document summarization models, an additional goal is [3]:\n\n\u2022 G4.Relevancy: A multi-document summarization model aims to identify related information within multiple input texts while generating the summary.\n\nRecently, knowledge utilization in the summarization models has exhibited a huge potential for promoting the summarizer performance in terms of G1 to G4 and fueled one more document summarization capacity target:\n\n\u2022 G5.",
            "score": 0.5624466565447138,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 744
                },
                {
                    "start": 744,
                    "end": 1225
                },
                {
                    "start": 1225,
                    "end": 1305
                },
                {
                    "start": 1307,
                    "end": 1312
                },
                {
                    "start": 1312,
                    "end": 1464
                },
                {
                    "start": 1466,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1625
                },
                {
                    "start": 1627,
                    "end": 1632
                },
                {
                    "start": 1632,
                    "end": 1854
                },
                {
                    "start": 1856,
                    "end": 1861
                },
                {
                    "start": 1861,
                    "end": 2003
                },
                {
                    "start": 2005,
                    "end": 2217
                },
                {
                    "start": 2219,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "226289939"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "224955327"
                },
                {
                    "start": 1850,
                    "end": 1853,
                    "matchedPaperCorpusId": "14650402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "239050558",
            "title": "Topic-Guided Abstractive Multi-Document Summarization",
            "text": "Multi-document summarization is a challenging subtask of text summarization with a long history. Many previous methods are extractive partly due to the lack of sufficient training data. These methods usually compute sentence salience over graph structures (Mihalcea and Tarau, 2004;Wang et al., 2020a). Abstractive MDS studies have been fueled by the recent development of large-scale datasets (Fabbri et al., 2019) and representation learning of NLP (Vaswani et al., 2017). Among them, hierarchical networks (Liu and Lapata, 2019) and graph neural networks (Jin et al., 2020) are widely used to capture the cross-document relations. However, most of them build interaction based on word-or paragraph-level representations, which are not flexible or straightforward. In comparison, we propose to model multiple documents more effectively by mining their subtopics.\n\nDatasets for multi-document summarization Recently, Fabbri et al. (2019) released the first largescale news dataset for MDS. Each article is collected from real-life scenarios and the golden summaries are written by human, which ensures the data quality. Prior to them, some studies tried to construct dataset in automatic manners. For example, Liu et al. (2018) and Liu and Lapata (2019) built datasets based on Wikipedia pages, regarding the first section as the summary and others as different documents. However, modeling the relations among different documents is a different task from modeling that of paragraphs from a same document. Therefore, the generalization ability of models built on such data could be questionable. For this reason, we do not consider such auto-constructed datasets but focus on the Multi-News dataset curated by human.\n\nTopic modeling for text summarization Topic model is widely used for document modeling. Nevertheless, few studies have applied it in summarization task. Previous studies regarded topical distributions as additional features to enrich word or sentence representations (Wei, 2012;Narayan et al., 2018;Wang et al., 2020b). However, these methods use a pipeline process where topic extraction and summary generation are separately performed. In comparison, we adopt a multi-task learning strategy so that the two tasks can learn complementary features from each",
            "score": 0.5621037782850636,
            "section_title": "Related Work",
            "char_start_offset": 4882,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 282,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 282,
                    "end": 301,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 394,
                    "end": 415,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 451,
                    "end": 473,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 509,
                    "end": 531,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 558,
                    "end": 576,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 918,
                    "end": 938,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1211,
                    "end": 1228,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1233,
                    "end": 1254,
                    "matchedPaperCorpusId": "170079112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "248571519",
            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
            "text": "Abstractive multi document summarization is the task of writing a single summary of the key points and content in multiple related documents. This task has evolved from research in single document abstractive and extractive summarization; however, it faces unique challenges due to input documents having duplicate and conflicting content as well a larger body of text. (Radev et al., 2000). This task has evolved from early approaches using sequence to sequence (Seq2Seq) neural architectures to transformer based architectures with the introduction of large-scale datasets (Liu et al., 2018), (Fabbri et al., 2019). Beyond the introduction of approaches now commonly used for single document abstractive summarization, cross document attention and graphs that capture relations between text in various documents have further improved the state of the art for multi document summarization tasks. (Liu et al., 2018), (Li et al., 2014). These graphs aim to better represent the inter dependencies between articles by representing text spans as nodes in the graph and capturing the relations between these sentences as edge weights. \n\nDespite the advances made with these approaches, a significant challenge remains in multi document summarization with respect to how to deal with contradictory information present in the multiple source documents. It is critical to both learn the relationships between different documents as well as to extract salient information that is consistent with the output viewpoint. This is a situation often faced with summarizing multiple news articles where different viewpoints on an issue can significantly change the semantic structure of the content present in each article making it challenging for the abstractive summarization model to learn the relationships between inconsistent or conflicting information. In this work we define conflicting opinions as a combined measure of the polarity and sentiment of text. By this definition, two pieces of text on the same topic that have a differing sentiment and/ or polarity are determined to have different viewpoints. This definition is used throughout the paper. \n\nThis paper proposes ACM, attribute conditioned multi document summarization, a novel approach to multi document summarization that incorporates an attribute conditioning module with an abstractive multi document summarization model in order to condition for a particular attribute when generating the multi document summary.",
            "score": 0.5617418573685862,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2147
                },
                {
                    "start": 2150,
                    "end": 2474
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 390,
                    "matchedPaperCorpusId": "1320"
                },
                {
                    "start": 917,
                    "end": 934,
                    "matchedPaperCorpusId": "10112929"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "274964986",
            "title": "Multi-LLM Text Summarization",
            "text": "In this work, we propose a novel multi-LLM summarization framework that leverages multiple large language models to enhance summarization quality of long document input. Through the distribution of generation and evaluation of candidate summaries across multiple models, our framework aims to provide better summaries than single LLM methods, leveraging expertise from different models. We present two interaction topologies, centralized and decentralized, to guide the collaboration, evaluation, and refinement of summaries between LLMs. Visually these two methods can be represented at a high level in Figure 1. In the datasets we test, articles are typically tens of thousands of words long and exceed the context window of most standard LLMs. To handle this, we establish a two stage process that involves chunking the source document, independently summarizing each chunk of the source document, and then applying a second round of chunking and summarization on the concatenated intermediate results. Throughout both these stages, both frameworks allow multiple LLMs to collaborate and converge on a single final high quality summary of the entire original reference document. Table 1 provides an overview of our framework's four main variations.",
            "score": 0.5613321644161383,
            "section_title": "Multi-LLM Summarization Framework",
            "char_start_offset": 5156,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1251
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "258378312",
            "title": "Enhancing Multi-Document Summarization with Cross-Document Graph-based Information Extraction",
            "text": "Our problem definition follows the typical formulation of abstractive multi-document summarization (MDS). Specifically, given a cluster of input documents D = {D 1 , D 2 , \u00b7 \u00b7 \u00b7 , D N }, we aim to build a model to generate a summary S of the document cluster. In this paper, we particularly focus on using IE to enhance summarization using the IE graph G merged from the individual graphs {G 1 , G 2 , \u00b7 \u00b7 \u00b7 , G N } extracted from N documents.",
            "score": 0.5612131518173039,
            "section_title": "Problem Formulation",
            "char_start_offset": 7758,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85595703125
        },
        {
            "corpus_id": "247593888",
            "title": "Read Top News First: A Document Reordering Approach for Multi-Document News Summarization",
            "text": "Multi-document news extractive summarization (MDS) aims to extract the salient information from multiple related news documents into a concise summary. Some approaches use task-specific architectures for this problem. For example, Wang et al. (2020) organize multiple documents as a heterogeneous graph before summarizing them. Zhong et al. (2020) formulate the extractive summarization task as a semantic matching problem. Recent works also explored reformulating this problem as a single-document summarization (SDS) problem by concatenating all documents into a single meta-document and then using an SDS model to summarize it (Cao et al., 2017;Lebanoff et al., 2018;Fabbri et al., 2019).\n\nDue to the conventions of news writing (Hong and Nenkova, 2014;Hicks et al., 2016), salient information often appears at the beginning of a news article. As a result, many summarization systems, including recent neural models (Kedzie et al., 2018;Zhong et al., 2019), pay more attention to the beginning of the document. Therefore, in MDS, it is important to consider the order in which the docu- * Equal Contribution ments are concatenated to form the meta-document before applying the summarization model.\n\nSpecifically, we argue that the various documents in the input are not equally important. Some documents contain more salient or detailed information and are more important. Therefore, compared with concatenating documents in an arbitrary order, it would be beneficial to reorder the documents such that the important ones are in the front of the meta-document and it becomes easier for the summarization model to learn the salient content.\n\nMotivated by these factors, we propose a simple yet effective approach to reorder the input documents according to their relative importance before applying a summarization model. We evaluate the effectiveness of our approach on Multi-News (Fabbri et al., 2019) and DUC-2004. 1 Results show that our simple reordering approach significantly outperforms the state-of-the-art methods with more complex model architectures. We also observe that this approach brings more performance gain with the increase in the number of input documents",
            "score": 0.5596348045065476,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 249,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 630,
                    "end": 648,
                    "matchedPaperCorpusId": "14651945"
                },
                {
                    "start": 648,
                    "end": 670,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 670,
                    "end": 690,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 732,
                    "end": 756,
                    "matchedPaperCorpusId": "2342155"
                },
                {
                    "start": 919,
                    "end": 940,
                    "matchedPaperCorpusId": "53083054"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "8294822",
            "title": "Multi-Document Summarization By Sentence Extraction",
            "text": "Conclusions and Future Work \n\nThis paper presented a statistical method of generating extraction based multi-document summaries. It builds upon previous work in single-document summarization and takes into account some of the major differences between single-document and multi-document summarization: (i) the need to carefully eliminate redundant information from multiple documents, and achieve high compression ratios, (ii) take into account information about document and passage similarities, and weight different passages accordingly, and (iii) take temporal information into account. \n\nOur approach differs from others in several ways: it is completely domain-independent, is based mainly on fast, statistical processing, it attempts to maximize the novelty of the information being selected, and different genres or corpora characteristics can be taken into account easily. Since our system is not based on the use of sophisticated natural language understanding or information extraction techniques, summaries lack co-reference resolution, passages may be disjoint from one another, and in some cases may have false implicature. \n\nIn future work, we will integrate work on multidocument summarization with work on clustering to provide summaries for clusters produced by topic detection and tracking. We also plan to investigate how to generate coherent temporally based event summaries. We will also investigate how users can effectively use multidocument summarization through interactive interfaces to browse and explore large document sets.",
            "score": 0.5593343958724772,
            "section_title": "7",
            "char_start_offset": 25267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 30,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1553
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "1299971",
            "title": "Summarizing Encyclopedic Term Descriptions on the Web",
            "text": "Given a set of paragraph-style descriptions for a single term in a specific domain (e.g., descriptions for \"hub\" in the computer domain), our summarization method produces a concise text describing the term from different viewpoints. These descriptions are obtained by the organization module in Figure 2. Thus, the related term extraction module is independent of our summarization method. \n\nOur method is multi-document summarization (MDS) (Mani, 2001). Because a set of input documents (in our case, the paragraphs for a single term) were written by different authors and/or different time, the redundancy and divergence of the topics in the input are greater than that for single document summarization. Thus, the recognition of similarity and difference among multiple contents is crucial. \n\nThe following two questions have to be answered: \n\n\u2022 by which language unit (e.g., words, phrases, or sentences) should two contents be compared? \n\n\u2022 by which criterion should two contents be regarded as \"similar\" or \"different\"? \n\nThe answers for these questions can be different depending on the application and the type of input documents. \n\nOur purpose is to include as many viewpoints as possible in a concise description. Thus, we compare two contents on a viewpoint-by-viewpoint basis. In addition, if two contents are associated with the same viewpoint, we determine that those contents are similar and that they should not be repeated in the summary. \n\nOur viewpoint-based summarization (VBS) method consists of the following four steps: \n\n1. identification, which recognizes the language unit associated with a viewpoint, 2. classification, which merges the identified units associated with the same viewpoint into a single group, 3. selection, which determines one or more representative units for each group, 4. presentation, which produces a summary in a specific format. \n\nThe model is similar to those in existing MDS methods. However, the implementation of each step varies depending on the application. We elaborate on the four steps in Sections 3.2-3.5, respectively.",
            "score": 0.5593089430988546,
            "section_title": "Overview",
            "char_start_offset": 5610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2082
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72314453125
        },
        {
            "corpus_id": "170079112",
            "title": "Hierarchical Transformers for Multi-Document Summarization",
            "text": "Previous solutions include model transfer (Zhang et al., 2018;Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives (Ma et al., 2016;Chu and Liu, 2018). Liu et al. (2018) propose a methodology for constructing large-scale summarization datasets and a two-stage model which first extracts salient information from source documents and then uses a decoder-only architecture (that can attend to very long sequences) to generate the summary. We follow their setup in viewing multi-document summarization as a supervised machine learning prob- lem and for this purpose assume access to large, labeled datasets (i.e., source documents-summary pairs). In contrast to their approach, we use a learning-based ranker and our abstractive model can hierarchically encode the input documents, with the ability to learn latent relations across documents and additionally incorporate information encoded in well-known graph representations.",
            "score": 0.559223419855666,
            "section_title": "Related Work",
            "char_start_offset": 6846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1093
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 62,
                    "matchedPaperCorpusId": "53223447"
                },
                {
                    "start": 62,
                    "end": 85,
                    "matchedPaperCorpusId": "52051402"
                },
                {
                    "start": 284,
                    "end": 301,
                    "matchedPaperCorpusId": "5845797"
                },
                {
                    "start": 321,
                    "end": 338,
                    "matchedPaperCorpusId": "39871772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "273849874",
            "title": "Summarization of Opinionated Political Documents with Varied Perspectives",
            "text": "Thus, the dataset enables a multi-document, multi-summary task. As with traditional news summarization tasks, the summary for each perspective should condense the information present in the passages. At the same time, the generated left and right-perspective summaries should accurately represent the perspectives reflected in their respective passages.",
            "score": 0.5584364303347085,
            "section_title": "POLISUM Dataset",
            "char_start_offset": 4116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 353
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39794921875
        },
        {
            "corpus_id": "16149757",
            "title": "Document Fusion for Comprehensive Event Description",
            "text": "Conventional text retrieval systems respond to a user's query by providing a (ranked) list of documents which potentially satisfy the information need. After having identified a number of documents which are actually relevant, the user reads some of those documents to get the information requested. To be sure to get a comprehensive account of a particular topic, the list of documents one has to read may be rather long, including a severe amount of redundancy; i.e., documents partially conveying the same information. \n\nAlthough this problems basically holds for any text retrieval situation, where comprehensiveness is relevant, it becomes particularly evident in the retrieval of news texts. \n\nNews agencies, such as AP, BBC, CNN, or Reuters, often describe the same event differently. For instance, they provide different background information, helping the reader to situate the story, they interview different people to comment on an event, and they provide additional, conflicting or more accurate information, depending on their sources. \n\nTo get a description of an event which is as comprehensive as possible and also as short as possible, a user has to compile his or her own description by taking parts of the original news stories, ignoring duplicate information. Typical users include journalists and intelligence analysts, for whom compiling and fusing information is an integral part of their work (Carbonell et al., 2000). Obviously, if done manually, this process can be rather laborious as it involves numerous comparisons, depending on the number and length of the documents. \n\nThe aim of this paper is to describe an approach automatizing this process by fusing information stemming from different documents to generate a single comprehensive document, containing the information of all original documents without repeating information which is conveyed by two or more documents. \n\nThe work described in this paper is closely related to the area of multi-document summarization (Barzilay et al., 1999;Mani and Bloedorn, 1999;McKeown and Radev, 1995;Radev, 2000), where related documents are analyzed to use frequently occurring segments for identifying relevant information that has to be included in the summary. Our work differs from the work on multi-document summarization as we focus on document fusion disregarding summarization. On the contrary, we are not aiming for the shortest description containing the most relevant information, but for the shortest description containing all information.",
            "score": 0.5580219149529994,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2359
                },
                {
                    "start": 2360,
                    "end": 2526
                }
            ],
            "ref_mentions": [
                {
                    "start": 2002,
                    "end": 2025,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 2025,
                    "end": 2049,
                    "matchedPaperCorpusId": "9177142"
                },
                {
                    "start": 2049,
                    "end": 2073,
                    "matchedPaperCorpusId": "2446679"
                },
                {
                    "start": 2073,
                    "end": 2085,
                    "matchedPaperCorpusId": "10103200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "202785778",
            "title": "Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs",
            "text": "Previous work in multi-document summarization (Barzilay et al., 1999) applies various techniques to handle long input, including clustering to find similar information (Honarpisheh et al., 2008), extractive methods to select relevant sentences (Daum\u00e9 III and Marcu, 2002;Gillick and Favre, 2009;Berg-Kirkpatrick et al., 2011;Di Fabbrizio et al., 2014;Bing et al., 2015;Cao et al., 2017) including maximal marginal relevance (Fabbri et al., 2019), and incorporating queries (Baumel et al., 2018) and graphs (Ganesan et al., 2010;Yasunaga et al., 2017). However, there are few large scale multi-document summarization datasets and many approaches have focused on extractive selection or hybrid extractive-abstractive models. In this work, we use graph construction to re-structure multidocument input for abstractive generation.\n\nAdvancements in question answering have examined performance on datasets with multidocument input, such as TriviaQA (Joshi et al., 2017). Various approaches have been proposed, including leveraging TF-IDF and bigram hashing with an RNN to find relevant information (Chen et al., 2017), models that score individual paragraphs for sub-selection (Clark and Gardner, 2017), and nearest neighbor search with paragraph re-ranking (Das et al., 2018a). However, these approaches have been applied to extractive question answering tasks that require span identification, rather than abstractive text generation in an information synthesis setting.",
            "score": 0.5578973897475901,
            "section_title": "Multi-Document Input",
            "char_start_offset": 2786,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 69,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 168,
                    "end": 194,
                    "matchedPaperCorpusId": "15955042"
                },
                {
                    "start": 244,
                    "end": 271,
                    "matchedPaperCorpusId": "189898"
                },
                {
                    "start": 271,
                    "end": 295,
                    "matchedPaperCorpusId": "167874"
                },
                {
                    "start": 295,
                    "end": 325,
                    "matchedPaperCorpusId": "15467396"
                },
                {
                    "start": 325,
                    "end": 351,
                    "matchedPaperCorpusId": "12682781"
                },
                {
                    "start": 369,
                    "end": 386,
                    "matchedPaperCorpusId": "14651945"
                },
                {
                    "start": 506,
                    "end": 528,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 944,
                    "end": 964,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 1093,
                    "end": 1112,
                    "matchedPaperCorpusId": "3618568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "10496513",
            "title": "Beyond SumBasic: Task-focused summarization with sentence simplification and lexical expansion",
            "text": "primarily interested in maximizing the content of our system's summaries, which, due to many issues of semantic realization such as paraphrase, cannot always be captured by measuring bigram overlap with four model summaries, we also evaluate our results with human evaluation metrics. Human evaluation is time-consuming and must be conducted carefully, in the same experimental setting, in order to ensure comparison across systems. We participated in DUC2006 in order to gain access to a human evaluation of our system, specifically the NIST content responsiveness score. In addition, a second human evaluation is coordinated by Columbia University, the Pyramid method . The Pyramid method requires two steps: first, a set of semantic equivalence classes are built for the sets of model summaries, with higher scores assigned to content represented in multiple model summaries, and second, a person identifies the content units in a peer summary that are found in the set of semantic equivalence classes. 3 The scores reported measure the amount of content overlap between the peer summary and the four model summaries.\n\nIn this paper, we describe the multi-document summarization system we submitted to DUC2006. Our contribution in DUC2006, identified as System 10, builds on an earlier system, SumBasic (Nenkova & Vanderwende, 2005) which produces generic multi-document summaries; we provide a description of SumBasic in Section 2. We then describe each of the remaining three main components that comprise our system: a task-focused extractive summarization system, sentence simplification, and lexical expansion of topic words. We will provide experiments using automatic metrics designed to quantify the contributions of each component. Human evaluation metrics, discussed in Section 6.2, indicate that this is a relatively successful approach to multi-document summarization; in the Pyramid evaluation, our system ranked first out of 22 systems and in the NIST metrics for content responsiveness, our system ranked third out of 35 systems.\n\nWith regard to our system design, it must be noted that this system, similar to almost all multi-document summarization systems, produces summaries by selecting sentences from the document set, either verbatim or with some simplification. Using sentence simplification is a step towards generating new summary text, rather than extracting",
            "score": 0.5576844680947342,
            "section_title": "Introduction",
            "char_start_offset": 1856,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "18711007",
            "title": "Multi-Document Summarization via Discriminative Summary Reranking",
            "text": "Given a set of documents about a topic, multi-document summarization systems aim to produce a short and fluent summary to deliver the salient information in the document set. Most existing summarization systems are based on sentence extraction, and they rely on a specific method to rank some kinds of units (e.g. words, bigrams, or sentences) and then extract summary sentences according to the ranking results. With the development of the DUC and TAC benchmark tests, document summarization has been well studied and many different summarization methods have been proposed, e.g., centroid-based method (Radev et al. 2004), graph-based ranking methods (Erkan and Radev 2004) and ILP-based methods (McDonald 2007;Gillick et al. 2008). \n\nIn an existing document summarization system, a single summarization model (i.e., a summarization method with a specific parameter setting) is used for extracting summaries from different document sets. For example, there are 50 different document sets in the DUC2004 dataset, and a summarization system usually adopts a single summarization model (e.g. an ILP-based method with a specific parameter setting) to extract summaries for all the 50 document sets. The common assumption is that a single summarization model can well deal with all the different document sets. However, according to our quantitative data analysis, none of the existing summarization models can always produce high-quality summaries for different document sets, and even a summarization model with good overall performance may produce low-quality summaries for some document sets. On the contrary, a baseline summarization model may produce high-quality summaries for some document sets. The reason lies in the different characteristics of different document sets and none of the summarization models can be fit for all different document sets. \n\nBased on the above observations, we attempt to improve the overall summarization performance by leveraging the summaries produced by multiple different summarization models (i.e., different summarization methods, or a summarization method with different parameter settings) for each document set.",
            "score": 0.5573256685568915,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 653,
                    "end": 675,
                    "matchedPaperCorpusId": "10418456"
                },
                {
                    "start": 713,
                    "end": 733,
                    "matchedPaperCorpusId": "18752763"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "2115491",
            "title": "Sub-event based multi-document summarization",
            "text": "Multiple articles on a particular topic tend to contain redundant information as well as information that is unique to each article. For instance, different news sources covering the same topic may take different angles, or new information may become available in a later report. So, while all the articles are related to the larger topic, each article may be associated with any of several sub-events. We wanted to find a way to capture the unique sub-event information that is characteristic in multiple-document coverage of a single topic. We predicted that breaking documents down to their sub-events and capturing those sentences in each sub-event with the highest utility would produce an accurate, thorough, and diverse multidocument summary. \n\nIn our first experiment, we compared six methods of summarization to see which produces the best summaries. The methods included three automatic and three manual methods of producing summaries. We used relative utility to capture and measure subtleties in determining sentence relevance. We created multiple document summaries using both a sub-event based approach and a topic-based approach. Generally, we expected to find that the manual summaries performed better than the automatic summaries. In our second experiment, we designed a multi-document summarizer which relied on a clustering method, and we tested the three policies we devised for creating summaries from the manual summarization technique developed in our first experiment.",
            "score": 0.5572027057576677,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1493
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "1320",
            "title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies",
            "text": "We presented a new multi-document summarizer, MEAD. It summarizes clusters of news articles automatically grouped by a topic detection system. MEAD uses information from the centroids of the clusters to select sentences that are most likely to be relevant to the cluster topic. \n\nWe used a new utility-based technique, CBSU, for the evaluation of MEAD and of summarizers in general. We found that MEAD produces summaries that are similar in quality to the ones produced by humans. We also compared MEAD's performance to an alternative method, multi-document lead, and showed how MEAD's sentence scoring weights can be modified to produce summaries significantly better than the alternatives. \n\nWe also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in [Carbonell and Goldstein, 1998]) and showed how it can be used in evaluating multidocument summaries. \n\nAll our findings are backed by the analysis of two experiments that we performed with human subjects. We found that the interjudge agreement on sentence utility is very high while the agreement on crosssentence subsumption is moderately low, although promising. \n\nIn the future, we would like to test our multidocument summarizer on a larger corpus and improve the summarization algorithm. We would also like to explore how the techniques we proposed here can be used for multiligual multidocument summarization.",
            "score": 0.5570599652501886,
            "section_title": "Contributions and future work",
            "char_start_offset": 21158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1461
                }
            ],
            "ref_mentions": [
                {
                    "start": 845,
                    "end": 876,
                    "matchedPaperCorpusId": "4508623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "256416214",
            "title": "Do Multi-Document Summarization Models Synthesize?",
            "text": "Automatic (multi-document) summarization (Nenkova and McKeown, 2011;Maybury, 1999) has been an active subfield within NLP for decades. We have focused our analysis on modern, neural abstractive models for conditional text generation (Bahdanau et al., 2015). In light of their empirical success, we have specifically evaluated a set of Transformer-based (Vaswani et al., 2017) models which have recently been used for multidocument summarization (Beltagy et al., 2020;Zhang et al., 2020;Xiao et al., 2022;Raffel et al., 2020). There has been some work on highlighting conflicting evidence in health literature specifically (Shah et al., 2021b,a), though this focused primarily on highlighting conflicting evidence and explicitly aggregating extracted content. \n\nMultiple works have attempted gauge the difficulty of multi-document summarization. Wolhandler et al. (2022) measures the difficulty of abstractive multi-document news summarization as a function of inputs necessary to produce a final summary; they find that two to four well-chosen documents can cover a news topic sufficiently for the summarizer. They also find systematic reviews are particularly ill-suited to this minimal covering approach. Giorgi et al. (2022) studies the impact of document retrieval behaviors on multidocument summarization performance, and find that models are sensitive to missing inputs. \n\nSentence fusion One view on synthesis might be that is a particular kind of sentence fusion (Barzilay and McKeown, 2005). However, past work on \"fusing\" sentences has assumed that the aim is to generate an output that contains the information common to similar sentences (Thadani and McKeown, 2013). This is intuitive in the context of, e.g., summarizing multiple news articles covering the same event. But here we are interested in the more challenging setting in which the output should reflect an aggregate measure of potentially conflicting evidence or opinions. \n\nReview and opinion summarization considers a similar task to ours: Aggregating (usually product) reviews and opinions into a single coherent text.",
            "score": 0.5569913819153017,
            "section_title": "Related Work",
            "char_start_offset": 29821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1945
                },
                {
                    "start": 1948,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 256,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 467,
                    "end": 486,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 504,
                    "end": 524,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 845,
                    "end": 869,
                    "matchedPaperCorpusId": "253098164"
                },
                {
                    "start": 1471,
                    "end": 1499,
                    "matchedPaperCorpusId": "16188305"
                },
                {
                    "start": 1650,
                    "end": 1677,
                    "matchedPaperCorpusId": "12635978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "15760330",
            "title": "Summarizing multiple spoken documents: finding evidence from untranscribed audio",
            "text": "Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995;Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multi-Gen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. \n\nAbstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for the time being. As in single-spoken-document summarization, this paper focuses on the extractive approach. \n\nAmong the extractive models, MMR (Carbonell and Goldstein, 1998) and MEAD (Radev et al., 2004), are possibly the most widely known. Both of them are linear models that balance salience and redundancy. Although in principle, these models allow for any estimates of salience and redundancy, they themselves calculate these scores with word reoccurrence statistics, e.g., tf.idf, and yield state-of-the-art performance. MMR it-eratively selects sentences that are similar to the entire documents, but dissimilar to the previously selected sentences to avoid redundancy. Its details will be revisited below. MEAD uses a redundancy removal mechanism similar to MMR, but to decide the salience of a sentence to the whole topic, MEAD uses not only its similarity score but also sentence position, e.g., the first sentence of each new story is considered important. Our work adopts the general framework of MMR and MEAD to study the effectiveness of the acoustic pattern evidence found in untranscribed audio.",
            "score": 0.5569509413747074,
            "section_title": "Multiple-document summarization",
            "char_start_offset": 7909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2080
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 342,
                    "matchedPaperCorpusId": "2446679"
                },
                {
                    "start": 342,
                    "end": 366,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 424,
                    "end": 455,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 467,
                    "end": 490,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 830,
                    "end": 852,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 1112,
                    "end": 1143,
                    "matchedPaperCorpusId": "4508623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "225075639",
            "title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
            "text": "We propose Multi-XScience, a large-scale dataset for multi-document summarization using scientific articles. We introduce a challenging multi-document summarization task: write the related work section of a paper using its abstract (source 1 in Tab. 1) and reference papers (additional sources). \n\nMulti-XScience is inspired by the XSum dataset and can be seen as a multi-document version of extreme summarization (Narayan et al., 2018b). Similar to XSum, the \"extremeness\" makes our dataset more amenable to abstractive summarization strategies. Moreover, Table 4 shows that Multi-XScience contains fewer positional and extractive biases than previous MDS datasets. High positional and extractive biases can undesirably enable models to achieve high summarization scores by copying sentences from certain (fixed) positions, e.g. lead sentences in news summarization (Grenander et al., 2019;Narayan et al., 2018a). Empirical results show that our dataset is challenging and requires models having high-level of text abstractiveness.",
            "score": 0.5566824210812378,
            "section_title": "Introduction",
            "char_start_offset": 2923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1032
                }
            ],
            "ref_mentions": [
                {
                    "start": 414,
                    "end": 437,
                    "matchedPaperCorpusId": "215768182"
                },
                {
                    "start": 867,
                    "end": 891,
                    "matchedPaperCorpusId": "202542690"
                },
                {
                    "start": 891,
                    "end": 913,
                    "matchedPaperCorpusId": "215768182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "272146191",
            "title": "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section",
            "text": "Document summarization is a field of artificial intelligence aimed at creating concise and informative summaries from extensive texts [13]. This process allows users to quickly grasp the main information of a document without needing to read the entire original content. Summarization can be performed in two main ways: extractive and abstractive [13]. Extractive summarization selects and copies key parts of the original text to compose the summary, preserving the verbatim of the source text. On the other hand, abstractive summarization rewrites the content, generating a summary that may include paraphrasing and synthesizing information more creatively and flexibly. \n\nMulti-document summarization (MDS) extends these concepts to handle multiple texts simultaneously [13]. The goal is to generate a single summary that incorporates information from a set of documents, eliminating redundancies and highlighting essential cross-referenced information. This technique is useful in situations where information on a specific topic is spread across various documents. \n\nMDS datasets advance MDS research by providing essential data for training, testing, and validating models. These datasets support the development of algorithms capable of handling the complexities of summarizing multiple documents, enabling researchers to compare different summarization methods effectively [8]. Essentially, an MDS dataset is composed of summaries generated from multiple source documents. The composition of this dataset can be done in different ways, such as by selecting a set of documents and asking humans to write summaries, or by using existing summaries and their source documents. The latter is the case for the Multi-News dataset [4], which uses news articles and their summaries to create an MDS dataset. \n\nA comprehensive survey is a review that provides a broad overview of a specific topic, covering the most relevant and recent research in the field. These surveys are essential for researchers to understand the state-of-the-art and identify gaps in the literature. The automatic generation of a survey's text can be considered a MDS task, where a set of scientific articles is summarized to produce a cohesive text. This task is challenging due to the complexity of scientific literature and the need to maintain the technical accuracy of the original content.",
            "score": 0.5565090660932305,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1806
                },
                {
                    "start": 1809,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 138,
                    "matchedPaperCorpusId": "47179013"
                },
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "47179013"
                },
                {
                    "start": 773,
                    "end": 777,
                    "matchedPaperCorpusId": "47179013"
                },
                {
                    "start": 1381,
                    "end": 1384,
                    "matchedPaperCorpusId": "250118028"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "270000888",
            "title": "Advancing automatic text summarization: Unleashing enhanced binary multi-objective grey wolf optimization with mutation",
            "text": "The research work's key contributions are outlined as follows:\n\n1. Proposes an extractive multi-document ATS model designed to maximize coverage and minimize redundancy using BMOGWO-M.\n\n2. Proposes BGWO with the integration of a mutation operator to address the multi-document extractive text summarization problem.\n\n3. Conducts experiments on the DUC 2002 dataset, a standard benchmark for text summarization.\n\n4. Evaluate the proposed model's performance using ROUGE-2 and ROUGE-L metrics.",
            "score": 0.5560021189486174,
            "section_title": "Contribution of the research",
            "char_start_offset": 7259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 64,
                    "end": 184
                },
                {
                    "start": 186,
                    "end": 315
                },
                {
                    "start": 317,
                    "end": 410
                },
                {
                    "start": 412,
                    "end": 491
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "10216129",
            "title": "Multi-document Summarization by Visualizing Topical Content",
            "text": "This paper describes a framework for multi-document summarization which combines three premises: coherent themes can be identified reliably; highly representative themes, running across subsets of the document collection, can function as multi-document summary surrogates; and effective end-use of such themes should be facilitated by a visualization environment which clarifies the relationship between themes and documents. We present algorithms that formalize our framework, describe an implementation, and demonstrate a prototype system and interface.",
            "score": 0.5557167075767279,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "269762702",
            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
            "text": "Along with the prosperity of knowledge-aware research in the natural language processing field, more and more document summarization models attempted to incorporate knowledge graphs to enhance the quality of generated summaries.SDS with KG Gunel et al. [6]  MDS with KG Zhou et al. [26] presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. [18] presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News [5] and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model [13] with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. [18] utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
            "score": 0.5553767710830457,
            "section_title": "STATE OF THE ART",
            "char_start_offset": 6022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 879
                },
                {
                    "start": 879,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1466
                },
                {
                    "start": 1466,
                    "end": 1729
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "204735695"
                },
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "235097309"
                },
                {
                    "start": 1112,
                    "end": 1115,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1210,
                    "end": 1214,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1525,
                    "end": 1529,
                    "matchedPaperCorpusId": "235097309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "247519084",
            "title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization",
            "text": "Multi-Document Summarization is the task of generating a summary from a cluster of related documents. State-of-the-art approaches to multi-document summarization are primarily either graph-based (Liao et al., 2018;Li et al., 2020;Pasunuru et al., 2021), leveraging graph neural networks to connect information between the documents, or hierarchical (Liu and Lapata, 2019a;Fabbri et al., 2019;Jin et al., 2020), building intermediate representations of individual documents and then aggregating information across. While effective, these models either require domain-specific additional information e.g. Abstract Meaning Representation (Liao et al., 2018), or discourse graphs (Christensen et al., 2013;Li et al., 2020), or use dataset-specific, customized architectures, making it difficult to leverage pretrained language models. Simultaneously, recent pretrained language models (typically encoder-decoder transformers) * Work mainly done during an internship at AI2. 1 The code and pre-trained models can be found at https: //github.com/allenai/PRIMER  have shown the advantages of pretraining and transfer learning for generation and summarization (Raffel et al., 2020;Lewis et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). Yet, existing pretrained models either use single-document pretraining objectives or use encoder-only models that do not work for generation tasks like summarization (e.g., CDLM, Caciularu et al., 2021).\n\nTherefore, we argue that these pretrained models are not necessarily the best fit for multi-document summarization. Alternatively, we propose a simple pretraining approach for multi-document summarization, reducing the need for dataset-specific architectures and large fine-tuning labeled data (See Figure 1 to compare with other pretrained models). Our method is designed to teach the model to identify and aggregate salient information across a \"cluster\" of related documents during pretraining. Specifically, our approach uses the Gap Sentence Generation objective (GSG) (Zhang et al., 2020)",
            "score": 0.5552866642623346,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "202889056",
            "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
            "text": "Today's increasing flood of information on the web creates a need for automated multi-document summarization systems that produce high quality summaries. However, producing summaries in a multi-document setting is difficult, as the language used to display the same information in a sentence can vary significantly, making it difficult for summarization models to capture. Given the complexity of the task and the lack of datasets, most researchers use extractive summarization, where the final summary is composed of existing sentences in the input documents. More specifically, extractive summarization systems output summaries in two steps : via sentence ranking, where an importance score is assigned to each sentence, and via the subsequent sentence selection, where the most appropriate sentence is chosen, by considering 1) their importance and 2) their frequency among all documents. Due to data sparcity, models heavily rely on well-designed features at the word level (Hong and Nenkova, 2014;Cao et al., 2015;Christensen et al., 2013;Yasunaga et al., 2017) or take advantage of other large, manually annotated datasets and then apply transfer learning (Cao et al., 2017). Additionally, most of the time, all sentences in the same collection of documents are processed independently and therefore, their relationships are lost. \n\nIn realistic scenarios, features are hard to craft, gathering additional annotated data is costly, and the large variety in expressing the same fact cannot be handled by the use of word-based features only, as is often the case. In this paper, we address these obstacles by proposing to simultaneously leverage two types of sentence embeddings, namely embeddings pre-trained on a large corpus that capture a variety of meanings and domain-specific embeddings learned during training. The former is typically trained on an unrelated corpus composed of high quality texts, allowing to cover additional contexts for each encountered word and sentence. Hereby, we build on the assumption that sentence embeddings capture both the syntactic and semantic content of sentences.",
            "score": 0.5543336960537912,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2109
                }
            ],
            "ref_mentions": [
                {
                    "start": 1002,
                    "end": 1019,
                    "matchedPaperCorpusId": "10675728"
                },
                {
                    "start": 1019,
                    "end": 1044,
                    "matchedPaperCorpusId": "337730"
                },
                {
                    "start": 1044,
                    "end": 1066,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1162,
                    "end": 1180,
                    "matchedPaperCorpusId": "14651945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84765625
        },
        {
            "corpus_id": "266244733",
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "text": "Introduced by Fabbri et al. (2019), the Multi-News dataset is a comprehensive collection designed specifically for multi-document summarization. It encompasses article-summary pairs, each consisting of 2-10 original articles alongside a human-generated summary. During the experiment, the dataset is divided into training, validation, and testing sets, with the input articles truncated to a maximum of 500 tokens.",
            "score": 0.5542435114418354,
            "section_title": "Multi-News",
            "char_start_offset": 23473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 414
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8427734375
        },
        {
            "corpus_id": "236478143",
            "title": "Entity-Aware Abstractive Multi-Document Summarization",
            "text": "We conduct experiments on two major datasets used in the literature of multi-document summarization, namely WikiSum (Liu et al., 2018) and MultiNews (Fabbri et al., 2019). \n\nWikiSum Dataset Liu et al. (2018) treat the generation of Wikipedia section titles as a supervised multi-document summarization task. For each instance, we get 13.3 clusters on average and each cluster has 9.9 tokens on average.",
            "score": 0.5541934471990512,
            "section_title": "Experimental Setup",
            "char_start_offset": 18092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 174,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 402
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 170,
                    "matchedPaperCorpusId": "174799390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "52053741",
            "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization",
            "text": "Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors.",
            "score": 0.5540277581334669,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "18400586",
            "title": "Revisions that improve cohesion in multi-document summaries: a preliminary study",
            "text": "Using multiple documents to generate a summary further complicates the situation. As contended by [Goldstein et al, 2000] a multi-document summary may contain redundant messages, since a cluster of news articles tends to cover the same main point and shared background. In addition, articles from various sources could contradict one another, as to how or when an event developed. Finally, since the source articles are not all written simultaneously, they may describe different stages of the development of an event. Not only do news stories come to different conclusions at various stages in an event, but also the attitudes of writers may change. \n\nMulti-document summaries may suffer further from problems of cohesion since their source articles may be written by different authors. Not only do writers have their own styles, they have the overarching structure of the article in mind when producing it. As a result, in MDS we are more likely to encounter text that is not cohesive. \n\nPrevious research has addressed revision in single-document summaries [Jing & McKeown, 2000] [Mani et al, 1999] and has suggested that revising summaries can make them more informative and correct errors. We believe that a generateand-revise strategy might also be used in creating better multiple-document summaries, within the framework of current extractive summarization systems. However, as mentioned previously, there is reason to believe that multi-document summaries suffer from many different coherence problems and that such problems occur more often than in single-document summaries. Therefore, an important preliminary step in determining how we might revise such summaries is to closely examine the cohesion problems that occur in multidocument summaries. \n\nIn the current paper we analyze a small corpus of manually revised multi-document summaries. We present a taxonomy of pragmatic concerns with respect to cohesion in the summaries, as well as the operators that can address them. Finally, we will discuss the feasibility of implementing such revisions automatically, which we hope to address in our future work.",
            "score": 0.5536257405231626,
            "section_title": "Text cohesion in MDS",
            "char_start_offset": 2818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 121,
                    "matchedPaperCorpusId": "11218013"
                },
                {
                    "start": 1060,
                    "end": 1081,
                    "matchedPaperCorpusId": "11218013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76416015625
        },
        {
            "corpus_id": "173990628",
            "title": "Scoring Sentence Singletons and Pairs for Abstractive Summarization",
            "text": "Our method does not require a massive amount of annotated data. We thus report results on singleand multi-document summarization datasets. \n\nWe experiment with (i) XSum (Narayan et al., 2018), a new dataset created for extreme, abstractive summarization. The task is to reduce a news article to a short, one-sentence summary. Both source articles and reference summaries are gathered from the BBC website. The training set contains about 204k article-summary pairs and the test contains 11k pairs. (ii) CNN/DM (Hermann et al., 2015), an abstractive summarization dataset frequently exploited by recent studies. The task is to reduce a news article to a multi-sentence summary (4 sentences on average). The training set contains about 287k article-summary pairs and the test set contains 11k pairs. We use the non-anonymzied version of the dataset. (iii) DUC-04 (Over and Yen, 2004), a benchmark multi-document summarization dataset. The task is to create an abstractive summary (5 sentences on average) from a set of 10 documents discussing a given topic. The dataset contains 50 sets of documents used for testing purpose only. Each document set is associated with four human reference summaries. \n\nWe build a training set for both tasks of content selection and summary generation. This is done by creating ground-truth sets of instances based on document-summary pairs. Each document and summary pair (D, S) is a collection of sentences D = {d 1 , d 2 , ..., d M } and S = {s 1 , s 2 , ..., s N }. We wish to associate each summary sentence s n with a subset of the document sentences D \u2286 D, which are the sentences that are merged to form s n . Our method chooses multiple sentences that work together to capture the most overlap with summary sentence s n , in the following way. \n\nWe use averaged ROUGE-1, -2, -L scores (Lin, 2004) to represent sentence similarity. The source sentence most similar to s n is chosen, which we call d1 .",
            "score": 0.5532205247332198,
            "section_title": "Data",
            "char_start_offset": 18698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 191,
                    "matchedPaperCorpusId": "215768182"
                },
                {
                    "start": 510,
                    "end": 532,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 1825,
                    "end": 1836,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6708984375
        },
        {
            "corpus_id": "269983093",
            "title": "Which Information Matters? Dissecting Human-written Multi-document Summaries with Partial Information Decomposition",
            "text": "Multi-document Summarization (MDS) consists of providing an abridged version of multiple documents.While some abstractive summarization approaches concatenate all documents into a single input (Johner et al., 2021;Xiao et al., 2022), the large size of input text represents a major challenge in MDS.Therefore, most methods implement two-stage approaches that extract salient text spans based on different heuristics (Lebanoff et al., 2018;Liu et al., 2018;Liu and Lapata, 2019;Zhu et al., 2021).The text is then fed into a summarization model under the assumption that a high-quality summary is based on such information.While earlier work quantifies the properties of human-written MDS based on n-gram matching (Banko and Vanderwende, 2004), we argue that there is a lack of indepth analyses.Without an understanding of the nature of summaries, improving the quality of MDS remains vague and without clear interpretability.\n\nThis work sheds light on what information constitutes a high-quality multi-document summary.In particular, we propose to categorise the summary information into information provided by at least one source (union) or by a unique source, redundant information from all source documents, and even new information derived from considering them jointly (synergy).In information theory, Partial Information Decomposition (PID) decomposes information in the same way to assess how information about a target is distributed among multiple source variables (Williams and Beer, 2010).\n\nWe therefore implement PID in MDS and present SPIDer; a novel approach to quantify the degree to which the PID components-such as redundancy or unique information-contribute to a summary. 1 We then perform an empirical analysis on human-written summaries from different MDS datasets using our approach.Our results demonstrate that the number of sources has a direct dependence on how they contribute to the summary.We also show that, surprisingly, the order of the source documents matters, and the first three documents are frequently considered as the main source of unique information for any number of sources.\n\nTo the best of our knowledge, we present the first fine-grained information analysis in human-written MDS.",
            "score": 0.553088851837875,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 495
                },
                {
                    "start": 495,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 924
                },
                {
                    "start": 926,
                    "end": 1018
                },
                {
                    "start": 1018,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1500
                },
                {
                    "start": 1502,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 1917
                },
                {
                    "start": 1917,
                    "end": 2116
                },
                {
                    "start": 2118,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 214,
                    "matchedPaperCorpusId": "235258298"
                },
                {
                    "start": 214,
                    "end": 232,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 416,
                    "end": 439,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 456,
                    "end": 477,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 477,
                    "end": 494,
                    "matchedPaperCorpusId": "235669622"
                },
                {
                    "start": 712,
                    "end": 741,
                    "matchedPaperCorpusId": "1465750"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "263672150",
            "title": "LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction",
            "text": "In this current work, we introduced a main event-focused method for multi-document news summarization. Our approach consistently outperformed peer systems in terms of ROUGE metrics. Summaries generated by the method exhibited better coherence, as per human evaluation. Future work could explore improved counterparts for each component of the method, namely main event extraction, context extraction, and rewriting.",
            "score": 0.5524562449035557,
            "section_title": "Conclusion",
            "char_start_offset": 21688,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 415
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.837890625
        },
        {
            "corpus_id": "62223738",
            "title": "Minimum redundancy and maximum relevance for single and multi-document Arabic text summarization",
            "text": "In the multi-document summarization task, sentence scoring is even more difficult because of the high probability of the crosssentence informational subsumption CIDR, if the documents discuss the same topic. CIDR reflects the fact that certain sentences repeat some of the information present in other sentences. Radev et al. (2004) proposed to compute the CIDR as the number of terms two sentences have in common, normalized by the length of each one. \n\nBecause we use terms as the central scoring elements, our adapted mRMR could be easily used to produce multidocument summaries. Sentence position is not incorporated in the scoring formula, and we can thus use one of the clustering methods: \n\n(a) Each document corresponds to a cluster; (b) Consider the bag-of-sentences model and apply a clustering algorithm (Naive Bayes, SVM, and HCLUST) to produce clusters and define the classification variable. \n\nThe following steps, terms and sentences scoring, remain unchanged and could be performed similar to the single-document summarization task.",
            "score": 0.551916013805562,
            "section_title": "mRMR for multi-document summarization",
            "char_start_offset": 25189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 695
                },
                {
                    "start": 698,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1048
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 332,
                    "matchedPaperCorpusId": "6354619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "16259680",
            "title": "Improving Update Summarization by Revisiting the MMR Criterion",
            "text": "Text summarization is the process of automatically creating a compressed version of a given text that provides useful information for the user [10]. Queryoriented summaries focus on a user's need, and extract the information related to the specified topic given explicitly in the form of a query [8]. On the other hand, generic summaries try to cover as much as possible the information content. Over the past few years, extensive experiments on query-oriented multidocument summarization have been carried out. Extractive summarization produces summaries by choosing a subset of sentences in the original documents. Sentences are then ordered and assembled according to their relevance to generate the summary [22]. This contrasts with abstractive summarization that involves rephrasing information in the text. Although human beings typically produce summaries in an abstractive way, most of the research is on extractive summarization. This is due to the fact that tools needed to construct semantic representations or generate natural language have not reached a mature stage today. Moreover, existing abstractive summarizers often depend on an extractive component. For example, [25] use a language generation component on top of a multi-document extractive summarizer to produce the final summary. In this paper, we focus on query-oriented multi-document text summarization, where the goal is to produce a summary of multiple documents about a specified topic. \n\nWith the ever increasing popularity of news search engines, displaying the information in a more practical and pleasant way is becoming a challenging and important issue. One possible solution is to summarize multiple news so as to propose only one short text instead of raw aggregated headlines. This is, intuitively, a reasonable solution though producing summaries from large collection of documents is a very complicated task. However, as the number of documents increases, facts that are considered as important -and have to appear in the summary-also become more numerous. In this case, a choice must then be made to drop important facts in order to satisfy size constraints. One way to tackle this problem is to remove facts that the user is already aware of. This variant of text summarization is called update summarization.",
            "score": 0.5502585868165991,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1184,
                    "end": 1188,
                    "matchedPaperCorpusId": "10019526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.681640625
        },
        {
            "corpus_id": "226283949",
            "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
            "text": "the benchmark multi-document summarization datasets, Multi-News and DUC-04, and it brings substantial improvements over several strong baselines for multi-document summarization. We leverage CNN/DailyMail, a single-document summarization dataset, to perform joint learning with Multi-News. We also test the performance on CNN/DailyMail test set, and joint learning also brings certain performance improvement for the single-document summarization baselines.\n\nIn summary, we make the following contributions in this paper:\n\n\u2022 To the best of our knowledge, we are the first to explore joint learning for neural abstractive single-document and multi-document summarizations.\n\n\u2022 We propose a unified model by fully sharing encoder and decoder and utilizing a decoding controller to aggregate the decoder's outputs for multiple input documents.\n\n\u2022 Experimental results show that our approach substantially outperforms several strong baselines, and single document summarization is verified to be very helpful to neural abstractive multi-document summarization. Our code is publicly available at https://github.com/ zhongxia96/MDS-and-SDS.\n\n2 Related Work",
            "score": 0.5495265038076145,
            "section_title": "Introduction",
            "char_start_offset": 4124,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "226283949",
            "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
            "text": "We jointly learn the single-document and multidocument summarizations in a unified model. Our goal is to maximize the probability of output summary Y given a single document S or a document set D. We use T s to denote the single-document training set and T m to denote the multi-document training set. We calculate negative logarithm likelihood function for single-document and multidocument summarizations, respectively.\n\nFor simplicity, we optimize the sum of the above losses.",
            "score": 0.5494875037408549,
            "section_title": "Objective Function",
            "char_start_offset": 15944,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "125321446",
            "title": "Density peaks clustering based integrate framework for multi-document summarization",
            "text": "In this paper, we proposed a novel unsupervised method to  handle the task of multi-document summarization. For ranking sentences, we proposed an integrated score framework. Informative content words are used to get the informativeness, while DPC was employed to measure the relevance and diversity of sentences at the same time. We combined those scores with a length constraint and selected sentences based dynamic programming at last. Extensive experiments on standard datasets show that our method is quite effective for multi-document summarization. \n\nIn the future, we will introduce external resources such as Wordnet and Wikipedia to calculate the sentence semantic similarity, which can solve the problems of the synonym and the multivocal word. We will then apply our proposed method in topicfocused and updated summarization, to which the tasks of summarization have turned.",
            "score": 0.5489509343440939,
            "section_title": "Conclusion",
            "char_start_offset": 16002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 885
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "258686261",
            "title": "A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization",
            "text": "Pre-trained language models (PLMs) have achieved outstanding achievements in abstractive single-document summarization (SDS). However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex. Previous works either design new MDS architectures or apply PLMs bluntly with concatenated source documents as a reformulated SDS task. While the former does not utilize previous pre-training efforts and may not generalize well across different domains, the latter may not sufficiently attend to the intricate cross-document relationships unique to MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task. Across 10 MDS benchmarks from various domains, our method outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters. It outperforms its corresponding PLM backbone by up to 3 Rouge-L and is favored by humans.",
            "score": 0.548766211770112,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "8244856",
            "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention",
            "text": "In this work, we focus on the query-focused multi-document summarization task. The experiments are conducted on the DUC 2005 \u223c 2007 datasets. All the documents are from news websites and grouped into various thematic clusters. In each cluster, there are four reference summaries created by NIST assessors. We use Stanford CoreNLP2 to process the datasets, including sentence splitting, tokenization and lemmatization. Our summarization model compiles the documents in a cluster into a single document. Table 1 shows the basic information of the three datasets. We can find that the data sizes of DUC are quite different. The sentence number of DUC 2007 is only about a half of DUC 2005's. For each cluster, a summarization system is requested to generate a summary with the length limit of 250 words. We conduct a 3-fold crossvalidation on DUC datasets, with two years of data as the training set and one year of data as the test set.",
            "score": 0.5475538512785904,
            "section_title": "Dataset",
            "char_start_offset": 12195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "248496597",
            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
            "text": "In this section we describe the framework proposed for summarizing multiple documents that can contain different gist. It consists of three stages, each with its own objective: (i) per-document information extraction, (ii) information redundancy minimization and (iii) RCR. The first stage aims at identifying and extracting salient information from each coreference-resolved individual document. This is under the assumption that most documents in a document group, e.g. a group of news articles from a time period, are independent of each other. Therefore, the task of finding key information in one document is unrelated to finding key information in another. The second stage aims at identifying sentences with similar information and merging those sentences into one so that the final summary is free of redundancy. This is under the assumption that some documents in a document group discuss the same story and hence the extracted information from the first stage might be overlapping. Lastly, the third stage aims at removing excessive mentions of entities due to the application of passage-wise coreference resolution. An overview of our framework is shown in Figure 1.",
            "score": 0.5473260482125347,
            "section_title": "Methodology",
            "char_start_offset": 14236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1177
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "258865156",
            "title": "Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval",
            "text": "Multi-document summarization (MDS) assumes a set of topic-related documents are provided as input. In practice, this document set is not always available; it would need to be retrieved given an information need, i.e. a question or topic statement, a setting we dub\"open-domain\"MDS. We study this more challenging setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive automatic and human evaluation, we determine: (1) state-of-the-art summarizers suffer large reductions in performance when applied to open-domain MDS, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provide practical guidelines to enable future work on open-domain MDS, e.g. how to choose the number of retrieved documents to summarize. Our results suggest that new retrieval and summarization methods and annotated resources for training and evaluation are necessary for further progress in the open-domain setting.",
            "score": 0.5472815937174063,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8447265625
        },
        {
            "corpus_id": "237593064",
            "title": "MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization",
            "text": "One of the most challenging aspects of current single-document news summarization is that the summary often contains 'extrinsic hallucinations', i.e., facts that are not present in the source document, which are often derived via world knowledge. This causes summarization systems to act more like open-ended language models tending to hallucinate facts that are erroneous. In this paper, we mitigate this problem with the help of multiple supplementary resource documents assisting the task. We present a new dataset MiRANews and benchmark existing summarization models. In contrast to multi-document summarization, which addresses multiple events from several source documents, we still aim at generating a summary for a single document. We show via data analysis that it's not only the models which are to blame: more than 27% of facts mentioned in the gold summaries of MiRANews are better grounded on assisting documents than in the main source articles. An error analysis of generated summaries from pretrained models fine-tuned on MiRANews reveals that this has an even bigger effects on models: assisted summarization reduces 55% of hallucinations when compared to single-document summarization models trained on the main article only. Our code and data are available at https://github.com/XinnuoXu/MiRANews.",
            "score": 0.5467484238152301,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50927734375
        },
        {
            "corpus_id": "258060196",
            "title": "LBMT team at VLSP2022-Abmusu: Hybrid method with text correlation and generative models for Vietnamese multi-document summarization",
            "text": "Summarization is an efficient way to process information in the context of big data, especially for non-expert users who want to get a quick overview of the most crucial ideas in a document (Radev et al., 2002). This problem has been applied to many natural language processing tasks such as document retrieval, text classification, and question answering (Guo et al., 2020;Mishra et al., 2021;Nguyen et al., 2022;Vuong et al., 2022). With the increasing amount of online content, it is becoming increasingly difficult for users to find interesting and useful information. \n\nMulti-document summarization is the task of generating a summary from a collection of documents. More specifically, this paper considers the task of generating summaries from a collection of documents in Vietnamese. Multi-document summarization is a challenging task because the summaries should not only describe the most important information from all documents but also provide a coherent interpretation of the documents. \n\nIn the past few years, several deep learning models have been proposed for the task of multidocument summarization. Extractive models are based on the idea of selecting sentences from the input documents and then combining the selected sentences to form the summary. Abstractive models are based on the idea of generating summary sentences from scratch. Abstractive models have shown better performance than extractive models but are more difficult to train. The reason is that the abstractive models need to understand the content of the input documents and then generate summary sentences. \n\nThanks to the development of pretrained language models, abstractive models have shown good performance in text summarization tasks. Pretrained language models are based on the idea of training a model on a large amount of unannotated data and then fine-tuning the model on the specific task and data set. Available pretrained language models in Vietnamese (Tran et al., 2021;Phan et al., 2022) allow us to apply these models to summarize Vietnamese text. \n\nThis paper proposes a pipeline abstractive method for multi-document summarization that includes three main phases. The first phase is single document extraction. In the second phase, we concatenate the candidate sentences from single documents. Then we perform a multi-document extraction to produce quickview summaries.",
            "score": 0.5462687553216439,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2051
                },
                {
                    "start": 2054,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2299
                },
                {
                    "start": 2300,
                    "end": 2375
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 210,
                    "matchedPaperCorpusId": "94818"
                },
                {
                    "start": 356,
                    "end": 374,
                    "matchedPaperCorpusId": "81977235"
                },
                {
                    "start": 374,
                    "end": 394,
                    "matchedPaperCorpusId": "235097442"
                },
                {
                    "start": 394,
                    "end": 414,
                    "matchedPaperCorpusId": "246281564"
                },
                {
                    "start": 414,
                    "end": 433,
                    "matchedPaperCorpusId": "251508558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "225678727",
            "title": "Extractive Multi-Document Summarization Model Based On Different Integrations of Double Similarity Measures",
            "text": "Multiple document summarization aims at extracting information relevant to an implicit or explicit subject from different documents written about that subject or topic [2]. \n\nThe approaches of extraction-based summarization can be categorized as supervised or unsupervised. Supervised approaches are constructed on algorithms that use a large number of summaries generated by human, and as an outcome, are most convenient for documents related to the summarizer model. Accordingly, they do not necessarily yield an adequate summary for documents that are dissimilar to the model. Furthermore, when the summarization purpose or documents' features are modified by the users, it becomes essential for reeducating the model or rebuilding the training data. Unsupervised approaches do not necessitate training data for training the summarizer. Automatic summary can either involves the most significant information overall (generic summarization) or the most relevant information considering an information need of the user (querybased summarization). Generic summarization approaches focus on covering diversity of the summary for delivering broader content coverage. Usually, they are described in terms of certain key features which relate to the concepts of intent, focus, and coverage. \n\nConsidering the usage, the summary can be indicative or informative. A condensed information on the key topics of a document can be provided through an Indicative summary. Document's most important passages should be preserved in this summary type and often used as the end part of the information retrieval systems, being retrieved by search system rather than full document. Their target should be to aid the user for deciding whether the reading for the original document is valuable or not. The typical length of an indicative summary ranges from 5% to 10% of the whole text. Dissimilarly, informative summaries deliver a condensation for a complete document, retaining significant information, while decreasing its volume. An informative summary is normally 20-30% of the original text [3]. \n\nThe main contribution of this paper is to model the multi-document text summarization task as an optimization problem. The proposed model emphasizes the discovery of essential sentences that cover the main topic of the document collection while transcending the occurrence of redundant sentences. Different integrations of double metric similarity measure are introduced to the proposed model for measuring similarity to improve system performance.",
            "score": 0.5455070658962016,
            "section_title": "Introduction",
            "char_start_offset": 2309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2084
                },
                {
                    "start": 2087,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2383
                },
                {
                    "start": 2384,
                    "end": 2535
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "35052244"
                },
                {
                    "start": 2080,
                    "end": 2083,
                    "matchedPaperCorpusId": "18873196"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "268146783",
            "title": "TOMDS (Topic-Oriented Multi-Document Summarization): Enabling Personalized Customization of Multi-Document Summaries",
            "text": "In a multi-document summarization task, if the user can decide on the summary topic, the generated summary can better align with the reader\u2019s specific needs and preferences. This paper addresses the issue of overly general content generation by common multi-document summarization models and proposes a topic-oriented multi-document summarization (TOMDS) approach. The method is divided into two stages: extraction and abstraction. During the extractive stage, it primarily identifies and retrieves paragraphs relevant to the designated topic, subsequently sorting them based on their relevance to the topic and forming an initial subset of documents. In the abstractive stage, building upon the transformer architecture, the process includes two parts: encoding and decoding. In the encoding part, we integrated an external discourse parsing module that focuses on both micro-level within-paragraph semantic relationships and macro-level inter-paragraph connections, effectively combining these with the implicit relationships in the source document to produce more enriched semantic features. In the decoding part, we incorporated a topic-aware attention mechanism that dynamically zeroes in on information pertinent to the chosen topic, thus guiding the summary generation process more effectively. The proposed model was primarily evaluated using the standard text summary dataset WikiSum. The experimental results show that our model significantly enhanced the thematic relevance and flexibility of the summaries and improved the accuracy of grammatical and semantic comprehension in the generated summaries.",
            "score": 0.5453779120264444,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "234743927",
            "title": "Generating Instructive Questions from Multiple Articles to Guide Reading in E-Bibliotherapy",
            "text": "To overcome the problems of Solution 1, our second solution (Figure 8) is to first synthesize multiple articles with the help of multi-document summarization techniques, and then encode the summary into an overall context vector, based on which the decoder obtains an output as the question for these articles. Multi-document summarization could identify and eliminate the redundancy across articles, recognizes novel information in different articles and try to make the final summary both coherent and complete. It could be very applicable to deal with the low information density and content incompleteness problems of Solution 1. In the study, we apply 6 different multi-document summarization techniques to summarize the N input articles into an s-word text. \n\n(1) Coverage summarization. This is the baseline summarization method. It successively takes the lead sentences from all articles until the summary length threshold is reached. (2) Centroid summarization [91]. Centroid-based method first constructs a centroid of the articles, which consists of words whose TF-IDF scores are above a pre-defined threshold. The salience of each sentence is computed as the weighted average of the similarity between the sentence and the centroid, sentence position within an article and the similarity between the sentence and the first sentence of the article. (3) TextRank summarization [92]. TextRank is a graph-based sentence ranking model where each sentence is added as a vertex and the sentence similarity (e.g., words overlap) is added as an edge between sentences. After the graph-based ranking algorithm converges, we can sort sentences based on their final scores. (4) ILP summarization [93]. Integer Linear Programming (ILP) method takes document summarization tasks as a combinatorial optimization problem, whose optimization goal is to cover as many word n-gram concepts as possible within the length constraint. (5) ClusterCMRW summarization [94]. Cluster-based Conditional Markov Random Walk (ClusterCMRW) method first detects the theme clusters in articles, and then incorporates the cluster-level information and the sentence-to-cluster relationship to compute the saliency score of the sentences based on a Conditional Markov Random Walk model.",
            "score": 0.5452988582416476,
            "section_title": "Solution 2: Encoder-Decoder with Summary on Inputs (ED-SoI)",
            "char_start_offset": 44572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2261
                }
            ],
            "ref_mentions": [
                {
                    "start": 970,
                    "end": 974,
                    "matchedPaperCorpusId": "6354619"
                },
                {
                    "start": 1696,
                    "end": 1700,
                    "matchedPaperCorpusId": "167874"
                },
                {
                    "start": 1955,
                    "end": 1959,
                    "matchedPaperCorpusId": "9849366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "233296301",
            "title": "Generating Related Work",
            "text": "Multi-document Summarization: Approaches in neural sequence-to-sequence learning (Rush et al., 2015;Cheng and Lapata, 2016;See et al., 2017;Subramanian et al., 2019) for document summarization have shown promise and have been adapted successfully for multi-document summarization (Zhang et al., 2018;Lebanoff et al., 2018;Baumel et al., 2018;Amplayo and Lapata, 2019;Fabbri et al., 2019;Lu et al., 2020). Trained on large amounts of data, these methods have improved upon traditional extractive (Carbonell and Goldstein, 1998;Radev and McKeown, 1998;Haghighi and Vanderwende, 2009) and abstractive approaches (McKeown and Radev, 1995;Ganesan et al., 2010). The key aspect of typical multidocument summarization solutions is to capture repetitions and similarities in the multiple input documents (Barzilay et al., 1999;Fabbri et al., 2019). However, in scientific writing, the goal is typically to highlight differences and identify dissimilarities amongst past work. Shah et al. ( 2021) addresses the contrastive nature of scientific studies, however, in related work generation, we are interested in generating an entire section describing past works and their differences in context of the new idea. \n\nQuery Driven Summarization: Several tasks such as article generation (Liu et al., 2018), dialogue (Moghe et al., 2018;Weston et al., 2018;Dinan et al., 2018;Fan et al., 2020), translation (Gu et al., 2018) and language modeling (Guu et al., 2018;Khandelwal et al., 2019) can be categorized as query driven generation. Our work can also be considered as part of this framework, with the new work being a query. However, we are interested in generating a coherent summary which highlights the comparative aspects of past works.",
            "score": 0.5447943929394885,
            "section_title": "Related Work",
            "char_start_offset": 3612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1730
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 100,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 100,
                    "end": 123,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 123,
                    "end": 140,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 140,
                    "end": 165,
                    "matchedPaperCorpusId": "202541012"
                },
                {
                    "start": 280,
                    "end": 300,
                    "matchedPaperCorpusId": "53223447"
                },
                {
                    "start": 367,
                    "end": 387,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 387,
                    "end": 403,
                    "matchedPaperCorpusId": "225075639"
                },
                {
                    "start": 495,
                    "end": 526,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 526,
                    "end": 550,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 550,
                    "end": 581,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 609,
                    "end": 634,
                    "matchedPaperCorpusId": "2446679"
                },
                {
                    "start": 634,
                    "end": 655,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 796,
                    "end": 819,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 819,
                    "end": 839,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1303,
                    "end": 1323,
                    "matchedPaperCorpusId": "52333947"
                },
                {
                    "start": 1323,
                    "end": 1343,
                    "matchedPaperCorpusId": "52006529"
                },
                {
                    "start": 1393,
                    "end": 1410,
                    "matchedPaperCorpusId": "19206366"
                },
                {
                    "start": 1433,
                    "end": 1451,
                    "matchedPaperCorpusId": "2318481"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "262043584",
            "title": "ODSum: New Benchmarks for Open Domain Multi-Document Summarization",
            "text": "Summarization is an established NLP task that generates concise and coherent summaries from provided texts (Hermann et al., 2015a;Xu and Lapata, 2020;Zhong et al., 2021;DeYoung et al., 2021a;Giorgi et al., 2022). Given the current information surge, the need to extract crucial points from a large collection of documents has become imperative. This introduces the concept of Open-domain multi-document summarization (ODMDS) (Ji et al., 2013;Giorgi et al., 2022). ODMDS can be analogized to extracting knowledge from diverse pieces of information across a large number of documents, and connecting and aggregate the information into a clear, coherent, and brief summary. While conventional (multi-document) summarization methods typically work within a specified setting where the source documents are predetermined and limited in scope, ODMDS addresses other broader, real-world challenges. It serves as a crucial tool for efficient knowledge extraction, facilitating a more comprehensive understanding of a topic without the need to review a multitude of documents. ODMDS has close connections with information retrieval (Zhang et al., 2021) and multi-document summarization (Wallace et al., 2020;DeYoung et al., 2021b;Zhong et al., 2021), bridging the gap between extracting relevant documents from large corpora and generating concise summaries from multiple related texts. \n\nODMDS differs from prior work in three main ways: 1) unlike open-domain QA (Yang et al., 2015;Lewis et al., 2020b), open-domain MDS requires long-form generations that address a given query, 2) unlike Query-focused MDS (Xu and Lapata, 2020;Pasunuru et al., 2021a) where the input documents are given and relevant to the query, ODMDS tackles a more challenging scenario where documents must be retrieved from a largescale collection of mostly irrelevant documents, 3) compared to previous MDS attempts (Wallace et al., 2020;DeYoung et al., 2021b), which involved hun-arXiv:2309.08960v1",
            "score": 0.544708707317129,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 130,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 130,
                    "end": 150,
                    "matchedPaperCorpusId": "226262229"
                },
                {
                    "start": 150,
                    "end": 169,
                    "matchedPaperCorpusId": "233219904"
                },
                {
                    "start": 169,
                    "end": 191,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 425,
                    "end": 442,
                    "matchedPaperCorpusId": "17131989"
                },
                {
                    "start": 1221,
                    "end": 1240,
                    "matchedPaperCorpusId": "233219904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "10735032",
            "title": "Learning to Rank Definitions to Generate Quizzes for Interactive Information Presentation",
            "text": "In multi-document summarization, the focus has been mainly on creating cohesive texts. (Lapata, 2003) uses the probability of words in adjacent sentences as constraints to maximize the coherence of all sentence-pairs in texts. Although we acknowledge that having cohesive definitions is important, since we are not creating a single text and the dialogue that we aim to achieve would involve frequent user/system interaction ( Fig. 2), we do not deal with the coherence of definitions in this paper.  Figure 2: Example dialogue based on the quiz-style ranking of definitions. S stands for a system utterance and U for a user utterance.",
            "score": 0.5446784708735701,
            "section_title": "Ranking Definitions for Quizzes",
            "char_start_offset": 4142,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 101,
                    "matchedPaperCorpusId": "10135300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4609375
        },
        {
            "corpus_id": "210919915",
            "title": "Generating Representative Headlines for News Stories",
            "text": "the summary or headline of a set of documents is also much more time consuming than that in the context of single-document summarization. Hence, recent multi-document summarization (MDS) models either seek to adapt from single-document models [22,67] or leverage external resources such as Wikipedia pages [26,27]. [11] recently provides a crowd-sourced dataset for multi-document summarization, but such resources remain absent for multi-document headline generation.\n\nTo facilitate standard study and evaluation, we publish the first dataset for multi-document headline generation. The published dataset consists of 367K news stories with human-curated headlines, 6.5 times larger than the biggest public dataset for multi-document summarization [11]. Large as it may seem, 367K news stories is still a drop in the ocean compared with the entire news corpus on the Web. More importantly, manual curation is slow and expensive, and can hardly scale to web-scale applications with millions of emerging articles every day. To this end, we propose to further leverage the unlabeled news corpus in two ways. Existing articles are first treated as a knowledge base and we automatically annotate unseen news stories by distant supervision (i.e., with one of the article titles in the news story). We then propose a multi-level pre-training framework, which initializes the model with a language model learned from the raw news corpus, and transfers knowledge from single-document article-title pairs. The distant supervision framework enables us to generate another dataset for training without human effort, which is 6 times larger than the aforementioned human-curated dataset. We show that our model solely based on distant supervision can already outperform the same model trained on the human-curated dataset. In addition, fine-tuning the distantly-trained model with a small number of human-labeled examples further boosts its performance (Section 7.3). In real-world applications, the process of grouping news stories, which is viewed as a prerequisite, is not always perfect. To tackle this problem, we design a self-voting-based documentlevel attention model, which proves to be robust to the noisy articles in the news stories (Section 7.4). Improving the quality of clustering",
            "score": 0.5444517732237506,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3980,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "53223447"
                },
                {
                    "start": 310,
                    "end": 313,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 748,
                    "end": 752,
                    "matchedPaperCorpusId": "174799390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "236150987",
            "title": "Multi-Document Summarization with Determinantal Point Process Attention",
            "text": "Multi-Document Summarization Most previous solutions to multi-document summarization adopt non-neural, extractive methods (Carbonell & Goldstein, 1998;Radev, Jing, Sty\u015b, & Tam, 2004;Erkan & Radev, 2004;Barzilay, McKeown, & Elhadad, 1999). \n\nMore recently, various encoder-decoder architectures (Liu & Lapata, 2019;Fabbri et al., 2019;Perez-Beltrachini et al., 2019;Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, & Shazeer, 2018;Zhang, Tan, & Wan, 2018;Lebanoff, Song, & Liu, 2018) have been ported to this task thanks to the development of large-scale datasets for model training. Among these, two approaches are closely related to our work on account of handling redundancy explicitly. Lebanoff et al. (2018) first pre-train an abstractive summarization model on single-document data and then fine-tune it on smaller multi-document benchmarks. They use a separately trained Maximal Marginal Relevance (MMR, Carbonell & Goldstein, 1998) module to select a relevant and non-redundant sentence from the input documents for the generation of the next summary sentence. Fabbri et al. (2019) incorporate this MMR mechanism as hierarchical attention into an end-to-end trained Pointer-Generator network (See et al., 2017). \n\nOur proposal differs from both approaches in terms of granularity; they operate at the sentence level while our model operates at the word level, resembling more the phrase selection approach of Barzilay et al. (1999). Another important difference lies in the way previously selected content is modelled. Lebanoff et al. (2018) explicitly select distinct sentences from the input, while Fabbri et al. (2019) do not track previously selected content and compute diversity as self-attention among all source sentences at each time step. In contrast, our DPP guided attention computes diversity at each time step between input tokens and a summary of previous context decisions.",
            "score": 0.54402856855123,
            "section_title": "Related Work",
            "char_start_offset": 6143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1213
                },
                {
                    "start": 1216,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 151,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 151,
                    "end": 182,
                    "matchedPaperCorpusId": "6354619"
                },
                {
                    "start": 182,
                    "end": 202,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 202,
                    "end": 237,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 365,
                    "end": 425,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 425,
                    "end": 449,
                    "matchedPaperCorpusId": "53223447"
                },
                {
                    "start": 905,
                    "end": 933,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 1411,
                    "end": 1433,
                    "matchedPaperCorpusId": "7031344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "145993667",
            "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis",
            "text": "In multi-document summarization, a single summary is sought from across many documents that describe the same topic. These documents, which are written by different authors, are normally taken from different news sources. Unlike single document summarisation, the process of summarising a collection of related documents poses a number of other challenges including a high degree of redundancy, which conceivably results from merging multiple descriptions of the same topic, and the ordering of the extracted summary sentences. To reduce redundancy, different summarization approaches used different methods such as measuring sentence similarity [54,58], using the seminal MMR algorithm and its derivative [1,48,56], and exploiting clustering algorithms [57]. Besides, sentence ordering remains a less studied problem in MDS. On this subject, Bollegala [59] combined four criteria (chronology, topicalcloseness, precedence, and succession) to develop a sentence ordering approach for multi document summarization. In this work, we have designed a pre-processing stage to mitigate these challenges. Firstly, related documents of each cluster to be summarised are merged together to form a single cluster document while arranging the entire text in the order of the source documents' timeline. We then iteratively removed similar sentences to exclude repeated content. This is done by finding the similarity of each sentence with the rest of the cluster sentences and removing those with high similarity scores. This produces a unified cluster document with minimized information repetition. More formally, if C = {D 1 , D 2 , D 3 , ..., D M } is a cluster of M documents to be summarised, we combine all sentences of the document collection to obtain a flattened cluster, C = {S 1 , S 2 , S 3 , S 4 , S 4 , S 4 , ..., S N }, where N is the total number of cluster sentences. Next, a filtering process is applied to C in order to sieve cluster sentences by discarding all highly similar sentences to the current one. Figure 4 describes the cluster merging process. For better readability, the figure indicates outward arrows for S 1 only, but the same logic applies to the rest of the sentences. By this merging, we remove (N \u2212 K) sentences where N \u2265 K.",
            "score": 0.5439502735553862,
            "section_title": "Merging Multi-document Clusters",
            "char_start_offset": 18359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "24465182"
                },
                {
                    "start": 709,
                    "end": 712,
                    "matchedPaperCorpusId": "13524290"
                },
                {
                    "start": 712,
                    "end": 715,
                    "matchedPaperCorpusId": "4508623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "264817506",
            "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders",
            "text": "Table 2 shows the comparison of Multi-news summarization. Given that all frameworks employing our pre-trained representations outperform the First-3 baseline, our approach effectively mitigates position bias (Dong et al., 2021). This bias often results in incomplete summaries that neglect essential information located in the middle of the document. \n\nThe results demonstrate two key findings: (i) Our method adeptly captures essential summary-worthy sentences, thereby consolidating the process of sentence clustering and, in turn, improving extractive accuracy. (ii) The embedded, intra-sentential distinctive features and inter-sentential cohensive features are crucial in ranking significant sentences across multiple documents.",
            "score": 0.5436515320147843,
            "section_title": "Multi-Document Experiments",
            "char_start_offset": 17770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 733
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 227,
                    "matchedPaperCorpusId": "231592914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67236328125
        },
        {
            "corpus_id": "255545906",
            "title": "A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding",
            "text": "of the multi-document summarization task, and we call the reader's attention when it happens.\n\nA more recent approach to multi-document summarization is known as update summarization. The idea of update-based summarization is to generate short multi-document summaries of recent documents under the assumption that the earlier documents were previously considered. Thus, the objective of an update summary is to update the reader with new information about a particular topic and the ATS system has to decide which piece of information in the set of new documents is novel and which is redundant.\n\nWe may find another kind of multi-document summarization if we consider jointly to summarize the original document and the content generated by the users (such as comments or other online network contents) after the publication of the original document. This approach of summarization is known as social context summarization. We may find examples of social context summarization in Sections 6.1.2 and 6.4.\n\n5. The type of method used to choose the sentences to be included in the summary: We may classify it in terms of being a supervised or an unsupervised method. This is particularly important because while in the former case we need data to train the model, in the latter case that is not necessary. Supervised methods arise in Sections 6.4, 7.2 and 8.\n\n3 An overview of other ATS surveys Table 1 presents an overview of other surveys about ATS systems. The first column presents the source document. The second column presents a summary of its content. The third column presents the date range of papers cited in the survey. The last column presents the number of papers cited in the review. The intention of the last two columns is to provide an indication of the coverage of the work. The most complete surveys to date are the ones presented in El-Kassas et al. (2021) and Mridha et al. (2021). Like ours, they intend to cover most aspects of ATS systems. However, we may find differences among them in terms of content and presentation. Although there is no doubt that most classical papers are present in our work and also in these two works, the presentation of our work is naturally model-guided. In terms of content, our work also presents additional sections that are not available elsewhere,",
            "score": 0.543292150892568,
            "section_title": "Classification of ATS systems",
            "char_start_offset": 9360,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1852,
                    "end": 1875,
                    "matchedPaperCorpusId": "224955327"
                },
                {
                    "start": 1880,
                    "end": 1900,
                    "matchedPaperCorpusId": "244510269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "173990832",
            "title": "Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization",
            "text": "Multi-document summarization is arguably one of the most important tools for information aggregation. It seeks to produce a succinct summary from a collection of textual documents created by multiple authors concerning a single topic (Nenkova and McKeown, 2011). The summarization technique has seen growing interest in a broad spectrum of domains that include summarizing product reviews (Gerani et al., 2014;, student survey responses (Luo and Litman, 2015;Luo et al., 2016), forum discussion threads (Ding and Jiang, 2015;Tarnpradab et al., 2017), and news articles about a particular event (Hong et al., 2014). Despite the empirical success, most of the datasets remain small, and the cost of hiring hu-man annotators to create ground-truth summaries for multi-document inputs can be prohibitive.\n\nImpressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015;See et al., 2017;Paulus et al., 2017;Chen and Bansal, 2018). These models, nonetheless, are data-hungry and learn poorly from small datasets, as is often the case with multidocument summarization. To date, studies have primarily focused on single-document summarization (See et al., 2017;Celikyilmaz et al., 2018;Kryscinski et al., 2018) and sentence summarization (Nallapati et al., 2016;Cao et al., 2018; in part because parallel training data are abundant and they can be conveniently acquired from the Web. Further, a notable issue with abstractive summarization is the reliability. These models are equipped with the capability of generating new words not present in the source. With greater freedom of lexical choices, the system summaries can contain inaccurate factual details and falsified content that prevent them from staying \"true-to-original.\"\n\nIn this paper we instead focus on an extractive method exploiting the determinantal point process (DPP; Kulesza and Taskar, 2012) for multidocument summarization. DPP can be trained on small data, and because extractive summar",
            "score": 0.5426782068021327,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 389,
                    "end": 410,
                    "matchedPaperCorpusId": "2767900"
                },
                {
                    "start": 437,
                    "end": 459,
                    "matchedPaperCorpusId": "5910159"
                },
                {
                    "start": 459,
                    "end": 476,
                    "matchedPaperCorpusId": "10939739"
                },
                {
                    "start": 503,
                    "end": 525,
                    "matchedPaperCorpusId": "15468908"
                },
                {
                    "start": 525,
                    "end": 549,
                    "matchedPaperCorpusId": "39210805"
                },
                {
                    "start": 594,
                    "end": 612,
                    "matchedPaperCorpusId": "16482037"
                },
                {
                    "start": 920,
                    "end": 937,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 937,
                    "end": 957,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 957,
                    "end": 979,
                    "matchedPaperCorpusId": "44129061"
                },
                {
                    "start": 1190,
                    "end": 1208,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1208,
                    "end": 1233,
                    "matchedPaperCorpusId": "4406182"
                },
                {
                    "start": 1233,
                    "end": 1257,
                    "matchedPaperCorpusId": "52091366"
                },
                {
                    "start": 1285,
                    "end": 1309,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1309,
                    "end": 1326,
                    "matchedPaperCorpusId": "19198109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "237593064",
            "title": "MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization",
            "text": "another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way.\n\nNote that, as shown in Figure 2 (left), our task is different from both Single-document Summarization (SDS, middle) and Multi-document Summarization (MDS, right): SDS aims at generating a summary for a single main document, while we aim to generate a target summary S for a single document D with supporting facts from multiple assisting documents A. In this paper:\n\n\u2022 We introduce a new task, Multi-Resource-Assisted News summarization, aiming at generating a summary for the corresponding news article with the support of related assisting documents.\n\n\u2022 We create and release a new dataset (MIRANEWS) introducing a novel automatic data collection method which gathers multiple assisting news articles from different news resources for a document-summary pair.\n\n\u2022 We introduce new referenceless metrics, which quantitatively evaluate extrinsic hallucinations both in summarization datasets and output summaries, and confirm that introducing assisting documents offers better grounding to more than 27% of facts mentioned in the reference summaries.\n\n\u2022 We report benchmark results using models both fine-tuned and trained from scratch on MI-RANEWS. We show that modeling assisting documents effectively introduces external facts in the summaries that are grounded on the assisting docustill over 90% of the total hallucinations are incorrect. ments, resulting in 55% less counterfactual hallucinations than SDS systems.",
            "score": 0.5421409883410084,
            "section_title": "Introduction",
            "char_start_offset": 3645,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "170079112",
            "title": "Hierarchical Transformers for Multi-Document Summarization",
            "text": "In this paper we conceptualized abstractive multidocument summarization as a machine learning problem. We proposed a new model which is able to encode multiple input documents hierarchically, learn latent relations across them, and additionally incorporate structural information from well-known graph representations. We have also demonstrated the importance of a learning-based approach for selecting which documents to summarize. Experimental results show that our model produces summaries which are both fluent and in-formative outperforming competitive systems by a wide margin. In the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks.",
            "score": 0.5418199205531851,
            "section_title": "Conclusions",
            "char_start_offset": 28299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 708
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "260937229",
            "title": "A Comparison of Summarization Methods for Duplicate Software Bug Reports",
            "text": "Figure 1 represents the architecture of the model. The multi-document summarization model consists of two main modules: input processing and extractive summarization. Sections 3.1.1 and 3.1.2 explain each module, respectively.",
            "score": 0.5417723204062548,
            "section_title": "Six Summarization Methods for Duplicate Bug Reports",
            "char_start_offset": 15224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 226
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302001953125
        },
        {
            "corpus_id": "266374529",
            "title": "Survey on Multi-Document Summarization: Systematic Literature Review",
            "text": "The latest trend is the desire to learn about the specific field without dedicating months and hours to it. The need to meet this challenge motivates demanding work related to text summarization technology. Nowadays, no one has the time to read a huge number of documents from the internet and then extract the most relevant and useful information from them. In this time, multi-document summarization gives a more precise and comprehensive summary from the collection of documents into a single document and saves them time and efforts of the readers. Today, a lot of methods are already developed in this field for improving the performance of technique but existing work needed more accuracy.",
            "score": 0.5416269932452585,
            "section_title": "Motivation:",
            "char_start_offset": 7628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 695
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "304226",
            "title": "Text Summarization Techniques: A Brief Survey",
            "text": "LDA has been extensively used for multi-document summarization recently. For example, Daume et al. [19] proposed B S , a Bayesian summarization model for query-focused summarization. Wang et al. [77] introduced a Bayesian sentence-based topic model for summarization which used both term-document and term-sentence associations. Their system achieved significance performance and outperformed many other summarization methods. Celikyilmaz et al. [13] describe multi-document summarization as a prediction problem based on a two-phase hybrid model. First, they propose a hierarchical topic model to discover the topic structures of all sentences. Then, they compute the similarities of candidate sentences with human-provided summaries using a novel tree-based sentence scoring function. In the second step they make use of these scores and train a regression model according the lexical and structural characteristics of the sentences, and employ the model to score sentences of new documents (unseen documents) to form a summary.",
            "score": 0.541479928648499,
            "section_title": "Bayesian Topic Models",
            "char_start_offset": 17382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1030
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 103,
                    "matchedPaperCorpusId": "6241932"
                },
                {
                    "start": 195,
                    "end": 199,
                    "matchedPaperCorpusId": "189209"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "5833592"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87646484375
        },
        {
            "corpus_id": "196363",
            "title": "Multi-document summarization using off the shelf compression software",
            "text": "A standard way for producing summaries of text documents is sentence extraction. In sentence extraction, the summary of a document (or a cluster of related documents) is a subset of the sentences in the original text (Mani, 2001). A number of techniques for choosing the right sentences to extract have been proposed in the literature, ranging from word counts (Luhn, 1958), key phrases (Edmundson, 1969), naive Bayesian classification (Kupiec et al., 1995), lexical chains (Barzilay and Elhadad, 1997), topic signatures (Hovy and Lin, 1999) and cluster centroids (Radev et al., 2000). Most techniques for sentence extraction compute a score for each individual sentence, although some recent work has started to pay attention to interactions between sentences. On the other hand, and particularly in multidocument summarization, some sentences may be redundant in the presence of others and such redundancy should lead to a lower score for each sentence proportional to the degree of overlap with other sentences in the summary. The Maximal Marginal Relevance (MMR) method (Carbonell and Goldstein, 1998) does just that.\n\nIn this paper, we are taking the idea of penalizing redundancy for multi-document summaries further. We want to explore existing techniques for identifying redundant information and using them for producing better summaries.\n\nAs in many areas in NLP, one of the biggest challenges in multi-document summarization is deciding on a way of calculating the similarity between two sentences or two groups of sentences. In extractive multi-document summarization, the goal is, on the one hand, to select the sentences which best represent the main point of the documents and, on the other, to pick sentences which do not overlap much with those sentences which have already been selected. To accomplish the task of sentence comparison, researchers have relied on stemming and counting n-gram similarity between two sentences. So, for example, if we have the following two sentences: \"The dogs go to the parks\" and \"The dog is going to the park,\" they would be nearly identical after stemming: \"the dog [be] go to the park,\" and any word overlap measure would be quite high (unigram cosine of .943).\n\nIn some ways",
            "score": 0.5413515510875929,
            "section_title": "The connection between text compression and multidocument summarization",
            "char_start_offset": 88,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 373,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 387,
                    "end": 404,
                    "matchedPaperCorpusId": "1177942"
                },
                {
                    "start": 436,
                    "end": 457,
                    "matchedPaperCorpusId": "5775833"
                },
                {
                    "start": 474,
                    "end": 502,
                    "matchedPaperCorpusId": "1584325"
                },
                {
                    "start": 564,
                    "end": 584,
                    "matchedPaperCorpusId": "1320"
                },
                {
                    "start": 1074,
                    "end": 1105,
                    "matchedPaperCorpusId": "4508623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "269983093",
            "title": "Which Information Matters? Dissecting Human-written Multi-document Summaries with Partial Information Decomposition",
            "text": "Understanding the nature of high-quality summaries is crucial to further improve the performance of multi-document summarization. We propose an approach to characterize human-written summaries using partial information decomposition, which decomposes the mutual information provided by all source documents into union, redundancy, synergy, and unique information. Our empirical analysis on different MDS datasets shows that there is a direct dependency between the number of sources and their contribution to the summary.",
            "score": 0.5413346563693769,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77294921875
        },
        {
            "corpus_id": "261660414",
            "title": "Unsupervised Multi-document Summarization with Holistic Inference",
            "text": "Multi-document summarization typically takes a collection of n documents D = {D (1) , . . . , D (n) } as inputs. Each document contains a varying number of sentences \n\nl i }, where l i is the number of sentences in the i-th document. Let S be the collection of all sentences, i.e. S = D (1) \u222a \u2022 \u2022 \u2022 \u222a D (n) . Additionally, let e i,j denote the similarity score between sentence s i and sentence s j . Our goal is to select a representative subset of sentences S \u2032 \u2282 S that maximizes the total importance of the subset while minimizing the redundancy within sentences in the subset at the same time.",
            "score": 0.5407939556473436,
            "section_title": "Problem Formulation",
            "char_start_offset": 7769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 598
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "229371202",
            "title": "Adaptive Summaries: A Personalized Concept-based Summarization Approach by Learning from Users' Feedback",
            "text": "We propose an interactive and personalized multi-document summarization approach using users' feedback. The selection or rejection of concepts, defining the importance of a concept, and the level of confidence engage users in making their desired summary. We empirically checked the validity of our approach on standard datasets using simulated user feedback. We observed that our framework shows promising results in terms of ROUGE score and also human evaluation. Results show that users' feedback can help them to find their desired information. As future work, we plan to include the reasons behind any action to optimize the system's performance.",
            "score": 0.540601068101373,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 22176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 651
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "256826757",
            "title": "PDSum: Prototype-driven Continuous Summarization of Evolving Multi-document Sets Stream",
            "text": "For the new task EMDS, we prepared strong baselines by adopting a centroid-based model [13,38] with a pretrained language model: DocCent and SentCent with document-and sentence-based centers, respectively, and their incremental versions IncDocCent, and IncSentCent. We also compared three popular unsupervised algorithms for multi-document summarization: the graph-based model Lexrank [9], the state-of-the-art extractive model Summpip [57], and the state-of-the-art abstractive model PRIMERA [47]. We fed each document set in a context to them so that they can only infer the temporally correlated documents to update their set summaries. See Appendix A.2 for details.",
            "score": 0.5405784102739608,
            "section_title": "Compared Algorithms.",
            "char_start_offset": 22425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 669
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 91,
                    "matchedPaperCorpusId": "1564849"
                },
                {
                    "start": 91,
                    "end": 94,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 493,
                    "end": 497,
                    "matchedPaperCorpusId": "247519084"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "174799390",
            "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
            "text": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",
            "score": 0.5402305007395213,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "7550466",
            "title": "iNeATS: Interactive Multi-Document Summarization",
            "text": "The goal of a good document summary is to provide a user with a presentation of the substance of a body of material in a coherent and concise form. Ideally, a summary would contain only the \"right\" amount of the interesting information and it would omit all the redundant and \"uninteresting\" material. The quality of the summary depends strongly on users' present need -a summary that focuses on one of several topics contained in the material may prove to be either very useful or completely useless depending on what users' interests are. \n\nAn automatic multi-document summarization system generally works by extracting relevant sentences from the documents and arranging them in a coherent order (McKeown et al., 2001;Over, 2001). The system has to make decisions on the summary's size, redundancy, and focus. Any of these decisions may have a significant impact on the quality of the output. We believe a system that directly involves the user in the summary generation process and adapts to her input will produce better summaries. Additionally, it has been shown that users are more satisfied with systems that visualize their decisions and give the user a sense of control over the process (Koenemann and Belkin, 1996). \n\nWe see three ways in which interactivity and visualization can be incorporated into the multidocument summarization process: \n\n1. give the user direct control over the summarization parameters such as size, redundancy, and focus of the summaries. \n\n2. support rapid browsing of the document set using the summary as the starting point and combining the multi-document summary with summaries for individual documents. \n\n3. incorporate alternative formats for organizing and displaying the summary, e.g., a set of news stories can be summarized by placing the stories on a world map based on the locations of the events described in the stories. \n\nIn this paper we describe iNeATS (Interactive NExt generation Text Summarization) which addresses these three directions. The iNeATS system is built on top of the NeATS multi-document summarization system. In the following section we give a brief overview of the NeATS system and in Section 3 describe the interactive version. \n\nNeATS (Lin and Hovy, 2002) is an extractionbased multi-document summarization system.",
            "score": 0.5401012973616747,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1872
                },
                {
                    "start": 1875,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2201
                },
                {
                    "start": 2204,
                    "end": 2289
                }
            ],
            "ref_mentions": [
                {
                    "start": 699,
                    "end": 721,
                    "matchedPaperCorpusId": "60913775"
                },
                {
                    "start": 721,
                    "end": 732,
                    "matchedPaperCorpusId": "59835348"
                },
                {
                    "start": 1197,
                    "end": 1225,
                    "matchedPaperCorpusId": "102257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81640625
        },
        {
            "corpus_id": "236477774",
            "title": "Improving Unsupervised Extractive Summarization with Facet-Aware Modeling",
            "text": "We introduce the datasets used in our experiments in this section. \n\nCNN/DM dataset contains 93k articles from CNN, and 220k articles from Daily Mail newspapers (Hermann et al., 2015). We use the nonanonymous version. Following (Zheng and Lapata, 2019), documents whose length of summaries are shorter than 30 tokens are filtered out. \n\nNYT dataset contains articles published by the New York Times between January 1, 1987and June 19, 2007(Li et al., 2016). The summaries are written by library scientists. Different from CNNDM, salient sentences distribute evenly in an article (Durrett et al., 2016). We filter out documents whose length of summaries are shorter than 50 tokens (Zheng and Lapata, 2019). \n\nMultiNews dataset consists of news articles and human-written summaries. The dataset is the first large-scale Multi-Documents Summarization (MDS) news dataset and comes from a diverse set of news sources (over 1500 sites) (Fabbri et al., 2019). \n\narXiv&PubMed datasets are two long document datasets of scientific publications from arXiv.org (113k) and PubMed (215k) (Cohan et al., 2018). The task is to generate the abstract from the paper body. \n\nWikiSum dataset is a multi-documents summarization dataset from Wikipedia (Liu et al., 2018). \n\nWe use the version provided by (Liu and Lapata, 2019a), which selects ranked top-40 paragraphs as input. For this dataset, we filter out documents whose summary length is less than 100 tokens. After the process, WikiSum test set contains 15,795 examples and the average length of summaries is 198. \n\nWikiHow dataset is a large-scale dataset of instructions from the online WikiHow.com website (Koupaee and Wang, 2018). The task is to generate the concatenated summary-sentences from the paragraphs.",
            "score": 0.5384716353818828,
            "section_title": "Datasets",
            "char_start_offset": 11000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 69,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1751
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 183,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 228,
                    "end": 252,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 439,
                    "end": 456,
                    "matchedPaperCorpusId": "3937849"
                },
                {
                    "start": 579,
                    "end": 601,
                    "matchedPaperCorpusId": "5125975"
                },
                {
                    "start": 680,
                    "end": 704,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 930,
                    "end": 951,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1075,
                    "end": 1095,
                    "matchedPaperCorpusId": "4894594"
                },
                {
                    "start": 1284,
                    "end": 1307,
                    "matchedPaperCorpusId": "170079112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.552734375
        },
        {
            "corpus_id": "53669855",
            "title": "Automatic Text Document Summarization using Semantic-based Analysis",
            "text": "According to (Ou, Khoo, & Goh, 2009) single document summarization can be defined as a \"process of representing the main content of one document\", and Multi-document Summarization also a process \"of representing the main content of a set of related documents on a topic, instead of only one document\". There are two possible approaches for multi-document summarization in the first approach, combine all documents in the single document then apply single document summary. The second possible approach generates a summary for each document, then combine all summary into one document later perform single document summarization on the combined summary to get a multi-document summary. Whereas, according to (Sood, 2013), It is important to note that concatenation of individual single document summaries may not necessarily produce a multi-document summary. Since, the issue with the later approach (first generate single summary and then again combine to generate a summary) is that in this process relative sentences position changes, and coherence lost, and this research gap opens new dimensions in research.",
            "score": 0.5383150572349463,
            "section_title": "Single and Multi-Document Summarization",
            "char_start_offset": 6165,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "44256387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "226283949",
            "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
            "text": "We conduct experiments on a latest released Multi-News dataset (Fabbri et al., 2019) and a standard DUC multi-document summarization dataset (Over et al., 2007). The Multi-News dataset contains 44,972 documents-summary pairs for training, 5,622 for development, and 5,622 for test. The number of source documents per summary ranges from 2 to 10. DUC-03 and DUC-04 contain 30 and 50 topics, respectively. Each topic has 10 documents paired with 4 different human-written references. CNN/Dailymail (Hermann et al., 2015;Nallapati et al., 2016) is a large scale single document summarization dataset, which contains 287,226 document-summary pairs for training, 13,368 for development and 11,490 for test.",
            "score": 0.5379100268886826,
            "section_title": "Datasets",
            "char_start_offset": 16447,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 518,
                    "end": 541,
                    "matchedPaperCorpusId": "8928715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88134765625
        },
        {
            "corpus_id": "7444",
            "title": "Columbia Newsblaster: Multilingual News Summarization on the Web",
            "text": "Our baseline approach to multilingual multi-document summarization is to apply our English-based summarization system, the Columbia Summarizer , to document clusters containing machinetranslated versions of non-English documents. The Columbia Summarizer routes to one of two multidocument summarization systems based on the similarity of the documents in the cluster. If the documents are highly similar, the Multigen summarization system (McKeown et al., 1999) is used. Multigen clusters sentences based on similarity, and then parses and fuses information from similar sentences to form a summary. The second summarization system used is DEMS, the Dissimilarity Engine for Multi-document Summarization , which uses a sentence extraction approach to summarization. The resulting summary is then run through a named entity recovery tool (Nenkova and McKeown, 2003), which repairs named entity references in the summary by making the first reference descriptive, and shortening subsequent reference mentions in the summary. Using an unmodified version of DEMS, summaries might contain sentences from translated documents which are not grammatically correct. The DEMS summarization system was modified to prefer choosing a sentence from an English article if there are sentences that express similar content in multiple languages. By setting different weight penalties we can take the quality of the translation system for a given language pair into",
            "score": 0.5375452209182873,
            "section_title": "Multilingual Summarization Baseline",
            "char_start_offset": 8732,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 461,
                    "matchedPaperCorpusId": "8115414"
                },
                {
                    "start": 837,
                    "end": 864,
                    "matchedPaperCorpusId": "6955819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75048828125
        },
        {
            "corpus_id": "2224274",
            "title": "Hierarchical Summarization: Scaling Up Multi-Document Summarization",
            "text": "The explosion in the number of documents on the Web necessitates automated approaches that organize and summarize large document collections on a complex topic. Existing methods for multi-document summarization (MDS) are designed to produce short summaries of 10-15 documents. 1 MDS systems do not scale to data sets ten times larger and proportionately longer summaries: they either cannot run on large input or produce a disorganized summary that is difficult to understand.\n\nWe present a novel MDS paradigm, hierarchical summarization, which operates on large document collections, creating summaries that organize the information coherently. It mimics how someone with a general interest in a complex topic would learn about it from an expert -first, the expert would provide an overview, and then more",
            "score": 0.537231931641685,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79931640625
        },
        {
            "corpus_id": "1005929",
            "title": "MUDOS-NG: Multi-document Summaries Using N-gram Graphs (Tech Report)",
            "text": "Since the late 50s and Luhn [Luhn, 1958] the information community has expressed its interest in summarizing texts. The domains of application of such methodologies range from news summarization [Wu and Liu, 2003, Barzilay and McKeown, 2005, Radev et al., 2005] to scientific article summarization [Teufel and Moens, 2002] and meeting summarization [Niekrasz et al., 2005, Erol et al., 2003]. \n\nSummarization has been defined as a reductive transformation of a given set of texts, usually described as a three-step process: selection of salient portions of text, aggregation of the information for various selected portions and abstraction of this information, and finally, presentation of the final summary text [Mani andBloedorn, 1999, Jones, 1999]. The summarization community aims to address major problems that arise during the summarization process. \n\n\u2022 How can one detect and select salient information to be included in the summary? Does the use of a query drive the information-selection task, and how? \n\n\u2022 How can one assure that the final summary does not contain redundant or repeated information, especially when multiple documents are used as input to the summarization process? \n\n\u2022 Can one develop methods that will function independently from the language of documents and on what degree can this be achieved? \n\nUp to date, many summarization systems have been developed and presented, especially within such endeavors as the Document Understanding Conferences (DUC) and Text Analysis Conferences (TAC)1 . The summarization community appears to have moved from single-text to multi-text input and has also reached such domains as opinion summarization and \"trend\" summarization, as in the case of NTCIR2 . However, different evaluations performed in recent years have proved that the multi-summarization task is highly complex and demanding, and that automatic summarizers have a long way to go to perform equally well to humans [Dang, 2005, Dang, 2006, Dang and Owczarzak, 2008]. It was recently shown [Genest et al., 2009] that the extractive approach has an upper limit of performance, which is lower when compared to the abstractive approach of humans.",
            "score": 0.5370268922422701,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 40,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 212,
                    "end": 240,
                    "matchedPaperCorpusId": "16188305"
                },
                {
                    "start": 298,
                    "end": 322,
                    "matchedPaperCorpusId": "261944073"
                },
                {
                    "start": 349,
                    "end": 371,
                    "matchedPaperCorpusId": "613148"
                },
                {
                    "start": 371,
                    "end": 391,
                    "matchedPaperCorpusId": "7668207"
                },
                {
                    "start": 713,
                    "end": 722,
                    "matchedPaperCorpusId": "9466119"
                },
                {
                    "start": 1956,
                    "end": 1968,
                    "matchedPaperCorpusId": "18691214"
                },
                {
                    "start": 1968,
                    "end": 1995,
                    "matchedPaperCorpusId": "26768540"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "218718706",
            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
            "text": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
            "score": 0.5368085336638815,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "272969413",
            "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications",
            "text": "Multi-document summarization often involves processing datasets with diverse formats, styles, and topics, which can pose challenges for LLMs [59]. LLMs need to effectively handle the complexity and heterogeneity of the input data, including variations in document length, structure, and quality [60] Addressing these challenges requires robust data preprocessing techniques, such as document segmentation, noise reduction, and format normalization [61]. Processing large volumes of documents for multi-document summarization can be computationally intensive, requiring significant resources and time [14]. Techniques such as model compression, distributed computing, and hardware acceleration can be employed to improve the scalability and efficiency of LLMs for multi-document summarization [56].",
            "score": 0.5364093119891168,
            "section_title": "Technical Considerations",
            "char_start_offset": 21188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 797
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "237347130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "239050558",
            "title": "Topic-Guided Abstractive Multi-Document Summarization",
            "text": "Multi-document summarization (MDS) is the task to create a fluent and concise summary for a collection of thematically related documents. Compared to single document summarization, it requires the ability to incorporate the perspective from multiple sources and therefore is arguably more challenging (Lin and Ng, 2019). Broadly, existing studies can be classified into two categories: extractive and abstractive. Extractive approaches directly select important sentences from the input documents, which is usually regarded as a sentence labeling (Nallapati et al., 2016;Zhang et al., 2018;Dong et al., 2018) or sentence ranking task (Narayan et al., 2018). By contrast, abstractive models typically use the natural language generation technology to produce a word-by-word summary. In general, extractive methods are more efficient and can avoid grammatical errors (Cui et al., 2020), while abstractive methods are more flexible and human-like because they can generate absent words (Lin and Ng, 2019).\n\nRecently, with the development of representation learning for NLP (Vaswani et al., 2017;Devlin et al., 2018) and large-scale datasets (Fabbri et al., 2019), some studies have achieved promising results on abstractive MDS (Liu and Lapata, 2019;Jin et al., 2020). Nevertheless, we found there are two limitations that have not been addressed by previous studies. First, some works simply concatenate multiple documents into a flat sequence and then apply single-document summarization approaches (Liu et al., 2018;Fabbri et al., 2019). However, this paradigm fails to consider the hierarchical document structures, which plays a key role in MDS task (Jin et al., 2020). Also, the concatenation operation inevitably produces a lengthy sequence, and encoding long texts for summarization is a challenge (Cohan et al., 2018).\n\nSecond, when dealing with multiple documents, a critical point is to learn the cross-document relations. Some studies address this problem by mining the co-occurrence words or entities (Wang et al., 2020a), which can hardly capture implicit associations due to the diverse language expressions. Some other studies (Jin et al., 2020;Liu and Lapata,",
            "score": 0.5359832991118967,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 301,
                    "end": 319,
                    "matchedPaperCorpusId": "198189946"
                },
                {
                    "start": 547,
                    "end": 571,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 571,
                    "end": 590,
                    "matchedPaperCorpusId": "52069762"
                },
                {
                    "start": 590,
                    "end": 608,
                    "matchedPaperCorpusId": "52843977"
                },
                {
                    "start": 634,
                    "end": 656,
                    "matchedPaperCorpusId": "215768182"
                },
                {
                    "start": 865,
                    "end": 883,
                    "matchedPaperCorpusId": "222310577"
                },
                {
                    "start": 983,
                    "end": 1001,
                    "matchedPaperCorpusId": "198189946"
                },
                {
                    "start": 1070,
                    "end": 1092,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1092,
                    "end": 1112,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1138,
                    "end": 1159,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1225,
                    "end": 1247,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1247,
                    "end": 1264,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1498,
                    "end": 1516,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1516,
                    "end": 1536,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1652,
                    "end": 1670,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1803,
                    "end": 1823,
                    "matchedPaperCorpusId": "4894594"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87255859375
        },
        {
            "corpus_id": "218674916",
            "title": "Multilayer network based comparative document analysis (MUNCoDA)",
            "text": "Multi-document summarization extracts information from multiple texts written about the same topic. The purpose of the method is to objectively explore the relationships between documents, identify key topics and compare documents according to the explored set of focal points. Contrary to classical multi-document summarization the extracted information is represented in a multiplex network which layers represent the network of the most informative word-pairs of the documents. The automated analysis of the network provides objective information about the topics and similarities of the documents. \n\nThe steps of the proposed method are shown in Fig. 1 . In the first step, the scope of the analysis needs to be identified that determines which documents are available for the study of the topic (e.g., sustainability reports, scientific papers, etc.). The next step is the preprocessing of the text that means the removal of stopwords, stemming, removal of short/long terms, and removal of frequent/infrequent terms (locally or globally) and term weighting and normalization. The informative word pairs are extracted from the word co-occurrences define the edges of the networks of the documents. Finally, the multiplex network is generated where the layers represent the networks of the documents. In the final step, the nodes and the layers are clustered based on their similarities.",
            "score": 0.5359052536255234,
            "section_title": "Method details",
            "char_start_offset": 439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1390
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "273606761",
            "title": "Discrete Bat Algorithm for Efficient Multi-Document Summarization",
            "text": "An automated procedure that creates a succinct and complete document from numerous documents is called Multi-Document Summarization. For summarizing the contents of multiple documents into a single concise document that holds the information of complete documents contents can be processed in three phases namely preprocessing, Computation of sentence score and Sentence similarity computation. In this section, all three phases are discussed in detail.",
            "score": 0.5352236416567224,
            "section_title": "Multi-document summarization",
            "char_start_offset": 15333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 453
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "248571519",
            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
            "text": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013). These papers primarily aim to address de-duplicating information and learning relationships between the different topics shared across documents however none of these architectures are built to deal with conflicting information.",
            "score": 0.5346926398026174,
            "section_title": "Multi Document Summarization",
            "char_start_offset": 8653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 231,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 241,
                    "end": 264,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 491,
                    "end": 508,
                    "matchedPaperCorpusId": "10112929"
                },
                {
                    "start": 1345,
                    "end": 1362,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1448,
                    "end": 1465,
                    "matchedPaperCorpusId": "218718706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9453125
        },
        {
            "corpus_id": "237304154",
            "title": "Alleviating Exposure Bias via Contrastive Learning for Abstractive Text Summarization",
            "text": "We evaluate our method on three datasets, i.e., XSum, CNNDM and Multi-News, since they represent different kinds of text summarization tasks: \n\n(1) Xsum contains news articles each associated with a summary. It is designed for single-document summarization and each summary contains one single sentence. (2) CNNDM is a collection of news articles accompanied with several highlights as their summaries. It is also used for singledocument summarization but the summaries often contain more than one sentence. (3) Multi-News is a multi-document summarization dataset, which consists of news articles and human-written summaries. In general, Multi-News summaries are longer than CNNDM summaries.",
            "score": 0.5346627954060517,
            "section_title": "Dataset",
            "char_start_offset": 7911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 144,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 692
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65380859375
        },
        {
            "corpus_id": "258865847",
            "title": "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering",
            "text": "demonstrated state-ofthe-art performance on a variety of multi-document downstream tasks, and outperformed underlying LMs and task-specific architectures. Such models are often pre-trained using a dataset where each instance is a set of related documents (e.g., news articles all discussing a specific event), which facilitates modeling of cross-text relationships. Existing multi-document pre-training objectives involve unmasking tokens in a document (Caciularu et al., 2021), or generating a salient masked sentence (Zhang et al., 2020;Xiao et al., 2022), encouraging the model to recover missing information using other documents. While successful, these models are either limited to classification tasks (Caciularu et al., 2021) or primarily designed for summarization (Zhang et al., 2020;Xiao et al., 2022).\n\nIn this work, we propose a novel pre-training objective that supports both short and long text generation, resulting in a versatile and general multidocument language model. In particular, we hypothesize that using questions and answers involving multiple documents can encourage the model to better learn and incorporate both fine-grained information (by asking questions about core information units in a specific sentence) as well as coarsegrained cross-document relationships required to generate a long text such as a summary. We show that this approach holds not only for summarization, but for other multi-document downstream tasks as well.\n\nDuring the pre-training of existing multidocument language models, the goal is to unmask spans (for encoder-only models) or generate masked textual spans (for encoder-decoder models) under a multi-document context. To that end, multiple concatenated sequences of related documents are fed during pre-training, thus requiring a large number of sets of related documents for an effective pre-training phase (Hoffmann et al., 2022). In a variety of existing multi-document benchmarks, such as multi-document summarization, only small to medium-scale document clusters are readily available. These are acquired either automatically with lexical similarity and retrieval (Fabbri et al., 2019) or semi-automatically (Gu et al., 2020), but generally,",
            "score": 0.5346490978662833,
            "section_title": "Pre-train by querying the context documents",
            "char_start_offset": 2146,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 519,
                    "end": 539,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 539,
                    "end": 557,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 774,
                    "end": 794,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 794,
                    "end": 812,
                    "matchedPaperCorpusId": "247519084"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "15795297",
            "title": "Multi-Document Abstractive Summarization Using ILP Based Multi-Sentence Compression",
            "text": "In multidocument summarization, all documents are not equally important; some documents contain more information on the main topics in the document set. Our first step estimates the importance of a document in the whole dataset using LexRank [Erkan and Radev, 2004], Pairwise Cosine Similarity and Overall Document Collection Similarity. Each sentence from the most important document are initialized into separate clusters. Thereafter, each sentence from the other documents are assigned to the cluster that has the highest similarity with the sentence. In the generation step, we first generate a word-graph structure from the sentences in each cluster and construct K shortest paths from the graph between the start and end nodes. We formulate a novel integer linear programming (ILP) problem that maximizes the information content and linguistic quality of the generated summary. Our ILP problem represents each of the K shortest paths as a binary variable. The coefficients of each variable in the objective function is obtained by combining the information score of the path and the linguistic quality score. We introduce several constraints into our ILP model. We ensure that only one sentence is generated from each cluster. Second, we avoid redundant sentences that carry the same or similar information from different clusters. The solution to the optimization problem decides the paths that would be included in the final abstractive summary. \n\nOn the DUC2004 and DUC2005 datasets, we demonstrate the effectiveness of our proposed method. Our proposed method outperforms not only some popular baselines but also the state-of-the-art extractive summarization systems. ROUGE scores [Lin, 2004] obtained by our system outperforms the best extractive summarizer on both the datasets. Our method also outperforms an abstractive summarizer based on multi-sentence compression [Filippova, 2010] when measured by ROUGE-2, ROUGE-L and ROUGE-SU4 scores. Further, manual evaluation by human judges shows that our technique produces summaries with acceptable linguistic quality and high informativeness.",
            "score": 0.5342803727967043,
            "section_title": "Introduction",
            "char_start_offset": 2158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 1691,
                    "end": 1702,
                    "matchedPaperCorpusId": "12299544"
                },
                {
                    "start": 1881,
                    "end": 1898,
                    "matchedPaperCorpusId": "14750088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "272969413",
            "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications",
            "text": "As the field of multi-document summarization evolves, new challenges are emerging. Multimodal summarization, involving documents with text, images, and other media types, requires models to integrate information across different modalities coherently. \n\nMultilingual and cross-lingual summarization present unique challenges in an increasingly globalized world [68]. Developing comprehensive evaluation metrics for summarization quality remains an open challenge [69]. Current metrics often fail to capture nuanced aspects of summary quality, such as coherence, relevance, and faithfulness to the original documents. \n\nThe \"black box\" nature of LLMs raises concerns about explainability and transparency in the summarization process [70]. As these models are increasingly used in critical applications, there is a growing need for methods to interpret and explain their decision-making processes. \n\nAddressing these challenges will require ongoing research and collaboration across disciplines. As we continue to advance the capabilities of long-context LLMs for multidocument summarization, it is crucial to remain mindful of these considerations to ensure the development of effective, ethical, and reliable summarization systems.",
            "score": 0.5333491844663835,
            "section_title": "Emerging Challenges",
            "char_start_offset": 23903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1232
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 365,
                    "matchedPaperCorpusId": "49310285"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "5981909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "269587755",
            "title": "Understanding Position Bias Effects on Fairness in Social Multi-Document Summarization",
            "text": "As the use of natural language processing models gets more prevalent in various industries, academic and social settings, it is imperative that we assess not only the quality of these models but also their fairness when exposed to data originating from diverse social groups (Czarnowska et al., 2021).Text summarization models, in particular, facilitate the processing of large collections of a wide variety of text data by distilling documents into short, concise, and informative summaries while preserving the most relevant points from the source document (Nallapati et al., 2017;Zhang et al., 2018;Liu and Lapata, 2019).Multi-document summarization (MDS) is the task of generating a coherent summary from a set of input documents, usually centered around a topic, as opposed to single document summarization (SDS) which takes one document as input.The input in MDS consists of multiple documents, that may have been written by distinct users, varying in linguistic diversity, styles, or dialects.\n\nMDS can be of type extractive, where the models extract the salient points directly from the source document to form the summary, or of type abstractive where the models generate summaries by rewriting salient information using novel words or phrases.In both cases, the resulting summary should be of good quality in terms of informativeness, coherence and relevance to the source document.At the same time, a good summary should be unbiased and should reflect the diversity of thoughts and perspectives present in the source documents.\n\nThe notion of fairness describes equal or fair treatment without favoritism or discrimination.However, plenty of evidence suggests intrinsic societal biases in language models (Bolukbasi et al., 2016;Bommasani et al., 2021;Deas et al., 2023).More specific to the task of summarization, fairness is measured by the ability of algorithms to capture the peculiarity in all represented groups (Shandilya et al., 2018;Dash et al., 2019;Keswani and Celis, 2021;Olabisi et al., 2022;Ladhak et al., 2023).",
            "score": 0.5332531224068208,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 852
                },
                {
                    "start": 852,
                    "end": 1000
                },
                {
                    "start": 1002,
                    "end": 1253
                },
                {
                    "start": 1253,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1538
                },
                {
                    "start": 1540,
                    "end": 1634
                },
                {
                    "start": 1634,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 300,
                    "matchedPaperCorpusId": "235658325"
                },
                {
                    "start": 559,
                    "end": 583,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1716,
                    "end": 1740,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "16694744",
            "title": "MultiLing 2015: Multilingual Summarization of Single and Multi-Documents, On-line Fora, and Call-center Conversations",
            "text": "This multilingual multi-document summarization (MMS) (Giannakopoulos, 2015) task aims to evaluate the application of partially or fully language-independent summarization algorithms. Each system participating in the task was called upon to provide summaries for a range of different languages, based on corresponding language-specific corpora. Systems were to summarize texts in at least two of the ten different languages: Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian, Spanish. \n\nThe task aims at the real problem of summarizing news topics, parts of which may be described or may happen in different moments in time. We consider, similarly to previous MultiLing efforts (Giannakopoulos et al., 2011;Li et al., 2013) that news topics can be seen as event sequences: Definition 1. An event sequence is a set of atomic (self-sufficient) event descriptions, sequenced in time, that share main actors, location of occurrence or some other important factor. Event sequences may refer to topics such as a natural disaster, a crime investigation, a set of negotiations focused on a single political issue, a sports event. \n\nThe multi-document summarization task required participants to generate a fluent and representative summary from the set of documents describing an event sequence. The language of each document set belonged to one of the aforementioned set of languages and all the documents in a set were of the same language. The output summary was expected to be in the same language and between 240 and 250 words, with the exception of Chinese, where the output summary size was expected to be 333 characters (i.e., 1000 bytes in UTF-8 encoding). \n\nThe task corpus is based on a set of WikiNews English news articles comprising 15 topics, each containing ten documents. Each English document was translated into the other nine languages to create sentence-parallel translations. (Li et al., 2013;Elhadad et al., 2013).",
            "score": 0.5331871206150317,
            "section_title": "Task Description",
            "char_start_offset": 4510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 728,
                    "end": 744,
                    "matchedPaperCorpusId": "14291870"
                },
                {
                    "start": 1911,
                    "end": 1928,
                    "matchedPaperCorpusId": "14291870"
                },
                {
                    "start": 1928,
                    "end": 1949,
                    "matchedPaperCorpusId": "7376440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6455078125
        },
        {
            "corpus_id": "270000888",
            "title": "Advancing automatic text summarization: Unleashing enhanced binary multi-objective grey wolf optimization with mutation",
            "text": "Multi-document extractive text summarization holds great potential in various real-world scenarios.In the media industry, it can be invaluable for extracting pertinent information from multiple documents covering a specific topic.For instance, in the aftermath of a national disaster, where there are numerous digital news documents reporting on the event, extractive text summarization can compile a concise summary of the key points across all articles.Similarly, in the medical field, analyzing the medical records of patients from a particular group can reveal patterns and trends.Extractive text summarization can efficiently summarize these records, aiding in identifying important insights for further study and analysis.In another realworld case study of financial news summarization, our proposed scheme begins with preprocessing raw articles, followed by transforming sentences into numerical vectors and documents into averaged vectors.The process then optimizes sentence selection for summary generation using our BMOGWO-M algorithm.Finally, the summarized news articles are presented to users, aiding them in making informed financial decisions.This demonstrates the scheme's effectiveness in providing clear and actionable summaries of financial news.",
            "score": 0.5331814090908736,
            "section_title": "A real world case study",
            "char_start_offset": 38157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 455
                },
                {
                    "start": 455,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1045
                },
                {
                    "start": 1045,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1265
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "678258",
            "title": "Exploring Content Models for Multi-Document Summarization",
            "text": "In this paper we have presented an exploration of content models for multi-document summarization and demonstrated that the use of structured topic models can benefit summarization quality as measured by automatic and manual metrics.",
            "score": 0.5326705194146916,
            "section_title": "Conclusion",
            "char_start_offset": 22042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81982421875
        },
        {
            "corpus_id": "266374529",
            "title": "Survey on Multi-Document Summarization: Systematic Literature Review",
            "text": "A Survey on Multi-Document Summarization [30] The paper handles three components of multidocument summarization. \n\n1. Centroid-based summarization 2. Information fusion algorithm 3. Sentence compression by using two algorithms. \n\nPaper [30] only considered 3 component of multi-document summarization whereas this research discusses and compare multiple components of multi-document summarization in detail. Also, compare the survey based on multiple objects.",
            "score": 0.5325812981673115,
            "section_title": "Year Survey Topic Methods Enhancement in our paper 2004",
            "char_start_offset": 6129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 115,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 459
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 45,
                    "matchedPaperCorpusId": "212806923"
                },
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "212806923"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "15567696",
            "title": "Unsupervised Text Recap Extraction for TV Series",
            "text": "Text summarization is widely explored in the news domain (Hong and Nenkova, 2014;McKeown, 2005). Generally, there are two approaches: extractive and abstractive summarization. \n\nExtractive summarization forms a summary by choosing the most representative sentences from the original corpus. The early system LEAD (Wasson, 1998) was pioneering work. It selected leading text of the document as the summary, and was applied in news searching to help online customers focus their queries on the beginning of news documents. He et al. (2012) assumed that summarization should consist of sentences that could best reconstruct the original document. They modeled relationship among sentences by forming an optimization problem. Moreover, Sipos et al. (2012) and Lin and Bilmes (2010) studied multi-document summarization using coverage-based methods. Among them, Lin and Bilmes (2010) proposed to approximate the optimal solution of a class of functions by exploiting submodularity. \n\nAbstractive summarization automatically create new sentences. For example, compared with the sentence-level analysis in extractive summarization, Bing et al. (2015) explored fine-grained syntactic units, i.e. noun/verb phrases, to represent concepts in input documents. The informative phrases were then used to generate sentences. \n\nIn this paper, we generalize the idea of text summarization to text recap extraction. Instead of summarizing a given document or collection, our model emphasizes plot contingency with the next episode.",
            "score": 0.5321358858723143,
            "section_title": "Generic Text Summarization Alogrithms",
            "char_start_offset": 5708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 81,
                    "matchedPaperCorpusId": "2342155"
                },
                {
                    "start": 81,
                    "end": 95,
                    "matchedPaperCorpusId": "34668305"
                },
                {
                    "start": 521,
                    "end": 537,
                    "matchedPaperCorpusId": "16471324"
                },
                {
                    "start": 732,
                    "end": 751,
                    "matchedPaperCorpusId": "1457503"
                },
                {
                    "start": 756,
                    "end": 777,
                    "matchedPaperCorpusId": "1803710"
                },
                {
                    "start": 857,
                    "end": 878,
                    "matchedPaperCorpusId": "1803710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "234480545",
            "title": "A Novel Framework for Multi-Document Temporal Summarization (MDTS)",
            "text": "A novel framework for multi-document summarization is presented in this paper. The given documents are preprocessed for annotation. The event, times, and event-time relations are extracted and stored in a database. Depending on the number of events and times present in the sentences. Sentences scores are ranked based on their scores. Redundant sentences are eliminated, and top ranked sentences are selected based to generate summary. DUC 2006 and DUC 2007 datasets are used for experimentation. The generated summaries of various modes are evaluated using the ROUGE tool, in which the recall and precision scores are observed and tabulated. The results of the proposed model have resulted high recall, precision and F score when compared with the existing methods like PSOS, CSOS, MDSCSA methods. The proposed method has retrieved relevant and non-redundant sentences into the summaries which resulted in the highest scores when compared with the existing methods. This framework can be applied to legal documents and medical domain data in future.",
            "score": 0.5317177237818543,
            "section_title": "4-Conclusion",
            "char_start_offset": 12496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77294921875
        },
        {
            "corpus_id": "222133287",
            "title": "Corpora Evaluation and System Bias detection in Multi Document Summarization",
            "text": "Multi-document summarization (MDS) is the task of reflecting key points from any set of documents into a concise text paragraph. In the past, it has been used to aggregate news, tweets, product reviews, etc. from various sources. Owing to no standard definition of the task, we encounter a plethora of datasets with varying levels of overlap and conflict between participating documents. There is also no standard regarding what constitutes summary information in MDS. Adding to the challenge is the fact that new systems report results on a set of chosen datasets, which might not correlate with their performance on the other datasets. In this paper, we study this heterogeneous task with the help of a few widely used MDS corpora and a suite of state-of-theart models. We make an attempt to quantify the quality of summarization corpus and prescribe a list of points to consider while proposing a new MDS corpus. Next, we analyze the reason behind the absence of an MDS system which achieves superior performance across all corpora. We then observe the extent to which system metrics are influenced, and bias is propagated due to corpus properties. The scripts to reproduce the experiments in this work are available at https://github.com/LCS2-IIITD/summarization_bias.git",
            "score": 0.5316603323421203,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "11989149",
            "title": "Sentence Position revisited: A robust light-weight Update Summarization \u2018baseline\u2019 Algorithm",
            "text": "In recent years, at the Document Understanding Conferences (DUC 1 ), Text Summarization research evolved through task focused evaluations ranging from 'generic single-document summarization' to 'query-focused multi-document summarization (QFMDS)'. The QFMDS task models the realworld complex question answering task wherein, given a topic and a set of 25 relevant documents, the task is to synthesize a fluent, well-organized 250word summary of the documents that answers the question(s) in the topic statement. Recent focus in the community has been towards query-focused update-summarization task at DUC and the Text Analysis Conference (TAC2 ). The update task was to produce short (~100 words) multi-document update summaries of newswire articles under the assumption that the user has already read a set of earlier articles. The purpose of each update summary will be to inform the reader of new information about a particular topic. \n\nThe rest of the paper is organized as follows. In Section 2, we describe a Sub-optimal Position Policy (SPP) based on Pyramid Annotated Data, then we derive a simple algorithm for summarization based on the SPP in Section 3, and show evaluation results. Next, in Section 4, we explain the current baselines and evaluation for Multi-Document Summarization and finally in Section 5, we discuss the need for an older baseline in the current context of the short summary task of update summarization.",
            "score": 0.5313260766018197,
            "section_title": "Introduction",
            "char_start_offset": 1890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1437
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61474609375
        },
        {
            "corpus_id": "14651945",
            "title": "Improving Multi-Document Summarization via Text Classification",
            "text": "Developed so far, multi-document summarization has reached its bottleneck due to the lack of sufficient training data and diverse categories of documents. Text classification just makes up for these deficiencies. In this paper, we propose a novel summarization system called TCSum, which leverages plentiful text classification data to improve the performance of multi-document summarization. TCSum projects documents onto distributed representations which act as a bridge between text classification and summarization. It also utilizes the classification results to produce summaries of different styles. Extensive experiments on DUC generic multi-document summarization datasets show that, TCSum can achieve the state-of-the-art performance without using any hand-crafted features and has the capability to catch the variations of summary styles with respect to different text categories.",
            "score": 0.5311053872279341,
            "section_title": "abstract",
            "char_start_offset": 4,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "257496469",
            "title": "Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization",
            "text": "Multi-document summarization (MDS) aims to automatically generate a concise and informative summary for a cluster of topically related source documents (Ma et al. 2020;Radev, Hovy, and McKeown 2002). It has a wide range of applications such as creating news digests (Fabbri et al. 2019), product review summaries (Gerani et al. 2014), and summaries for scientific literature (Moro et al. 2022;Otmakhova et al. 2022). Our work targets abstractive MDS, which generates summaries with words that do not necessarily come from the source documents, resembling the summarization process of human beings. \n\nState-of-the-art text summarization models use pretrained language models (PLMs) including both generalpurpose PLMs for text generation (Beltagy, Peters, and Cohan 2020;Lewis et al. 2020) and PLMs designed for text summarization (Zhang et al. 2020a;Xiao et al. 2022). When applied to the abstractive MDS task, these models take a flat concatenation of the (multiple) source documents, which may not capture cross-document relationships such as contradiction, redundancy, or complementary information very Figure 1: The structure of the heterogeneous graph given three documents in a document cluster: The orange triangles denote document nodes d, the blue quadrates denote sentence nodes s, the green circles denote word nodes w, and the line (or curve) segments between nodes denote edges. A detailed description of the graph is in the Preliminaries. \n\nwell (Radev 2000). Ma et al. (2020) argue that explicit modeling of cross-document relationships can potentially improve the quality of summaries. Following this, several recent studies (Li et al. 2020;Jin, Wang, and Wan 2020;Cui and Hu 2021) explore graphs to model source documents to improve abstractive MDS. However, these graphs are homogeneous in that the nodes or edges are not distinguished for different semantic units (e.g., words, sentences, and paragraphs) in the encoding process. This means these MDS models cannot capture the diverse cross-document relationships among different types of semantic units.",
            "score": 0.5297501995535554,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1451
                },
                {
                    "start": 1454,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 168,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 266,
                    "end": 286,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 313,
                    "end": 333,
                    "matchedPaperCorpusId": "2767900"
                },
                {
                    "start": 393,
                    "end": 415,
                    "matchedPaperCorpusId": "248780431"
                },
                {
                    "start": 829,
                    "end": 849,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 849,
                    "end": 866,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 1459,
                    "end": 1471,
                    "matchedPaperCorpusId": "10103200"
                },
                {
                    "start": 1473,
                    "end": 1489,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 1640,
                    "end": 1656,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1656,
                    "end": 1680,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1680,
                    "end": 1695,
                    "matchedPaperCorpusId": "239050558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "paperId": "73c751a6096a169666f5cc63a7a49b13c840df34",
            "corpusId": 267662945,
            "title": "Chain-of-event prompting for multi-document summarization by large language models",
            "venue": "International Journal of Web Information Systems",
            "year": 2024,
            "referenceCount": 33,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1108/ijwis-12-2023-0249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/ijwis-12-2023-0249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284099871",
                    "name": "Songlin Bao"
                },
                {
                    "authorId": "2222616638",
                    "name": "Tiantian Li"
                },
                {
                    "authorId": "2222186730",
                    "name": "Bin Cao"
                }
            ],
            "abstract": "\nPurpose\nIn the era of big data, various industries are generating large amounts of text data every day. Simplifying and summarizing these data can effectively serve users and improve efficiency. Recently, zero-shot prompting in large language models (LLMs) has demonstrated remarkable performance on various language tasks. However, generating a very \u201cconcise\u201d multi-document summary is a difficult task for it. When conciseness is specified in the zero-shot prompting, the generated multi-document summary still contains some unimportant information, even with the few-shot prompting. This paper aims to propose a LLMs prompting for multi-document summarization task.\n\n\nDesign/methodology/approach\nTo overcome this challenge, the authors propose chain-of-event (CoE) prompting for multi-document summarization (MDS) task. In this prompting, the authors take events as the center and propose a four-step summary reasoning process: specific event extraction; event abstraction and generalization; common event statistics; and summary generation. To further improve the performance of LLMs, the authors extend CoE prompting with the example of summary reasoning.\n\n\nFindings\nSummaries generated by CoE prompting are more abstractive, concise and accurate. The authors evaluate the authors\u2019 proposed prompting on two data sets. The experimental results over ChatGLM2-6b show that the authors\u2019 proposed CoE prompting consistently outperforms other typical promptings across all data sets.\n\n\nOriginality/value\nThis paper proposes CoE prompting to solve MDS tasks by the LLMs. CoE prompting can not only identify the key events but also ensure the conciseness of the summary. By this method, users can access the most relevant and important information quickly, improving their decision-making processes.\n",
            "corpus_id": "267662945",
            "text": "\nPurpose\nIn the era of big data, various industries are generating large amounts of text data every day. Simplifying and summarizing these data can effectively serve users and improve efficiency. Recently, zero-shot prompting in large language models (LLMs) has demonstrated remarkable performance on various language tasks. However, generating a very \u201cconcise\u201d multi-document summary is a difficult task for it. When conciseness is specified in the zero-shot prompting, the generated multi-document summary still contains some unimportant information, even with the few-shot prompting. This paper aims to propose a LLMs prompting for multi-document summarization task.\n\n\nDesign/methodology/approach\nTo overcome this challenge, the authors propose chain-of-event (CoE) prompting for multi-document summarization (MDS) task. In this prompting, the authors take events as the center and propose a four-step summary reasoning process: specific event extraction; event abstraction and generalization; common event statistics; and summary generation. To further improve the performance of LLMs, the authors extend CoE prompting with the example of summary reasoning.\n\n\nFindings\nSummaries generated by CoE prompting are more abstractive, concise and accurate. The authors evaluate the authors\u2019 proposed prompting on two data sets. The experimental results over ChatGLM2-6b show that the authors\u2019 proposed CoE prompting consistently outperforms other typical promptings across all data sets.\n\n\nOriginality/value\nThis paper proposes CoE prompting to solve MDS tasks by the LLMs. CoE prompting can not only identify the key events but also ensure the conciseness of the summary. By this method, users can access the most relevant and important information quickly, improving their decision-making processes.\n",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.84814453125
        },
        {
            "paperId": "1221aa62d85770e1712c98fbe2fbaf8bad512861",
            "corpusId": 262045067,
            "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "referenceCount": 50,
            "citationCount": 32,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.09369",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.09369, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1420116116",
                    "name": "Kung-Hsiang Huang"
                },
                {
                    "authorId": "46180754",
                    "name": "Philippe Laban"
                },
                {
                    "authorId": "22281632",
                    "name": "A. R. Fabbri"
                },
                {
                    "authorId": "3466801",
                    "name": "Prafulla Kumar Choubey"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "30340989",
                    "name": "Chien-Sheng Wu"
                }
            ],
            "abstract": "Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40% of the diverse information on average.",
            "corpus_id": "262045067",
            "text": "Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40% of the diverse information on average.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.703125
        },
        {
            "paperId": "c747c6ce74bf80f1c0af97fb3aeb37875415d238",
            "corpusId": 258841548,
            "title": "Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "referenceCount": 62,
            "citationCount": 22,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.13693",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "31860505",
                    "name": "Lucy Lu Wang"
                },
                {
                    "authorId": "1881453937",
                    "name": "Yulia Otmakhova"
                },
                {
                    "authorId": "48727916",
                    "name": "Jay DeYoung"
                },
                {
                    "authorId": "153574160",
                    "name": "Thinh Hung Truong"
                },
                {
                    "authorId": "2003338023",
                    "name": "Bailey Kuehl"
                },
                {
                    "authorId": "2203427167",
                    "name": "Erin Bransom"
                },
                {
                    "authorId": "2111879324",
                    "name": "Byron Wallace"
                }
            ],
            "abstract": "Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics when they are proposed. Therefore, we introduce a dataset of human-assessed summary quality facets and pairwise preferences to encourage and support the development of better automated evaluation methods for literature review MDS. We take advantage of community submissions to the Multi-document Summarization for Literature Review (MSLR) shared task to compile a diverse and representative sample of generated summaries. We analyze how automated summarization evaluation metrics correlate with lexical features of generated summaries, to other automated metrics including several we propose in this work, and to aspects of human-assessed summary quality. We find that not only do automated metrics fail to capture aspects of quality as assessed by humans, in many cases the system rankings produced by these metrics are anti-correlated with rankings according to human annotators.",
            "corpus_id": "258841548",
            "text": "Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics when they are proposed. Therefore, we introduce a dataset of human-assessed summary quality facets and pairwise preferences to encourage and support the development of better automated evaluation methods for literature review MDS. We take advantage of community submissions to the Multi-document Summarization for Literature Review (MSLR) shared task to compile a diverse and representative sample of generated summaries. We analyze how automated summarization evaluation metrics correlate with lexical features of generated summaries, to other automated metrics including several we propose in this work, and to aspects of human-assessed summary quality. We find that not only do automated metrics fail to capture aspects of quality as assessed by humans, in many cases the system rankings produced by these metrics are anti-correlated with rankings according to human annotators.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.67919921875
        },
        {
            "paperId": "d25121da56c9050137800c69520111b30201d1ed",
            "corpusId": 233231380,
            "title": "MS\u02c62: Multi-Document Summarization of Medical Studies",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "referenceCount": 92,
            "citationCount": 113,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.emnlp-main.594.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.06486, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48727916",
                    "name": "Jay DeYoung"
                },
                {
                    "authorId": "46181066",
                    "name": "Iz Beltagy"
                },
                {
                    "authorId": "15292561",
                    "name": "Madeleine van Zuylen"
                },
                {
                    "authorId": "2003338023",
                    "name": "Bailey Kuehl"
                },
                {
                    "authorId": "31860505",
                    "name": "Lucy Lu Wang"
                }
            ],
            "abstract": "To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system\u2019s generated summaries. Data and models are available at https://github.com/allenai/ms2.",
            "corpus_id": "233231380",
            "text": "To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system\u2019s generated summaries. Data and models are available at https://github.com/allenai/ms2.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7236328125
        },
        {
            "paperId": "90812d2cd9e1697821c4b4014f44f5bb1873bffe",
            "corpusId": 266164114,
            "title": "Promoting Topic Coherence and Inter-Document Consorts in Multi-Document Summarization via Simplicial Complex and Sheaf Graph",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 61,
            "citationCount": 9,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.emnlp-main.133.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.emnlp-main.133?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.emnlp-main.133, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1986291522",
                    "name": "Yash Kumar Atri"
                },
                {
                    "authorId": "2273535747",
                    "name": "Arun Iyer"
                },
                {
                    "authorId": "2273556645",
                    "name": "Tanmoy Chakraborty"
                },
                {
                    "authorId": "9313418",
                    "name": "Vikram Goyal"
                }
            ],
            "abstract": "Multi-document Summarization (MDS) characterizes compressing information from multiple source documents to its succinct summary. An ideal summary should encompass all topics and accurately model cross-document relations expounded upon in the source documents. However, existing systems either impose constraints on the length of tokens during the encoding or falter in capturing the intricate cross-document relationships. These limitations impel the systems to produce summaries that are non-factual and unfaithful, thereby imparting an unfair comprehension of the topic to the readers. To counter these limitations and promote the information equivalence between the source document and generated summary, we propose FABRIC , a novel encoder-decoder model that uses pre-trained BART to comprehensively analyze linguistic nuances, simplicial complex layer to apprehend inherent properties that transcend pairwise associations and sheaf graph attention to effectively capture the het-erophilic properties. We benchmark FABRIC with eleven baselines over four widely-used MDS datasets \u2013 Multinews, CQASumm, DUC and Opinosis, and show that FABRIC achieves consistent performance improvement across all the evaluation metrics (syntactical, semantical and faithfulness). We corroborate these improvements further through qualitative human evaluation. The source code is available at https://github.com/LCS2-IIITD/FABRIC",
            "corpus_id": "266164114",
            "text": "Multi-document Summarization (MDS) characterizes compressing information from multiple source documents to its succinct summary. An ideal summary should encompass all topics and accurately model cross-document relations expounded upon in the source documents. However, existing systems either impose constraints on the length of tokens during the encoding or falter in capturing the intricate cross-document relationships. These limitations impel the systems to produce summaries that are non-factual and unfaithful, thereby imparting an unfair comprehension of the topic to the readers. To counter these limitations and promote the information equivalence between the source document and generated summary, we propose FABRIC , a novel encoder-decoder model that uses pre-trained BART to comprehensively analyze linguistic nuances, simplicial complex layer to apprehend inherent properties that transcend pairwise associations and sheaf graph attention to effectively capture the het-erophilic properties. We benchmark FABRIC with eleven baselines over four widely-used MDS datasets \u2013 Multinews, CQASumm, DUC and Opinosis, and show that FABRIC achieves consistent performance improvement across all the evaluation metrics (syntactical, semantical and faithfulness). We corroborate these improvements further through qualitative human evaluation. The source code is available at https://github.com/LCS2-IIITD/FABRIC",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.82861328125
        },
        {
            "paperId": "35dee171e843f07d13622f57e475fcbcab7aad31",
            "corpusId": 262185901,
            "title": "Multi-Document Summarization Using Selective Attention Span and Reinforcement Learning",
            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
            "year": 2023,
            "referenceCount": 81,
            "citationCount": 8,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASLP.2023.3316459?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASLP.2023.3316459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1986291522",
                    "name": "Yash Kumar Atri"
                },
                {
                    "authorId": "1744939",
                    "name": "Vikram Goyal"
                },
                {
                    "authorId": "144054829",
                    "name": "Tanmoy Chakraborty"
                }
            ],
            "abstract": "Abstractive text summarization systems using recently improved RNN-based sequence-to-sequence architecture have shown great promise for single-document summarization. However, such neural models fail to perpetuate the performance in the multi-document summarization setting owing to the long-range dependencies within the documents, overlapping/contradicting facts and extrinsic model hallucinations. These shortcomings augment the model to generate inconsistent, repetitive and non-factual summaries. In this work, we introduce <monospace>REISA</monospace>, a sequence-to-sequence model with a novel <italic>reinforced selective attention span</italic> that attends over the input and recalibrates the local attention weights to focus on important segments while generating output at each time step. <monospace>REISA</monospace> utilizes a reinforcement learning-based policy gradient algorithm to reward the model and formulate attention distributions over the encoder input. We further benchmark <monospace>REISA</monospace> on two widely-used multi-document summarization corpora \u2013 Multinews and CQASumm, and observe an improvement of <inline-formula><tex-math notation=\"LaTeX\">$+2.91$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$+6.64$</tex-math></inline-formula> ROUGE-L scores, respectively. The qualitative analyses on semantic similarity by BERTScore, faithfulness by question-answer evaluation and human evaluation show significant improvement over the baseline-generated summaries.",
            "corpus_id": "262185901",
            "text": "Abstractive text summarization systems using recently improved RNN-based sequence-to-sequence architecture have shown great promise for single-document summarization. However, such neural models fail to perpetuate the performance in the multi-document summarization setting owing to the long-range dependencies within the documents, overlapping/contradicting facts and extrinsic model hallucinations. These shortcomings augment the model to generate inconsistent, repetitive and non-factual summaries. In this work, we introduce <monospace>REISA</monospace>, a sequence-to-sequence model with a novel <italic>reinforced selective attention span</italic> that attends over the input and recalibrates the local attention weights to focus on important segments while generating output at each time step. <monospace>REISA</monospace> utilizes a reinforcement learning-based policy gradient algorithm to reward the model and formulate attention distributions over the encoder input. We further benchmark <monospace>REISA</monospace> on two widely-used multi-document summarization corpora \u2013 Multinews and CQASumm, and observe an improvement of <inline-formula><tex-math notation=\"LaTeX\">$+2.91$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$+6.64$</tex-math></inline-formula> ROUGE-L scores, respectively. The qualitative analyses on semantic similarity by BERTScore, faithfulness by question-answer evaluation and human evaluation show significant improvement over the baseline-generated summaries.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.85205078125
        },
        {
            "paperId": "f99401ed0b8c0b7f2a150150e3c17d2d9b097587",
            "corpusId": 258688045,
            "title": "Review on Query-focused Multi-document Summarization (QMDS) with Comparative Analysis",
            "venue": "ACM Computing Surveys",
            "year": 2023,
            "referenceCount": 240,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3597299?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3597299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2217264091",
                    "name": "Prasenjeet Roy"
                },
                {
                    "authorId": "2205181100",
                    "name": "Suman Kundu"
                }
            ],
            "abstract": "The problem of query-focused multi-document summarization (QMDS) is to generate a summary from multiple source documents on identical/similar topics based on the query submitted by the users. This article provides a systematic review of the literature of QMDS. The research works are classified into six major categories based on the summarization methodologies used. Different techniques used for finding query-relevant summaries for different algorithms under each of the six major groups are reported. Further, 17 evaluation metrics used for evaluating algorithms for text summaries against the human-curated summaries are compiled here in this article. Extensive experiments are performed on eight different datasets. Comparative results of nine methodologies, each representing one of the six different groups, are presented. Seven different evaluation metrics are used in the comparative study. It is observed that DL- and ML-based QMDS methods perform. better in comparison to the other methods.",
            "corpus_id": "258688045",
            "text": "The problem of query-focused multi-document summarization (QMDS) is to generate a summary from multiple source documents on identical/similar topics based on the query submitted by the users. This article provides a systematic review of the literature of QMDS. The research works are classified into six major categories based on the summarization methodologies used. Different techniques used for finding query-relevant summaries for different algorithms under each of the six major groups are reported. Further, 17 evaluation metrics used for evaluating algorithms for text summaries against the human-curated summaries are compiled here in this article. Extensive experiments are performed on eight different datasets. Comparative results of nine methodologies, each representing one of the six different groups, are presented. Seven different evaluation metrics are used in the comparative study. It is observed that DL- and ML-based QMDS methods perform. better in comparison to the other methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8642578125
        },
        {
            "paperId": "eafb78056fe05aea600cf03818716d7c2761a651",
            "corpusId": 258231820,
            "title": "Grapharizer: A Graph-Based Technique for Extractive Multi-Document Summarization",
            "venue": "Electronics",
            "year": 2023,
            "referenceCount": 48,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/12/8/1895/pdf?version=1681740423",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics12081895?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics12081895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "70642020",
                    "name": "Zakia Jalil"
                },
                {
                    "authorId": "2128893061",
                    "name": "Muhammad Nasir"
                },
                {
                    "authorId": "2268000",
                    "name": "M. Alazab"
                },
                {
                    "authorId": "2152458353",
                    "name": "Jamal Nasir"
                },
                {
                    "authorId": "1742471",
                    "name": "Tehmina Amjad"
                },
                {
                    "authorId": "2078200186",
                    "name": "Abdullah Alqammaz"
                }
            ],
            "abstract": "In the age of big data, there is increasing growth of data on the Internet. It becomes frustrating for users to locate the desired data. Therefore, text summarization emerges as a solution to this problem. It summarizes and presents the users with the gist of the provided documents. However, summarizer systems face challenges, such as poor grammaticality, missing important information, and redundancy, particularly in multi-document summarization. This study involves the development of a graph-based extractive generic MDS technique, named Grapharizer (GRAPH-based summARIZER), focusing on resolving these challenges. Grapharizer addresses the grammaticality problems of the summary using lemmatization during pre-processing. Furthermore, synonym mapping, multi-word expression mapping, and anaphora and cataphora resolution, contribute positively to improving the grammaticality of the generated summary. Challenges, such as redundancy and proper coverage of all topics, are dealt with to achieve informativity and representativeness. Grapharizer is a novel approach which can also be used in combination with different machine learning models. The system was tested on DUC 2004 and Recent News Article datasets against various state-of-the-art techniques. Use of Grapharizer with machine learning increased accuracy by up to 23.05% compared with different baseline techniques on ROUGE scores. Expert evaluation of the proposed system indicated the accuracy to be more than 55%.",
            "corpus_id": "258231820",
            "text": "In the age of big data, there is increasing growth of data on the Internet. It becomes frustrating for users to locate the desired data. Therefore, text summarization emerges as a solution to this problem. It summarizes and presents the users with the gist of the provided documents. However, summarizer systems face challenges, such as poor grammaticality, missing important information, and redundancy, particularly in multi-document summarization. This study involves the development of a graph-based extractive generic MDS technique, named Grapharizer (GRAPH-based summARIZER), focusing on resolving these challenges. Grapharizer addresses the grammaticality problems of the summary using lemmatization during pre-processing. Furthermore, synonym mapping, multi-word expression mapping, and anaphora and cataphora resolution, contribute positively to improving the grammaticality of the generated summary. Challenges, such as redundancy and proper coverage of all topics, are dealt with to achieve informativity and representativeness. Grapharizer is a novel approach which can also be used in combination with different machine learning models. The system was tested on DUC 2004 and Recent News Article datasets against various state-of-the-art techniques. Use of Grapharizer with machine learning increased accuracy by up to 23.05% compared with different baseline techniques on ROUGE scores. Expert evaluation of the proposed system indicated the accuracy to be more than 55%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.78466796875
        },
        {
            "paperId": "e9c9d287e98fa543ad533c14c9267163b01d3fc2",
            "corpusId": 261682317,
            "title": "Multi-Document Summarization: A Comparative Evaluation",
            "venue": "International Conference on Industrial and Information Systems",
            "year": 2023,
            "referenceCount": 33,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.04951",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.04951, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238954441",
                    "name": "Kushan Hewapathirana"
                },
                {
                    "authorId": "39077183",
                    "name": "Nisansa de Silva"
                },
                {
                    "authorId": "2238953704",
                    "name": "C. D. A. D. O. C. ScienceEngineering"
                },
                {
                    "authorId": "2238954361",
                    "name": "University of Moratuwa"
                },
                {
                    "authorId": "2244276351",
                    "name": "Sri Lanka"
                },
                {
                    "authorId": "2238953913",
                    "name": "ConscientAI"
                }
            ],
            "abstract": "This paper is aimed at evaluating state-of-the-art models for Multi-document Summarization (MDS) on different types of datasets in various domains and investigating the limitations of existing models to determine future research directions. To address this gap, we conducted an extensive literature review to identify state-of-the-art models and datasets. We analyzed the performance of PRIMERA and PEGASUS models on BigSurvey-MDS and MS^2 datasets, which posed unique challenges due to their varied domains. Our findings show that the General-Purpose Pretrained Model LED outperforms PRIMERA and PEGASUS on the MS^2 dataset. We used the ROUGE score as a performance metric to evaluate the identified models on different datasets. Our study provides valuable insights into the models' strengths and weaknesses, as well as their applicability in different domains. This work serves as a reference for future MDS research and contributes to the development of accurate and robust models which can be utilized on demanding datasets with academically and/or scientifically complex data as well as generalized, relatively simple datasets.",
            "corpus_id": "261682317",
            "text": "This paper is aimed at evaluating state-of-the-art models for Multi-document Summarization (MDS) on different types of datasets in various domains and investigating the limitations of existing models to determine future research directions. To address this gap, we conducted an extensive literature review to identify state-of-the-art models and datasets. We analyzed the performance of PRIMERA and PEGASUS models on BigSurvey-MDS and MS^2 datasets, which posed unique challenges due to their varied domains. Our findings show that the General-Purpose Pretrained Model LED outperforms PRIMERA and PEGASUS on the MS^2 dataset. We used the ROUGE score as a performance metric to evaluate the identified models on different datasets. Our study provides valuable insights into the models' strengths and weaknesses, as well as their applicability in different domains. This work serves as a reference for future MDS research and contributes to the development of accurate and robust models which can be utilized on demanding datasets with academically and/or scientifically complex data as well as generalized, relatively simple datasets.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8134765625
        }
    ],
    "quotes": {
        "cost": 0.24441000000000004,
        "quotes": [
            {
                "idx": 0,
                "key": "[1320 | Radev et al. | 2000 | Citations: 584]",
                "snippets": "We also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in (Carbonell et al., 1998)) and showed how it can be used in evaluating multidocument summaries.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Contributions and future work",
                        "pdf_hash": "",
                        "start": 694,
                        "end": 946,
                        "sentence_offsets": [
                            {
                                "start": 694,
                                "end": 946
                            }
                        ],
                        "ref_mentions": [
                            "4508623"
                        ],
                        "quote": "We also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in (Carbonell et al., 1998)) and showed how it can be used in evaluating multidocument summaries."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[152281 | Branavan et al. | 2008 | Citations: 122]",
                "snippets": "This paper also relates to a large body of work in multi-document summarization. Researchers have long noted that a central challenge of multi-document summarization is identifying redundant information over input documents (Radev et al., 1998)(Carbonell et al., 1998)(Mani et al., 1997)Barzilay, McKeown, & Elhadad, 1999). This task is of crucial significance because multi-document summarizers operate over related documents that describe the same facts multiple times. In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999;(Radev et al., 2000)(Nenkova et al., 2006). Many of these algorithms first cluster sentences together, and then extract or generate sentence representatives for the clusters.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1320 | Radev et al. | 2000 | Citations: 584]": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",
                    "[6025826 | Mani et al. | 1997 | Citations: 252]": "We describe a new method for summarizing similarities and differences in a pair of related documents using a graph representation for text. Concepts denoted by words, phrases, and proper names in the document are represented positionally as nodes in the graph along with edges corresponding to semantic relations between items. Given a perspective in terms of which the pair of documents is to be summarized, the algorithm first uses a spreading activation technique to discover, in each document, nodes semantically related to the topic. The activated graphs of each document are then matched to yield a graph corresponding to similarities and differences between the pair, which is rendered in natural language. An evaluation of these techniques has been carried out."
                },
                "metadata": [
                    {
                        "section_title": "Multidocument Summarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 827,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 80
                            },
                            {
                                "start": 81,
                                "end": 333
                            },
                            {
                                "start": 334,
                                "end": 481
                            },
                            {
                                "start": 482,
                                "end": 696
                            },
                            {
                                "start": 697,
                                "end": 827
                            }
                        ],
                        "ref_mentions": [
                            "10019526",
                            "4508623",
                            "6025826",
                            "1320",
                            "86903"
                        ],
                        "quote": "This paper also relates to a large body of work in multi-document summarization. Researchers have long noted that a central challenge of multi-document summarization is identifying redundant information over input documents (Radev et al., 1998)(Carbonell et al., 1998)(Mani et al., 1997)Barzilay, McKeown, & Elhadad, 1999). This task is of crucial significance because multi-document summarizers operate over related documents that describe the same facts multiple times. In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999;(Radev et al., 2000)(Nenkova et al., 2006). Many of these algorithms first cluster sentences together, and then extract or generate sentence representatives for the clusters."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[15795297 | Banerjee et al. | 2015 | Citations: 136]",
                "snippets": "On the DUC2004 and DUC2005 datasets, we demonstrate the effectiveness of our proposed method. Our proposed method outperforms not only some popular baselines but also the state-of-the-art extractive summarization systems. ROUGE scores (Oya et al., 2014) obtained by our system outperforms the best extractive summarizer on both the datasets. Our method also outperforms an abstractive summarizer based on multi-sentence compression (Filippova, 2010) when measured by ROUGE-2, ROUGE-L and ROUGE-SU4 scores.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[12299544 | Oya et al. | 2014 | Citations: 93]": "In this paper, we present an automatic abstractive summarization system of meeting conversations. Our system extends a novel multi-sentence fusion algorithm in order to generate abstract templates. It also leverages the relationship between summaries and their source meeting transcripts to select the best templates for generating abstractive summaries of meetings. Our manual and automatic evaluation results demonstrate the success of our system in achieving higher scores both in readability and informativeness."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1456,
                        "end": 1954,
                        "sentence_offsets": [
                            {
                                "start": 1456,
                                "end": 1549
                            },
                            {
                                "start": 1550,
                                "end": 1677
                            },
                            {
                                "start": 1678,
                                "end": 1790
                            },
                            {
                                "start": 1791,
                                "end": 1954
                            }
                        ],
                        "ref_mentions": [
                            "12299544",
                            "14750088"
                        ],
                        "quote": "On the DUC2004 and DUC2005 datasets, we demonstrate the effectiveness of our proposed method. Our proposed method outperforms not only some popular baselines but also the state-of-the-art extractive summarization systems. ROUGE scores (Oya et al., 2014) obtained by our system outperforms the best extractive summarizer on both the datasets. Our method also outperforms an abstractive summarizer based on multi-sentence compression (Filippova, 2010) when measured by ROUGE-2, ROUGE-L and ROUGE-SU4 scores."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[15926944 | Kumar et al. | 2012 | Citations: 72]",
                "snippets": "In this study, some survey on multi document summarization approaches has been presented. We will direct our focus notably on four well known approaches to multi document summarization namely the feature based method, cluster based method, graph based method and knowledge based method. The general ideas behind these methods have been described.\n\nA number of research study have addressed multi document summarization in academia (Erkan and Radev, 2004a, Wan and Yang, 2008, Haribagiu and Lacatusu, 2010) and illustrated different types of approaches and available systems for multi document summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this study, some survey on multi document summarization approaches has been presented. We will direct our focus notably on four well known approaches to multi document summarization namely the feature based method, cluster based method, graph based method and knowledge based method. The general ideas behind these methods have been described.\n\nA number of research study have addressed multi document summarization in academia (Erkan and Radev, 2004a, Wan and Yang, 2008, Haribagiu and Lacatusu, 2010) and illustrated different types of approaches and available systems for multi document summarization.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[17446655 | Wan | 2008 | Citations: 82]",
                "snippets": "Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the \"voting\" or \"recommendations\" between sentences in the documents (Erkan et al., 2004)(Mihalcea et al., 2005)(Wan et al., 2006).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[5457260 | Wan et al. | 2006 | Citations: 109]": "This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1554,
                        "end": 1810,
                        "sentence_offsets": [
                            {
                                "start": 1554,
                                "end": 1810
                            }
                        ],
                        "ref_mentions": [
                            "10418456",
                            "8878897",
                            "5457260"
                        ],
                        "quote": "Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the \"voting\" or \"recommendations\" between sentences in the documents (Erkan et al., 2004)(Mihalcea et al., 2005)(Wan et al., 2006)."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[174799390 | Fabbri et al. | 2019 | Citations: 590]",
                "snippets": "Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 133,
                        "end": 406,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[202785778 | Fan et al. | 2019 | Citations: 102]",
                "snippets": "Previous work in multi-document summarization (Barzilay et al., 1999) applies various techniques to handle long input, including clustering to find similar information (Honarpisheh et al., 2008), extractive methods to select relevant sentences (Daum\u00e9 et al., 2002)(Gillick et al., 2009)(Berg-Kirkpatrick et al., 2011)(Fabbrizio et al., 2014)Bing et al., 2015;(Cao et al., 2016) including maximal marginal relevance (Fabbri et al., 2019), and incorporating queries (Baumel et al., 2018) and graphs (Ganesan et al., 2010)Yasunaga et al., 2017).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7031344 | Barzilay et al. | 1999 | Citations: 484]": "We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents. Our approach is unique in its usage of language generation to reformulate the wording of the summary.",
                    "[12682781 | Fabbrizio et al. | 2014 | Citations: 71]": "We present a hybrid method to generate summaries of product and services reviews by combining natural language generation and salient sentence selection techniques. Our system, STARLET-H, receives as input textual reviews with associated rated topics, and produces as output a natural language document summarizing the opinions expressed in the reviews. STARLET-H operates as a hybrid",
                    "[14651945 | Cao et al. | 2016 | Citations: 104]": "\n \n Developed so far, multi-document summarization has reached its bottleneck due to the lack of sufficient training data and diverse categories of documents. Text classification just makes up for these deficiencies. In this paper, we propose a novel summarization system called TCSum, which leverages plentiful text classification data to improve the performance of multi-document summarization. TCSum projects documents onto distributed representations which act as a bridge between text classification and summarization. It also utilizes the classification results to produce summaries of different styles. Extensive experiments on DUC generic multi-document summarization datasets show that, TCSum can achieve the state-of-the-art performance without using any hand-crafted features and has the capability to catch the variations of summary styles with respect to different text categories.\n \n",
                    "[167874 | Gillick et al. | 2009 | Citations: 303]": "We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the sub-sentence or \"concept-level, to a sentence-level model, previously solved with an ILP. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences. We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously. The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics.",
                    "[189898 | Daume et al. | 2002 | Citations: 102]": "We present a document compression system that uses a hierarchical noisy-channel model of text production. Our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input. The system then uses a statistical hierarchical model of text production in order to drop non-important syntactic and discourse constituents so as to generate coherent, grammatical document compressions of arbitrary length. The system outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text. Our results support the claim that discourse knowledge plays an important role in document summarization."
                },
                "metadata": [
                    {
                        "section_title": "Multi-Document Input",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 551,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "7031344",
                            "15955042",
                            "189898",
                            "167874",
                            "15467396",
                            "12682781",
                            "14651945",
                            "988010"
                        ],
                        "quote": "Previous work in multi-document summarization (Barzilay et al., 1999) applies various techniques to handle long input, including clustering to find similar information (Honarpisheh et al., 2008), extractive methods to select relevant sentences (Daum\u00e9 et al., 2002)(Gillick et al., 2009)(Berg-Kirkpatrick et al., 2011)(Fabbrizio et al., 2014)Bing et al., 2015;(Cao et al., 2016) including maximal marginal relevance (Fabbri et al., 2019), and incorporating queries (Baumel et al., 2018) and graphs (Ganesan et al., 2010)Yasunaga et al., 2017)."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[202889056 | Antognini et al. | 2019 | Citations: 22]",
                "snippets": "Extractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988).\n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10103200 | Radev | 2000 | Citations: 248]": "We introduce CST (cross-document structure theory), a paradigm for multi-document analysis. CST takes into account the rhetorical structure of clusters of related textual documents. We present a taxonomy of cross-document relationships. We argue that CST can be the basis for multi-document summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering of facts.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[5457260 | Wan et al. | 2006 | Citations: 109]": "This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems.",
                    "[5879376 | Galley | 2006 | Citations: 188]": "We describe a probabilistic approach to content selection for meeting summarization. We use skipchain Conditional Random Fields (CRF) to model non-local pragmatic dependencies between paired utterances such as Question-Answer that typically appear together in summaries, and show that these models outperform linear-chain CRFs and Bayesian models in the task. We also discuss different approaches for ranking all utterances in a sequence using CRFs. Our best performing system achieves 91.3% of human performance when evaluated with the Pyramid evaluation metric, which represents a 3.9% absolute increase compared to our most competitive non-sequential classifier."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 371,
                        "end": 1847,
                        "sentence_offsets": [
                            {
                                "start": 206,
                                "end": 392
                            },
                            {
                                "start": 393,
                                "end": 477
                            },
                            {
                                "start": 480,
                                "end": 570
                            },
                            {
                                "start": 571,
                                "end": 614
                            },
                            {
                                "start": 615,
                                "end": 718
                            },
                            {
                                "start": 719,
                                "end": 873
                            },
                            {
                                "start": 874,
                                "end": 990
                            },
                            {
                                "start": 991,
                                "end": 1042
                            },
                            {
                                "start": 1043,
                                "end": 1072
                            },
                            {
                                "start": 1073,
                                "end": 1194
                            },
                            {
                                "start": 1195,
                                "end": 1404
                            },
                            {
                                "start": 1407,
                                "end": 1772
                            },
                            {
                                "start": 1773,
                                "end": 1957
                            }
                        ],
                        "ref_mentions": [
                            "10103200",
                            "506350",
                            "5457260",
                            "17064982",
                            "9849366",
                            "11977708",
                            "337730",
                            "60514661",
                            "4508623",
                            "1207010",
                            "5879376"
                        ],
                        "quote": "Extractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988).\n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[218718706 | Li et al. | 2020 | Citations: 136]",
                "snippets": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 459,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[220045815 | Jin et al. | 2020 | Citations: 99]",
                "snippets": "The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u0327elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[21850704 | Paulus et al. | 2017 | Citations: 1559]": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
                    "[4406182 | Celikyilmaz et al. | 2018 | Citations: 307]": "We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[52053741 | Lebanoff et al. | 2018 | Citations: 157]": "Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors.",
                    "[52144157 | Gehrmann et al. | 2018 | Citations: 689]": "Neural summarization produces outputs that are fluent and readable, but which can be poor at content selection, for instance often copying full sentences from the source document. This work explores the use of data-efficient content selectors to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences making it easy to transfer a trained summarizer to a new domain.",
                    "[6532096 | Yasunaga et al. | 2017 | Citations: 258]": "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.",
                    "[8314118 | See et al. | 2017 | Citations: 4028]": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1261,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 104
                            },
                            {
                                "start": 105,
                                "end": 339
                            },
                            {
                                "start": 340,
                                "end": 522
                            },
                            {
                                "start": 523,
                                "end": 864
                            },
                            {
                                "start": 865,
                                "end": 1039
                            },
                            {
                                "start": 1040,
                                "end": 1261
                            }
                        ],
                        "ref_mentions": [
                            "506350",
                            "6532096",
                            "8314118",
                            "21850704",
                            "52144157",
                            "4406182",
                            "52053741"
                        ],
                        "quote": "The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u0327elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[221293184 | Zhang et al. | 2020 | Citations: 17]",
                "snippets": "Query-based Multi-document Summarization. Query-based multi-document summarization is the process of automatically generating natural summaries of text documents in the context of a given query. An early work for extractive query-based multidocument summarization is presented by (Goldstein-Stewart et al., 1999), which ranked sentences using a weighted combination of statistical and linguistic features. (Daum\u00e9 et al., 2006) presented to extract sentences based on the language model, Bayesian model, and graphical model. [40] introduced the graph information to look for relevant sentences. (Schilder et al., 2008) used the multi-modality manifold-ranking algorithm to extract topic-focused summary from multiple documents. Recently, some works employ the encoder-decoder framework to produce the query-based summaries. [24] trained a pointer-generator model, and [3] incorporated relevance into a neural seq2seq models for query-based abstractive summarization. (Nema et al., 2017) introduced a new diversity based attention mechanism to alleviate the problem of repeating phrases.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[22109805 | Schilder et al. | 2008 | Citations: 109]": "We present a fast query-based multi-document summarizer called FastSum based solely on word-frequency features of clusters, documents and topics. Summary sentences are ranked by a regression SVM. The summarizer does not use any expensive NLP techniques such as parsing, tagging of names or even part of speech information. Still, the achieved accuracy is comparable to the best systems presented in recent academic competitions (i.e., Document Understanding Conference (DUC)). Because of a detailed feature analysis using Least Angle Regression (LARS), FastSum can rely on a minimal set of features leading to fast processing times: 1250 news documents in 60 seconds.",
                    "[5673925 | Nema et al. | 2017 | Citations: 169]": "Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.",
                    "[6241932 | Daume et al. | 2006 | Citations: 285]": "We present BAYESUM (for \"Bayesian summarization\"), a model for sentence extraction in query-focused summarization. BAYESUM leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BAYESUM is not afflicted by the paucity of information in short queries. We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system. Furthermore, we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework."
                },
                "metadata": [
                    {
                        "section_title": "2.2.2",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1007,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 41
                            },
                            {
                                "start": 42,
                                "end": 194
                            },
                            {
                                "start": 195,
                                "end": 377
                            },
                            {
                                "start": 378,
                                "end": 479
                            },
                            {
                                "start": 480,
                                "end": 549
                            },
                            {
                                "start": 550,
                                "end": 663
                            },
                            {
                                "start": 664,
                                "end": 759
                            },
                            {
                                "start": 760,
                                "end": 902
                            },
                            {
                                "start": 903,
                                "end": 1007
                            }
                        ],
                        "ref_mentions": [
                            "11218013",
                            "6241932",
                            "22109805",
                            "5673925"
                        ],
                        "quote": "Query-based Multi-document Summarization. Query-based multi-document summarization is the process of automatically generating natural summaries of text documents in the context of a given query. An early work for extractive query-based multidocument summarization is presented by (Goldstein-Stewart et al., 1999), which ranked sentences using a weighted combination of statistical and linguistic features. (Daum\u00e9 et al., 2006) presented to extract sentences based on the language model, Bayesian model, and graphical model. [40] introduced the graph information to look for relevant sentences. (Schilder et al., 2008) used the multi-modality manifold-ranking algorithm to extract topic-focused summary from multiple documents. Recently, some works employ the encoder-decoder framework to produce the query-based summaries. [24] trained a pointer-generator model, and [3] incorporated relevance into a neural seq2seq models for query-based abstractive summarization. (Nema et al., 2017) introduced a new diversity based attention mechanism to alleviate the problem of repeating phrases."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[222140880 | Bista et al. | 2020 | Citations: 2]",
                "snippets": "Most existing work on this topic has focused on the generic summarization task. However, update summarization is of equal practical interest.\n\nMulti-document extractive summarization methods can be unsupervised or supervised. Unsupervised methods typically define salience (or coverage) using a global model of sentence-sentence similarity. Methods based on retrieval (Goldstein et al., 1999), centroids (Radev et al., 2004), graph centrality (Erkan and Radev, 2004), or utility maximization (Lin andBilmes, 2010, 2011;Gillick and Favre, 2009) have been well explored. However, sentence salience also depends on surface features (e.g., position, length, presence of cue words); effectively capturing these requires supervised models specific to the dataset and task. A body of work has incorporated such information through supervised learning, for example based on point processes (Kulesza and Taskar, 2012), learning important words (Hong and Nenkova, 2014), graph neural networks (Yasunaga et al., 2017), and support vector regression (Varma et al., 2009).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Most existing work on this topic has focused on the generic summarization task. However, update summarization is of equal practical interest.\n\nMulti-document extractive summarization methods can be unsupervised or supervised. Unsupervised methods typically define salience (or coverage) using a global model of sentence-sentence similarity. Methods based on retrieval (Goldstein et al., 1999), centroids (Radev et al., 2004), graph centrality (Erkan and Radev, 2004), or utility maximization (Lin andBilmes, 2010, 2011;Gillick and Favre, 2009) have been well explored. However, sentence salience also depends on surface features (e.g., position, length, presence of cue words); effectively capturing these requires supervised models specific to the dataset and task. A body of work has incorporated such information through supervised learning, for example based on point processes (Kulesza and Taskar, 2012), learning important words (Hong and Nenkova, 2014), graph neural networks (Yasunaga et al., 2017), and support vector regression (Varma et al., 2009).",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[226283949 | Jin et al. | 2020 | Citations: 15]",
                "snippets": "Multi-document summarization aims at producing a fluent, condensed summary for the given document set...Compared with single-document summarization, multi-document summarization needs to handle multiple input documents. A simple method is to concatenate multiple documents into a long flat text and treat it as a long sequence-tosequence task.\n\nEmpowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017; Paulus et al., 2018; Tan et al., 2017; \u00c7 elikyilmaz et al., 2018)...Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization.\n\nThe methods for multi-document summarization can generally be categorized to extractive and abstractive...Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004; Christensen et al., 2013; Yasunaga et al., 2017).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                },
                "metadata": [
                    {
                        "quote": "Multi-document summarization aims at producing a fluent, condensed summary for the given document set",
                        "pdf_hash": ""
                    },
                    {
                        "quote": "Compared with single-document summarization, multi-document summarization needs to handle multiple input documents. A simple method is to concatenate multiple documents into a long flat text and treat it as a long sequence-tosequence task.\n\nEmpowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017; Paulus et al., 2018; Tan et al., 2017; \u00c7 elikyilmaz et al., 2018)",
                        "pdf_hash": ""
                    },
                    {
                        "quote": "Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization.\n\nThe methods for multi-document summarization can generally be categorized to extractive and abstractive",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Multi-Document Summarization",
                        "pdf_hash": "",
                        "start": 289,
                        "end": 474,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "506350"
                        ],
                        "quote": "Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004; Christensen et al., 2013; Yasunaga et al., 2017)."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[235097309 | Pasunuru et al. | 2021 | Citations: 73]",
                "snippets": "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[49210924 | Liao et al. | 2018 | Citations: 105]": "Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.",
                    "[5001921 | Liu et al. | 2018 | Citations: 302]": "We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-tograph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-totext generator. The framework is data-driven, trainable, and not specifically designed for a particular domain. Experiments on goldstandard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[6025826 | Mani et al. | 1997 | Citations: 252]": "We describe a new method for summarizing similarities and differences in a pair of related documents using a graph representation for text. Concepts denoted by words, phrases, and proper names in the document are represented positionally as nodes in the graph along with edges corresponding to semantic relations between items. Given a perspective in terms of which the pair of documents is to be summarized, the algorithm first uses a spreading activation technique to discover, in each document, nodes semantically related to the topic. The activated graphs of each document are then matched to yield a graph corresponding to similarities and differences between the pair, which is rendered in natural language. An evaluation of these techniques has been carried out."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2271,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 102
                            },
                            {
                                "start": 103,
                                "end": 251
                            },
                            {
                                "start": 252,
                                "end": 372
                            },
                            {
                                "start": 373,
                                "end": 547
                            },
                            {
                                "start": 548,
                                "end": 697
                            },
                            {
                                "start": 700,
                                "end": 883
                            },
                            {
                                "start": 884,
                                "end": 1100
                            },
                            {
                                "start": 1101,
                                "end": 1288
                            },
                            {
                                "start": 1289,
                                "end": 1387
                            },
                            {
                                "start": 1388,
                                "end": 1465
                            },
                            {
                                "start": 1466,
                                "end": 1613
                            },
                            {
                                "start": 1614,
                                "end": 1765
                            },
                            {
                                "start": 1768,
                                "end": 1907
                            },
                            {
                                "start": 1908,
                                "end": 2011
                            },
                            {
                                "start": 2012,
                                "end": 2271
                            }
                        ],
                        "ref_mentions": [
                            "6025826",
                            "10019526",
                            "6025826",
                            "10019526",
                            "506350",
                            "16587947",
                            "15709889",
                            "6788641",
                            "5001921",
                            "49210924"
                        ],
                        "quote": "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com"
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[236150987 | Perez-Beltrachini et al. | 2021 | Citations: 29]",
                "snippets": "Multi-Document Summarization Most previous solutions to multi-document summarization adopt non-neural, extractive methods (Carbonell et al., 1998)(Radev et al., 2004)(Erkan et al., 2004)(Barzilay et al., 1999). \n\nMore recently, various encoder-decoder architectures (Liu & Lapata, 2019;Fabbri et al., 2019;Perez-Beltrachini et al., 2019;(Liu et al., 2018)(Zhang et al., 2018)Lebanoff, Song, & Liu, 2018) have been ported to this task thanks to the development of large-scale datasets for model training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7031344 | Barzilay et al. | 1999 | Citations: 484]": "We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents. Our approach is unique in its usage of language generation to reformulate the wording of the summary.",
                    "[3608234 | Liu et al. | 2018 | Citations: 801]": "We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[53223447 | Zhang et al. | 2018 | Citations: 35]": "Till now, neural abstractive summarization methods have achieved great success for single document summarization (SDS). However, due to the lack of large scale multi-document summaries, such methods can be hardly applied to multi-document summarization (MDS). In this paper, we investigate neural abstractive methods for MDS by adapting a state-of-the-art neural abstractive summarization model for SDS. We propose an approach to extend the neural abstractive model trained on large scale SDS data to the MDS task. Our approach only makes use of a small number of multi-document summaries for fine tuning. Experimental results on two benchmark DUC datasets demonstrate that our approach can outperform a variety of baseline neural models."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 577,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 238
                            },
                            {
                                "start": 241,
                                "end": 577
                            }
                        ],
                        "ref_mentions": [
                            "4508623",
                            "6354619",
                            "506350",
                            "7031344",
                            "3608234",
                            "53223447"
                        ],
                        "quote": "Multi-Document Summarization Most previous solutions to multi-document summarization adopt non-neural, extractive methods (Carbonell et al., 1998)(Radev et al., 2004)(Erkan et al., 2004)(Barzilay et al., 1999). \n\nMore recently, various encoder-decoder architectures (Liu & Lapata, 2019;Fabbri et al., 2019;Perez-Beltrachini et al., 2019;(Liu et al., 2018)(Zhang et al., 2018)Lebanoff, Song, & Liu, 2018) have been ported to this task thanks to the development of large-scale datasets for model training."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[236478143 | Zhou et al. | 2021 | Citations: 28]",
                "snippets": "Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020).\n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018).\n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[170079112 | Liu et al. | 2019 | Citations: 298]": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[52053741 | Lebanoff et al. | 2018 | Citations: 157]": "Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 966,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 120
                            },
                            {
                                "start": 121,
                                "end": 287
                            },
                            {
                                "start": 290,
                                "end": 585
                            },
                            {
                                "start": 586,
                                "end": 796
                            },
                            {
                                "start": 799,
                                "end": 968
                            }
                        ],
                        "ref_mentions": [
                            "52053741",
                            "170079112"
                        ],
                        "quote": "Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020).\n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018).\n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020)."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[238221709 | Jalil et al. | 2021 | Citations: 14]",
                "snippets": "For instance, the same contents may be covered from multiple sources, so at times, a number of documents may be available to gain an insight into the same event (Glavas et al., 2014). In this regard, a multi-document summary becomes a representation of the information contained in a cluster of documents which helps users understand the gist of those documents (Alguliyev et al., 2012), (Luo et al., 2013). A multi-document summary represents the information contained in the cluster of documents and helps users understand those documents (Alguliyev et al., 2013).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 493,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 165
                            },
                            {
                                "start": 166,
                                "end": 354
                            },
                            {
                                "start": 355,
                                "end": 493
                            }
                        ],
                        "ref_mentions": [
                            "45592507",
                            "596849",
                            "38686738",
                            "4490918"
                        ],
                        "quote": "For instance, the same contents may be covered from multiple sources, so at times, a number of documents may be available to gain an insight into the same event (Glavas et al., 2014). In this regard, a multi-document summary becomes a representation of the information contained in a cluster of documents which helps users understand the gist of those documents (Alguliyev et al., 2012), (Luo et al., 2013). A multi-document summary represents the information contained in the cluster of documents and helps users understand those documents (Alguliyev et al., 2013)."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[239050558 | Cui et al. | 2021 | Citations: 41]",
                "snippets": "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1062,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[248496597 | Wang et al. | 2022 | Citations: 1]",
                "snippets": "Multi-document summarization (MDS) tries to solve the task of generating a summary for a collection of documents sharing the same topic. This field has been studied for a long time (Kathleen McKeown and Dragomir R Radev, 1995; Carbonell and Goldstein, 1998). Traditional approaches to MDS have been extractive, where part of the original text is selected and then organized to form a summary. After that many abstractive methods have been developed. In recent years, neural network-based methods have been predominant.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Multi-document summarization (MDS) tries to solve the task of generating a summary for a collection of documents sharing the same topic. This field has been studied for a long time (Kathleen McKeown and Dragomir R Radev, 1995; Carbonell and Goldstein, 1998). Traditional approaches to MDS have been extractive, where part of the original text is selected and then organized to form a summary. After that many abstractive methods have been developed. In recent years, neural network-based methods have been predominant.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[248571519 | Sankar et al. | 2022 | Citations: 0]",
                "snippets": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[10112929 | Li et al. | 2014 | Citations: 46]": "In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status \u2013 remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences. Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary. Compared with state-ofthe-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                },
                "metadata": [
                    {
                        "section_title": "Multi Document Summarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1615,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 109
                            },
                            {
                                "start": 110,
                                "end": 276
                            },
                            {
                                "start": 277,
                                "end": 452
                            },
                            {
                                "start": 453,
                                "end": 619
                            },
                            {
                                "start": 620,
                                "end": 994
                            },
                            {
                                "start": 995,
                                "end": 1150
                            },
                            {
                                "start": 1151,
                                "end": 1363
                            },
                            {
                                "start": 1364,
                                "end": 1615
                            }
                        ],
                        "ref_mentions": [
                            "577937",
                            "506350",
                            "10112929",
                            "218718706",
                            "218718706"
                        ],
                        "quote": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[248780330 | Moro et al. | 2022 | Citations: 32]",
                "snippets": "Flat solutions. Flat concatenation is a simple yet powerful solution because the generation of the multi-document summary is treated as a singledocument summarization task, thus it can leverage state-of-the-art pre-trained summarization models. \n\nConsequently, processing all documents as a flat input requires models capable of handling long sequences. As previously experimented by (DeYoung et al., 2021), Xiao et al. (2021) proposed to leverage the Longformer-Encoder-Decoder model (Beltagy et al., 2020) pre-trained with a novel multi-document summarization specific task. They proved that a long-range Transformer that encodes all documents is a straightforward yet effective solution, and they achieved new state-of-the-art results in several multi-document summarization datasets. However, such models may struggle to handle a massive cluster of topic-related documents since they need to truncate them because of architectural limits. Further, processing all documents in a cluster could be noisy if some of them are not relevant or factual with respect to the summary. \n\nHierarchical solutions. To better preserve crossdocument relations and obtain semantic-rich representations, hierarchical concatenation solutions leverage graph-based techniques to work from word and sentence-level (Wan et al., 2006)(Liao et al., 2018)(Nayeem et al., 2018)Antognini and Faltings, 2019;(Li et al., 2020) to documentlevel (Amplayo and Lapata, 2021). Other hierarchical approaches include multi-head pooling and inter-paragraph attention architectures (Liu et al., 2019), attention models with maximal marginal relevance (Fabbri et al., 2019), and attention across different granularity representations (Jin et al., 2020). Such models are often datasetspecific because of the custom architecture, so they struggle to adapt to other datasets and effectively leverage pre-trained state-of-the-art Transformers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[170079112 | Liu et al. | 2019 | Citations: 298]": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[174799390 | Fabbri et al. | 2019 | Citations: 590]": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[220045815 | Jin et al. | 2020 | Citations: 99]": "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.",
                    "[233231380 | DeYoung et al. | 2021 | Citations: 113]": "To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system\u2019s generated summaries. Data and models are available at https://github.com/allenai/ms2.",
                    "[49210924 | Liao et al. | 2018 | Citations: 105]": "Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.",
                    "[5457260 | Wan et al. | 2006 | Citations: 109]": "This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 96,
                        "end": 2001,
                        "sentence_offsets": [
                            {
                                "start": 96,
                                "end": 111
                            },
                            {
                                "start": 112,
                                "end": 340
                            },
                            {
                                "start": 343,
                                "end": 449
                            },
                            {
                                "start": 450,
                                "end": 671
                            },
                            {
                                "start": 672,
                                "end": 882
                            },
                            {
                                "start": 883,
                                "end": 1037
                            },
                            {
                                "start": 1038,
                                "end": 1172
                            },
                            {
                                "start": 1175,
                                "end": 1198
                            },
                            {
                                "start": 1199,
                                "end": 1538
                            },
                            {
                                "start": 1539,
                                "end": 1815
                            },
                            {
                                "start": 1816,
                                "end": 2001
                            }
                        ],
                        "ref_mentions": [
                            "233231380",
                            "5457260",
                            "49210924",
                            "52011473",
                            "218718706",
                            "170079112",
                            "174799390",
                            "220045815"
                        ],
                        "quote": "Flat solutions. Flat concatenation is a simple yet powerful solution because the generation of the multi-document summary is treated as a singledocument summarization task, thus it can leverage state-of-the-art pre-trained summarization models. \n\nConsequently, processing all documents as a flat input requires models capable of handling long sequences. As previously experimented by (DeYoung et al., 2021), Xiao et al. (2021) proposed to leverage the Longformer-Encoder-Decoder model (Beltagy et al., 2020) pre-trained with a novel multi-document summarization specific task. They proved that a long-range Transformer that encodes all documents is a straightforward yet effective solution, and they achieved new state-of-the-art results in several multi-document summarization datasets. However, such models may struggle to handle a massive cluster of topic-related documents since they need to truncate them because of architectural limits. Further, processing all documents in a cluster could be noisy if some of them are not relevant or factual with respect to the summary. \n\nHierarchical solutions. To better preserve crossdocument relations and obtain semantic-rich representations, hierarchical concatenation solutions leverage graph-based techniques to work from word and sentence-level (Wan et al., 2006)(Liao et al., 2018)(Nayeem et al., 2018)Antognini and Faltings, 2019;(Li et al., 2020) to documentlevel (Amplayo and Lapata, 2021). Other hierarchical approaches include multi-head pooling and inter-paragraph attention architectures (Liu et al., 2019), attention models with maximal marginal relevance (Fabbri et al., 2019), and attention across different granularity representations (Jin et al., 2020). Such models are often datasetspecific because of the custom architecture, so they struggle to adapt to other datasets and effectively leverage pre-trained state-of-the-art Transformers."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[256416214 | DeYoung et al. | 2023 | Citations: 8]",
                "snippets": "Automatic (multi-document) summarization (Nenkova and McKeown, 2011;Maybury, 1999) has been an active subfield within NLP for decades. We have focused our analysis on modern, neural abstractive models for conditional text generation (Bahdanau et al., 2014). In light of their empirical success, we have specifically evaluated a set of Transformer-based (Vaswani et al., 2017) models which have recently been used for multidocument summarization (Beltagy et al., 2020;(Zhang et al., 2019)Xiao et al., 2022;(Raffel et al., 2019). There has been some work on highlighting conflicting evidence in health literature specifically (Shah et al., 2021b,a), though this focused primarily on highlighting conflicting evidence and explicitly aggregating extracted content.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[11212020 | Bahdanau et al. | 2014 | Citations: 27329]": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[209405420 | Zhang et al. | 2019 | Citations: 2054]": "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 758,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 134
                            },
                            {
                                "start": 135,
                                "end": 257
                            },
                            {
                                "start": 258,
                                "end": 525
                            },
                            {
                                "start": 526,
                                "end": 758
                            }
                        ],
                        "ref_mentions": [
                            "11212020",
                            "209405420",
                            "204838007"
                        ],
                        "quote": "Automatic (multi-document) summarization (Nenkova and McKeown, 2011;Maybury, 1999) has been an active subfield within NLP for decades. We have focused our analysis on modern, neural abstractive models for conditional text generation (Bahdanau et al., 2014). In light of their empirical success, we have specifically evaluated a set of Transformer-based (Vaswani et al., 2017) models which have recently been used for multidocument summarization (Beltagy et al., 2020;(Zhang et al., 2019)Xiao et al., 2022;(Raffel et al., 2019). There has been some work on highlighting conflicting evidence in health literature specifically (Shah et al., 2021b,a), though this focused primarily on highlighting conflicting evidence and explicitly aggregating extracted content."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[257364970 | Ma | 2023 | Citations: 3]",
                "snippets": "Most extractive multi-document summarization approaches splice all the sentences contained in the original documents into a larger text, and then generate a summary by selecting sentences from the larger text (Lamsiyah et al., 2021)(Yang et al., 2014)(Erkan et al., 2004)...The centroid-based summarization approaches focus on the commonality of all documents or all sentences and they select sentences based on the centroid words of all documents (Radev et al., 2004;(Rossiello et al., 2017) or the centroid embedding of all sentences (Lamsiyah et al., 2021). The clustering-based summarization approaches divide sentences into multiple groups and select sentences from each group (Yang et al., 2014)Sarkar, 2009).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[2346086 | Rossiello et al. | 2017 | Citations: 128]": "The textual similarity is a crucial aspect for many extractive text summarization methods. A bag-of-words representation does not allow to grasp the semantic relationships between concepts when comparing strongly related sentences with no words in common. To overcome this issue, in this paper we propose a centroid-based method for text summarization that exploits the compositional capabilities of word embeddings. The evaluations on multi-document and multilingual datasets prove the effectiveness of the continuous vector representation of words compared to the bag-of-words model. Despite its simplicity, our method achieves good performance even in comparison to more complex deep learning models. Our method is unsupervised and it can be adopted in other summarization tasks.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 738,
                        "end": 1010,
                        "sentence_offsets": [
                            {
                                "start": 738,
                                "end": 1011
                            }
                        ],
                        "ref_mentions": [
                            "228954621",
                            "5792920",
                            "506350"
                        ],
                        "quote": "Most extractive multi-document summarization approaches splice all the sentences contained in the original documents into a larger text, and then generate a summary by selecting sentences from the larger text (Lamsiyah et al., 2021)(Yang et al., 2014)(Erkan et al., 2004)"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1571,
                        "end": 2012,
                        "sentence_offsets": [
                            {
                                "start": 1571,
                                "end": 1856
                            },
                            {
                                "start": 1857,
                                "end": 2011
                            }
                        ],
                        "ref_mentions": [
                            "2346086",
                            "228954621",
                            "5792920"
                        ],
                        "quote": "The centroid-based summarization approaches focus on the commonality of all documents or all sentences and they select sentences based on the centroid words of all documents (Radev et al., 2004;(Rossiello et al., 2017) or the centroid embedding of all sentences (Lamsiyah et al., 2021). The clustering-based summarization approaches divide sentences into multiple groups and select sentences from each group (Yang et al., 2014)Sarkar, 2009)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[263610015 | Mascarell et al. | 2023 | Citations: 1]",
                "snippets": "Multi-document Summarization (MDS) aims at condensing the most important information from different documents. Despite the advances in single-document summarization (Zhang et al., 2019), summarizing multiple related documents remains a greater challenge due to its input length and the presence of redundant information (Fan et al., 2019)(Song et al., 2022). Therefore, some research focuses on implementing multi-stage approaches that first identify the relevant information to then feed it into a summarization model (Lebanoff et al., 2018)(Liu et al., 2019). More recent works utilize pre-trained language models (Lewis et al., 2019)(Raffel et al., 2019)(Xiao et al., 2021) finetuned for the summarization task and feed them with the source documents concatenated (Johner et al., 2021)(Xiao et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[170079112 | Liu et al. | 2019 | Citations: 298]": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[202785778 | Fan et al. | 2019 | Citations: 102]": "Query-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query, which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input, long-form question answering and multi-document summarization, feeding graph representations as input can achieve better performance than using retrieved text portions.",
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[204960716 | Lewis et al. | 2019 | Citations: 10856]": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
                    "[209405420 | Zhang et al. | 2019 | Citations: 2054]": "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.",
                    "[247519084 | Xiao et al. | 2021 | Citations: 119]": "We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins.",
                    "[248512466 | Song et al. | 2022 | Citations: 22]": "A notable challenge in Multi-Document Summarization (MDS) is the extremely-long length of the input. In this paper, we present an extract-then-abstract Transformer framework to overcome the problem. Specifically, we leverage pre-trained language models to construct a hierarchical extractor for salient sentence selection across documents and an abstractor for rewriting the selected contents as summaries. However, learning such a framework is challenging since the optimal contents for the abstractor are generally unknown. Previous works typically create pseudo extraction oracle to enable the supervised learning for both the extractor and the abstractor. Nevertheless, we argue that the performance of such methods could be restricted due to the insufficient information for prediction and inconsistent objectives between training and testing. To this end, we propose a loss weighting mechanism that makes the model aware of the unequal importance for the sentences not in the pseudo extraction oracle, and leverage the fine-tuned abstractor to generate summary references as auxiliary signals for learning the extractor. Moreover, we propose a reinforcement learning method that can efficiently apply to the extractor for harmonizing the optimization between training and testing. Experiment results show that our framework substantially outperforms strong baselines with comparable model sizes and achieves the best results on the Multi-News, Multi-XScience, and WikiCatSum corpora.",
                    "[52053741 | Lebanoff et al. | 2018 | Citations: 157]": "Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 155,
                        "end": 963,
                        "sentence_offsets": [
                            {
                                "start": 155,
                                "end": 265
                            },
                            {
                                "start": 266,
                                "end": 512
                            },
                            {
                                "start": 513,
                                "end": 719
                            },
                            {
                                "start": 720,
                                "end": 963
                            }
                        ],
                        "ref_mentions": [
                            "209405420",
                            "202785778",
                            "248512466",
                            "52053741",
                            "170079112",
                            "204960716",
                            "204838007",
                            "247519084",
                            "235258298",
                            "247519084"
                        ],
                        "quote": "Multi-document Summarization (MDS) aims at condensing the most important information from different documents. Despite the advances in single-document summarization (Zhang et al., 2019), summarizing multiple related documents remains a greater challenge due to its input length and the presence of redundant information (Fan et al., 2019)(Song et al., 2022). Therefore, some research focuses on implementing multi-stage approaches that first identify the relevant information to then feed it into a summarization model (Lebanoff et al., 2018)(Liu et al., 2019). More recent works utilize pre-trained language models (Lewis et al., 2019)(Raffel et al., 2019)(Xiao et al., 2021) finetuned for the summarization task and feed them with the source documents concatenated (Johner et al., 2021)(Xiao et al., 2021)."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[266244733 | Su et al. | 2023 | Citations: 1]",
                "snippets": "In the context of multi-document summarization experiments, the model is configured for multi-document analysis, and multiple single documents are concatenated to form an extensive document. The task of multi-document summarization prohibits the use of triple blocking, as the task demands comprehension of several individual documents to generate a comprehensive summary. Results are displayed in Table 3. \n\nIt is observed that the model outperforms prior approaches in multi-document contexts, with particular improvements in various multi-document metrics. The incorporation of topic nodes and adaptive layers contributes positively to the performance of the multi-document summarization model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results on multi-news",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 697,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 190
                            },
                            {
                                "start": 191,
                                "end": 372
                            },
                            {
                                "start": 373,
                                "end": 406
                            },
                            {
                                "start": 409,
                                "end": 559
                            },
                            {
                                "start": 560,
                                "end": 697
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the context of multi-document summarization experiments, the model is configured for multi-document analysis, and multiple single documents are concatenated to form an extensive document. The task of multi-document summarization prohibits the use of triple blocking, as the task demands comprehension of several individual documents to generate a comprehensive summary. Results are displayed in Table 3. \n\nIt is observed that the model outperforms prior approaches in multi-document contexts, with particular improvements in various multi-document metrics. The incorporation of topic nodes and adaptive layers contributes positively to the performance of the multi-document summarization model."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[266599825 | Sakaji et al. | 2023 | Citations: 0]",
                "snippets": "Related research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain (Moro et al., 2022). Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization (Liao et al., 2018). Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset (Fabbri et al., 2019). Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing (Nayeem et al., 2018). They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries (Liu et al., 2019). Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries (Li et al., 2020). Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words (Jin et al., 2020). Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature (DeYoung et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[170079112 | Liu et al. | 2019 | Citations: 298]": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[174799390 | Fabbri et al. | 2019 | Citations: 590]": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[220045815 | Jin et al. | 2020 | Citations: 99]": "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.",
                    "[233231380 | DeYoung et al. | 2021 | Citations: 113]": "To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system\u2019s generated summaries. Data and models are available at https://github.com/allenai/ms2.",
                    "[248780330 | Moro et al. | 2022 | Citations: 32]": "Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can be vital. Others leverage linear model approximations to apply multi-input concatenation, worsening the results because all information is considered, even if it is conflicting or noisy with respect to a shared background. Despite the importance and social impact of medicine, there are no ad-hoc solutions for multi-document summarization. For this reason, we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization. Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews. Moreover, we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method.",
                    "[49210924 | Liao et al. | 2018 | Citations: 105]": "Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research."
                },
                "metadata": [
                    {
                        "section_title": "VII. RELATED WORKS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1702,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 79
                            },
                            {
                                "start": 80,
                                "end": 241
                            },
                            {
                                "start": 242,
                                "end": 374
                            },
                            {
                                "start": 375,
                                "end": 475
                            },
                            {
                                "start": 476,
                                "end": 699
                            },
                            {
                                "start": 700,
                                "end": 846
                            },
                            {
                                "start": 847,
                                "end": 1010
                            },
                            {
                                "start": 1011,
                                "end": 1158
                            },
                            {
                                "start": 1159,
                                "end": 1388
                            },
                            {
                                "start": 1389,
                                "end": 1526
                            },
                            {
                                "start": 1527,
                                "end": 1702
                            }
                        ],
                        "ref_mentions": [
                            "248780330",
                            "49210924",
                            "174799390",
                            "52011473",
                            "170079112",
                            "218718706",
                            "220045815",
                            "233231380"
                        ],
                        "quote": "Related research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain (Moro et al., 2022). Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization (Liao et al., 2018). Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset (Fabbri et al., 2019). Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing (Nayeem et al., 2018). They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries (Liu et al., 2019). Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries (Li et al., 2020). Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words (Jin et al., 2020). Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature (DeYoung et al., 2021)."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[269762702 | Qu | 2024 | Citations: 0]",
                "snippets": "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[174799390 | Fabbri et al. | 2019 | Citations: 590]": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",
                    "[235097309 | Pasunuru et al. | 2021 | Citations: 73]": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.",
                    "[236478143 | Zhou et al. | 2021 | Citations: 28]": ",",
                    "[204960716 | Lewis et al. | 2019 | Citations: 10856]": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
                },
                "metadata": [
                    {
                        "section_title": "STATE OF THE ART",
                        "pdf_hash": "",
                        "start": 270,
                        "end": 1729,
                        "sentence_offsets": [
                            {
                                "start": 228,
                                "end": 547
                            },
                            {
                                "start": 547,
                                "end": 740
                            },
                            {
                                "start": 740,
                                "end": 879
                            },
                            {
                                "start": 879,
                                "end": 1129
                            },
                            {
                                "start": 1129,
                                "end": 1335
                            },
                            {
                                "start": 1335,
                                "end": 1466
                            },
                            {
                                "start": 1466,
                                "end": 1729
                            }
                        ],
                        "ref_mentions": [
                            "236478143",
                            "235097309",
                            "174799390",
                            "204960716",
                            "235097309"
                        ],
                        "quote": "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[271525553 | Shakil et al. | 2024 | Citations: 24]",
                "snippets": "Although much research focuses on single-document summarization, multi-document summarization presents unique challenges (Narayan et al., 2018)(Lamsiyah et al., 2021). Unlike its single-document counterpart, multi-document summarization involves synthesizing information from multiple sources, often necessitating the alignment of documents, identification, and resolution of redundancies, contradictions, and varying perspectives. These complexities introduce unique challenges such as ensuring coherence in the face of diverse inputs and maintaining a balanced representation of all source documents. The enormous amount of information that needs to be processed during multi-document summarization is one of the main challenges. The amount of data increases rapidly with many documents, causing computational difficulties and extending processing times [183]. The possibility of conflicting information across documents presents another challenge. Finding the most precise or relevant information can be troublesome, particularly if the source text comprises different authors or viewpoints (Erkan et al., 2004). Furthermore, the temporal part of the information can present difficulties. For instance, while summing up news articles, recent data may be more pertinent than older information, expecting models to have a sense of temporality (Wan et al., 2007).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[233948337 | Lamsiyah et al. | 2021 | Citations: 39]": "Text representation is a fundamental cornerstone that impacts the effectiveness of several text summarization methods. Transfer learning using pre-trained word embedding models has shown promising results. However, most of these representations do not consider the order and the semantic relationships between words in a sentence, and thus they do not carry the meaning of a full sentence. To overcome this issue, the current study proposes an unsupervised method for extractive multi-document summarization based on transfer learning from BERT sentence embedding model. Moreover, to improve sentence representation learning, we fine-tune BERT model on supervised intermediate tasks from GLUE benchmark datasets using single-task and multi-task fine-tuning methods. Experiments are performed on the standard DUC\u20192002\u20132004 datasets. The obtained results show that our method has significantly outperformed several baseline methods and achieves a comparable and sometimes better performance than the recent state-of-the-art deep learning\u2013based methods. Furthermore, the results show that fine-tuning BERT using multi-task learning has considerably improved the performance.",
                    "[3510042 | Narayan et al. | 2018 | Citations: 550]": "Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                },
                "metadata": [
                    {
                        "section_title": "Multi-Document Summarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1298,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 131
                            },
                            {
                                "start": 132,
                                "end": 395
                            },
                            {
                                "start": 396,
                                "end": 566
                            },
                            {
                                "start": 567,
                                "end": 695
                            },
                            {
                                "start": 696,
                                "end": 826
                            },
                            {
                                "start": 827,
                                "end": 914
                            },
                            {
                                "start": 915,
                                "end": 1063
                            },
                            {
                                "start": 1064,
                                "end": 1139
                            },
                            {
                                "start": 1140,
                                "end": 1298
                            }
                        ],
                        "ref_mentions": [
                            "3510042",
                            "233948337",
                            "506350",
                            "532313"
                        ],
                        "quote": "Although much research focuses on single-document summarization, multi-document summarization presents unique challenges (Narayan et al., 2018)(Lamsiyah et al., 2021). Unlike its single-document counterpart, multi-document summarization involves synthesizing information from multiple sources, often necessitating the alignment of documents, identification, and resolution of redundancies, contradictions, and varying perspectives. These complexities introduce unique challenges such as ensuring coherence in the face of diverse inputs and maintaining a balanced representation of all source documents. The enormous amount of information that needs to be processed during multi-document summarization is one of the main challenges. The amount of data increases rapidly with many documents, causing computational difficulties and extending processing times [183]. The possibility of conflicting information across documents presents another challenge. Finding the most precise or relevant information can be troublesome, particularly if the source text comprises different authors or viewpoints (Erkan et al., 2004). Furthermore, the temporal part of the information can present difficulties. For instance, while summing up news articles, recent data may be more pertinent than older information, expecting models to have a sense of temporality (Wan et al., 2007)."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[271903777 | Liu et al. | 2024 | Citations: 0]",
                "snippets": "Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise.\n\nNon-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary.\n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise.\n\nNon-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary.\n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020).",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[272146191 | Fernandes et al. | 2024 | Citations: 2]",
                "snippets": "Multi-document summarization (MDS) addresses the problem of extracting information that is spread across multiple documents, making it more challenging than single-document summarization. It is still an evolving field, with no single approach to solve this problem. The development of this field relies on datasets that serve as benchmarks for evaluating and comparing different summarization methods. The scientific literature includes some examples of such datasets, each varying in domain, structure, size, and summarization objective. These datasets are important for advancing research and improving summarization models. \n\nOutside the scientific domain, Multi-News [4] presents a large volume of news articles and summaries, focusing on the abstractive summarization of multiple documents in the journalistic context. Also in this context, Ghalandari et al. [5] proposed a large-scale dataset for multi-document summarization that contains concise human-written summaries of news events. In the legal field, Multi-LexSum tackles the summarization of legal cases, presenting a set of civil rights litigation summaries with multiple granularities. The summarization of instructional content is the focus of the HowSumm dataset, derived from WikiHow articles [1]. The FINDSum dataset focuses on the challenge of summarizing long text and multiple tables [10]. \n\nIn the scientific domain, the need to manage the vast amount of literature has generated interest in datasets specific to the summarization of scientific articles. Multi-XScience [12] is a dataset focused on the generation of \"related work\" sections by summarizing multiple scientific articles. BigSurvey (Liu et al., 2022) was designed to create structured summaries of academic articles, focusing on consolidating literature reviews. SumPubMed (Gupta et al., 2021) and ScisummNet (Yasunaga et al., 2019) aim the summarization of single scientific articles. \n\nSeveral other datasets can be used for multi-document summarization. Koh et al. [8] work is an extensive survey on long document summarization, including ten other MDS datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[236772881 | Gupta et al. | 2021 | Citations: 48]": "Most earlier work on text summarization is carried out on news article datasets. The summary in these datasets is naturally located at the beginning of the text. Hence, a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize. To address this issue, we constructed a new dataset, SumPubMed , using scientific articles from the PubMed archive. We conducted a human analysis of summary coverage, redundancy, readability, coherence, and informativeness on SumPubMed . SumPubMed is challenging because (a) the summary is distributed throughout the text (not-localized on top), and (b) it contains rare domain-specific scientific terms. We observe that seq2seq models that adequately summarize news articles struggle to summarize SumPubMed . Thus, SumPubMed opens new avenues for the future improvement of models as well as the development of new evaluation metrics.",
                    "[250636132 | Liu et al. | 2022 | Citations: 18]": "Writing a survey paper on one research topic usually needs to cover the salient content from numerous related papers, which can be modeled as a multi-document summarization (MDS) task. Existing MDS datasets usually focus on producing the structureless summary covering a few input documents. Meanwhile, previous structured summary generation works focus on summarizing a single document into a multi-section summary. These existing datasets and methods cannot meet the requirements of summarizing numerous academic papers into a structured summary. To deal with the scarcity of available data, we propose BigSurvey, the first large-scale dataset for generating comprehensive summaries of numerous academic papers on each topic. We collect target summaries from more than seven thousand survey papers and utilize their 430 thousand reference papers\u2019 abstracts as input documents. To organize the diverse content from dozens of input documents and ensure the efficiency of processing long text sequences, we propose a summarization method named category-based alignment and sparse transformer (CAST). The experimental results show that our CAST method outperforms various advanced summarization methods.",
                    "[58053521 | Yasunaga et al. | 2019 | Citations: 209]": "Scientific article summarization is challenging: large, annotated corpora are not available, and the summary should ideally include the article\u2019s impacts on research community. This paper provides novel solutions to these two challenges. We 1) develop and release the first large-scale manually-annotated corpus for scientific papers (on computational linguistics) by enabling faster annotation, and 2) propose summarization methods that integrate the authors\u2019 original highlights (abstract) and the article\u2019s actual impacts on the community (citations), to create comprehensive, hybrid summaries. We conduct experiments to demonstrate the efficacy of our corpus in training data-driven models for scientific paper summarization and the advantage of our hybrid summaries over abstracts and traditional citation-based summaries. Our large annotated corpus and hybrid methods provide a new framework for scientific paper summarization research."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2052,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 187
                            },
                            {
                                "start": 188,
                                "end": 265
                            },
                            {
                                "start": 266,
                                "end": 401
                            },
                            {
                                "start": 402,
                                "end": 538
                            },
                            {
                                "start": 539,
                                "end": 626
                            },
                            {
                                "start": 629,
                                "end": 823
                            },
                            {
                                "start": 824,
                                "end": 993
                            },
                            {
                                "start": 994,
                                "end": 1151
                            },
                            {
                                "start": 1152,
                                "end": 1266
                            },
                            {
                                "start": 1267,
                                "end": 1362
                            },
                            {
                                "start": 1365,
                                "end": 1528
                            },
                            {
                                "start": 1529,
                                "end": 1659
                            },
                            {
                                "start": 1660,
                                "end": 1785
                            },
                            {
                                "start": 1786,
                                "end": 1872
                            },
                            {
                                "start": 1875,
                                "end": 1943
                            },
                            {
                                "start": 1944,
                                "end": 2052
                            }
                        ],
                        "ref_mentions": [
                            "250636132",
                            "236772881",
                            "58053521"
                        ],
                        "quote": "Multi-document summarization (MDS) addresses the problem of extracting information that is spread across multiple documents, making it more challenging than single-document summarization. It is still an evolving field, with no single approach to solve this problem. The development of this field relies on datasets that serve as benchmarks for evaluating and comparing different summarization methods. The scientific literature includes some examples of such datasets, each varying in domain, structure, size, and summarization objective. These datasets are important for advancing research and improving summarization models. \n\nOutside the scientific domain, Multi-News [4] presents a large volume of news articles and summaries, focusing on the abstractive summarization of multiple documents in the journalistic context. Also in this context, Ghalandari et al. [5] proposed a large-scale dataset for multi-document summarization that contains concise human-written summaries of news events. In the legal field, Multi-LexSum tackles the summarization of legal cases, presenting a set of civil rights litigation summaries with multiple granularities. The summarization of instructional content is the focus of the HowSumm dataset, derived from WikiHow articles [1]. The FINDSum dataset focuses on the challenge of summarizing long text and multiple tables [10]. \n\nIn the scientific domain, the need to manage the vast amount of literature has generated interest in datasets specific to the summarization of scientific articles. Multi-XScience [12] is a dataset focused on the generation of \"related work\" sections by summarizing multiple scientific articles. BigSurvey (Liu et al., 2022) was designed to create structured summaries of academic articles, focusing on consolidating literature reviews. SumPubMed (Gupta et al., 2021) and ScisummNet (Yasunaga et al., 2019) aim the summarization of single scientific articles. \n\nSeveral other datasets can be used for multi-document summarization. Koh et al. [8] work is an extensive survey on long document summarization, including ten other MDS datasets."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[272969413 | Godbole et al. | 2024 | Citations: 5]",
                "snippets": "Multi-document summarization presents unique challenges due to the need for synthesizing information from diverse sources, which may contain redundant, complementary, or contradictory information across documents [4]. Variations in writing style and level of detail add complexity to the task. Determining the relevance and importance of information from each source is crucial for creating a coherent and comprehensive summary [5].\n\nTraditional document summarization techniques often struggle with redundancy, inconsistency, lack of context understanding, scalability issues for multiple document summarization tasks, inability to capture cross-document relationships, difficulty handling diverse formats, and lack of domain adaptability [6,7]8]. These limitations highlight the need for more advanced approaches to multi-document summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[19198109 | Cao et al. | 2017 | Citations: 371]": "\n \n Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. Our preliminary study reveals nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from this problem. While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. To avoid generating fake facts in a summary, we leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text. The dual-attention sequence-to-sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions. Experiments on the Gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80%. Notably, the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text.\n \n",
                    "[269225 | Parveen et al. | 2015 | Citations: 117]": "We present an approach for extractive single-document summarization. Our approach is based on a weighted graphical representation of documents obtained by topic modeling. We optimize importance, coherence and non-redundancy simultaneously using ILP. We compare ROUGE scores of our system with state-of-the-art results on scientific articles from PLOS Medicine and on DUC 2002 data. Human judges evaluate the coherence of summaries generated by our system in comparision to two baselines. Our approach obtains competitive performance.",
                    "[8377315 | Bing et al. | 2015 | Citations: 144]": "We propose an abstraction-based multi-document summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences, namely, noun/verb phrases. Different from existing abstraction-based approaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-of-the-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 455,
                        "end": 1302,
                        "sentence_offsets": [
                            {
                                "start": 415,
                                "end": 539
                            },
                            {
                                "start": 540,
                                "end": 757
                            },
                            {
                                "start": 758,
                                "end": 833
                            },
                            {
                                "start": 834,
                                "end": 972
                            },
                            {
                                "start": 975,
                                "end": 1289
                            },
                            {
                                "start": 1290,
                                "end": 1388
                            }
                        ],
                        "ref_mentions": [
                            "8377315",
                            "337730",
                            "1296465",
                            "269225",
                            "19198109"
                        ],
                        "quote": "Multi-document summarization presents unique challenges due to the need for synthesizing information from diverse sources, which may contain redundant, complementary, or contradictory information across documents [4]. Variations in writing style and level of detail add complexity to the task. Determining the relevance and importance of information from each source is crucial for creating a coherent and comprehensive summary [5].\n\nTraditional document summarization techniques often struggle with redundancy, inconsistency, lack of context understanding, scalability issues for multiple document summarization tasks, inability to capture cross-document relationships, difficulty handling diverse formats, and lack of domain adaptability [6,7]8]. These limitations highlight the need for more advanced approaches to multi-document summarization."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[278000561 | Tan et al. | 2025 | Citations: 0]",
                "snippets": "Multi-document summarization (MDS) is a task that aims to generate concise and coherent summaries by synthesizing information from multiple documents on the same topic (Jin et al., 2020)(Li et al., 2020)(Ma, 2021)(Mao et al., 2020)(Pang et al., 2021). MDS can lead to diverse applications, such as news aggregation (Chen et al., 2024)(Fabbri et al., 2019)(Khatuya et al., 2024), scientific research (DeYoung et al., 2021)(Lu et al., 2020)(Wang et al., 2024), and legal document analysis [17,(Malik et al., 2024)(Shen et al., 2022). Current MDS approaches can be categorized into two classes: Graph-based models (Cui et al., 2021)(Li et al., 2023)(Pasunuru et al., 2021)(Qu, 2024)(Zhang et al., 2023) and pre-trained language models [2](Puduppully et al., 2022)(Xiao et al., 2021). Graph-based models rely on auxiliary information (e.g., discourse structures) as an input graph to capture the cross-document relationships, while pre-trained language models use the attention mechanisms to capture them.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[174799390 | Fabbri et al. | 2019 | Citations: 590]": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[220045815 | Jin et al. | 2020 | Citations: 99]": "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.",
                    "[233231380 | DeYoung et al. | 2021 | Citations: 113]": "To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system\u2019s generated summaries. Data and models are available at https://github.com/allenai/ms2.",
                    "[235097309 | Pasunuru et al. | 2021 | Citations: 73]": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.",
                    "[239050558 | Cui et al. | 2021 | Citations: 41]": "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics.",
                    "[269762702 | Qu | 2024 | Citations: 0]": "With the development of information technology, a large amount of information and corpora has been incrementally sparked from the Web, stimulating an increasingly high demand for summarizing. Document Summarization is one of Natural Language Processing tasks, which aims to generate abridged versions of a given single or multiple documents as concise and coherent as possible while preserving salient information from the source texts. Recent research in the area has started to use knowledge graphs as they can capture more factual and applicable information from more facets along with source information, benefiting fact consistency and informativeness of generated summaries, rather than just from a linguistic perspective. However, there is no explicit investigation of the effects of different kinds of knowledge graphs on document summarization. The proposed method is to use structured informative and knowledgeable auxiliary information, especially knowledge graphs, into pre-trained summarization models, advancing summary qualities. Expected outcomes are exploring knowledge and knowledge graph incorporation for multi-document summarization, and achieving more informative, coherent, and factually consistent summaries.",
                    "[222090788 | Mao et al. | 2020 | Citations: 47]": "While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning. Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.",
                    "[225075639 | Lu et al. | 2020 | Citations: 120]": "Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results\u2014using several state-of-the-art models trained on the Multi-XScience dataset\u2014reveal that Multi-XScience is well suited for abstractive models.",
                    "[235352668 | Pang et al. | 2021 | Citations: 17]": "We aim to renew interest in a particular multi-document summarization (MDS) task which we call AgreeSum: agreement-oriented multi-document summarization. Given a cluster of articles, the goal is to provide abstractive summaries that represent information common and faithful to all input articles. Given the lack of existing datasets, we create a dataset for AgreeSum, and provide annotations on article-summary entailment relations for a subset of the clusters in the dataset. We aim to create strong baselines for the task by applying the top-performing pretrained single-document summarization model PEGASUS onto AgreeSum, leveraging both annotated clusters by supervised losses, and unannotated clusters by T5-based entailment-related and language-related losses. Compared to other baselines, both automatic evaluation and human evaluation show better article-summary and cluster-summary entailment in generated summaries. On a separate note, we hope that our article-summary entailment annotations contribute to the community's effort in improving abstractive summarization faithfulness.",
                    "[235792259 | Ma | 2021 | Citations: 2]": "Multi-document summarization is one of the most important tasks in the field of Natural Language Processing (NLP) and it gains increasing attention in recent years. It aims to generate one summary across several topic-related documents. Compared with extractive summarization, abstractive summarization is more similar to human-written ones. Proposing effective and efficient abstractive multi-document summarization models is significant to the NLP community. Existing deep learning based multi-document summarization models rely on the exceptional ability of neural networks to extract distinct features. However, they have missed out important linguistic knowledge such as dependencies between words since linguistics information in texts is full of meaningful knowledge with respect to the input documents. Besides, how models automatically evaluate the quality of the summary is crucial to design a high-performance summarization model since the evaluation indicator objectively measures the effectiveness of a method. In this proposal, we bring forward two research questions and corresponding solutions for the abstractive multi-document summarization task.",
                    "[247519084 | Xiao et al. | 2021 | Citations: 119]": "We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins.",
                    "[249927023 | Shen et al. | 2022 | Citations: 48]": "With the advent of large language models, methods for abstractive summarization have made great strides, creating potential for use in applications to aid knowledge workers processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC) (https://clearinghouse.net),which posts information about large-scale civil rights lawsuits, serving lawyers, scholars, and the general public. Today, summarization in the CRLC requires extensive training of lawyers and law students who spend hours per case understanding multiple relevant documents in order to produce high-quality summaries of key events and outcomes. Motivated by this ongoing real-world summarization effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing. Multi-LexSum presents a challenging multi-document summarization task given the length of the source documents, often exceeding two hundred pages per case. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence\"extreme\"summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task. We release Multi-LexSum for further research in summarization methods as well as to facilitate development of applications to assist in the CRLC's mission at https://multilexsum.github.io.",
                    "[251224184 | Puduppully et al. | 2022 | Citations: 12]": "In Multi-Document Summarization (MDS), the input can be modeled as a set of documents, and the output is its summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a novel pretraining objective, which involves selecting the ROUGE-based centroid of each document cluster as a proxy for its summary. Our objective thus does not require human written summaries and can be utilized for pretraining on a dataset consisting solely of document sets. Through zero-shot, few-shot, and fully supervised experiments on multiple MDS datasets, we show that our model Centrum is better or comparable to a state-of-the-art model. We make the pretrained and fine-tuned models freely available to the research communityhttps://github.com/ratishsp/centrum.",
                    "[257496469 | Li et al. | 2023 | Citations: 11]": "Multi-document summarization (MDS) aims to generate a summary for a number of related documents. We propose HGSum \u2014 an MDS model that extends an encoder-decoder architecture to incorporate a heterogeneous graph to represent different semantic units (e.g., words and sentences) of the documents. This contrasts with existing MDS models which do not consider different edge types of graphs and as such do not capture the diversity of relationships in the documents. To preserve only key information and relationships of the documents in the heterogeneous graph, HGSum uses graph pooling to compress the input graph. And to guide HGSum to learn the compression, we introduce an additional objective that maximizes the similarity between the compressed graph and the graph constructed from the ground-truth summary during training. HGSum is trained end-to-end with the graph similarity and standard cross-entropy objectives. Experimental results over Multi-News, WCEP-100, and Arxiv show that HGSum outperforms state-of-the-art MDS models. The code for our model and experiments is available at: https://github.com/oaimli/HGSum.",
                    "[258378312 | Zhang et al. | 2023 | Citations: 16]": "Information extraction (IE) and summarization are closely related, both tasked with presenting a subset of the information contained in a natural language text. However, while IE extracts structural representations, summarization aims to abstract the most salient information into a generated text summary \u2013 thus potentially encountering the technical limitations of current text generation methods (e.g., hallucination). To mitigate this risk, this work uses structured IE graphs to enhance the abstractive summarization task. Specifically, we focus on improving Multi-Document Summarization (MDS) performance by using cross-document IE output, incorporating two novel components: (1) the use of auxiliary entity and event recognition systems to focus the summary generation model; (2) incorporating an alignment loss between IE nodes and their text spans to reduce inconsistencies between the IE graphs and text representations. Operationally, both the IE nodes and corresponding text spans are projected into the same embedding space and pairwise distance is minimized. Experimental results on multiple MDS benchmarks show that summaries generated by our model are more factually consistent with the source documents than baseline models while maintaining the same level of abstractiveness.",
                    "[269157041 | Wang et al. | 2024 | Citations: 2]": "Automatically condensing multiple topic-related scientific papers into a succinct and concise summary is referred to as Multi-Document Scientific Summarization (MDSS). Currently, while commonly used abstractive MDSS methods can generate flexible and coherent summaries, the difficulty in handling global information and the lack of guidance during decoding still make it challenging to generate better summaries. To alleviate these two shortcomings, this paper introduces summary candidates into MDSS, utilizing the global information of the document set and additional guidance from the summary candidates to guide the decoding process. Our insights are twofold: Firstly, summary candidates can provide instructive information from both positive and negative perspectives, and secondly, selecting higher-quality candidates from multiple options contributes to producing better summaries. Drawing on the insights, we propose a summary candidates fusion framework - Disentangling Instructive information from Ranked candidates (DIR) for MDSS. Specifically, DIR first uses a specialized pairwise comparison method towards multiple candidates to pick out those of higher quality. Then DIR disentangles the instructive information of summary candidates into positive and negative latent variables with Conditional Variational Autoencoder. These variables are further incorporated into the decoder to guide generation. We evaluate our approach with three different types of Transformer-based models and three different types of candidates, and consistently observe noticeable performance improvements according to automatic and human evaluation. More analyses further demonstrate the effectiveness of our model in handling global information and enhancing decoding controllability.",
                    "[269756893 | Khatuya et al. | 2024 | Citations: 4]": "While automatic summarization techniques have made significant advancements, their primary focus has been on summarizing short news articles or documents that have clear structural patterns like scientific articles or government reports. There has not been much exploration into developing efficient methods for summarizing financial documents, which often contain complex facts and figures. Here, we study the problem of bullet point summarization of long Earning Call Transcripts (ECTs) using the recently released ECTSum dataset. We leverage an unsupervised question-based extractive module followed by a parameter efficient instruction-tuned abstractive module to solve this task. Our proposed model FLANFinBPS achieves new state-of-the-art performances outperforming the strongest baseline with 14.88% average ROUGE score gain, and is capable of generating factually consistent bullet point summaries that capture the important facts discussed in the ECTs. We make the codebase publicly available at https://github.com/subhendukhatuya/FLAN-FinBPS.",
                    "[270371298 | Chen et al. | 2024 | Citations: 5]": "A proficient summarization model should exhibit both flexibility -- the capacity to handle a range of in-domain summarization tasks, and adaptability -- the competence to acquire new knowledge and adjust to unseen out-of-domain tasks. Unlike large language models (LLMs) that achieve this through parameter scaling, we propose a more parameter-efficient approach in this study. Our motivation rests on the principle that the general summarization ability to capture salient information can be shared across different tasks, while the domain-specific summarization abilities need to be distinct and tailored. Concretely, we propose MoeSumm, a Mixture-of-Expert Summarization architecture, which utilizes a main expert for gaining the general summarization capability and deputy experts that selectively collaborate to meet specific summarization task requirements. We further propose a max-margin loss to stimulate the separation of these abilities. Our model's distinct separation of general and domain-specific summarization abilities grants it with notable flexibility and adaptability, all while maintaining parameter efficiency. MoeSumm achieves flexibility by managing summarization across multiple domains with a single model, utilizing a shared main expert and selected deputy experts. It exhibits adaptability by tailoring deputy experts to cater to out-of-domain few-shot and zero-shot scenarios. Experimental results on 11 datasets show the superiority of our model compared with recent baselines and LLMs. We also provide statistical and visual evidence of the distinct separation of the two abilities in MoeSumm https://github.com/iriscxy/MoE_Summ",
                    "[271114508 | Malik et al. | 2024 | Citations: 1]": "Extracting relevant information from legal documents is a challenging task due to the technical complexity and volume of their content. These factors also increase the costs of annotating large datasets, which are required to train state-of-the-art summarization systems. To address these challenges, we introduce CivilSum, a collection of 23,350 legal case decisions from the Supreme Court of India and other Indian High Courts paired with human-written summaries. Compared to previous datasets such as IN-Abs, CivilSum not only has more legal decisions but also provides shorter and more abstractive summaries, thus offering a challenging benchmark for legal summarization. Unlike other domains such as news articles, our analysis shows the most important content tends to appear at the end of the documents. We measure the effect of this tail bias on summarization performance using strong architectures for long-document abstractive summarization, and the results highlight the importance of long sequence modeling for the proposed task. CivilSum and related code are publicly available to the research community to advance text summarization in the legal domain."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 690,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 185
                            },
                            {
                                "start": 186,
                                "end": 331
                            },
                            {
                                "start": 332,
                                "end": 469
                            },
                            {
                                "start": 470,
                                "end": 690
                            }
                        ],
                        "ref_mentions": [
                            "220045815",
                            "218718706",
                            "235792259",
                            "222090788",
                            "235352668",
                            "270371298",
                            "174799390",
                            "269756893",
                            "233231380",
                            "225075639",
                            "269157041",
                            "271114508",
                            "249927023",
                            "239050558",
                            "257496469",
                            "235097309",
                            "269762702",
                            "258378312",
                            "251224184",
                            "247519084"
                        ],
                        "quote": "Multi-document summarization (MDS) is a task that aims to generate concise and coherent summaries by synthesizing information from multiple documents on the same topic (Jin et al., 2020)(Li et al., 2020)(Ma, 2021)(Mao et al., 2020)(Pang et al., 2021). MDS can lead to diverse applications, such as news aggregation (Chen et al., 2024)(Fabbri et al., 2019)(Khatuya et al., 2024), scientific research (DeYoung et al., 2021)(Lu et al., 2020)(Wang et al., 2024), and legal document analysis [17,(Malik et al., 2024)(Shen et al., 2022). Current MDS approaches can be categorized into two classes: Graph-based models (Cui et al., 2021)(Li et al., 2023)(Pasunuru et al., 2021)(Qu, 2024)(Zhang et al., 2023) and pre-trained language models [2](Puduppully et al., 2022)(Xiao et al., 2021). Graph-based models rely on auxiliary information (e.g., discourse structures) as an input graph to capture the cross-document relationships, while pre-trained language models use the attention mechanisms to capture them."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[678258 | Haghighi et al. | 2009 | Citations: 560]",
                "snippets": "We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HierSum, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HierSum, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[7031344 | Barzilay et al. | 1999 | Citations: 484]",
                "snippets": "Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin et al., 1997)(Marcu, 1997)(Salton et al., 1994). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while ex-tracting some similar sentences could produce a summary biased towards some sources.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[5519987 | Lin et al. | 1997 | Citations: 316]": "This paper addresses the problem of identifying likely topics of texts by their position in the text. It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure. This method can be used in applications such as information retrieval, routing, and text summarization."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 899,
                        "end": 1419,
                        "sentence_offsets": [
                            {
                                "start": 899,
                                "end": 1089
                            },
                            {
                                "start": 1090,
                                "end": 1244
                            },
                            {
                                "start": 1245,
                                "end": 1419
                            }
                        ],
                        "ref_mentions": [
                            "5519987",
                            "11680756",
                            "32296317"
                        ],
                        "quote": "Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin et al., 1997)(Marcu, 1997)(Salton et al., 1994). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while ex-tracting some similar sentences could produce a summary biased towards some sources."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[8294822 | Goldstein et al. | 2000 | Citations: 447]",
                "snippets": "Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani et al., 1997)(Carbonell et al., 1998)TIPSTER, 1998b;Radev and McKeown, 1998;Mani and Bloedorn, 1999;McKeown et al., .!999;(Stein, 1999). These include comparing templates filled in by extracting information -using specialized, domain specific knowledge sources -from the doc-\"ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com--\u2022 paring named-entities -extracted using specialized lists between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related lexical items (identity mappings, synonyms, hypernyms, etc.) to extract text spans from the document set (Mani et al., 1997). Another system (Stein, 1999) creates a multi-document summary from multiple single document summaries, an approach that can be sub-optimal in some cases, due to the fact that the process of generating the final multi-document summary takes as input the individual summaries and not the complete documents. (Particularly if the single-document summaries can contain much overlapping information.) The Columbia University system (McKeown et al., 1999) creates a multi-document summary using machine learning and statistical techniques to identify similar sections and language generation to reformulate the summary.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[60778976 | Carbonell et al. | 1998 | Citations: 14]": "This paper presents a method for combining\nquery-relevance with information-novelty in the context\nof text retrieval and summarization. The Maximal\nMarginal Relevance (MMR) criterion strives to reduce\nredundancy while maintaining query relevance in\nre-ranking retrieved documents and in selecting appropriate passages for text summarization. Preliminary results\nindicate some benefits for MMR diversity ranking\nin document retrieval and in single document summarization.\nThe latter are borne out by the recent results of the\nSUMMAC conference in the evaluation of summarization\nsystems. However, the clearest advantage is demonstrated\nin constructing non-redundant multi-document\nsummaries, where MMR results are clearly superior to\nnon-MMR passage selection.",
                    "[8115414 | McKeown et al. | 1999 | Citations: 245]": "By synthesizing information common to retrieved documents, multi-document summarization can help users of information retrieval systems to find relevant documents with a minimal amount of reading. We are developing a multidocument summarization system to automatically generate a concise summary by identifying and synthesizing similarities across a set of related documents. Our approach is unique in its integration of machine learning and statistical techniques to identify similar paragraphs, intersection of similar phrases within paragraphs, and language generation to reformulate the wording of the summary. Our evaluation of system components shows that learning over multiple extracted linguistic features is more effective than information retrieval approaches at identifying similar text units for summarization and that it is possible to generate a fluent summary that conveys similarities among documents even when full semantic interpretations of the input text are not available."
                },
                "metadata": [
                    {
                        "section_title": "Background and Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1560,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 279
                            },
                            {
                                "start": 280,
                                "end": 939
                            },
                            {
                                "start": 940,
                                "end": 1252
                            },
                            {
                                "start": 1253,
                                "end": 1342
                            },
                            {
                                "start": 1343,
                                "end": 1560
                            }
                        ],
                        "ref_mentions": [
                            "9177142",
                            "60778976",
                            "232822009",
                            "9177142",
                            "232822009",
                            "8115414"
                        ],
                        "quote": "Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani et al., 1997)(Carbonell et al., 1998)TIPSTER, 1998b;Radev and McKeown, 1998;Mani and Bloedorn, 1999;McKeown et al., .!999;(Stein, 1999). These include comparing templates filled in by extracting information -using specialized, domain specific knowledge sources -from the doc-\"ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com--\u2022 paring named-entities -extracted using specialized lists between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related lexical items (identity mappings, synonyms, hypernyms, etc.) to extract text spans from the document set (Mani et al., 1997). Another system (Stein, 1999) creates a multi-document summary from multiple single document summaries, an approach that can be sub-optimal in some cases, due to the fact that the process of generating the final multi-document summary takes as input the individual summaries and not the complete documents. (Particularly if the single-document summaries can contain much overlapping information.) The Columbia University system (McKeown et al., 1999) creates a multi-document summary using machine learning and statistical techniques to identify similar sections and language generation to reformulate the summary."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[9174081 | Marujo et al. | 2015 | Citations: 17]",
                "snippets": "The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.\n\nIn general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics.\n\nWe propose to extend a state-of-the-art single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013), capable of focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.\n\nIn general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics.\n\nWe propose to extend a state-of-the-art single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013), capable of focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization.",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.04047,
        "cot": "For this query on multi-document summarization references, I need to organize the quotes into logical dimensions that will help create a comprehensive overview of work in this field.\n\nFirst, I'll create an \"Introduction/Background\" section to provide context about what multi-document summarization is and why it's important. This will help frame the rest of the content even though the query is primarily asking for references.\n\nNext, I'll organize the references into several key dimensions:\n\n1. \"Evolution of Multi-Document Summarization Approaches\" - This will be a synthesis section covering the historical progression of methods over time. Many quotes describe this evolution, and a cohesive narrative will be more valuable than a bullet list.\n\n2. \"Extractive Approaches to Multi-Document Summarization\" - This will be a list format section, as the user wants references to specific methods and approaches. This category represents a major branch of techniques in the field.\n\n3. \"Abstractive Approaches to Multi-Document Summarization\" - Another list format section covering the other major branch of techniques, particularly neural approaches that have become dominant in recent years.\n\n4. \"Graph-Based Methods for Multi-Document Summarization\" - This will be a list format section since graph-based methods are frequently mentioned in the quotes and represent an important specific approach within the field.\n\n5. \"Datasets and Evaluation for Multi-Document Summarization\" - A list format section covering the important datasets and evaluation metrics mentioned in the quotes, which are critical for researchers to know about.\n\n6. \"Challenges in Multi-Document Summarization\" - A synthesis section discussing the various challenges mentioned across quotes that make multi-document summarization difficult compared to single-document summarization.\n\nThese dimensions cover the major aspects of the field mentioned in the quotes and provide a structured way to present references to the user that will be useful for research purposes.",
        "plan": {
            "Introduction and Background (synthesis)": [
                5,
                11,
                12,
                15,
                16,
                28,
                30,
                31
            ],
            "Evolution of Multi-Document Summarization Approaches (synthesis)": [
                13,
                18,
                19,
                20,
                34,
                35
            ],
            "Extractive Approaches to Multi-Document Summarization (list)": [
                1,
                3,
                4,
                7,
                9,
                11,
                14,
                22,
                28,
                33
            ],
            "Abstractive Approaches to Multi-Document Summarization (list)": [
                2,
                6,
                9,
                12,
                14,
                17,
                23,
                24,
                25,
                26,
                28,
                32
            ],
            "Graph-Based Methods for Multi-Document Summarization (list)": [
                0,
                4,
                7,
                8,
                13,
                17,
                19,
                26,
                31
            ],
            "Datasets and Evaluation for Multi-Document Summarization (list)": [
                2,
                5,
                13,
                19,
                21,
                23,
                25,
                29
            ],
            "Challenges in Multi-Document Summarization (synthesis)": [
                10,
                15,
                27,
                30
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction and Background",
                "tldr": "Multi-document summarization (MDS) aims to generate concise summaries from multiple topic-related documents, requiring systems to handle redundancy, conflicting information, and cross-document relationships. This challenging task has evolved from traditional extractive methods to neural approaches, with recent advances leveraging graph-based techniques and pre-trained language models. (11 sources)",
                "text": "\nMulti-document summarization (MDS) is the task of generating a concise and informative summary from a set of topic-related documents <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"238221709\" paperTitle=\"(Jalil et al., 2021)\" isShortName></Paper>. Unlike single-document summarization, MDS presents unique challenges due to the need to synthesize information from diverse sources that may contain redundant, complementary, or contradictory information <Paper corpusId=\"272969413\" paperTitle=\"(Godbole et al., 2024)\" isShortName></Paper> <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>. A multi-document summary serves as a representation of the information contained in a cluster of documents, helping users understand the gist of those documents without having to read them all <Paper corpusId=\"238221709\" paperTitle=\"(Jalil et al., 2021)\" isShortName></Paper>.\n\nMDS methods can be broadly categorized into extractive and abstractive approaches <Paper corpusId=\"226283949\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper> <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Extractive approaches select and arrange key sentences from the original documents based on their importance <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Early extractive methods relied on techniques such as retrieval, centroids, graph centrality, and utility maximization <Paper corpusId=\"222140880\" paperTitle=\"(Bista et al., 2020)\" isShortName></Paper>. The LexRank algorithm, which uses graph-based centrality measures to identify important sentences, has been particularly influential in this area <Paper corpusId=\"226283949\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper> <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>.\n\nIn contrast, abstractive approaches can generate more flexible and coherent summaries by creating new sentences that capture the essence of the source documents <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. However, due to the historical lack of large-scale training data, most earlier MDS methods were extractive <Paper corpusId=\"226283949\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper>. This situation has been changing with the introduction of datasets like Multi-News, which has enabled more research into neural abstractive MDS <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper>.\n\nRecent advances in MDS have been driven by several key developments. First, the application of pre-trained language models, which have shown promising results in single-document summarization, to the multi-document setting <Paper corpusId=\"278000561\" paperTitle=\"(Tan et al., 2025)\" isShortName></Paper>. Second, the use of hierarchical models that can extract different-grained features and select important information across documents <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Third, the incorporation of graph-based methods to model relationships between textual units and enhance representation <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278000561\" paperTitle=\"(Tan et al., 2025)\" isShortName></Paper>.\n\nDespite these advances, MDS still faces significant challenges. Simply adopting models that work well for single-document summarization may not lead to ideal results in the multi-document setting <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"52053741\" paperTitle=\"(Lebanoff et al., 2018)\" isShortName></Paper>. Cross-document relationships need to be effectively modeled <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"170079112\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>, and systems must deal with issues of redundancy, inconsistency, lack of context understanding, and difficulty handling diverse formats <Paper corpusId=\"272969413\" paperTitle=\"(Godbole et al., 2024)\" isShortName></Paper>.\n\nMDS has diverse applications across domains, including news aggregation, scientific research, and legal document analysis <Paper corpusId=\"278000561\" paperTitle=\"(Tan et al., 2025)\" isShortName></Paper>. Each domain presents its own unique challenges and requirements, driving the need for specialized approaches and domain-specific datasets.",
                "citations": [
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise.\n\nNon-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary.\n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 271903777,
                            "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2316447969",
                                    "name": "Ran Liu"
                                },
                                {
                                    "authorId": "2287803134",
                                    "name": "Ming Liu"
                                },
                                {
                                    "authorId": "2316535336",
                                    "name": "Min Yu"
                                },
                                {
                                    "authorId": "152634491",
                                    "name": "Jianguo Jiang"
                                },
                                {
                                    "authorId": "1996030117",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2316503511",
                                    "name": "Dan Zhang"
                                },
                                {
                                    "authorId": "2316432820",
                                    "name": "Jingyuan Li"
                                },
                                {
                                    "authorId": "2282447587",
                                    "name": "Xiang Meng"
                                },
                                {
                                    "authorId": "2282485475",
                                    "name": "Weiqing Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.90869140625
                    },
                    {
                        "id": "(Jalil et al., 2021)",
                        "snippets": [
                            "For instance, the same contents may be covered from multiple sources, so at times, a number of documents may be available to gain an insight into the same event (Glavas et al., 2014). In this regard, a multi-document summary becomes a representation of the information contained in a cluster of documents which helps users understand the gist of those documents (Alguliyev et al., 2012), (Luo et al., 2013). A multi-document summary represents the information contained in the cluster of documents and helps users understand those documents (Alguliyev et al., 2013)."
                        ],
                        "paper": {
                            "corpus_id": 238221709,
                            "title": "Extractive Multi-Document Summarization: A Review of Progress in the Last Decade",
                            "authors": [
                                {
                                    "authorId": "70642020",
                                    "name": "Zakia Jalil"
                                },
                                {
                                    "authorId": "35560924",
                                    "name": "Jamal Abdul Nasir"
                                },
                                {
                                    "authorId": "2128893061",
                                    "name": "Muhammad Nasir"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Access",
                            "n_citations": 14
                        },
                        "score": 0.904296875
                    },
                    {
                        "id": "(Godbole et al., 2024)",
                        "snippets": [
                            "Multi-document summarization presents unique challenges due to the need for synthesizing information from diverse sources, which may contain redundant, complementary, or contradictory information across documents [4]. Variations in writing style and level of detail add complexity to the task. Determining the relevance and importance of information from each source is crucial for creating a coherent and comprehensive summary [5].\n\nTraditional document summarization techniques often struggle with redundancy, inconsistency, lack of context understanding, scalability issues for multiple document summarization tasks, inability to capture cross-document relationships, difficulty handling diverse formats, and lack of domain adaptability [6,7]8]. These limitations highlight the need for more advanced approaches to multi-document summarization."
                        ],
                        "paper": {
                            "corpus_id": 272969413,
                            "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications",
                            "authors": [
                                {
                                    "authorId": "2350511520",
                                    "name": "Aditi Godbole"
                                },
                                {
                                    "authorId": "2301049091",
                                    "name": "Jabin Geevarghese George"
                                },
                                {
                                    "authorId": "48781397",
                                    "name": "Smita Shandilya"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.93115234375
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020).\n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018).\n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 236478143,
                            "title": "Entity-Aware Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2053308860",
                                    "name": "Weidong Ren"
                                },
                                {
                                    "authorId": "150112803",
                                    "name": "Gongshen Liu"
                                },
                                {
                                    "authorId": "153253583",
                                    "name": "Bo Su"
                                },
                                {
                                    "authorId": "143844110",
                                    "name": "Wei Lu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 28
                        },
                        "score": 0.93408203125
                    },
                    {
                        "id": "(Jin et al., 2020)",
                        "snippets": [
                            "Multi-document summarization aims at producing a fluent, condensed summary for the given document set",
                            "Compared with single-document summarization, multi-document summarization needs to handle multiple input documents. A simple method is to concatenate multiple documents into a long flat text and treat it as a long sequence-tosequence task.\n\nEmpowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017; Paulus et al., 2018; Tan et al., 2017; \u00c7 elikyilmaz et al., 2018)",
                            "Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization.\n\nThe methods for multi-document summarization can generally be categorized to extractive and abstractive",
                            "Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004; Christensen et al., 2013; Yasunaga et al., 2017)."
                        ],
                        "paper": {
                            "corpus_id": 226283949,
                            "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1491212422",
                                    "name": "Hanqi Jin"
                                },
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2020,
                            "venue": "Findings",
                            "n_citations": 15
                        },
                        "score": 0.958984375
                    },
                    {
                        "id": "(Bista et al., 2020)",
                        "snippets": [
                            "Most existing work on this topic has focused on the generic summarization task. However, update summarization is of equal practical interest.\n\nMulti-document extractive summarization methods can be unsupervised or supervised. Unsupervised methods typically define salience (or coverage) using a global model of sentence-sentence similarity. Methods based on retrieval (Goldstein et al., 1999), centroids (Radev et al., 2004), graph centrality (Erkan and Radev, 2004), or utility maximization (Lin andBilmes, 2010, 2011;Gillick and Favre, 2009) have been well explored. However, sentence salience also depends on surface features (e.g., position, length, presence of cue words); effectively capturing these requires supervised models specific to the dataset and task. A body of work has incorporated such information through supervised learning, for example based on point processes (Kulesza and Taskar, 2012), learning important words (Hong and Nenkova, 2014), graph neural networks (Yasunaga et al., 2017), and support vector regression (Varma et al., 2009)."
                        ],
                        "paper": {
                            "corpus_id": 222140880,
                            "title": "SupMMD: A Sentence Importance Model for Extractive Summarisation using Maximum Mean Discrepancy",
                            "authors": [
                                {
                                    "authorId": "2168767",
                                    "name": "Umanga Bista"
                                },
                                {
                                    "authorId": "3175685",
                                    "name": "A. Mathews"
                                },
                                {
                                    "authorId": "2844480",
                                    "name": "A. Menon"
                                },
                                {
                                    "authorId": "33650938",
                                    "name": "Lexing Xie"
                                }
                            ],
                            "year": 2020,
                            "venue": "Findings",
                            "n_citations": 2
                        },
                        "score": 0.9072265625
                    },
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fabbri et al., 2019)",
                        "snippets": [
                            "Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples."
                        ],
                        "paper": {
                            "corpus_id": 174799390,
                            "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
                            "authors": [
                                {
                                    "authorId": "46255971",
                                    "name": "Alexander R. Fabbri"
                                },
                                {
                                    "authorId": "46331602",
                                    "name": "Irene Li"
                                },
                                {
                                    "authorId": "2106009217",
                                    "name": "Tianwei She"
                                },
                                {
                                    "authorId": "50341789",
                                    "name": "Suyi Li"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 590
                        },
                        "score": 0.9287109375
                    },
                    {
                        "id": "(Tan et al., 2025)",
                        "snippets": [
                            "Multi-document summarization (MDS) is a task that aims to generate concise and coherent summaries by synthesizing information from multiple documents on the same topic (Jin et al., 2020)(Li et al., 2020)(Ma, 2021)(Mao et al., 2020)(Pang et al., 2021). MDS can lead to diverse applications, such as news aggregation (Chen et al., 2024)(Fabbri et al., 2019)(Khatuya et al., 2024), scientific research (DeYoung et al., 2021)(Lu et al., 2020)(Wang et al., 2024), and legal document analysis [17,(Malik et al., 2024)(Shen et al., 2022). Current MDS approaches can be categorized into two classes: Graph-based models (Cui et al., 2021)(Li et al., 2023)(Pasunuru et al., 2021)(Qu, 2024)(Zhang et al., 2023) and pre-trained language models [2](Puduppully et al., 2022)(Xiao et al., 2021). Graph-based models rely on auxiliary information (e.g., discourse structures) as an input graph to capture the cross-document relationships, while pre-trained language models use the attention mechanisms to capture them."
                        ],
                        "paper": {
                            "corpus_id": 278000561,
                            "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization",
                            "authors": [
                                {
                                    "authorId": "148149386",
                                    "name": "Shiyin Tan"
                                },
                                {
                                    "authorId": "2357102667",
                                    "name": "Jaeeon Park"
                                },
                                {
                                    "authorId": "2242195007",
                                    "name": "Dongyuan Li"
                                },
                                {
                                    "authorId": "2299193401",
                                    "name": "Renhe Jiang"
                                },
                                {
                                    "authorId": "2283854880",
                                    "name": "Manabu Okumura"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.92041015625
                    },
                    {
                        "id": "(Lebanoff et al., 2018)",
                        "snippets": [
                            "Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors."
                        ],
                        "paper": {
                            "corpus_id": 52053741,
                            "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "50827114",
                                    "name": "Logan Lebanoff"
                                },
                                {
                                    "authorId": "50982080",
                                    "name": "Kaiqiang Song"
                                },
                                {
                                    "authorId": "144544919",
                                    "name": "Fei Liu"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 157
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2019)",
                        "snippets": [
                            "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines."
                        ],
                        "paper": {
                            "corpus_id": 170079112,
                            "title": "Hierarchical Transformers for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "39798499",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 298
                        },
                        "score": 0.9189453125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Evolution of Multi-Document Summarization Approaches",
                "tldr": "Multi-document summarization has evolved from early graph-based extractive approaches in the late 1990s to sophisticated neural abstractive methods in recent years, with four major developmental phases: graph ranking methods, compression-based approaches, neural sequence-to-sequence models, and more recent hierarchical and graph-enhanced neural architectures. (13 sources)",
                "text": "\nThe development of multi-document summarization (MDS) began in the late 1990s, motivated largely by the growing popularity of the World Wide Web and the increasing need to synthesize information from multiple sources <Paper corpusId=\"6025826\" paperTitle=\"(Mani et al., 1997)\" isShortName></Paper> <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>. These early approaches were primarily graph-based, with Mani et al. developing methods to represent documents as graphs and perform graph matching to identify salient regions across documents <Paper corpusId=\"6025826\" paperTitle=\"(Mani et al., 1997)\" isShortName></Paper>. Similarly, Radev et al. explored mapping documents to template representations for summarization <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>.\n\nThe early 2000s marked a significant advancement with the introduction of the Document Understanding Conference (DUC) datasets, which included human-written summaries for multi-document clusters <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>. This period saw the development of influential algorithms like LexRank, which extracted salient sentences by constructing similarity graphs and applying PageRank-like algorithms <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. Other researchers extended single-document summarization approaches to the multi-document setting using various techniques, including template filling, named entity extraction, and co-reference chains <Paper corpusId=\"8294822\" paperTitle=\"(Goldstein et al., 2000)\" isShortName></Paper>.\n\nThe field has evolved through four primary approaches over time <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>. The first phase focused on graph ranking-based extractive methods like TextRank and LexRank <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. The second phase introduced syntax and structure-based compression methods to address redundancy and paraphrasing issues across documents, as demonstrated by Li et al. <Paper corpusId=\"10112929\" paperTitle=\"(Li et al., 2014)\" isShortName></Paper> <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>.\n\nThe third major shift came with neural sequence-to-sequence abstractive methods around 2017, building upon advances in single-document summarization <Paper corpusId=\"248496597\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. This transition was facilitated by the introduction of larger datasets like WikiSum and MultiNews, which enabled training of neural models for MDS <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper>.\n\nMost recently, approaches have become more sophisticated, with hierarchical and flat architectures addressing the complexities of multi-document input. Flat solutions treat MDS as a single-document task, leveraging pre-trained models but struggling with long inputs <Paper corpusId=\"248780330\" paperTitle=\"(Moro et al., 2022)\" isShortName></Paper>. Hierarchical solutions better preserve cross-document relations using graph-based techniques at word, sentence, and document levels <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"170079112\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>. These include attention-based models with maximal marginal relevance <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper> and models incorporating attention across different granularity representations <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"248780330\" paperTitle=\"(Moro et al., 2022)\" isShortName></Paper>.\n\nAnother noteworthy approach involves hierarchically combining single-document summaries to create multi-document summaries, which has achieved competitive results <Paper corpusId=\"9174081\" paperTitle=\"(Marujo et al., 2015)\" isShortName></Paper>. The latest research continues to integrate graph-based approaches with neural methods to better model relationships between textual units across documents <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Mani et al., 1997)",
                        "snippets": [
                            "We describe a new method for summarizing similarities and differences in a pair of related documents using a graph representation for text. Concepts denoted by words, phrases, and proper names in the document are represented positionally as nodes in the graph along with edges corresponding to semantic relations between items. Given a perspective in terms of which the pair of documents is to be summarized, the algorithm first uses a spreading activation technique to discover, in each document, nodes semantically related to the topic. The activated graphs of each document are then matched to yield a graph corresponding to similarities and differences between the pair, which is rendered in natural language. An evaluation of these techniques has been carried out."
                        ],
                        "paper": {
                            "corpus_id": 6025826,
                            "title": "Multi-Document Summarization by Graph Search and Matching",
                            "authors": [
                                {
                                    "authorId": "1729172",
                                    "name": "I. Mani"
                                },
                                {
                                    "authorId": "2740861",
                                    "name": "E. Bloedorn"
                                }
                            ],
                            "year": 1997,
                            "venue": "AAAI/IAAI",
                            "n_citations": 252
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pasunuru et al., 2021)",
                        "snippets": [
                            "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com"
                        ],
                        "paper": {
                            "corpus_id": 235097309,
                            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                            "authors": [
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "2940333",
                                    "name": "Mengwen Liu"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "120209444",
                                    "name": "Sujith Ravi"
                                },
                                {
                                    "authorId": "40262269",
                                    "name": "Markus Dreyer"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 73
                        },
                        "score": 0.91064453125
                    },
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0
                    },
                    {
                        "id": "(Goldstein et al., 2000)",
                        "snippets": [
                            "Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani et al., 1997)(Carbonell et al., 1998)TIPSTER, 1998b;Radev and McKeown, 1998;Mani and Bloedorn, 1999;McKeown et al., .!999;(Stein, 1999). These include comparing templates filled in by extracting information -using specialized, domain specific knowledge sources -from the doc-\"ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com--\u2022 paring named-entities -extracted using specialized lists between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related lexical items (identity mappings, synonyms, hypernyms, etc.) to extract text spans from the document set (Mani et al., 1997). Another system (Stein, 1999) creates a multi-document summary from multiple single document summaries, an approach that can be sub-optimal in some cases, due to the fact that the process of generating the final multi-document summary takes as input the individual summaries and not the complete documents. (Particularly if the single-document summaries can contain much overlapping information.) The Columbia University system (McKeown et al., 1999) creates a multi-document summary using machine learning and statistical techniques to identify similar sections and language generation to reformulate the summary."
                        ],
                        "paper": {
                            "corpus_id": 8294822,
                            "title": "Multi-Document Summarization By Sentence Extraction",
                            "authors": [
                                {
                                    "authorId": "40561307",
                                    "name": "J. Goldstein"
                                },
                                {
                                    "authorId": "1751139",
                                    "name": "Vibhu Mittal"
                                },
                                {
                                    "authorId": "143712374",
                                    "name": "J. Carbonell"
                                },
                                {
                                    "authorId": "50404544",
                                    "name": "M. Kantrowitz"
                                }
                            ],
                            "year": 2000,
                            "venue": "",
                            "n_citations": 447
                        },
                        "score": 0.9462890625
                    },
                    {
                        "id": "(Sankar et al., 2022)",
                        "snippets": [
                            "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."
                        ],
                        "paper": {
                            "corpus_id": 248571519,
                            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2064325789",
                                    "name": "Aiswarya Sankar"
                                },
                                {
                                    "authorId": "145934595",
                                    "name": "Ankit R. Chadha"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9453125
                    },
                    {
                        "id": "(Li et al., 2014)",
                        "snippets": [
                            "In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status \u2013 remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences. Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary. Compared with state-ofthe-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data."
                        ],
                        "paper": {
                            "corpus_id": 10112929,
                            "title": "Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees",
                            "authors": [
                                {
                                    "authorId": "40475614",
                                    "name": "Chen Li"
                                },
                                {
                                    "authorId": "2152797181",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "144544919",
                                    "name": "Fei Liu"
                                },
                                {
                                    "authorId": "46586837",
                                    "name": "Lin Zhao"
                                },
                                {
                                    "authorId": "1807350",
                                    "name": "F. Weng"
                                }
                            ],
                            "year": 2014,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 46
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Multi-document summarization (MDS) tries to solve the task of generating a summary for a collection of documents sharing the same topic. This field has been studied for a long time (Kathleen McKeown and Dragomir R Radev, 1995; Carbonell and Goldstein, 1998). Traditional approaches to MDS have been extractive, where part of the original text is selected and then organized to form a summary. After that many abstractive methods have been developed. In recent years, neural network-based methods have been predominant."
                        ],
                        "paper": {
                            "corpus_id": 248496597,
                            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
                            "authors": [
                                {
                                    "authorId": "2152171283",
                                    "name": "Ning Wang"
                                },
                                {
                                    "authorId": "2140162523",
                                    "name": "Han Liu"
                                },
                                {
                                    "authorId": "1753376",
                                    "name": "D. Klabjan"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Fabbri et al., 2019)",
                        "snippets": [
                            "Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples."
                        ],
                        "paper": {
                            "corpus_id": 174799390,
                            "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
                            "authors": [
                                {
                                    "authorId": "46255971",
                                    "name": "Alexander R. Fabbri"
                                },
                                {
                                    "authorId": "46331602",
                                    "name": "Irene Li"
                                },
                                {
                                    "authorId": "2106009217",
                                    "name": "Tianwei She"
                                },
                                {
                                    "authorId": "50341789",
                                    "name": "Suyi Li"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 590
                        },
                        "score": 0.9287109375
                    },
                    {
                        "id": "(Moro et al., 2022)",
                        "snippets": [
                            "Flat solutions. Flat concatenation is a simple yet powerful solution because the generation of the multi-document summary is treated as a singledocument summarization task, thus it can leverage state-of-the-art pre-trained summarization models. \n\nConsequently, processing all documents as a flat input requires models capable of handling long sequences. As previously experimented by (DeYoung et al., 2021), Xiao et al. (2021) proposed to leverage the Longformer-Encoder-Decoder model (Beltagy et al., 2020) pre-trained with a novel multi-document summarization specific task. They proved that a long-range Transformer that encodes all documents is a straightforward yet effective solution, and they achieved new state-of-the-art results in several multi-document summarization datasets. However, such models may struggle to handle a massive cluster of topic-related documents since they need to truncate them because of architectural limits. Further, processing all documents in a cluster could be noisy if some of them are not relevant or factual with respect to the summary. \n\nHierarchical solutions. To better preserve crossdocument relations and obtain semantic-rich representations, hierarchical concatenation solutions leverage graph-based techniques to work from word and sentence-level (Wan et al., 2006)(Liao et al., 2018)(Nayeem et al., 2018)Antognini and Faltings, 2019;(Li et al., 2020) to documentlevel (Amplayo and Lapata, 2021). Other hierarchical approaches include multi-head pooling and inter-paragraph attention architectures (Liu et al., 2019), attention models with maximal marginal relevance (Fabbri et al., 2019), and attention across different granularity representations (Jin et al., 2020). Such models are often datasetspecific because of the custom architecture, so they struggle to adapt to other datasets and effectively leverage pre-trained state-of-the-art Transformers."
                        ],
                        "paper": {
                            "corpus_id": 248780330,
                            "title": "Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature",
                            "authors": [
                                {
                                    "authorId": "143853729",
                                    "name": "G. Moro"
                                },
                                {
                                    "authorId": "134327204",
                                    "name": "Luca Ragazzi"
                                },
                                {
                                    "authorId": "2132084411",
                                    "name": "Lorenzo Valgimigli"
                                },
                                {
                                    "authorId": "2165227434",
                                    "name": "Davide Freddi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 32
                        },
                        "score": 0.9375
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0.93505859375
                    },
                    {
                        "id": "(Liu et al., 2019)",
                        "snippets": [
                            "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines."
                        ],
                        "paper": {
                            "corpus_id": 170079112,
                            "title": "Hierarchical Transformers for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "39798499",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 298
                        },
                        "score": 0.9189453125
                    },
                    {
                        "id": "(Jin et al._1, 2020)",
                        "snippets": [
                            "The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u0327elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model."
                        ],
                        "paper": {
                            "corpus_id": 220045815,
                            "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1491212422",
                                    "name": "Hanqi Jin"
                                },
                                {
                                    "authorId": "1751960",
                                    "name": "Tian-ming Wang"
                                },
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 99
                        },
                        "score": 0.94775390625
                    },
                    {
                        "id": "(Marujo et al., 2015)",
                        "snippets": [
                            "The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.\n\nIn general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics.\n\nWe propose to extend a state-of-the-art single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013), capable of focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization."
                        ],
                        "paper": {
                            "corpus_id": 9174081,
                            "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach",
                            "authors": [
                                {
                                    "authorId": "1728026",
                                    "name": "Lu\u00eds Marujo"
                                },
                                {
                                    "authorId": "145648625",
                                    "name": "Ricardo Ribeiro"
                                },
                                {
                                    "authorId": "4284219",
                                    "name": "David Martins de Matos"
                                },
                                {
                                    "authorId": "1723288",
                                    "name": "J. Neto"
                                },
                                {
                                    "authorId": "145001267",
                                    "name": "A. Gershman"
                                },
                                {
                                    "authorId": "143712374",
                                    "name": "J. Carbonell"
                                }
                            ],
                            "year": 2015,
                            "venue": "International Workshop on Semantic Evaluation",
                            "n_citations": 17
                        },
                        "score": 0.93408203125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Extractive Approaches to Multi-Document Summarization",
                "tldr": "Extractive multi-document summarization selects important sentences from source documents using various computational approaches to identify the most salient information while addressing redundancy. The most influential methods include graph-based models like LexRank, centroid-based approaches, clustering techniques, and feature-based methods. (14 sources)",
                "text": "\nExtractive approaches to multi-document summarization aim to identify and select the most important sentences from input documents to produce a concise summary. These methods face a central challenge of identifying redundant information across related documents while determining which repeated content signals importance <Paper corpusId=\"152281\" paperTitle=\"(Branavan et al., 2008)\" isShortName></Paper> <Paper corpusId=\"7031344\" paperTitle=\"(Barzilay et al., 1999)\" isShortName></Paper>.\n\nGraph-based methods have been particularly influential in extractive MDS. LexRank, introduced by Erkan and Radev, uses eigenvector centrality in a graph representation where sentences are connected based on similarity, applying PageRank-like algorithms to identify salient sentences <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. This approach has proven remarkably effective, ranking first in DUC 2004 evaluations and outperforming both centroid-based methods and other competing systems. LexRank's effectiveness partly stems from its robustness to noise in document clustering <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>.\n\nOther notable graph-based approaches include the affinity graph method proposed by Wan et al., which incorporates a diffusion process to capture semantic relationships between sentences and differentiates between intra-document and inter-document links <Paper corpusId=\"5457260\" paperTitle=\"(Wan et al., 2006)\" isShortName></Paper>. Graph models effectively leverage the \"voting\" or \"recommendations\" between sentences to determine importance <Paper corpusId=\"17446655\" paperTitle=\"(Wan, 2008)\" isShortName></Paper>.\n\nCentroid-based approaches represent another important category of extractive methods. These techniques focus on identifying centroid words or embeddings that represent the commonality across all documents <Paper corpusId=\"257364970\" paperTitle=\"(Ma, 2023)\" isShortName></Paper>. Radev et al. introduced MEAD, a system that generates summaries using cluster centroids produced by topic detection and tracking <Paper corpusId=\"1320\" paperTitle=\"(Radev et al., 2000)\" isShortName></Paper>. More recently, Rossiello et al. enhanced centroid-based methods by exploiting word embeddings to capture semantic relationships between concepts, proving effective for multilingual datasets <Paper corpusId=\"2346086\" paperTitle=\"(Rossiello et al., 2017)\" isShortName></Paper>.\n\nClustering-based methods divide sentences into multiple groups and select representative sentences from each cluster <Paper corpusId=\"257364970\" paperTitle=\"(Ma, 2023)\" isShortName></Paper>. This approach helps address redundancy issues by grouping similar information together. Many algorithms first cluster sentences and then extract or generate representatives for those clusters <Paper corpusId=\"152281\" paperTitle=\"(Branavan et al., 2008)\" isShortName></Paper>.\n\nFeature-based methods utilize various sentence attributes to determine importance. Kumar et al. identified four primary approaches to multi-document summarization: feature-based, cluster-based, graph-based, and knowledge-based methods <Paper corpusId=\"15926944\" paperTitle=\"(Kumar et al., 2012)\" isShortName></Paper>. Supervised approaches incorporate surface features like sentence position, length, and presence of cue words through supervised learning algorithms, including support vector regression <Paper corpusId=\"222140880\" paperTitle=\"(Bista et al., 2020)\" isShortName></Paper>.\n\nOther significant extractive approaches include Maximum Marginal Relevance (MMR), which uses a greedy approach to select sentences while balancing relevance and redundancy <Paper corpusId=\"202889056\" paperTitle=\"(Antognini et al., 2019)\" isShortName></Paper>. Cross-document structure theory, proposed by Radev, provides a theoretical foundation for multi-document summarization by accounting for rhetorical structures across related documents <Paper corpusId=\"10103200\" paperTitle=\"(Radev, 2000)\" isShortName></Paper>.\n\nDespite the emergence of neural abstractive methods, extractive approaches remain important due to their effectiveness and relatively lower resource requirements <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al._1, 2020)\" isShortName></Paper>. Traditional extractive methods have been the dominant approach historically due to the scarcity of large-scale training data needed for abstractive approaches <Paper corpusId=\"236150987\" paperTitle=\"(Perez-Beltrachini et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Branavan et al., 2008)",
                        "snippets": [
                            "This paper also relates to a large body of work in multi-document summarization. Researchers have long noted that a central challenge of multi-document summarization is identifying redundant information over input documents (Radev et al., 1998)(Carbonell et al., 1998)(Mani et al., 1997)Barzilay, McKeown, & Elhadad, 1999). This task is of crucial significance because multi-document summarizers operate over related documents that describe the same facts multiple times. In fact, it is common to assume that repetition of information among related sources is an indicator of its importance (Barzilay et al., 1999;(Radev et al., 2000)(Nenkova et al., 2006). Many of these algorithms first cluster sentences together, and then extract or generate sentence representatives for the clusters."
                        ],
                        "paper": {
                            "corpus_id": 152281,
                            "title": "Learning Document-Level Semantic Properties from Free-Text Annotations",
                            "authors": [
                                {
                                    "authorId": "1741598",
                                    "name": "S. Branavan"
                                },
                                {
                                    "authorId": "3152698",
                                    "name": "Harr Chen"
                                },
                                {
                                    "authorId": "144154709",
                                    "name": "Jacob Eisenstein"
                                },
                                {
                                    "authorId": "1741283",
                                    "name": "R. Barzilay"
                                }
                            ],
                            "year": 2008,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 122
                        },
                        "score": 0.931640625
                    },
                    {
                        "id": "(Barzilay et al., 1999)",
                        "snippets": [
                            "Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin et al., 1997)(Marcu, 1997)(Salton et al., 1994). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while ex-tracting some similar sentences could produce a summary biased towards some sources."
                        ],
                        "paper": {
                            "corpus_id": 7031344,
                            "title": "Information Fusion in the Context of Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1741283",
                                    "name": "R. Barzilay"
                                },
                                {
                                    "authorId": "145590324",
                                    "name": "K. McKeown"
                                },
                                {
                                    "authorId": "1754680",
                                    "name": "Michael Elhadad"
                                }
                            ],
                            "year": 1999,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 484
                        },
                        "score": 0.90478515625
                    },
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wan et al., 2006)",
                        "snippets": [
                            "This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems."
                        ],
                        "paper": {
                            "corpus_id": 5457260,
                            "title": "Improved Affinity Graph Based Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                },
                                {
                                    "authorId": "1743923",
                                    "name": "Jianwu Yang"
                                }
                            ],
                            "year": 2006,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 109
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wan, 2008)",
                        "snippets": [
                            "Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the \"voting\" or \"recommendations\" between sentences in the documents (Erkan et al., 2004)(Mihalcea et al., 2005)(Wan et al., 2006)."
                        ],
                        "paper": {
                            "corpus_id": 17446655,
                            "title": "An Exploration of Document Impact on Graph-Based Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2008,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 82
                        },
                        "score": 0.9169921875
                    },
                    {
                        "id": "(Ma, 2023)",
                        "snippets": [
                            "Most extractive multi-document summarization approaches splice all the sentences contained in the original documents into a larger text, and then generate a summary by selecting sentences from the larger text (Lamsiyah et al., 2021)(Yang et al., 2014)(Erkan et al., 2004)",
                            "The centroid-based summarization approaches focus on the commonality of all documents or all sentences and they select sentences based on the centroid words of all documents (Radev et al., 2004;(Rossiello et al., 2017) or the centroid embedding of all sentences (Lamsiyah et al., 2021). The clustering-based summarization approaches divide sentences into multiple groups and select sentences from each group (Yang et al., 2014)Sarkar, 2009)."
                        ],
                        "paper": {
                            "corpus_id": 257364970,
                            "title": "Mining Both Commonality and Specificity From Multiple Documents for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2084599072",
                                    "name": "Bing Ma"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 3
                        },
                        "score": 0.91455078125
                    },
                    {
                        "id": "(Radev et al., 2000)",
                        "snippets": [
                            "We also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in (Carbonell et al., 1998)) and showed how it can be used in evaluating multidocument summaries."
                        ],
                        "paper": {
                            "corpus_id": 1320,
                            "title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies",
                            "authors": [
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                },
                                {
                                    "authorId": "40544823",
                                    "name": "Hongyan Jing"
                                },
                                {
                                    "authorId": "3166871",
                                    "name": "M. Budzikowska"
                                }
                            ],
                            "year": 2000,
                            "venue": "arXiv.org",
                            "n_citations": 584
                        },
                        "score": 0.94189453125
                    },
                    {
                        "id": "(Rossiello et al., 2017)",
                        "snippets": [
                            "The textual similarity is a crucial aspect for many extractive text summarization methods. A bag-of-words representation does not allow to grasp the semantic relationships between concepts when comparing strongly related sentences with no words in common. To overcome this issue, in this paper we propose a centroid-based method for text summarization that exploits the compositional capabilities of word embeddings. The evaluations on multi-document and multilingual datasets prove the effectiveness of the continuous vector representation of words compared to the bag-of-words model. Despite its simplicity, our method achieves good performance even in comparison to more complex deep learning models. Our method is unsupervised and it can be adopted in other summarization tasks."
                        ],
                        "paper": {
                            "corpus_id": 2346086,
                            "title": "Centroid-based Text Summarization through Compositionality of Word Embeddings",
                            "authors": [
                                {
                                    "authorId": "3415700",
                                    "name": "Gaetano Rossiello"
                                },
                                {
                                    "authorId": "1731651",
                                    "name": "Pierpaolo Basile"
                                },
                                {
                                    "authorId": "145467353",
                                    "name": "G. Semeraro"
                                }
                            ],
                            "year": 2017,
                            "venue": "MultiLing@EACL",
                            "n_citations": 128
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kumar et al., 2012)",
                        "snippets": [
                            "In this study, some survey on multi document summarization approaches has been presented. We will direct our focus notably on four well known approaches to multi document summarization namely the feature based method, cluster based method, graph based method and knowledge based method. The general ideas behind these methods have been described.\n\nA number of research study have addressed multi document summarization in academia (Erkan and Radev, 2004a, Wan and Yang, 2008, Haribagiu and Lacatusu, 2010) and illustrated different types of approaches and available systems for multi document summarization."
                        ],
                        "paper": {
                            "corpus_id": 15926944,
                            "title": "Automatic Multi Document Summarization Approaches",
                            "authors": [
                                {
                                    "authorId": "1734844",
                                    "name": "Y. J. Kumar"
                                },
                                {
                                    "authorId": "1680372",
                                    "name": "N. Salim"
                                }
                            ],
                            "year": 2012,
                            "venue": "",
                            "n_citations": 72
                        },
                        "score": 0.9248046875
                    },
                    {
                        "id": "(Bista et al., 2020)",
                        "snippets": [
                            "Most existing work on this topic has focused on the generic summarization task. However, update summarization is of equal practical interest.\n\nMulti-document extractive summarization methods can be unsupervised or supervised. Unsupervised methods typically define salience (or coverage) using a global model of sentence-sentence similarity. Methods based on retrieval (Goldstein et al., 1999), centroids (Radev et al., 2004), graph centrality (Erkan and Radev, 2004), or utility maximization (Lin andBilmes, 2010, 2011;Gillick and Favre, 2009) have been well explored. However, sentence salience also depends on surface features (e.g., position, length, presence of cue words); effectively capturing these requires supervised models specific to the dataset and task. A body of work has incorporated such information through supervised learning, for example based on point processes (Kulesza and Taskar, 2012), learning important words (Hong and Nenkova, 2014), graph neural networks (Yasunaga et al., 2017), and support vector regression (Varma et al., 2009)."
                        ],
                        "paper": {
                            "corpus_id": 222140880,
                            "title": "SupMMD: A Sentence Importance Model for Extractive Summarisation using Maximum Mean Discrepancy",
                            "authors": [
                                {
                                    "authorId": "2168767",
                                    "name": "Umanga Bista"
                                },
                                {
                                    "authorId": "3175685",
                                    "name": "A. Mathews"
                                },
                                {
                                    "authorId": "2844480",
                                    "name": "A. Menon"
                                },
                                {
                                    "authorId": "33650938",
                                    "name": "Lexing Xie"
                                }
                            ],
                            "year": 2020,
                            "venue": "Findings",
                            "n_citations": 2
                        },
                        "score": 0.9072265625
                    },
                    {
                        "id": "(Antognini et al., 2019)",
                        "snippets": [
                            "Extractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988).\n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges."
                        ],
                        "paper": {
                            "corpus_id": 202889056,
                            "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "26399699",
                                    "name": "Diego Antognini"
                                },
                                {
                                    "authorId": "1735128",
                                    "name": "B. Faltings"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 22
                        },
                        "score": 0.91748046875
                    },
                    {
                        "id": "(Radev, 2000)",
                        "snippets": [
                            "We introduce CST (cross-document structure theory), a paradigm for multi-document analysis. CST takes into account the rhetorical structure of clusters of related textual documents. We present a taxonomy of cross-document relationships. We argue that CST can be the basis for multi-document summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering of facts."
                        ],
                        "paper": {
                            "corpus_id": 10103200,
                            "title": "A Common Theory of Information Fusion from Multiple Text Sources Step One: Cross-Document Structure",
                            "authors": [
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2000,
                            "venue": "SIGDIAL Workshop",
                            "n_citations": 248
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jin et al._1, 2020)",
                        "snippets": [
                            "The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u0327elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model."
                        ],
                        "paper": {
                            "corpus_id": 220045815,
                            "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1491212422",
                                    "name": "Hanqi Jin"
                                },
                                {
                                    "authorId": "1751960",
                                    "name": "Tian-ming Wang"
                                },
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 99
                        },
                        "score": 0.94775390625
                    },
                    {
                        "id": "(Perez-Beltrachini et al., 2021)",
                        "snippets": [
                            "Multi-Document Summarization Most previous solutions to multi-document summarization adopt non-neural, extractive methods (Carbonell et al., 1998)(Radev et al., 2004)(Erkan et al., 2004)(Barzilay et al., 1999). \n\nMore recently, various encoder-decoder architectures (Liu & Lapata, 2019;Fabbri et al., 2019;Perez-Beltrachini et al., 2019;(Liu et al., 2018)(Zhang et al., 2018)Lebanoff, Song, & Liu, 2018) have been ported to this task thanks to the development of large-scale datasets for model training."
                        ],
                        "paper": {
                            "corpus_id": 236150987,
                            "title": "Multi-Document Summarization with Determinantal Point Process Attention",
                            "authors": [
                                {
                                    "authorId": "1400959575",
                                    "name": "Laura Perez-Beltrachini"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 29
                        },
                        "score": 0.91552734375
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Abstractive Approaches to Multi-Document Summarization",
                "tldr": "Abstractive multi-document summarization has evolved from early sentence fusion techniques to sophisticated neural approaches using pre-trained language models. Recent advances include hierarchical architectures, graph-based representations, and entity-aware models that effectively handle cross-document relations. (20 sources)",
                "text": "\nAbstractive approaches to multi-document summarization (MDS) aim to generate entirely new text that captures the essence of source documents, rather than simply extracting sentences. Unlike extractive methods, abstractive approaches can produce more flexible and coherent summaries by creating new sentences that synthesize information from multiple sources <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al._1, 2020)\" isShortName></Paper>.\n\nEarly abstractive MDS methods focused on identifying and synthesizing similar elements across documents. Barzilay et al. introduced an approach that used language generation to reformulate the wording of the summary <Paper corpusId=\"7031344\" paperTitle=\"(Barzilay et al., 1999)\" isShortName></Paper> <Paper corpusId=\"236150987\" paperTitle=\"(Perez-Beltrachini et al., 2021)\" isShortName></Paper>. Banerjee et al. demonstrated that their abstractive summarizer outperformed extractive systems on DUC2004 and DUC2005 datasets, showing the potential of abstractive methods <Paper corpusId=\"15795297\" paperTitle=\"(Banerjee et al., 2015)\" isShortName></Paper>.\n\nThe development of neural abstractive MDS was initially constrained by the scarcity of large-scale training data, making extractive methods more prevalent <Paper corpusId=\"226283949\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper>. This situation changed with the introduction of large parallel datasets such as MultiNews, which enabled more research into neural abstractive MDS <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper> <Paper corpusId=\"266599825\" paperTitle=\"(Sakaji et al., 2023)\" isShortName></Paper>.\n\nRecent neural approaches to abstractive MDS can be categorized into several types:\n\n1. **Sequential models**: Some approaches treat MDS as a long sequence-to-sequence task by concatenating multiple documents into a flat text <Paper corpusId=\"226283949\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper>. This includes fine-tuning pre-trained models like BART <Paper corpusId=\"204960716\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper> and T5 <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper> for the summarization task <Paper corpusId=\"263610015\" paperTitle=\"(Mascarell et al., 2023)\" isShortName></Paper>.\n\n2. **Hierarchical models**: These architectures extract different-grained features (document, sentence, and word level) and select important information across documents <Paper corpusId=\"271903777\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Jin et al. proposed a multi-granularity interaction network that jointly learns semantic representations for words, sentences, and documents <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al._1, 2020)\" isShortName></Paper>.\n\n3. **Adaptation from single-document models**: Several works have adapted neural encoder-decoder models trained on single-document summarization to the multi-document setting. Zhang et al. added a document set encoder to extend models trained on large-scale single-document summarization corpora <Paper corpusId=\"53223447\" paperTitle=\"(Zhang et al., 2018)\" isShortName></Paper>. Lebanoff et al. incorporated the maximal marginal relevance method into neural encoder-decoder models to address redundancy <Paper corpusId=\"52053741\" paperTitle=\"(Lebanoff et al., 2018)\" isShortName></Paper>.\n\n4. **Graph-based neural approaches**: These methods model relationships between textual units across documents to enhance representation. Li et al. developed a neural abstractive MDS model leveraging graph representations of documents to more effectively process multiple input documents <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. Cui et al. proposed representing multiple documents as a heterogeneous graph, considering semantic nodes of different granularities <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>.\n\n5. **Entity-aware models**: Zhou et al. presented EMSum, an entity-aware model augmenting Transformer-based encoders with knowledge graphs consisting of text units and entities as nodes, allowing it to capture cross-document information <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>.\n\n6. **Topic-based approaches**: Some models incorporate topic modeling to bridge different documents. Haghighi and Vanderwende utilized hierarchical LDA-style models to represent content specificity as a hierarchy of topic vocabulary distributions <Paper corpusId=\"678258\" paperTitle=\"(Haghighi et al., 2009)\" isShortName></Paper>. Cui et al. employed a neural topic model to discover latent topics that act as cross-document semantic units <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>.\n\n7. **Multi-stage approaches**: These methods first identify relevant information before feeding it into a summarization model. Pasunuru et al. presented BART-Long-Graph, which integrated a Longformer with both local and global attention mechanisms for encoding long texts, and leveraged a knowledge graph <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>.\n\nRecent advances in pre-trained language models have significantly improved abstractive MDS performance. Models like PRIMERA (Pre-trained Multi-document Representation for Abstractive summarization) reduce the need for dataset-specific architectures and large amounts of fine-tuning labeled data <Paper corpusId=\"247519084\" paperTitle=\"(Xiao et al., 2021)\" isShortName></Paper> <Paper corpusId=\"263610015\" paperTitle=\"(Mascarell et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Jin et al._1, 2020)",
                        "snippets": [
                            "The methods for multi-document summarization can generally be categorized to extractive and abstractive. The extractive methods produce a summary by extracting and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004;Christensen et al., 2013;Yasunaga et al., 2017). Since the neural abstractive models have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Gehrmann et al., 2018;C \u0327elikyilmaz et al., 2018), some works trained abstractive summarization models on a single document dataset and adjusted the model to adapt the multi-document summarization task. Zhang et al. (2018) added a document set encoder into the single document summarization framework and tuned the pre-trained model on the multi-document summarization dataset. Lebanoff et al. (2018) combined an extractive summarization algorithm (MMR) for sentence extraction to reweight the original sentence importance distribution learned in the single document abstractive summarization model."
                        ],
                        "paper": {
                            "corpus_id": 220045815,
                            "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1491212422",
                                    "name": "Hanqi Jin"
                                },
                                {
                                    "authorId": "1751960",
                                    "name": "Tian-ming Wang"
                                },
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 99
                        },
                        "score": 0.94775390625
                    },
                    {
                        "id": "(Barzilay et al., 1999)",
                        "snippets": [
                            "Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin et al., 1997)(Marcu, 1997)(Salton et al., 1994). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while ex-tracting some similar sentences could produce a summary biased towards some sources."
                        ],
                        "paper": {
                            "corpus_id": 7031344,
                            "title": "Information Fusion in the Context of Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1741283",
                                    "name": "R. Barzilay"
                                },
                                {
                                    "authorId": "145590324",
                                    "name": "K. McKeown"
                                },
                                {
                                    "authorId": "1754680",
                                    "name": "Michael Elhadad"
                                }
                            ],
                            "year": 1999,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 484
                        },
                        "score": 0.90478515625
                    },
                    {
                        "id": "(Perez-Beltrachini et al., 2021)",
                        "snippets": [
                            "Multi-Document Summarization Most previous solutions to multi-document summarization adopt non-neural, extractive methods (Carbonell et al., 1998)(Radev et al., 2004)(Erkan et al., 2004)(Barzilay et al., 1999). \n\nMore recently, various encoder-decoder architectures (Liu & Lapata, 2019;Fabbri et al., 2019;Perez-Beltrachini et al., 2019;(Liu et al., 2018)(Zhang et al., 2018)Lebanoff, Song, & Liu, 2018) have been ported to this task thanks to the development of large-scale datasets for model training."
                        ],
                        "paper": {
                            "corpus_id": 236150987,
                            "title": "Multi-Document Summarization with Determinantal Point Process Attention",
                            "authors": [
                                {
                                    "authorId": "1400959575",
                                    "name": "Laura Perez-Beltrachini"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 29
                        },
                        "score": 0.91552734375
                    },
                    {
                        "id": "(Banerjee et al., 2015)",
                        "snippets": [
                            "On the DUC2004 and DUC2005 datasets, we demonstrate the effectiveness of our proposed method. Our proposed method outperforms not only some popular baselines but also the state-of-the-art extractive summarization systems. ROUGE scores (Oya et al., 2014) obtained by our system outperforms the best extractive summarizer on both the datasets. Our method also outperforms an abstractive summarizer based on multi-sentence compression (Filippova, 2010) when measured by ROUGE-2, ROUGE-L and ROUGE-SU4 scores."
                        ],
                        "paper": {
                            "corpus_id": 15795297,
                            "title": "Multi-Document Abstractive Summarization Using ILP Based Multi-Sentence Compression",
                            "authors": [
                                {
                                    "authorId": "2169453878",
                                    "name": "Siddhartha Banerjee"
                                },
                                {
                                    "authorId": "143930195",
                                    "name": "P. Mitra"
                                },
                                {
                                    "authorId": "3060386",
                                    "name": "Kazunari Sugiyama"
                                }
                            ],
                            "year": 2015,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 136
                        },
                        "score": 0.9091796875
                    },
                    {
                        "id": "(Jin et al., 2020)",
                        "snippets": [
                            "Multi-document summarization aims at producing a fluent, condensed summary for the given document set",
                            "Compared with single-document summarization, multi-document summarization needs to handle multiple input documents. A simple method is to concatenate multiple documents into a long flat text and treat it as a long sequence-tosequence task.\n\nEmpowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017; Paulus et al., 2018; Tan et al., 2017; \u00c7 elikyilmaz et al., 2018)",
                            "Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization.\n\nThe methods for multi-document summarization can generally be categorized to extractive and abstractive",
                            "Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004; Christensen et al., 2013; Yasunaga et al., 2017)."
                        ],
                        "paper": {
                            "corpus_id": 226283949,
                            "title": "Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1491212422",
                                    "name": "Hanqi Jin"
                                },
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2020,
                            "venue": "Findings",
                            "n_citations": 15
                        },
                        "score": 0.958984375
                    },
                    {
                        "id": "(Fabbri et al., 2019)",
                        "snippets": [
                            "Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples."
                        ],
                        "paper": {
                            "corpus_id": 174799390,
                            "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
                            "authors": [
                                {
                                    "authorId": "46255971",
                                    "name": "Alexander R. Fabbri"
                                },
                                {
                                    "authorId": "46331602",
                                    "name": "Irene Li"
                                },
                                {
                                    "authorId": "2106009217",
                                    "name": "Tianwei She"
                                },
                                {
                                    "authorId": "50341789",
                                    "name": "Suyi Li"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 590
                        },
                        "score": 0.9287109375
                    },
                    {
                        "id": "(Sakaji et al., 2023)",
                        "snippets": [
                            "Related research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain (Moro et al., 2022). Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization (Liao et al., 2018). Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset (Fabbri et al., 2019). Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing (Nayeem et al., 2018). They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries (Liu et al., 2019). Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries (Li et al., 2020). Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words (Jin et al., 2020). Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature (DeYoung et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 266599825,
                            "title": "Summarization of Investment Reports Using Pre-trained Model",
                            "authors": [
                                {
                                    "authorId": "2879326",
                                    "name": "Hiroki Sakaji"
                                },
                                {
                                    "authorId": "2276796103",
                                    "name": "Ryotaro Kobayashi"
                                },
                                {
                                    "authorId": "2276798525",
                                    "name": "Kiyoshi Izumi"
                                },
                                {
                                    "authorId": "2276797477",
                                    "name": "Hiroyuki Mitsugi"
                                },
                                {
                                    "authorId": "2276798124",
                                    "name": "Wataru Kuramoto"
                                }
                            ],
                            "year": 2023,
                            "venue": "IIAI International Conference on Advanced Applied Informatics",
                            "n_citations": 0
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Lewis et al., 2019)",
                        "snippets": [
                            "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
                        ],
                        "paper": {
                            "corpus_id": 204960716,
                            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
                            "authors": [
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "11323179",
                                    "name": "Yinhan Liu"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "2320509",
                                    "name": "Marjan Ghazvininejad"
                                },
                                {
                                    "authorId": "113947684",
                                    "name": "Abdel-rahman Mohamed"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1759422",
                                    "name": "Veselin Stoyanov"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 10856
                        },
                        "score": 0
                    },
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mascarell et al., 2023)",
                        "snippets": [
                            "Multi-document Summarization (MDS) aims at condensing the most important information from different documents. Despite the advances in single-document summarization (Zhang et al., 2019), summarizing multiple related documents remains a greater challenge due to its input length and the presence of redundant information (Fan et al., 2019)(Song et al., 2022). Therefore, some research focuses on implementing multi-stage approaches that first identify the relevant information to then feed it into a summarization model (Lebanoff et al., 2018)(Liu et al., 2019). More recent works utilize pre-trained language models (Lewis et al., 2019)(Raffel et al., 2019)(Xiao et al., 2021) finetuned for the summarization task and feed them with the source documents concatenated (Johner et al., 2021)(Xiao et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 263610015,
                            "title": "Entropy-based Sampling for Abstractive Multi-document Summarization in Low-resource Settings",
                            "authors": [
                                {
                                    "authorId": "2121237",
                                    "name": "Laura Mascarell"
                                },
                                {
                                    "authorId": "1879120021",
                                    "name": "Ribin Chalumattu"
                                },
                                {
                                    "authorId": "2253607868",
                                    "name": "Julien Heitmann"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Natural Language Generation",
                            "n_citations": 1
                        },
                        "score": 0.9130859375
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise.\n\nNon-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary.\n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 271903777,
                            "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2316447969",
                                    "name": "Ran Liu"
                                },
                                {
                                    "authorId": "2287803134",
                                    "name": "Ming Liu"
                                },
                                {
                                    "authorId": "2316535336",
                                    "name": "Min Yu"
                                },
                                {
                                    "authorId": "152634491",
                                    "name": "Jianguo Jiang"
                                },
                                {
                                    "authorId": "1996030117",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2316503511",
                                    "name": "Dan Zhang"
                                },
                                {
                                    "authorId": "2316432820",
                                    "name": "Jingyuan Li"
                                },
                                {
                                    "authorId": "2282447587",
                                    "name": "Xiang Meng"
                                },
                                {
                                    "authorId": "2282485475",
                                    "name": "Weiqing Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.90869140625
                    },
                    {
                        "id": "(Zhang et al., 2018)",
                        "snippets": [
                            "Till now, neural abstractive summarization methods have achieved great success for single document summarization (SDS). However, due to the lack of large scale multi-document summaries, such methods can be hardly applied to multi-document summarization (MDS). In this paper, we investigate neural abstractive methods for MDS by adapting a state-of-the-art neural abstractive summarization model for SDS. We propose an approach to extend the neural abstractive model trained on large scale SDS data to the MDS task. Our approach only makes use of a small number of multi-document summaries for fine tuning. Experimental results on two benchmark DUC datasets demonstrate that our approach can outperform a variety of baseline neural models."
                        ],
                        "paper": {
                            "corpus_id": 53223447,
                            "title": "Adapting Neural Single-Document Summarization Model for Abstractive Multi-Document Summarization: A Pilot Study",
                            "authors": [
                                {
                                    "authorId": "2108172729",
                                    "name": "Jianmin Zhang"
                                },
                                {
                                    "authorId": "2796928",
                                    "name": "Jiwei Tan"
                                },
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Natural Language Generation",
                            "n_citations": 35
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lebanoff et al., 2018)",
                        "snippets": [
                            "Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors."
                        ],
                        "paper": {
                            "corpus_id": 52053741,
                            "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "50827114",
                                    "name": "Logan Lebanoff"
                                },
                                {
                                    "authorId": "50982080",
                                    "name": "Kaiqiang Song"
                                },
                                {
                                    "authorId": "144544919",
                                    "name": "Fei Liu"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 157
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0.93505859375
                    },
                    {
                        "id": "(Cui et al., 2021)",
                        "snippets": [
                            "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics."
                        ],
                        "paper": {
                            "corpus_id": 239050558,
                            "title": "Topic-Guided Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "143738684",
                                    "name": "Peng Cui"
                                },
                                {
                                    "authorId": "2109312896",
                                    "name": "Le Hu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 41
                        },
                        "score": 0.91259765625
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020).\n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018).\n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 236478143,
                            "title": "Entity-Aware Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2053308860",
                                    "name": "Weidong Ren"
                                },
                                {
                                    "authorId": "150112803",
                                    "name": "Gongshen Liu"
                                },
                                {
                                    "authorId": "153253583",
                                    "name": "Bo Su"
                                },
                                {
                                    "authorId": "143844110",
                                    "name": "Wei Lu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 28
                        },
                        "score": 0.93408203125
                    },
                    {
                        "id": "(Qu, 2024)",
                        "snippets": [
                            "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."
                        ],
                        "paper": {
                            "corpus_id": 269762702,
                            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
                            "authors": [
                                {
                                    "authorId": "2163451228",
                                    "name": "Yutong Qu"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Web Conference",
                            "n_citations": 0
                        },
                        "score": 0.92626953125
                    },
                    {
                        "id": "(Haghighi et al., 2009)",
                        "snippets": [
                            "We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HierSum, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions."
                        ],
                        "paper": {
                            "corpus_id": 678258,
                            "title": "Exploring Content Models for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1761880",
                                    "name": "A. Haghighi"
                                },
                                {
                                    "authorId": "1909300",
                                    "name": "Lucy Vanderwende"
                                }
                            ],
                            "year": 2009,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 560
                        },
                        "score": 0.94482421875
                    },
                    {
                        "id": "(Pasunuru et al., 2021)",
                        "snippets": [
                            "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com"
                        ],
                        "paper": {
                            "corpus_id": 235097309,
                            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                            "authors": [
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "2940333",
                                    "name": "Mengwen Liu"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "120209444",
                                    "name": "Sujith Ravi"
                                },
                                {
                                    "authorId": "40262269",
                                    "name": "Markus Dreyer"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 73
                        },
                        "score": 0.91064453125
                    },
                    {
                        "id": "(Xiao et al., 2021)",
                        "snippets": [
                            "We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins."
                        ],
                        "paper": {
                            "corpus_id": 247519084,
                            "title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization",
                            "authors": [
                                {
                                    "authorId": "49617120",
                                    "name": "Wen Xiao"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "1825424",
                                    "name": "G. Carenini"
                                },
                                {
                                    "authorId": "2527954",
                                    "name": "Arman Cohan"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 119
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Graph-Based Methods for Multi-Document Summarization",
                "tldr": "Graph-based methods for multi-document summarization represent documents as graphs with textual units (words, sentences, documents) as nodes connected by various relationships. These approaches have evolved from classical algorithms like LexRank to modern neural graph-enhanced models that capture cross-document relationships and semantic dependencies. (14 sources)",
                "text": "\nGraph-based methods have been foundational in multi-document summarization since the late 1990s. Early researchers modeled document collections as graph structures, with Mani et al. representing documents as graphs and performing graph matching to identify salient regions across documents <Paper corpusId=\"6025826\" paperTitle=\"(Mani et al., 1997)\" isShortName></Paper>. Around the same time, Radev introduced Cross-document Structure Theory (CST) as a paradigm for multi-document analysis, providing a theoretical foundation for representing rhetorical structures across related documents <Paper corpusId=\"10103200\" paperTitle=\"(Radev, 2000)\" isShortName></Paper>.\n\nA breakthrough came with LexRank, proposed by Erkan and Radev, which constructs a graph representation where sentences are nodes connected based on similarity, and applies a PageRank-like algorithm to identify important sentences <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. LexRank ranked first in Document Understanding Conference (DUC) 2004 evaluations, demonstrating remarkable effectiveness partly due to its robustness to noise in document clustering <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>.\n\nThese graph-based models gained popularity by leveraging the \"voting\" or \"recommendations\" between sentences to determine importance <Paper corpusId=\"17446655\" paperTitle=\"(Wan, 2008)\" isShortName></Paper>. Wan et al. further enhanced this approach with an affinity graph method that incorporated a diffusion process to capture semantic relationships between sentences while differentiating between intra-document and inter-document links <Paper corpusId=\"5457260\" paperTitle=\"(Wan et al., 2006)\" isShortName></Paper>.\n\nOther notable graph-based approaches include:\n\n1. **Maximum Marginal Relevance (MMR)**: This technique uses a greedy approach to select sentences while balancing relevance and redundancy, and has been incorporated into the evaluation of multi-document summaries <Paper corpusId=\"1320\" paperTitle=\"(Radev et al., 2000)\" isShortName></Paper>.\n\n2. **Discourse graphs**: Christensen et al. developed a system that performs sentence selection while balancing coherence and salience by building a graph that approximates discourse relations across sentences <Paper corpusId=\"202889056\" paperTitle=\"(Antognini et al., 2019)\" isShortName></Paper>.\n\n3. **Reinforced random walk models**: These improved results by using reinforced random walks to rank sentences and remove redundancy <Paper corpusId=\"202889056\" paperTitle=\"(Antognini et al., 2019)\" isShortName></Paper>.\n\nAs neural models emerged, researchers began combining graph-based approaches with neural architectures. Li et al. developed a neural abstractive MDS model that leverages document graphs (similarity graphs and discourse graphs) to better capture cross-document relations and guide summary generation <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. This approach was shown to bring substantial improvements over several strong baselines <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>.\n\nRecent advancements include heterogeneous graph representations that consider semantic nodes of different granularities. Cui et al. proposed representing multiple documents as a heterogeneous graph with semantic nodes at different levels and applying a graph-to-sequence framework to generate summaries <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>. They also employed a neural topic model to discover latent topics that bridge different documents and guide summary generation <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>.\n\nEntity-aware models have further enhanced graph-based approaches. Zhou et al. presented EMSum, which augments Transformer-based encoders with knowledge graphs consisting of text units and entities as nodes while utilizing Graph Attention Networks <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper> <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>. This design allows the model to capture cross-document information and identify relative information among documents <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>.\n\nPasunuru et al. introduced BART-Long-Graph, an efficient graph-enhanced approach that integrates a Longformer with both local and global attention mechanisms for encoding long texts, while leveraging a knowledge graph <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>. This approach led to significant improvements on the Multi-News dataset and produced summaries that are more abstractive and factually consistent <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>.\n\nGraph-based models remain a vibrant area of research in multi-document summarization, with recent work focusing on creating more sophisticated graph structures that can better capture the complexities of cross-document relationships <Paper corpusId=\"278000561\" paperTitle=\"(Tan et al., 2025)\" isShortName></Paper>. These approaches continue to demonstrate competitive performance against pre-trained language models, particularly in their ability to represent explicit relationships between textual units.",
                "citations": [
                    {
                        "id": "(Mani et al., 1997)",
                        "snippets": [
                            "We describe a new method for summarizing similarities and differences in a pair of related documents using a graph representation for text. Concepts denoted by words, phrases, and proper names in the document are represented positionally as nodes in the graph along with edges corresponding to semantic relations between items. Given a perspective in terms of which the pair of documents is to be summarized, the algorithm first uses a spreading activation technique to discover, in each document, nodes semantically related to the topic. The activated graphs of each document are then matched to yield a graph corresponding to similarities and differences between the pair, which is rendered in natural language. An evaluation of these techniques has been carried out."
                        ],
                        "paper": {
                            "corpus_id": 6025826,
                            "title": "Multi-Document Summarization by Graph Search and Matching",
                            "authors": [
                                {
                                    "authorId": "1729172",
                                    "name": "I. Mani"
                                },
                                {
                                    "authorId": "2740861",
                                    "name": "E. Bloedorn"
                                }
                            ],
                            "year": 1997,
                            "venue": "AAAI/IAAI",
                            "n_citations": 252
                        },
                        "score": 0
                    },
                    {
                        "id": "(Radev, 2000)",
                        "snippets": [
                            "We introduce CST (cross-document structure theory), a paradigm for multi-document analysis. CST takes into account the rhetorical structure of clusters of related textual documents. We present a taxonomy of cross-document relationships. We argue that CST can be the basis for multi-document summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering of facts."
                        ],
                        "paper": {
                            "corpus_id": 10103200,
                            "title": "A Common Theory of Information Fusion from Multiple Text Sources Step One: Cross-Document Structure",
                            "authors": [
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2000,
                            "venue": "SIGDIAL Workshop",
                            "n_citations": 248
                        },
                        "score": 0
                    },
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wan, 2008)",
                        "snippets": [
                            "Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the \"voting\" or \"recommendations\" between sentences in the documents (Erkan et al., 2004)(Mihalcea et al., 2005)(Wan et al., 2006)."
                        ],
                        "paper": {
                            "corpus_id": 17446655,
                            "title": "An Exploration of Document Impact on Graph-Based Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2008,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 82
                        },
                        "score": 0.9169921875
                    },
                    {
                        "id": "(Wan et al., 2006)",
                        "snippets": [
                            "This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems."
                        ],
                        "paper": {
                            "corpus_id": 5457260,
                            "title": "Improved Affinity Graph Based Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                },
                                {
                                    "authorId": "1743923",
                                    "name": "Jianwu Yang"
                                }
                            ],
                            "year": 2006,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 109
                        },
                        "score": 0
                    },
                    {
                        "id": "(Radev et al., 2000)",
                        "snippets": [
                            "We also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in (Carbonell et al., 1998)) and showed how it can be used in evaluating multidocument summaries."
                        ],
                        "paper": {
                            "corpus_id": 1320,
                            "title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies",
                            "authors": [
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                },
                                {
                                    "authorId": "40544823",
                                    "name": "Hongyan Jing"
                                },
                                {
                                    "authorId": "3166871",
                                    "name": "M. Budzikowska"
                                }
                            ],
                            "year": 2000,
                            "venue": "arXiv.org",
                            "n_citations": 584
                        },
                        "score": 0.94189453125
                    },
                    {
                        "id": "(Antognini et al., 2019)",
                        "snippets": [
                            "Extractive multi-document summarization has been addressed by a large range of approaches. Several of them employ graph-based methods. Radev (2000) introduced a cross-document structure theory, as a basis for multi-document summarization. Erkan and Radev (2004) proposed LexRank, an unsupervised multi-document summarizer based on the concept of eigenvector centrality in a graph of sentences. Other works exploit shallow or deep features from the graph's topology (Wan and Yang, 2006;Antiqueira et al., 2009). Wan and Yang (2008) pairs graph-based methods (e.g. random walk) with clustering. Mei et al. (2010) improved results by using a reinforced random walk model to rank sentences and keep non-redundant ones. The system by Christensen et al. (2013) does sentence selection, while balancing coherence and salience and by building a graph that approximates discourse relations across sentences (Mann and Thompson, 1988).\n\nBesides graph-based methods, other viable approaches include Maximum Marginal Relevance (Carbonell and Goldstein, 1998), which uses a greedy approach to select sentences and considers the tradeoff between relevance and redundancy ; support vector regression (Li et al., 2007) ; conditional random field (Galley, 2006) ; or hidden markov model (Conroy et al., 2004). Yet other approaches rely on n-grams regression as in Li et (Christensen et al., 2013), based on hand-crafted features, where sentence nodes are normalized over all the incoming edges."
                        ],
                        "paper": {
                            "corpus_id": 202889056,
                            "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "26399699",
                                    "name": "Diego Antognini"
                                },
                                {
                                    "authorId": "1735128",
                                    "name": "B. Faltings"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 22
                        },
                        "score": 0.91748046875
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0.93505859375
                    },
                    {
                        "id": "(Sankar et al., 2022)",
                        "snippets": [
                            "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."
                        ],
                        "paper": {
                            "corpus_id": 248571519,
                            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2064325789",
                                    "name": "Aiswarya Sankar"
                                },
                                {
                                    "authorId": "145934595",
                                    "name": "Ankit R. Chadha"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9453125
                    },
                    {
                        "id": "(Cui et al., 2021)",
                        "snippets": [
                            "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics."
                        ],
                        "paper": {
                            "corpus_id": 239050558,
                            "title": "Topic-Guided Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "143738684",
                                    "name": "Peng Cui"
                                },
                                {
                                    "authorId": "2109312896",
                                    "name": "Le Hu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 41
                        },
                        "score": 0.91259765625
                    },
                    {
                        "id": "(Qu, 2024)",
                        "snippets": [
                            "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."
                        ],
                        "paper": {
                            "corpus_id": 269762702,
                            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
                            "authors": [
                                {
                                    "authorId": "2163451228",
                                    "name": "Yutong Qu"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Web Conference",
                            "n_citations": 0
                        },
                        "score": 0.92626953125
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020).\n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018).\n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 236478143,
                            "title": "Entity-Aware Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2053308860",
                                    "name": "Weidong Ren"
                                },
                                {
                                    "authorId": "150112803",
                                    "name": "Gongshen Liu"
                                },
                                {
                                    "authorId": "153253583",
                                    "name": "Bo Su"
                                },
                                {
                                    "authorId": "143844110",
                                    "name": "Wei Lu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 28
                        },
                        "score": 0.93408203125
                    },
                    {
                        "id": "(Pasunuru et al., 2021)",
                        "snippets": [
                            "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com"
                        ],
                        "paper": {
                            "corpus_id": 235097309,
                            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                            "authors": [
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "2940333",
                                    "name": "Mengwen Liu"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "120209444",
                                    "name": "Sujith Ravi"
                                },
                                {
                                    "authorId": "40262269",
                                    "name": "Markus Dreyer"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 73
                        },
                        "score": 0.91064453125
                    },
                    {
                        "id": "(Tan et al., 2025)",
                        "snippets": [
                            "Multi-document summarization (MDS) is a task that aims to generate concise and coherent summaries by synthesizing information from multiple documents on the same topic (Jin et al., 2020)(Li et al., 2020)(Ma, 2021)(Mao et al., 2020)(Pang et al., 2021). MDS can lead to diverse applications, such as news aggregation (Chen et al., 2024)(Fabbri et al., 2019)(Khatuya et al., 2024), scientific research (DeYoung et al., 2021)(Lu et al., 2020)(Wang et al., 2024), and legal document analysis [17,(Malik et al., 2024)(Shen et al., 2022). Current MDS approaches can be categorized into two classes: Graph-based models (Cui et al., 2021)(Li et al., 2023)(Pasunuru et al., 2021)(Qu, 2024)(Zhang et al., 2023) and pre-trained language models [2](Puduppully et al., 2022)(Xiao et al., 2021). Graph-based models rely on auxiliary information (e.g., discourse structures) as an input graph to capture the cross-document relationships, while pre-trained language models use the attention mechanisms to capture them."
                        ],
                        "paper": {
                            "corpus_id": 278000561,
                            "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization",
                            "authors": [
                                {
                                    "authorId": "148149386",
                                    "name": "Shiyin Tan"
                                },
                                {
                                    "authorId": "2357102667",
                                    "name": "Jaeeon Park"
                                },
                                {
                                    "authorId": "2242195007",
                                    "name": "Dongyuan Li"
                                },
                                {
                                    "authorId": "2299193401",
                                    "name": "Renhe Jiang"
                                },
                                {
                                    "authorId": "2283854880",
                                    "name": "Manabu Okumura"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.92041015625
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Datasets and Evaluation for Multi-Document Summarization",
                "tldr": "Multi-document summarization research has evolved from small-scale benchmarks like DUC to large-scale datasets such as WikiSum, Multi-News, and MS\u00b2 that enable neural approaches. Evaluation predominantly relies on ROUGE metrics, though newer frameworks also incorporate human assessment of coherence, factual consistency, and informativeness. (13 sources)",
                "text": "\nThe availability of appropriate datasets has been crucial to the development of multi-document summarization (MDS) approaches. Early MDS research was supported by the Document Understanding Conference (DUC) datasets from the early 2000s, which included human-written summaries for multi-document clusters and sparked increased research interest <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>. These datasets, though small in scale, provided valuable benchmarks for evaluating early extractive approaches.\n\nA significant limitation in MDS research has been the scarcity of large-scale training data, particularly for neural abstractive approaches <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper>. This situation changed with the introduction of several comprehensive datasets:\n\n1. **WikiSum**: Introduced by Liu et al., this dataset is based on Wikipedia articles and enabled researchers to train neural models for MDS <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"49210924\" paperTitle=\"(Liao et al., 2018)\" isShortName></Paper>.\n\n2. **Multi-News**: Developed by Fabbri et al., this is the first large-scale MDS news dataset, containing news articles and their corresponding human-written summaries <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper> <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>.\n\n3. **Multi-XScience**: This dataset focuses on generating \"related work\" sections by summarizing multiple scientific articles <Paper corpusId=\"272146191\" paperTitle=\"(Fernandes et al., 2024)\" isShortName></Paper>.\n\n4. **MS\u00b2** (Multi-Document Summarization of Medical Studies): Released by DeYoung et al., this dataset contains over 470,000 documents and 20,000 summaries derived from scientific literature, facilitating development of systems that can assess and aggregate contradictory evidence across multiple studies <Paper corpusId=\"266599825\" paperTitle=\"(Sakaji et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233231380\" paperTitle=\"(DeYoung et al., 2021)\" isShortName></Paper>.\n\n5. **BigSurvey**: Designed for creating structured summaries of academic articles, focusing on consolidating literature reviews <Paper corpusId=\"272146191\" paperTitle=\"(Fernandes et al., 2024)\" isShortName></Paper> <Paper corpusId=\"250636132\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\n6. **HowSumm**: This dataset focuses on summarizing instructional content derived from WikiHow articles <Paper corpusId=\"272146191\" paperTitle=\"(Fernandes et al., 2024)\" isShortName></Paper>.\n\n7. **Multi-LexSum**: Tackles the summarization of legal cases, presenting civil rights litigation summaries with multiple granularities <Paper corpusId=\"272146191\" paperTitle=\"(Fernandes et al., 2024)\" isShortName></Paper>.\n\nFor evaluation, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores remain the dominant metric in MDS research <Paper corpusId=\"15795297\" paperTitle=\"(Banerjee et al., 2015)\" isShortName></Paper>. Studies typically report ROUGE-1, ROUGE-2, and ROUGE-L scores, which measure unigram, bigram, and longest common subsequence overlaps between generated and reference summaries. For example, Banerjee et al. demonstrated that their abstractive summarizer outperformed extractive systems on DUC2004 and DUC2005 datasets when measured by ROUGE-2, ROUGE-L, and ROUGE-SU4 scores <Paper corpusId=\"15795297\" paperTitle=\"(Banerjee et al., 2015)\" isShortName></Paper>.\n\nBeyond automated metrics, human evaluation plays a crucial role in assessing summary quality. Recent evaluation frameworks consider multiple dimensions including:\n\n1. **Coherence**: How well-structured and logically connected the summary is.\n2. **Factual consistency**: Whether the summary is faithful to the source documents.\n3. **Informativeness**: How well the summary captures key information.\n4. **Redundancy**: Whether the summary contains unnecessary repetitive information.\n5. **Readability**: How clear and fluent the language is <Paper corpusId=\"256416214\" paperTitle=\"(DeYoung et al., 2023)\" isShortName></Paper> <Paper corpusId=\"15795297\" paperTitle=\"(Banerjee et al., 2015)\" isShortName></Paper>.\n\nThe development of these datasets and evaluation metrics has enabled more rigorous comparison of different MDS approaches and facilitated the transition from traditional extractive methods to neural abstractive techniques <Paper corpusId=\"263610015\" paperTitle=\"(Mascarell et al., 2023)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper> <Paper corpusId=\"209405420\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Pasunuru et al., 2021)",
                        "snippets": [
                            "Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization. Liu et al. (2018) introduced a large-scale dataset for MDS called WikiSum, based on Wikipedia articles. Liu and Lapata (2019) introduced a hierarchical Transformer model to better encode global and local aspects in multiple documents and showed improvements on WikiSum. Fabbri et al. (2019) introduced an MDS dataset of human-written abstracts from the newser.com"
                        ],
                        "paper": {
                            "corpus_id": 235097309,
                            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                            "authors": [
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "2940333",
                                    "name": "Mengwen Liu"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "120209444",
                                    "name": "Sujith Ravi"
                                },
                                {
                                    "authorId": "40262269",
                                    "name": "Markus Dreyer"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 73
                        },
                        "score": 0.91064453125
                    },
                    {
                        "id": "(Fabbri et al., 2019)",
                        "snippets": [
                            "Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples."
                        ],
                        "paper": {
                            "corpus_id": 174799390,
                            "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
                            "authors": [
                                {
                                    "authorId": "46255971",
                                    "name": "Alexander R. Fabbri"
                                },
                                {
                                    "authorId": "46331602",
                                    "name": "Irene Li"
                                },
                                {
                                    "authorId": "2106009217",
                                    "name": "Tianwei She"
                                },
                                {
                                    "authorId": "50341789",
                                    "name": "Suyi Li"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 590
                        },
                        "score": 0.9287109375
                    },
                    {
                        "id": "(Liao et al., 2018)",
                        "snippets": [
                            "Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research."
                        ],
                        "paper": {
                            "corpus_id": 49210924,
                            "title": "Abstract Meaning Representation for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "49792730",
                                    "name": "Kexin Liao"
                                },
                                {
                                    "authorId": "50827114",
                                    "name": "Logan Lebanoff"
                                },
                                {
                                    "authorId": "144544919",
                                    "name": "Fei Liu"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 105
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sankar et al., 2022)",
                        "snippets": [
                            "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."
                        ],
                        "paper": {
                            "corpus_id": 248571519,
                            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2064325789",
                                    "name": "Aiswarya Sankar"
                                },
                                {
                                    "authorId": "145934595",
                                    "name": "Ankit R. Chadha"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9453125
                    },
                    {
                        "id": "(Fernandes et al., 2024)",
                        "snippets": [
                            "Multi-document summarization (MDS) addresses the problem of extracting information that is spread across multiple documents, making it more challenging than single-document summarization. It is still an evolving field, with no single approach to solve this problem. The development of this field relies on datasets that serve as benchmarks for evaluating and comparing different summarization methods. The scientific literature includes some examples of such datasets, each varying in domain, structure, size, and summarization objective. These datasets are important for advancing research and improving summarization models. \n\nOutside the scientific domain, Multi-News [4] presents a large volume of news articles and summaries, focusing on the abstractive summarization of multiple documents in the journalistic context. Also in this context, Ghalandari et al. [5] proposed a large-scale dataset for multi-document summarization that contains concise human-written summaries of news events. In the legal field, Multi-LexSum tackles the summarization of legal cases, presenting a set of civil rights litigation summaries with multiple granularities. The summarization of instructional content is the focus of the HowSumm dataset, derived from WikiHow articles [1]. The FINDSum dataset focuses on the challenge of summarizing long text and multiple tables [10]. \n\nIn the scientific domain, the need to manage the vast amount of literature has generated interest in datasets specific to the summarization of scientific articles. Multi-XScience [12] is a dataset focused on the generation of \"related work\" sections by summarizing multiple scientific articles. BigSurvey (Liu et al., 2022) was designed to create structured summaries of academic articles, focusing on consolidating literature reviews. SumPubMed (Gupta et al., 2021) and ScisummNet (Yasunaga et al., 2019) aim the summarization of single scientific articles. \n\nSeveral other datasets can be used for multi-document summarization. Koh et al. [8] work is an extensive survey on long document summarization, including ten other MDS datasets."
                        ],
                        "paper": {
                            "corpus_id": 272146191,
                            "title": "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section",
                            "authors": [
                                {
                                    "authorId": "2318273161",
                                    "name": "Leandro Car'isio Fernandes"
                                },
                                {
                                    "authorId": "2317979625",
                                    "name": "Gustavo Bartz Guedes"
                                },
                                {
                                    "authorId": "2188833716",
                                    "name": "T. Laitz"
                                },
                                {
                                    "authorId": "2188833452",
                                    "name": "Thales Sales Almeida"
                                },
                                {
                                    "authorId": "2268315298",
                                    "name": "R. Nogueira"
                                },
                                {
                                    "authorId": "2256889299",
                                    "name": "R.A. Lotufo"
                                },
                                {
                                    "authorId": "2257137831",
                                    "name": "Jayr Pereira"
                                }
                            ],
                            "year": 2024,
                            "venue": "Brazilian Conference on Intelligent Systems",
                            "n_citations": 2
                        },
                        "score": 0.90966796875
                    },
                    {
                        "id": "(Sakaji et al., 2023)",
                        "snippets": [
                            "Related research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain (Moro et al., 2022). Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization (Liao et al., 2018). Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset (Fabbri et al., 2019). Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing (Nayeem et al., 2018). They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries (Liu et al., 2019). Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries (Li et al., 2020). Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words (Jin et al., 2020). Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature (DeYoung et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 266599825,
                            "title": "Summarization of Investment Reports Using Pre-trained Model",
                            "authors": [
                                {
                                    "authorId": "2879326",
                                    "name": "Hiroki Sakaji"
                                },
                                {
                                    "authorId": "2276796103",
                                    "name": "Ryotaro Kobayashi"
                                },
                                {
                                    "authorId": "2276798525",
                                    "name": "Kiyoshi Izumi"
                                },
                                {
                                    "authorId": "2276797477",
                                    "name": "Hiroyuki Mitsugi"
                                },
                                {
                                    "authorId": "2276798124",
                                    "name": "Wataru Kuramoto"
                                }
                            ],
                            "year": 2023,
                            "venue": "IIAI International Conference on Advanced Applied Informatics",
                            "n_citations": 0
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(DeYoung et al., 2021)",
                        "snippets": [
                            "To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system\u2019s generated summaries. Data and models are available at https://github.com/allenai/ms2."
                        ],
                        "paper": {
                            "corpus_id": 233231380,
                            "title": "MS\u02c62: Multi-Document Summarization of Medical Studies",
                            "authors": [
                                {
                                    "authorId": "48727916",
                                    "name": "Jay DeYoung"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "15292561",
                                    "name": "Madeleine van Zuylen"
                                },
                                {
                                    "authorId": "2003338023",
                                    "name": "Bailey Kuehl"
                                },
                                {
                                    "authorId": "31860505",
                                    "name": "Lucy Lu Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 113
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Writing a survey paper on one research topic usually needs to cover the salient content from numerous related papers, which can be modeled as a multi-document summarization (MDS) task. Existing MDS datasets usually focus on producing the structureless summary covering a few input documents. Meanwhile, previous structured summary generation works focus on summarizing a single document into a multi-section summary. These existing datasets and methods cannot meet the requirements of summarizing numerous academic papers into a structured summary. To deal with the scarcity of available data, we propose BigSurvey, the first large-scale dataset for generating comprehensive summaries of numerous academic papers on each topic. We collect target summaries from more than seven thousand survey papers and utilize their 430 thousand reference papers\u2019 abstracts as input documents. To organize the diverse content from dozens of input documents and ensure the efficiency of processing long text sequences, we propose a summarization method named category-based alignment and sparse transformer (CAST). The experimental results show that our CAST method outperforms various advanced summarization methods."
                        ],
                        "paper": {
                            "corpus_id": 250636132,
                            "title": "Generating a Structured Summary of Numerous Academic Papers: Dataset and Method",
                            "authors": [
                                {
                                    "authorId": "3344531",
                                    "name": "Shuaiqi Liu"
                                },
                                {
                                    "authorId": "144115026",
                                    "name": "Jiannong Cao"
                                },
                                {
                                    "authorId": "8092850",
                                    "name": "Ruosong Yang"
                                },
                                {
                                    "authorId": "2000188918",
                                    "name": "Zhiyuan Wen"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 18
                        },
                        "score": 0
                    },
                    {
                        "id": "(Banerjee et al., 2015)",
                        "snippets": [
                            "On the DUC2004 and DUC2005 datasets, we demonstrate the effectiveness of our proposed method. Our proposed method outperforms not only some popular baselines but also the state-of-the-art extractive summarization systems. ROUGE scores (Oya et al., 2014) obtained by our system outperforms the best extractive summarizer on both the datasets. Our method also outperforms an abstractive summarizer based on multi-sentence compression (Filippova, 2010) when measured by ROUGE-2, ROUGE-L and ROUGE-SU4 scores."
                        ],
                        "paper": {
                            "corpus_id": 15795297,
                            "title": "Multi-Document Abstractive Summarization Using ILP Based Multi-Sentence Compression",
                            "authors": [
                                {
                                    "authorId": "2169453878",
                                    "name": "Siddhartha Banerjee"
                                },
                                {
                                    "authorId": "143930195",
                                    "name": "P. Mitra"
                                },
                                {
                                    "authorId": "3060386",
                                    "name": "Kazunari Sugiyama"
                                }
                            ],
                            "year": 2015,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 136
                        },
                        "score": 0.9091796875
                    },
                    {
                        "id": "(DeYoung et al., 2023)",
                        "snippets": [
                            "Automatic (multi-document) summarization (Nenkova and McKeown, 2011;Maybury, 1999) has been an active subfield within NLP for decades. We have focused our analysis on modern, neural abstractive models for conditional text generation (Bahdanau et al., 2014). In light of their empirical success, we have specifically evaluated a set of Transformer-based (Vaswani et al., 2017) models which have recently been used for multidocument summarization (Beltagy et al., 2020;(Zhang et al., 2019)Xiao et al., 2022;(Raffel et al., 2019). There has been some work on highlighting conflicting evidence in health literature specifically (Shah et al., 2021b,a), though this focused primarily on highlighting conflicting evidence and explicitly aggregating extracted content."
                        ],
                        "paper": {
                            "corpus_id": 256416214,
                            "title": "Do Multi-Document Summarization Models Synthesize?",
                            "authors": [
                                {
                                    "authorId": "48727916",
                                    "name": "Jay DeYoung"
                                },
                                {
                                    "authorId": "2203750000",
                                    "name": "Stephanie C. Martinez"
                                },
                                {
                                    "authorId": "1808775",
                                    "name": "I. Marshall"
                                },
                                {
                                    "authorId": "1912476",
                                    "name": "Byron C. Wallace"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.91259765625
                    },
                    {
                        "id": "(Mascarell et al., 2023)",
                        "snippets": [
                            "Multi-document Summarization (MDS) aims at condensing the most important information from different documents. Despite the advances in single-document summarization (Zhang et al., 2019), summarizing multiple related documents remains a greater challenge due to its input length and the presence of redundant information (Fan et al., 2019)(Song et al., 2022). Therefore, some research focuses on implementing multi-stage approaches that first identify the relevant information to then feed it into a summarization model (Lebanoff et al., 2018)(Liu et al., 2019). More recent works utilize pre-trained language models (Lewis et al., 2019)(Raffel et al., 2019)(Xiao et al., 2021) finetuned for the summarization task and feed them with the source documents concatenated (Johner et al., 2021)(Xiao et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 263610015,
                            "title": "Entropy-based Sampling for Abstractive Multi-document Summarization in Low-resource Settings",
                            "authors": [
                                {
                                    "authorId": "2121237",
                                    "name": "Laura Mascarell"
                                },
                                {
                                    "authorId": "1879120021",
                                    "name": "Ribin Chalumattu"
                                },
                                {
                                    "authorId": "2253607868",
                                    "name": "Julien Heitmann"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Natural Language Generation",
                            "n_citations": 1
                        },
                        "score": 0.9130859375
                    },
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2019)",
                        "snippets": [
                            "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets."
                        ],
                        "paper": {
                            "corpus_id": 209405420,
                            "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "47540100",
                                    "name": "Jingqing Zhang"
                                },
                                {
                                    "authorId": "2143397386",
                                    "name": "Yao Zhao"
                                },
                                {
                                    "authorId": "144413479",
                                    "name": "Mohammad Saleh"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2054
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Challenges in Multi-Document Summarization",
                "tldr": "Multi-document summarization faces unique challenges beyond single-document approaches, including handling redundancy, resolving contradictions, and modeling cross-document relationships. Systems must also address computational complexity, temporal relevance, factual consistency, and domain adaptation while maintaining coherence across diverse sources. (10 sources)",
                "text": "\nMulti-document summarization (MDS) presents several distinct challenges that distinguish it from single-document summarization, making it a particularly complex task in natural language processing. While single-document summarization models have made significant progress, simply adapting these approaches to multi-document settings often leads to suboptimal results <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"52053741\" paperTitle=\"(Lebanoff et al., 2018)\" isShortName></Paper>. These challenges can be categorized into several key areas:\n\nFirst, the volume of information that must be processed in MDS increases exponentially with the number of input documents, creating significant computational challenges and extending processing times <Paper corpusId=\"271525553\" paperTitle=\"(Shakil et al., 2024)\" isShortName></Paper>. This computational complexity often limits the practical application of sophisticated models to large document collections.\n\nInformation inconsistency represents another critical challenge, as multiple documents frequently contain contradictory information from different authors or viewpoints <Paper corpusId=\"271525553\" paperTitle=\"(Shakil et al., 2024)\" isShortName></Paper> <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. Systems must identify the most accurate or relevant information across sources, requiring sophisticated content selection mechanisms that can resolve conflicts while maintaining factual accuracy.\n\nThe temporal dimension of information adds another layer of complexity, particularly when summarizing news articles where recent information may be more relevant than older content <Paper corpusId=\"271525553\" paperTitle=\"(Shakil et al., 2024)\" isShortName></Paper>. This requires models to incorporate a sense of temporality in their content selection process.\n\nCross-document relationships must be effectively modeled to create coherent summaries, as shown by numerous research efforts <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"170079112\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>. Unlike single-document summarization, where relationships between sentences are often clear from the document structure, MDS requires identifying and modeling relationships across independent texts with potentially different styles, terminologies, and structures.\n\nRedundancy management is particularly challenging in MDS due to information overlap across documents. Traditional summarization techniques often struggle with identifying and eliminating redundant content while preserving important information <Paper corpusId=\"272969413\" paperTitle=\"(Godbole et al., 2024)\" isShortName></Paper>. This challenge is compounded by variations in how similar information may be expressed across different sources.\n\nMaintaining coherence while integrating information from diverse sources with varying writing styles and levels of detail represents another significant hurdle <Paper corpusId=\"272969413\" paperTitle=\"(Godbole et al., 2024)\" isShortName></Paper>. The summary must present a unified narrative despite drawing from potentially disparate sources.\n\nQuery-based multi-document summarization introduces additional complexities, as systems must not only synthesize information across documents but also ensure relevance to specific queries <Paper corpusId=\"221293184\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. This has led to specialized approaches incorporating query attention mechanisms <Paper corpusId=\"5673925\" paperTitle=\"(Nema et al., 2017)\" isShortName></Paper> and Bayesian models <Paper corpusId=\"6241932\" paperTitle=\"(Daume et al., 2006)\" isShortName></Paper>.\n\nDomain adaptability presents another challenge, as summarization systems often perform well on the domains they were trained on but struggle to generalize across different subject areas or document types <Paper corpusId=\"272969413\" paperTitle=\"(Godbole et al., 2024)\" isShortName></Paper>. This limitation is particularly relevant given the diverse applications of MDS across news, scientific research, legal documents, and other specialized domains.\n\nFinally, ensuring factual consistency is crucial for practical applications of MDS <Paper corpusId=\"233948337\" paperTitle=\"(Lamsiyah et al., 2021)\" isShortName></Paper>. Abstractive approaches in particular risk generating \"fake facts\" when fusing information from multiple sources, necessitating verification mechanisms to maintain faithfulness to the source documents.\n\nThese challenges highlight the need for more sophisticated approaches that can effectively process multiple documents, model their relationships, and generate coherent, non-redundant, and factually accurate summaries that represent the diverse information contained in the source documents.",
                "citations": [
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "Multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. It is a task that can be more challenging than single-document summarization due to the presence of diverse and potentially conflicting information (Ma et al., 2020).\n\nWhile significant progress has been made in single-document summarization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018;Zhang et al., 2018;Baumel et al., 2018).\n\nSeveral previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a;Li et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 236478143,
                            "title": "Entity-Aware Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2053308860",
                                    "name": "Weidong Ren"
                                },
                                {
                                    "authorId": "150112803",
                                    "name": "Gongshen Liu"
                                },
                                {
                                    "authorId": "153253583",
                                    "name": "Bo Su"
                                },
                                {
                                    "authorId": "143844110",
                                    "name": "Wei Lu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 28
                        },
                        "score": 0.93408203125
                    },
                    {
                        "id": "(Lebanoff et al., 2018)",
                        "snippets": [
                            "Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors."
                        ],
                        "paper": {
                            "corpus_id": 52053741,
                            "title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "50827114",
                                    "name": "Logan Lebanoff"
                                },
                                {
                                    "authorId": "50982080",
                                    "name": "Kaiqiang Song"
                                },
                                {
                                    "authorId": "144544919",
                                    "name": "Fei Liu"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 157
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shakil et al., 2024)",
                        "snippets": [
                            "Although much research focuses on single-document summarization, multi-document summarization presents unique challenges (Narayan et al., 2018)(Lamsiyah et al., 2021). Unlike its single-document counterpart, multi-document summarization involves synthesizing information from multiple sources, often necessitating the alignment of documents, identification, and resolution of redundancies, contradictions, and varying perspectives. These complexities introduce unique challenges such as ensuring coherence in the face of diverse inputs and maintaining a balanced representation of all source documents. The enormous amount of information that needs to be processed during multi-document summarization is one of the main challenges. The amount of data increases rapidly with many documents, causing computational difficulties and extending processing times [183]. The possibility of conflicting information across documents presents another challenge. Finding the most precise or relevant information can be troublesome, particularly if the source text comprises different authors or viewpoints (Erkan et al., 2004). Furthermore, the temporal part of the information can present difficulties. For instance, while summing up news articles, recent data may be more pertinent than older information, expecting models to have a sense of temporality (Wan et al., 2007)."
                        ],
                        "paper": {
                            "corpus_id": 271525553,
                            "title": "Abstractive text summarization: State of the art, challenges, and improvements",
                            "authors": [
                                {
                                    "authorId": "2300173312",
                                    "name": "Hassan Shakil"
                                },
                                {
                                    "authorId": "2313554804",
                                    "name": "Ahmad Farooq"
                                },
                                {
                                    "authorId": "2261083539",
                                    "name": "Jugal K. Kalita"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neurocomputing",
                            "n_citations": 24
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2019)",
                        "snippets": [
                            "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines."
                        ],
                        "paper": {
                            "corpus_id": 170079112,
                            "title": "Hierarchical Transformers for Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "39798499",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 298
                        },
                        "score": 0.9189453125
                    },
                    {
                        "id": "(Godbole et al., 2024)",
                        "snippets": [
                            "Multi-document summarization presents unique challenges due to the need for synthesizing information from diverse sources, which may contain redundant, complementary, or contradictory information across documents [4]. Variations in writing style and level of detail add complexity to the task. Determining the relevance and importance of information from each source is crucial for creating a coherent and comprehensive summary [5].\n\nTraditional document summarization techniques often struggle with redundancy, inconsistency, lack of context understanding, scalability issues for multiple document summarization tasks, inability to capture cross-document relationships, difficulty handling diverse formats, and lack of domain adaptability [6,7]8]. These limitations highlight the need for more advanced approaches to multi-document summarization."
                        ],
                        "paper": {
                            "corpus_id": 272969413,
                            "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications",
                            "authors": [
                                {
                                    "authorId": "2350511520",
                                    "name": "Aditi Godbole"
                                },
                                {
                                    "authorId": "2301049091",
                                    "name": "Jabin Geevarghese George"
                                },
                                {
                                    "authorId": "48781397",
                                    "name": "Smita Shandilya"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.93115234375
                    },
                    {
                        "id": "(Zhang et al., 2020)",
                        "snippets": [
                            "Query-based Multi-document Summarization. Query-based multi-document summarization is the process of automatically generating natural summaries of text documents in the context of a given query. An early work for extractive query-based multidocument summarization is presented by (Goldstein-Stewart et al., 1999), which ranked sentences using a weighted combination of statistical and linguistic features. (Daum\u00e9 et al., 2006) presented to extract sentences based on the language model, Bayesian model, and graphical model. [40] introduced the graph information to look for relevant sentences. (Schilder et al., 2008) used the multi-modality manifold-ranking algorithm to extract topic-focused summary from multiple documents. Recently, some works employ the encoder-decoder framework to produce the query-based summaries. [24] trained a pointer-generator model, and [3] incorporated relevance into a neural seq2seq models for query-based abstractive summarization. (Nema et al., 2017) introduced a new diversity based attention mechanism to alleviate the problem of repeating phrases."
                        ],
                        "paper": {
                            "corpus_id": 221293184,
                            "title": "Query Understanding via Intent Description Generation",
                            "authors": [
                                {
                                    "authorId": "2109960367",
                                    "name": "Ruqing Zhang"
                                },
                                {
                                    "authorId": "70414094",
                                    "name": "Jiafeng Guo"
                                },
                                {
                                    "authorId": "7888704",
                                    "name": "Yixing Fan"
                                },
                                {
                                    "authorId": "37510256",
                                    "name": "Yanyan Lan"
                                },
                                {
                                    "authorId": "1717004",
                                    "name": "Xueqi Cheng"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Information and Knowledge Management",
                            "n_citations": 17
                        },
                        "score": 0.9423828125
                    },
                    {
                        "id": "(Nema et al., 2017)",
                        "snippets": [
                            "Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores."
                        ],
                        "paper": {
                            "corpus_id": 5673925,
                            "title": "Diversity driven attention model for query-based abstractive summarization",
                            "authors": [
                                {
                                    "authorId": "9192775",
                                    "name": "Preksha Nema"
                                },
                                {
                                    "authorId": "2361078",
                                    "name": "Mitesh M. Khapra"
                                },
                                {
                                    "authorId": "2039596",
                                    "name": "Anirban Laha"
                                },
                                {
                                    "authorId": "1723632",
                                    "name": "Balaraman Ravindran"
                                }
                            ],
                            "year": 2017,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 169
                        },
                        "score": 0
                    },
                    {
                        "id": "(Daume et al., 2006)",
                        "snippets": [
                            "We present BAYESUM (for \"Bayesian summarization\"), a model for sentence extraction in query-focused summarization. BAYESUM leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BAYESUM is not afflicted by the paucity of information in short queries. We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system. Furthermore, we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework."
                        ],
                        "paper": {
                            "corpus_id": 6241932,
                            "title": "Bayesian Query-Focused Summarization",
                            "authors": [
                                {
                                    "authorId": "1722360",
                                    "name": "Hal Daum\u00e9"
                                },
                                {
                                    "authorId": "1695463",
                                    "name": "D. Marcu"
                                }
                            ],
                            "year": 2006,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 285
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lamsiyah et al., 2021)",
                        "snippets": [
                            "Text representation is a fundamental cornerstone that impacts the effectiveness of several text summarization methods. Transfer learning using pre-trained word embedding models has shown promising results. However, most of these representations do not consider the order and the semantic relationships between words in a sentence, and thus they do not carry the meaning of a full sentence. To overcome this issue, the current study proposes an unsupervised method for extractive multi-document summarization based on transfer learning from BERT sentence embedding model. Moreover, to improve sentence representation learning, we fine-tune BERT model on supervised intermediate tasks from GLUE benchmark datasets using single-task and multi-task fine-tuning methods. Experiments are performed on the standard DUC\u20192002\u20132004 datasets. The obtained results show that our method has significantly outperformed several baseline methods and achieves a comparable and sometimes better performance than the recent state-of-the-art deep learning\u2013based methods. Furthermore, the results show that fine-tuning BERT using multi-task learning has considerably improved the performance."
                        ],
                        "paper": {
                            "corpus_id": 233948337,
                            "title": "Unsupervised extractive multi-document summarization method based on transfer learning from BERT multi-task fine-tuning",
                            "authors": [
                                {
                                    "authorId": "81542199",
                                    "name": "Salima Lamsiyah"
                                },
                                {
                                    "authorId": "3196929",
                                    "name": "Abdelkader El Mahdaouy"
                                },
                                {
                                    "authorId": "122945591",
                                    "name": "S. Ouatik"
                                },
                                {
                                    "authorId": "2925748",
                                    "name": "B. Espinasse"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of information science",
                            "n_citations": 39
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.351552
    }
}